{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target filtering procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/07grn/02graph_filter'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk(df, tf, k):\n",
    "    return df[df['TF'] == tf].sort_values('importance', ascending=False)[:k]\n",
    "\n",
    "def filter_topk(grnboost_out, k=50):\n",
    "    tfs = grnboost_out['TF'].unique()\n",
    "    tf_dfs = []\n",
    "    for tf in tfs:\n",
    "        tf_dfs.append(get_topk(grnboost_out, tf, k=k))  \n",
    "    return pd.concat(tf_dfs)\n",
    "\n",
    "def get_pc(grnboost_out, pc=95):\n",
    "    return grnboost_out.sort_values('importance', ascending=False)[:int(len(grnboost_out)*(1-0.01*pc))]\n",
    "\n",
    "def get_filtered_adj_list(grnboost_out):\n",
    "    filters = {}\n",
    "    filters['top50'] = filter_topk(grnboost_out, k=50)\n",
    "    filters['95pc'] = get_pc(grnboost_out, pc=95)\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adamsonweissman2016_gsm2406675_1 finished\n",
      "adamsonweissman2016_gsm2406677_2 finished\n",
      "datlingerbock2017_stimulated finished\n",
      "datlingerbock2017_unstimulated finished\n",
      "datlingerbock2021_stimulated finished\n",
      "datlingerbock2021_unstimulated finished\n",
      "dixit_combined finished\n",
      "dixit_gsm2396858 finished\n",
      "dixit_gsm2396861 finished\n",
      "papalexisatija2021_eccite_arrayed_rna finished\n",
      "papalexisatija2021_eccite_rna finished\n",
      "tiankampmann2019_ipsc finished\n",
      "tiankampmann2019_day7neuron finished\n",
      "tiankampmann2021_crispra finished\n",
      "xucao2023 finished\n"
     ]
    }
   ],
   "source": [
    "# Generate filtered adjacency files for GRNboost graph\n",
    "\n",
    "names = ['adamsonweissman2016_gsm2406675_1', 'adamsonweissman2016_gsm2406677_2', 'datlingerbock2017_stimulated',\n",
    "        'datlingerbock2017_unstimulated', 'datlingerbock2021_stimulated', 'datlingerbock2021_unstimulated', 'dixit_combined',\n",
    "        'dixit_gsm2396858', 'dixit_gsm2396861', 'papalexisatija2021_eccite_arrayed_rna', 'papalexisatija2021_eccite_rna',\n",
    "        'tiankampmann2019_ipsc', 'tiankampmann2019_day7neuron', 'tiankampmann2021_crispra', 'xucao2023']\n",
    "\n",
    "for name in names:\n",
    "    for split in range(1,6):\n",
    "        # Read GRNboost output\n",
    "        grnboost_out = pd.read_csv(f'../01scenic_adj/{name}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "        filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "        # Save filtered graphs\n",
    "        directory = f'./{name}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        filtered['top50'].to_csv(f'./{name}/{name}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "        filtered['95pc'].to_csv(f'./{name}/{name}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)\n",
    "    print(f'{name} finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dixit_combined finished\n",
      "dixit_gsm2396858 finished\n",
      "dixit_gsm2396861 finished\n"
     ]
    }
   ],
   "source": [
    "names = ['dixit_combined', 'dixit_gsm2396858', 'dixit_gsm2396861']\n",
    "\n",
    "for name in names:\n",
    "    for split in range(1,6):\n",
    "        # Read GRNboost output\n",
    "        grnboost_out = pd.read_csv(f'../01scenic_adj/{name}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "        filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "        # Save filtered graphs\n",
    "        directory = f'./{name}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        filtered['top50'].to_csv(f'./{name}/{name}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "        filtered['95pc'].to_csv(f'./{name}/{name}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)\n",
    "    print(f'{name} finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adamsonweissman2016_gsm2406681_3 finished\n"
     ]
    }
   ],
   "source": [
    "names = 'adamsonweissman2016_gsm2406681_3'\n",
    "\n",
    "for split in range(1,6):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "    directory = f'./{names}'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)\n",
    "print(f'{names} finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normanweissman2019 finished\n"
     ]
    }
   ],
   "source": [
    "names = 'normanweissman2019'\n",
    "\n",
    "for split in range(1,6):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "    directory = f'./{names}'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)\n",
    "print(f'{names} finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'replogle_k562_essential'\n",
    "\n",
    "for split in range(1,2):\n",
    "    # Read GRNboost output\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'replogle_rpe1_essential'\n",
    "\n",
    "for split in range(5,6):\n",
    "    # Read GRNboost output\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate filtered adjacency files for GRNboost graph\n",
    "\n",
    "#names = ['norman'] \n",
    "#names = ['tian2019_neuron_hvg', 'tian2019_ipsc_hvg', 'jost2020_hvg', 'replogle2020_hvg']\n",
    "names = 'datlingerbock'\n",
    "\n",
    "for name in names:\n",
    "    for split in range(2,6):\n",
    "        # Read GRNboost output\n",
    "        grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "        filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "        # Save filtered graphs\n",
    "        filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "        filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate filtered adjacency files for GRNboost graph\n",
    "\n",
    "#names = ['norman'] \n",
    "#names = ['tian2019_neuron_hvg', 'tian2019_ipsc_hvg', 'jost2020_hvg', 'replogle2020_hvg']\n",
    "names = 'ps_arrayed_rna_5000'\n",
    "\n",
    "for split in range(1,6):\n",
    "    # Read GRNboost output\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "        # Save filtered graphs\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate filtered adjacency files for GRNboost graph\n",
    "\n",
    "#names = ['norman'] \n",
    "#names = ['tian2019_neuron_hvg', 'tian2019_ipsc_hvg', 'jost2020_hvg', 'replogle2020_hvg']\n",
    "names = 'ps_rna_5015'\n",
    "\n",
    "for split in range(5,6):\n",
    "    # Read GRNboost output\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'tk_a'\n",
    "for split in range(1,6):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'tk_i'\n",
    "\n",
    "for split in range(1,6):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'xucao'\n",
    "\n",
    "for split in range(4,6):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'xucao'\n",
    "\n",
    "for split in range(1,4):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'frangiehlzar'\n",
    "\n",
    "for split in range(1,6):\n",
    "    grnboost_out = pd.read_csv(f'../01scenic_adj/{names}_adjacencies_' + str(split) + '.csv', index_col =0)\n",
    "    filtered = get_filtered_adj_list(grnboost_out)\n",
    "\n",
    "    filtered['top50'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_top50.csv', \n",
    "                                 index=False, header=False)\n",
    "    filtered['95pc'].to_csv(f'./{names}/{names}_spilt' + str(split) + '_95pc.csv',\n",
    "                                 index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gears",
   "language": "python",
   "name": "gears"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
