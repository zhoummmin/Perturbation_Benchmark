Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_181527-2q5ga69a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_combined_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/2q5ga69a
wandb: WARNING Serializing object of type ndarray that is 8024128 bytes
  0%|                                                                                       | 0/3600 [00:00<?, ?it/s]  0%|‚ñè                                                                              | 7/3600 [00:00<00:54, 65.66it/s]  0%|‚ñé                                                                             | 15/3600 [00:00<00:50, 70.59it/s]  1%|‚ñç                                                                             | 23/3600 [00:00<00:49, 72.70it/s]  1%|‚ñã                                                                             | 33/3600 [00:00<00:46, 76.24it/s]  1%|‚ñâ                                                                             | 43/3600 [00:00<00:43, 81.50it/s]  1%|‚ñà‚ñè                                                                            | 52/3600 [00:00<00:43, 82.02it/s]  2%|‚ñà‚ñé                                                                            | 61/3600 [00:00<00:43, 81.87it/s]  2%|‚ñà‚ñå                                                                            | 70/3600 [00:00<00:42, 82.33it/s]  2%|‚ñà‚ñã                                                                            | 79/3600 [00:00<00:42, 82.49it/s]  2%|‚ñà‚ñâ                                                                            | 88/3600 [00:01<00:42, 82.12it/s]  3%|‚ñà‚ñà                                                                            | 97/3600 [00:01<00:43, 81.40it/s]  3%|‚ñà‚ñà‚ñé                                                                          | 106/3600 [00:01<00:42, 81.77it/s]  3%|‚ñà‚ñà‚ñç                                                                          | 115/3600 [00:01<00:42, 81.46it/s]  3%|‚ñà‚ñà‚ñã                                                                          | 124/3600 [00:01<00:42, 81.23it/s]  4%|‚ñà‚ñà‚ñä                                                                          | 133/3600 [00:01<00:43, 80.23it/s]  4%|‚ñà‚ñà‚ñà                                                                          | 142/3600 [00:01<00:43, 79.56it/s]  4%|‚ñà‚ñà‚ñà‚ñè                                                                         | 150/3600 [00:01<00:43, 79.34it/s]  4%|‚ñà‚ñà‚ñà‚ñç                                                                         | 158/3600 [00:01<00:43, 79.17it/s]  5%|‚ñà‚ñà‚ñà‚ñå                                                                         | 167/3600 [00:02<00:43, 79.66it/s]  5%|‚ñà‚ñà‚ñà‚ñã                                                                         | 175/3600 [00:02<00:44, 77.18it/s]  5%|‚ñà‚ñà‚ñà‚ñâ                                                                         | 185/3600 [00:02<00:43, 78.15it/s]  5%|‚ñà‚ñà‚ñà‚ñà‚ñè                                                                        | 195/3600 [00:02<00:41, 81.50it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñé                                                                        | 204/3600 [00:02<00:44, 76.37it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñå                                                                        | 213/3600 [00:02<00:42, 79.67it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñã                                                                        | 222/3600 [00:02<00:40, 82.40it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                        | 231/3600 [00:02<00:41, 81.50it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                       | 240/3600 [00:03<00:41, 81.04it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                       | 249/3600 [00:03<00:41, 80.74it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                       | 258/3600 [00:03<00:41, 80.07it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                       | 267/3600 [00:03<00:42, 79.11it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                       | 275/3600 [00:03<00:42, 78.51it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                       | 283/3600 [00:03<00:42, 78.84it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                      | 291/3600 [00:03<00:41, 79.09it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                      | 299/3600 [00:03<00:42, 76.79it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 309/3600 [00:03<00:40, 80.79it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                      | 318/3600 [00:03<00:40, 81.55it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                      | 327/3600 [00:04<00:40, 81.49it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                     | 336/3600 [00:04<00:39, 81.61it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                     | 345/3600 [00:04<00:39, 81.95it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                     | 354/3600 [00:04<00:39, 81.55it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                     | 363/3600 [00:04<00:39, 81.86it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                     | 372/3600 [00:04<00:39, 82.16it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                    | 381/3600 [00:04<00:39, 81.83it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                    | 390/3600 [00:04<00:39, 81.73it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                    | 399/3600 [00:04<00:39, 81.19it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                    | 408/3600 [00:05<00:39, 80.70it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                    | 417/3600 [00:05<00:39, 81.12it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                    | 426/3600 [00:05<00:39, 80.94it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                   | 435/3600 [00:05<00:40, 77.37it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                   | 443/3600 [00:05<00:40, 77.78it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                   | 452/3600 [00:05<00:40, 77.77it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                   | 462/3600 [00:05<00:38, 81.54it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                   | 471/3600 [00:05<00:38, 81.80it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                  | 480/3600 [00:05<00:39, 78.83it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                  | 490/3600 [00:06<00:37, 82.48it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                  | 499/3600 [00:06<00:37, 82.64it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                  | 508/3600 [00:06<00:37, 82.14it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 517/3600 [00:06<00:38, 79.58it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                 | 527/3600 [00:06<00:37, 82.87it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                 | 536/3600 [00:06<00:38, 79.84it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                 | 546/3600 [00:06<00:37, 80.83it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                 | 555/3600 [00:06<00:37, 81.08it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                 | 564/3600 [00:07<00:36, 83.11it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                | 573/3600 [00:07<00:37, 80.13it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                | 583/3600 [00:07<00:36, 83.70it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                | 592/3600 [00:07<00:37, 80.97it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                | 602/3600 [00:07<00:35, 84.25it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                | 611/3600 [00:07<00:36, 81.45it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                               | 620/3600 [00:07<00:36, 82.38it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                               | 629/3600 [00:07<00:36, 82.10it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                               | 639/3600 [00:07<00:34, 84.61it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                               | 648/3600 [00:08<00:37, 79.04it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                               | 656/3600 [00:08<00:37, 78.25it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                              | 665/3600 [00:08<00:38, 75.54it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                              | 674/3600 [00:08<00:38, 75.99it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                              | 682/3600 [00:08<00:38, 75.84it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                              | 692/3600 [00:08<00:40, 72.04it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                              | 700/3600 [00:08<00:40, 72.04it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                             | 709/3600 [00:08<00:38, 75.10it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                             | 718/3600 [00:08<00:36, 78.51it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                             | 726/3600 [00:09<00:37, 76.38it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             | 734/3600 [00:09<00:38, 74.99it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                             | 742/3600 [00:09<00:37, 75.57it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                             | 750/3600 [00:09<00:37, 76.68it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                            | 758/3600 [00:09<00:38, 74.77it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                            | 766/3600 [00:09<00:38, 73.87it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                            | 774/3600 [00:09<00:38, 73.96it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                            | 782/3600 [00:09<00:38, 73.59it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                            | 790/3600 [00:09<00:38, 72.92it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                            | 798/3600 [00:10<00:38, 73.03it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                           | 806/3600 [00:10<00:38, 72.68it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                           | 814/3600 [00:10<00:37, 74.04it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                           | 822/3600 [00:10<00:37, 73.27it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                           | 830/3600 [00:10<00:37, 73.59it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                           | 838/3600 [00:10<00:36, 74.80it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 846/3600 [00:10<00:36, 74.49it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                          | 854/3600 [00:10<00:38, 71.54it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                          | 863/3600 [00:10<00:37, 72.73it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 871/3600 [00:11<00:37, 72.11it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                          | 880/3600 [00:11<00:36, 75.11it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                          | 888/3600 [00:11<00:35, 76.16it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                         | 896/3600 [00:11<00:35, 75.74it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                         | 904/3600 [00:11<00:35, 75.08it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                         | 912/3600 [00:11<00:35, 75.96it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                         | 920/3600 [00:11<00:35, 75.93it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                         | 928/3600 [00:11<00:35, 75.23it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                         | 936/3600 [00:11<00:34, 76.55it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                        | 944/3600 [00:12<00:34, 76.16it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                        | 952/3600 [00:12<00:35, 74.75it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                        | 961/3600 [00:12<00:34, 76.65it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                        | 969/3600 [00:12<00:34, 76.78it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                        | 977/3600 [00:12<00:35, 74.92it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                        | 986/3600 [00:12<00:34, 76.44it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                       | 994/3600 [00:12<00:33, 76.88it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                      | 1002/3600 [00:12<00:35, 72.39it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                      | 1012/3600 [00:12<00:33, 77.54it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 1021/3600 [00:13<00:32, 78.56it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                      | 1029/3600 [00:13<00:34, 75.35it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                      | 1038/3600 [00:13<00:33, 76.32it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                      | 1047/3600 [00:13<00:32, 78.54it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                     | 1055/3600 [00:13<00:33, 75.82it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 1065/3600 [00:13<00:31, 80.14it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                     | 1074/3600 [00:13<00:32, 78.92it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                     | 1082/3600 [00:13<00:32, 77.22it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                     | 1091/3600 [00:13<00:31, 78.91it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 1100/3600 [00:14<00:31, 79.78it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                    | 1109/3600 [00:14<00:31, 78.11it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                    | 1118/3600 [00:14<00:33, 75.07it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                    | 1127/3600 [00:14<00:31, 77.57it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                    | 1136/3600 [00:14<00:31, 78.51it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                   | 1144/3600 [00:14<00:35, 68.76it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                   | 1155/3600 [00:14<00:30, 79.17it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                   | 1164/3600 [00:14<00:29, 81.43it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                   | 1173/3600 [00:14<00:31, 76.40it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                   | 1183/3600 [00:15<00:30, 79.27it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                  | 1192/3600 [00:15<00:30, 79.99it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                  | 1201/3600 [00:15<00:29, 81.79it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 1210/3600 [00:15<00:30, 79.56it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                  | 1219/3600 [00:15<00:29, 79.53it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                  | 1228/3600 [00:15<00:29, 81.37it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                  | 1237/3600 [00:15<00:28, 83.43it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                 | 1246/3600 [00:15<00:27, 84.67it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                 | 1255/3600 [00:15<00:28, 81.11it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                 | 1265/3600 [00:16<00:28, 82.98it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                 | 1275/3600 [00:16<00:27, 83.17it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                | 1285/3600 [00:16<00:27, 85.10it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                | 1295/3600 [00:16<00:26, 85.60it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                | 1305/3600 [00:16<00:27, 83.79it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                | 1315/3600 [00:16<00:26, 87.86it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                | 1324/3600 [00:16<00:26, 86.44it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                               | 1334/3600 [00:16<00:25, 87.99it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                               | 1343/3600 [00:17<00:25, 87.16it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                               | 1352/3600 [00:17<00:26, 86.16it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                               | 1362/3600 [00:17<00:24, 89.89it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                               | 1372/3600 [00:17<00:25, 85.95it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                              | 1381/3600 [00:17<00:25, 86.85it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                              | 1390/3600 [00:17<00:26, 82.73it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 1402/3600 [00:17<00:24, 88.48it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 1411/3600 [00:17<00:24, 88.45it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                              | 1420/3600 [00:17<00:24, 88.84it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                             | 1430/3600 [00:17<00:24, 88.40it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                             | 1440/3600 [00:18<00:24, 89.19it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                             | 1449/3600 [00:18<00:24, 87.99it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                             | 1459/3600 [00:18<00:24, 89.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                             | 1468/3600 [00:18<00:24, 88.30it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                            | 1477/3600 [00:18<00:24, 87.32it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                            | 1488/3600 [00:18<00:23, 88.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                            | 1497/3600 [00:18<00:24, 84.70it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                            | 1508/3600 [00:18<00:23, 89.59it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                            | 1517/3600 [00:18<00:23, 89.28it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                           | 1527/3600 [00:19<00:22, 90.73it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                           | 1537/3600 [00:19<00:22, 91.05it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                           | 1547/3600 [00:19<00:23, 89.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                           | 1556/3600 [00:19<00:23, 86.27it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                           | 1565/3600 [00:19<00:23, 86.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                          | 1574/3600 [00:19<00:23, 86.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                          | 1583/3600 [00:19<00:23, 87.16it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                          | 1593/3600 [00:19<00:22, 89.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                          | 1602/3600 [00:19<00:22, 87.57it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                          | 1611/3600 [00:20<00:23, 86.27it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                         | 1620/3600 [00:20<00:23, 83.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                         | 1630/3600 [00:20<00:22, 87.05it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                         | 1639/3600 [00:20<00:22, 86.95it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                         | 1648/3600 [00:20<00:22, 86.47it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                         | 1657/3600 [00:20<00:22, 86.59it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 1668/3600 [00:20<00:21, 87.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                        | 1677/3600 [00:20<00:22, 87.27it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                        | 1687/3600 [00:20<00:22, 84.03it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 1696/3600 [00:21<00:23, 82.04it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                        | 1705/3600 [00:21<00:24, 77.42it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 1713/3600 [00:21<00:26, 71.14it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                       | 1721/3600 [00:21<00:28, 66.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1728/3600 [00:21<00:29, 64.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                       | 1735/3600 [00:21<00:29, 64.29it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                       | 1742/3600 [00:21<00:30, 61.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                       | 1749/3600 [00:21<00:30, 60.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                       | 1756/3600 [00:22<00:30, 60.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                      | 1763/3600 [00:22<00:31, 58.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 1769/3600 [00:22<00:31, 58.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                      | 1775/3600 [00:22<00:31, 57.22it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                      | 1783/3600 [00:22<00:29, 60.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                      | 1790/3600 [00:22<00:30, 60.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                      | 1797/3600 [00:22<00:31, 57.59it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                      | 1804/3600 [00:22<00:30, 59.58it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 1810/3600 [00:22<00:30, 58.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 1816/3600 [00:23<00:31, 56.43it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                     | 1823/3600 [00:23<00:30, 58.31it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 1829/3600 [00:23<00:31, 55.97it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                     | 1836/3600 [00:23<00:31, 55.58it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 1845/3600 [00:23<00:27, 64.59it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 1852/3600 [00:23<00:26, 65.72it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                    | 1861/3600 [00:23<00:25, 69.23it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 1870/3600 [00:23<00:23, 72.20it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                    | 1879/3600 [00:23<00:22, 76.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                    | 1887/3600 [00:24<00:23, 74.11it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 1895/3600 [00:24<00:22, 75.14it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 1904/3600 [00:24<00:21, 78.35it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 1912/3600 [00:24<00:22, 75.47it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 1920/3600 [00:24<00:21, 76.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 1928/3600 [00:24<00:21, 76.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 1936/3600 [00:24<00:21, 76.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 1944/3600 [00:24<00:21, 75.88it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 1952/3600 [00:24<00:22, 74.67it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 1962/3600 [00:25<00:20, 78.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 1973/3600 [00:25<00:19, 82.83it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 1982/3600 [00:25<00:19, 82.31it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 1991/3600 [00:25<00:19, 80.75it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 2000/3600 [00:25<00:19, 82.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 2009/3600 [00:25<00:20, 77.00it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 2017/3600 [00:25<00:20, 77.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 2025/3600 [00:25<00:20, 77.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 2033/3600 [00:25<00:20, 77.71it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 2041/3600 [00:26<00:20, 74.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 2050/3600 [00:26<00:19, 79.01it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 2058/3600 [00:26<00:20, 75.00it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 2066/3600 [00:26<00:20, 76.06it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 2074/3600 [00:26<00:19, 76.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 2082/3600 [00:26<00:19, 76.59it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 2090/3600 [00:26<00:19, 76.91it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 2098/3600 [00:26<00:19, 76.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 2107/3600 [00:26<00:18, 80.55it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 2116/3600 [00:27<00:18, 79.65it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 2124/3600 [00:27<00:18, 79.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 2132/3600 [00:27<00:18, 78.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 2140/3600 [00:27<00:19, 75.20it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 2149/3600 [00:27<00:18, 79.02it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 2157/3600 [00:27<00:18, 77.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 2165/3600 [00:27<00:18, 77.86it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 2173/3600 [00:27<00:18, 77.96it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 2181/3600 [00:27<00:18, 77.94it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 2189/3600 [00:27<00:18, 75.49it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 2198/3600 [00:28<00:17, 79.35it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 2206/3600 [00:28<00:17, 78.57it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 2214/3600 [00:28<00:17, 78.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 2222/3600 [00:28<00:18, 74.92it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 2230/3600 [00:28<00:18, 75.60it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 2238/3600 [00:28<00:18, 75.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 2247/3600 [00:28<00:17, 75.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 2255/3600 [00:28<00:17, 75.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 2263/3600 [00:28<00:17, 76.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 2272/3600 [00:29<00:17, 76.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 2280/3600 [00:29<00:17, 76.57it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 2288/3600 [00:29<00:17, 76.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 2297/3600 [00:29<00:17, 76.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 2305/3600 [00:29<00:17, 73.57it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 2313/3600 [00:29<00:18, 68.89it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 2322/3600 [00:29<00:17, 72.07it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 2332/3600 [00:29<00:16, 77.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 2341/3600 [00:29<00:15, 79.69it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 2350/3600 [00:30<00:15, 80.06it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 2359/3600 [00:30<00:15, 78.59it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 2369/3600 [00:30<00:15, 79.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 2378/3600 [00:30<00:15, 80.74it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 2387/3600 [00:30<00:14, 81.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 2397/3600 [00:30<00:14, 85.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 2406/3600 [00:30<00:14, 84.89it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 2415/3600 [00:30<00:14, 84.20it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 2424/3600 [00:30<00:14, 83.86it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 2433/3600 [00:31<00:13, 84.24it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 2442/3600 [00:31<00:13, 84.25it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 2451/3600 [00:31<00:13, 84.30it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 2460/3600 [00:31<00:13, 84.24it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 2469/3600 [00:31<00:13, 81.99it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 2479/3600 [00:31<00:13, 85.59it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 2488/3600 [00:31<00:13, 84.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 2497/3600 [00:31<00:13, 84.67it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 2506/3600 [00:31<00:12, 84.71it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 2515/3600 [00:32<00:13, 82.06it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 2525/3600 [00:32<00:12, 85.78it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2534/3600 [00:32<00:13, 79.63it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2543/3600 [00:32<00:13, 81.13it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2553/3600 [00:32<00:12, 85.21it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2562/3600 [00:32<00:12, 85.56it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2571/3600 [00:32<00:13, 79.01it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2580/3600 [00:32<00:12, 79.47it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2591/3600 [00:32<00:11, 86.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2601/3600 [00:33<00:11, 85.19it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2610/3600 [00:33<00:11, 86.19it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2620/3600 [00:33<00:11, 87.66it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2629/3600 [00:33<00:11, 87.42it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2638/3600 [00:33<00:11, 84.92it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2650/3600 [00:33<00:10, 93.96it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2660/3600 [00:33<00:10, 90.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2670/3600 [00:33<00:10, 91.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2680/3600 [00:33<00:09, 92.68it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2690/3600 [00:34<00:09, 91.65it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2700/3600 [00:34<00:09, 93.75it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2710/3600 [00:34<00:09, 90.22it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2721/3600 [00:34<00:09, 94.46it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2731/3600 [00:34<00:09, 92.86it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2741/3600 [00:34<00:09, 91.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2752/3600 [00:34<00:09, 91.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2763/3600 [00:34<00:09, 92.33it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2774/3600 [00:34<00:08, 94.35it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2784/3600 [00:35<00:08, 90.82it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2795/3600 [00:35<00:08, 92.16it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2805/3600 [00:35<00:08, 93.09it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2815/3600 [00:35<00:08, 91.88it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2825/3600 [00:35<00:08, 91.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2835/3600 [00:35<00:08, 91.46it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2846/3600 [00:35<00:08, 94.04it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2856/3600 [00:35<00:08, 92.15it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2866/3600 [00:35<00:08, 90.88it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2877/3600 [00:36<00:07, 90.88it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2887/3600 [00:36<00:07, 92.33it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2897/3600 [00:36<00:07, 94.40it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2907/3600 [00:36<00:07, 87.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2918/3600 [00:36<00:07, 92.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2928/3600 [00:36<00:07, 91.78it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2938/3600 [00:36<00:07, 92.24it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2948/3600 [00:36<00:07, 91.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2958/3600 [00:36<00:07, 91.39it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2968/3600 [00:37<00:06, 92.88it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2978/3600 [00:37<00:06, 91.91it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2988/3600 [00:37<00:06, 92.78it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2999/3600 [00:37<00:06, 95.53it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 3009/3600 [00:37<00:06, 91.78it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3019/3600 [00:37<00:06, 93.39it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3029/3600 [00:37<00:06, 92.40it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3039/3600 [00:37<00:05, 93.65it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 3050/3600 [00:37<00:05, 96.21it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3060/3600 [00:38<00:05, 92.47it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 3071/3600 [00:38<00:05, 96.52it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3081/3600 [00:38<00:05, 93.84it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 3091/3600 [00:38<00:05, 89.49it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3102/3600 [00:38<00:05, 92.27it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3112/3600 [00:38<00:05, 93.19it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3123/3600 [00:38<00:04, 95.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3133/3600 [00:38<00:05, 93.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 3143/3600 [00:38<00:04, 91.69it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3153/3600 [00:39<00:04, 91.00it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3163/3600 [00:39<00:04, 92.57it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3174/3600 [00:39<00:04, 94.68it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3184/3600 [00:39<00:04, 93.85it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3194/3600 [00:39<00:04, 92.32it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3205/3600 [00:39<00:04, 95.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3215/3600 [00:39<00:04, 95.52it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3225/3600 [00:39<00:04, 91.25it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3235/3600 [00:39<00:03, 91.47it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3246/3600 [00:40<00:03, 91.98it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3257/3600 [00:40<00:03, 91.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3267/3600 [00:40<00:03, 92.44it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3277/3600 [00:40<00:03, 94.39it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3287/3600 [00:40<00:03, 91.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3298/3600 [00:40<00:03, 94.35it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3308/3600 [00:40<00:03, 90.81it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3318/3600 [00:40<00:03, 91.92it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3328/3600 [00:40<00:02, 91.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3339/3600 [00:41<00:02, 94.83it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3349/3600 [00:41<00:02, 89.85it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3359/3600 [00:41<00:02, 90.21it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3369/3600 [00:41<00:02, 89.34it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3380/3600 [00:41<00:02, 92.36it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3390/3600 [00:41<00:02, 92.37it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3400/3600 [00:41<00:02, 90.88it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3410/3600 [00:41<00:02, 88.49it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3420/3600 [00:41<00:02, 87.66it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3430/3600 [00:42<00:01, 88.26it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3439/3600 [00:42<00:01, 87.25it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3448/3600 [00:42<00:01, 79.55it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3458/3600 [00:42<00:01, 79.91it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3467/3600 [00:42<00:01, 77.67it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3476/3600 [00:42<00:01, 79.24it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3484/3600 [00:42<00:01, 74.68it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3492/3600 [00:42<00:01, 74.51it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3501/3600 [00:43<00:01, 76.99it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3509/3600 [00:43<00:01, 73.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3517/3600 [00:43<00:01, 73.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3526/3600 [00:43<00:00, 75.95it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3534/3600 [00:43<00:00, 72.25it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3543/3600 [00:43<00:00, 72.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3552/3600 [00:43<00:00, 75.21it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3560/3600 [00:43<00:00, 71.63it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3569/3600 [00:43<00:00, 72.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3578/3600 [00:44<00:00, 72.80it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3587/3600 [00:44<00:00, 75.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3595/3600 [00:44<00:00, 74.67it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3600/3600 [00:44<00:00, 81.10it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.5226
Epoch 1 Step 51 Train Loss: 0.5113
Epoch 1 Step 101 Train Loss: 0.6022
Epoch 1 Step 151 Train Loss: 0.5488
Epoch 1 Step 201 Train Loss: 0.4732
Epoch 1 Step 251 Train Loss: 0.5793
Epoch 1 Step 301 Train Loss: 0.5566
Epoch 1 Step 351 Train Loss: 0.5029
Epoch 1 Step 401 Train Loss: 0.4410
Epoch 1 Step 451 Train Loss: 0.5300
Epoch 1 Step 501 Train Loss: 0.5573
Epoch 1 Step 551 Train Loss: 0.5381
Epoch 1 Step 601 Train Loss: 0.5180
Epoch 1 Step 651 Train Loss: 0.5871
Epoch 1 Step 701 Train Loss: 0.4674
Epoch 1 Step 751 Train Loss: 0.4105
Epoch 1 Step 801 Train Loss: 0.4899
Epoch 1 Step 851 Train Loss: 0.5133
Epoch 1 Step 901 Train Loss: 0.5155
Epoch 1 Step 951 Train Loss: 0.5338
Epoch 1 Step 1001 Train Loss: 0.5737
Epoch 1: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0339 Validation Top 20 DE MSE: 0.0791. 
Epoch 2 Step 1 Train Loss: 0.6084
Epoch 2 Step 51 Train Loss: 0.5571
Epoch 2 Step 101 Train Loss: 0.5031
Epoch 2 Step 151 Train Loss: 0.4766
Epoch 2 Step 201 Train Loss: 0.5252
Epoch 2 Step 251 Train Loss: 0.5903
Epoch 2 Step 301 Train Loss: 0.4926
Epoch 2 Step 351 Train Loss: 0.4911
Epoch 2 Step 401 Train Loss: 0.4746
Epoch 2 Step 451 Train Loss: 0.5186
Epoch 2 Step 501 Train Loss: 0.5315
Epoch 2 Step 551 Train Loss: 0.4400
Epoch 2 Step 601 Train Loss: 0.5262
Epoch 2 Step 651 Train Loss: 0.6355
Epoch 2 Step 701 Train Loss: 0.4848
Epoch 2 Step 751 Train Loss: 0.5497
Epoch 2 Step 801 Train Loss: 0.4752
Epoch 2 Step 851 Train Loss: 0.4988
Epoch 2 Step 901 Train Loss: 0.5714
Epoch 2 Step 951 Train Loss: 0.5171
Epoch 2 Step 1001 Train Loss: 0.5999
Epoch 2: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0495 Validation Top 20 DE MSE: 0.0760. 
Epoch 3 Step 1 Train Loss: 0.4614
Epoch 3 Step 51 Train Loss: 0.6024
Epoch 3 Step 101 Train Loss: 0.5411
Epoch 3 Step 151 Train Loss: 0.5898
Epoch 3 Step 201 Train Loss: 0.5541
Epoch 3 Step 251 Train Loss: 0.4798
Epoch 3 Step 301 Train Loss: 0.4956
Epoch 3 Step 351 Train Loss: 0.5469
Epoch 3 Step 401 Train Loss: 0.4789
Epoch 3 Step 451 Train Loss: 0.5761
Epoch 3 Step 501 Train Loss: 0.6215
Epoch 3 Step 551 Train Loss: 0.5454
Epoch 3 Step 601 Train Loss: 0.5186
Epoch 3 Step 651 Train Loss: 0.4471
Epoch 3 Step 701 Train Loss: 0.5609
Epoch 3 Step 751 Train Loss: 0.6256
Epoch 3 Step 801 Train Loss: 0.6250
Epoch 3 Step 851 Train Loss: 0.6215
Epoch 3 Step 901 Train Loss: 0.4801
Epoch 3 Step 951 Train Loss: 0.4908
Epoch 3 Step 1001 Train Loss: 0.4108
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0191 Validation Top 20 DE MSE: 0.0416. 
Epoch 4 Step 1 Train Loss: 0.4185
Epoch 4 Step 51 Train Loss: 0.5669
Epoch 4 Step 101 Train Loss: 0.5853
Epoch 4 Step 151 Train Loss: 0.7293
Epoch 4 Step 201 Train Loss: 0.4014
Epoch 4 Step 251 Train Loss: 0.4704
Epoch 4 Step 301 Train Loss: 0.8143
Epoch 4 Step 351 Train Loss: 0.4972
Epoch 4 Step 401 Train Loss: 0.4829
Epoch 4 Step 451 Train Loss: 0.6266
Epoch 4 Step 501 Train Loss: 0.4916
Epoch 4 Step 551 Train Loss: 0.4808
Epoch 4 Step 601 Train Loss: 0.4608
Epoch 4 Step 651 Train Loss: 0.4724
Epoch 4 Step 701 Train Loss: 0.5097
Epoch 4 Step 751 Train Loss: 0.5717
Epoch 4 Step 801 Train Loss: 0.5963
Epoch 4 Step 851 Train Loss: 0.5908
Epoch 4 Step 901 Train Loss: 0.5136
Epoch 4 Step 951 Train Loss: 0.4672
Epoch 4 Step 1001 Train Loss: 0.4407
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0232 Validation Top 20 DE MSE: 0.0636. 
Epoch 5 Step 1 Train Loss: 0.4493
Epoch 5 Step 51 Train Loss: 0.5982
Epoch 5 Step 101 Train Loss: 0.4956
Epoch 5 Step 151 Train Loss: 0.4906
Epoch 5 Step 201 Train Loss: 0.5766
Epoch 5 Step 251 Train Loss: 0.6108
Epoch 5 Step 301 Train Loss: 0.5202
Epoch 5 Step 351 Train Loss: 0.5506
Epoch 5 Step 401 Train Loss: 0.4692
Epoch 5 Step 451 Train Loss: 0.5214
Epoch 5 Step 501 Train Loss: 0.4743
Epoch 5 Step 551 Train Loss: 0.4314
Epoch 5 Step 601 Train Loss: 0.6230
Epoch 5 Step 651 Train Loss: 0.5180
Epoch 5 Step 701 Train Loss: 0.6246
Epoch 5 Step 751 Train Loss: 0.4480
Epoch 5 Step 801 Train Loss: 0.4906
Epoch 5 Step 851 Train Loss: 0.4775
Epoch 5 Step 901 Train Loss: 0.4846
Epoch 5 Step 951 Train Loss: 0.5104
Epoch 5 Step 1001 Train Loss: 0.5577
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.0489. 
Epoch 6 Step 1 Train Loss: 0.4982
Epoch 6 Step 51 Train Loss: 0.7680
Epoch 6 Step 101 Train Loss: 0.4651
Epoch 6 Step 151 Train Loss: 0.4095
Epoch 6 Step 201 Train Loss: 0.6074
Epoch 6 Step 251 Train Loss: 0.5924
Epoch 6 Step 301 Train Loss: 0.4760
Epoch 6 Step 351 Train Loss: 0.5373
Epoch 6 Step 401 Train Loss: 0.6076
Epoch 6 Step 451 Train Loss: 0.6985
Epoch 6 Step 501 Train Loss: 0.5664
Epoch 6 Step 551 Train Loss: 0.4114
Epoch 6 Step 601 Train Loss: 0.4548
Epoch 6 Step 651 Train Loss: 0.5148
Epoch 6 Step 701 Train Loss: 0.5146
Epoch 6 Step 751 Train Loss: 0.4444
Epoch 6 Step 801 Train Loss: 0.5098
Epoch 6 Step 851 Train Loss: 0.7356
Epoch 6 Step 901 Train Loss: 0.4444
Epoch 6 Step 951 Train Loss: 0.5237
Epoch 6 Step 1001 Train Loss: 0.4479
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0153 Validation Top 20 DE MSE: 0.0457. 
Epoch 7 Step 1 Train Loss: 0.4838
Epoch 7 Step 51 Train Loss: 0.4380
Epoch 7 Step 101 Train Loss: 0.4400
Epoch 7 Step 151 Train Loss: 0.5000
Epoch 7 Step 201 Train Loss: 0.6129
Epoch 7 Step 251 Train Loss: 0.4485
Epoch 7 Step 301 Train Loss: 0.4879
Epoch 7 Step 351 Train Loss: 0.4606
Epoch 7 Step 401 Train Loss: 0.5081
Epoch 7 Step 451 Train Loss: 0.4102
Epoch 7 Step 501 Train Loss: 0.4448
Epoch 7 Step 551 Train Loss: 0.6100
Epoch 7 Step 601 Train Loss: 0.6955
Epoch 7 Step 651 Train Loss: 0.5101
Epoch 7 Step 701 Train Loss: 0.5125
Epoch 7 Step 751 Train Loss: 0.5117
Epoch 7 Step 801 Train Loss: 0.4789
Epoch 7 Step 851 Train Loss: 0.5259
Epoch 7 Step 901 Train Loss: 0.5494
Epoch 7 Step 951 Train Loss: 0.5292
Epoch 7 Step 1001 Train Loss: 0.5903
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.0416. 
Epoch 8 Step 1 Train Loss: 0.4843
Epoch 8 Step 51 Train Loss: 0.6338
Epoch 8 Step 101 Train Loss: 0.5406
Epoch 8 Step 151 Train Loss: 0.5116
Epoch 8 Step 201 Train Loss: 0.5846
Epoch 8 Step 251 Train Loss: 0.6274
Epoch 8 Step 301 Train Loss: 0.5444
Epoch 8 Step 351 Train Loss: 0.5753
Epoch 8 Step 401 Train Loss: 0.5375
Epoch 8 Step 451 Train Loss: 0.5214
Epoch 8 Step 501 Train Loss: 0.4804
Epoch 8 Step 551 Train Loss: 0.4489
Epoch 8 Step 601 Train Loss: 0.6778
Epoch 8 Step 651 Train Loss: 0.5687
Epoch 8 Step 701 Train Loss: 0.4896
Epoch 8 Step 751 Train Loss: 0.4347
Epoch 8 Step 801 Train Loss: 0.5205
Epoch 8 Step 851 Train Loss: 0.5268
Epoch 8 Step 901 Train Loss: 0.5378
Epoch 8 Step 951 Train Loss: 0.6144
Epoch 8 Step 1001 Train Loss: 0.7497
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0395. 
Epoch 9 Step 1 Train Loss: 0.4642
Epoch 9 Step 51 Train Loss: 0.5320
Epoch 9 Step 101 Train Loss: 0.6159
Epoch 9 Step 151 Train Loss: 0.5644
Epoch 9 Step 201 Train Loss: 0.5590
Epoch 9 Step 251 Train Loss: 0.5216
Epoch 9 Step 301 Train Loss: 0.5947
Epoch 9 Step 351 Train Loss: 0.4731
Epoch 9 Step 401 Train Loss: 0.4302
Epoch 9 Step 451 Train Loss: 0.5618
Epoch 9 Step 501 Train Loss: 0.6550
Epoch 9 Step 551 Train Loss: 0.5511
Epoch 9 Step 601 Train Loss: 0.5652
Epoch 9 Step 651 Train Loss: 0.4333
Epoch 9 Step 701 Train Loss: 0.4673
Epoch 9 Step 751 Train Loss: 0.6053
Epoch 9 Step 801 Train Loss: 0.4470
Epoch 9 Step 851 Train Loss: 0.5121
Epoch 9 Step 901 Train Loss: 0.5066
Epoch 9 Step 951 Train Loss: 0.4951
Epoch 9 Step 1001 Train Loss: 0.5633
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0425. 
Epoch 10 Step 1 Train Loss: 0.4451
Epoch 10 Step 51 Train Loss: 0.5831
Epoch 10 Step 101 Train Loss: 0.4799
Epoch 10 Step 151 Train Loss: 0.4638
Epoch 10 Step 201 Train Loss: 0.4565
Epoch 10 Step 251 Train Loss: 0.4668
Epoch 10 Step 301 Train Loss: 0.5164
Epoch 10 Step 351 Train Loss: 0.4755
Epoch 10 Step 401 Train Loss: 0.4785
Epoch 10 Step 451 Train Loss: 0.6392
Epoch 10 Step 501 Train Loss: 0.5836
Epoch 10 Step 551 Train Loss: 0.5187
Epoch 10 Step 601 Train Loss: 0.4590
Epoch 10 Step 651 Train Loss: 0.4387
Epoch 10 Step 701 Train Loss: 0.5603
Epoch 10 Step 751 Train Loss: 0.4896
Epoch 10 Step 801 Train Loss: 0.4941
Epoch 10 Step 851 Train Loss: 0.4841
Epoch 10 Step 901 Train Loss: 0.6239
Epoch 10 Step 951 Train Loss: 0.4639
Epoch 10 Step 1001 Train Loss: 0.5093
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0440. 
Epoch 11 Step 1 Train Loss: 0.4886
Epoch 11 Step 51 Train Loss: 0.4527
Epoch 11 Step 101 Train Loss: 0.5602
Epoch 11 Step 151 Train Loss: 0.5413
Epoch 11 Step 201 Train Loss: 0.6039
Epoch 11 Step 251 Train Loss: 0.4404
Epoch 11 Step 301 Train Loss: 0.4607
Epoch 11 Step 351 Train Loss: 0.4301
Epoch 11 Step 401 Train Loss: 0.4619
Epoch 11 Step 451 Train Loss: 0.6376
Epoch 11 Step 501 Train Loss: 0.5730
Epoch 11 Step 551 Train Loss: 0.5530
Epoch 11 Step 601 Train Loss: 0.4904
Epoch 11 Step 651 Train Loss: 0.5353
Epoch 11 Step 701 Train Loss: 0.4180
Epoch 11 Step 751 Train Loss: 0.4721
Epoch 11 Step 801 Train Loss: 0.4499
Epoch 11 Step 851 Train Loss: 0.5014
Epoch 11 Step 901 Train Loss: 0.4831
Epoch 11 Step 951 Train Loss: 0.4336
Epoch 11 Step 1001 Train Loss: 0.7004
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.0444. 
Epoch 12 Step 1 Train Loss: 0.7231
Epoch 12 Step 51 Train Loss: 0.4734
Epoch 12 Step 101 Train Loss: 0.4865
Epoch 12 Step 151 Train Loss: 0.5610
Epoch 12 Step 201 Train Loss: 0.6253
Epoch 12 Step 251 Train Loss: 0.4429
Epoch 12 Step 301 Train Loss: 0.5749
Epoch 12 Step 351 Train Loss: 0.4486
Epoch 12 Step 401 Train Loss: 0.4352
Epoch 12 Step 451 Train Loss: 0.6438
Epoch 12 Step 501 Train Loss: 0.5122
Epoch 12 Step 551 Train Loss: 0.4675
Epoch 12 Step 601 Train Loss: 0.6315
Epoch 12 Step 651 Train Loss: 0.5057
Epoch 12 Step 701 Train Loss: 0.5162
Epoch 12 Step 751 Train Loss: 0.5669
Epoch 12 Step 801 Train Loss: 0.5741
Epoch 12 Step 851 Train Loss: 0.4988
Epoch 12 Step 901 Train Loss: 0.4594
Epoch 12 Step 951 Train Loss: 0.4413
Epoch 12 Step 1001 Train Loss: 0.4855
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0117 Validation Top 20 DE MSE: 0.0481. 
Epoch 13 Step 1 Train Loss: 0.4959
Epoch 13 Step 51 Train Loss: 0.5182
Epoch 13 Step 101 Train Loss: 0.6708
Epoch 13 Step 151 Train Loss: 0.5756
Epoch 13 Step 201 Train Loss: 0.5272
Epoch 13 Step 251 Train Loss: 0.5144
Epoch 13 Step 301 Train Loss: 0.6069
Epoch 13 Step 351 Train Loss: 0.4772
Epoch 13 Step 401 Train Loss: 0.5002
Epoch 13 Step 451 Train Loss: 0.4562
Epoch 13 Step 501 Train Loss: 0.4196
Epoch 13 Step 551 Train Loss: 0.6403
Epoch 13 Step 601 Train Loss: 0.5526
Epoch 13 Step 651 Train Loss: 0.5595
Epoch 13 Step 701 Train Loss: 0.6860
Epoch 13 Step 751 Train Loss: 0.5576
Epoch 13 Step 801 Train Loss: 0.5409
Epoch 13 Step 851 Train Loss: 0.5054
Epoch 13 Step 901 Train Loss: 0.4857
Epoch 13 Step 951 Train Loss: 0.8061
Epoch 13 Step 1001 Train Loss: 0.4828
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0420. 
Epoch 14 Step 1 Train Loss: 0.6390
Epoch 14 Step 51 Train Loss: 0.4923
Epoch 14 Step 101 Train Loss: 0.4599
Epoch 14 Step 151 Train Loss: 0.5898
Epoch 14 Step 201 Train Loss: 0.5130
Epoch 14 Step 251 Train Loss: 0.5094
Epoch 14 Step 301 Train Loss: 0.6110
Epoch 14 Step 351 Train Loss: 0.4833
Epoch 14 Step 401 Train Loss: 0.4594
Epoch 14 Step 451 Train Loss: 0.5788
Epoch 14 Step 501 Train Loss: 0.5115
Epoch 14 Step 551 Train Loss: 0.5966
Epoch 14 Step 601 Train Loss: 0.4660
Epoch 14 Step 651 Train Loss: 0.5634
Epoch 14 Step 701 Train Loss: 0.5516
Epoch 14 Step 751 Train Loss: 0.4095
Epoch 14 Step 801 Train Loss: 0.5565
Epoch 14 Step 851 Train Loss: 0.4521
Epoch 14 Step 901 Train Loss: 0.4887
Epoch 14 Step 951 Train Loss: 0.4548
Epoch 14 Step 1001 Train Loss: 0.5100
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.0461. 
Epoch 15 Step 1 Train Loss: 0.5956
Epoch 15 Step 51 Train Loss: 0.4040
Epoch 15 Step 101 Train Loss: 0.6258
Epoch 15 Step 151 Train Loss: 0.4240
Epoch 15 Step 201 Train Loss: 0.7019
Epoch 15 Step 251 Train Loss: 0.5132
Epoch 15 Step 301 Train Loss: 0.5596
Epoch 15 Step 351 Train Loss: 0.5144
Epoch 15 Step 401 Train Loss: 0.4798
Epoch 15 Step 451 Train Loss: 0.7229
Epoch 15 Step 501 Train Loss: 0.4568
Epoch 15 Step 551 Train Loss: 0.4814
Epoch 15 Step 601 Train Loss: 0.4555
Epoch 15 Step 651 Train Loss: 0.5724
Epoch 15 Step 701 Train Loss: 0.4149
Epoch 15 Step 751 Train Loss: 0.6923
Epoch 15 Step 801 Train Loss: 0.5511
Epoch 15 Step 851 Train Loss: 0.6690
Epoch 15 Step 901 Train Loss: 0.4790
Epoch 15 Step 951 Train Loss: 0.4037
Epoch 15 Step 1001 Train Loss: 0.5625
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0437. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0474
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0010715205
test_unseen_single_pearson: 0.9980759609113079
test_unseen_single_mse_de: 0.047390822
test_unseen_single_pearson_de: 0.9866974923407462
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.7175887017034346
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.03333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9583333333333335
test_unseen_single_mse_top20_de_non_dropout: 0.047426880808073335
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.029 MB uploadedwandb: | 0.028 MB of 0.029 MB uploadedwandb: / 0.028 MB of 0.029 MB uploadedwandb: - 0.029 MB of 0.029 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÅ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñà
wandb:                                                   val_de_mse ‚ñà‚ñá‚ñÅ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.04739
wandb:                                              test_de_pearson 0.9867
wandb:               test_frac_opposite_direction_top20_non_dropout 0.03333
wandb:                          test_frac_sigma_below_1_non_dropout 0.95833
wandb:                                                     test_mse 0.00107
wandb:                                test_mse_top20_de_non_dropout 0.04743
wandb:                                                 test_pearson 0.99808
wandb:                                           test_pearson_delta 0.71759
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.03333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.95833
wandb:                                       test_unseen_single_mse 0.00107
wandb:                                    test_unseen_single_mse_de 0.04739
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.04743
wandb:                                   test_unseen_single_pearson 0.99808
wandb:                                test_unseen_single_pearson_de 0.9867
wandb:                             test_unseen_single_pearson_delta 0.71759
wandb:                                                 train_de_mse 0.00962
wandb:                                             train_de_pearson 0.9971
wandb:                                                    train_mse 0.00059
wandb:                                                train_pearson 0.99905
wandb:                                                training_loss 0.51793
wandb:                                                   val_de_mse 0.04371
wandb:                                               val_de_pearson 0.99194
wandb:                                                      val_mse 0.00105
wandb:                                                  val_pearson 0.99817
wandb: 
wandb: üöÄ View run scbert_Dixit_combined_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/2q5ga69a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_181527-2q5ga69a/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_183543-fd5fvig3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_combined_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/fd5fvig3
wandb: WARNING Serializing object of type ndarray that is 8024128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.6335
Epoch 1 Step 51 Train Loss: 0.6491
Epoch 1 Step 101 Train Loss: 0.5980
Epoch 1 Step 151 Train Loss: 0.4935
Epoch 1 Step 201 Train Loss: 0.6143
Epoch 1 Step 251 Train Loss: 0.6695
Epoch 1 Step 301 Train Loss: 0.4921
Epoch 1 Step 351 Train Loss: 0.4768
Epoch 1 Step 401 Train Loss: 0.5198
Epoch 1 Step 451 Train Loss: 0.6210
Epoch 1 Step 501 Train Loss: 0.7551
Epoch 1 Step 551 Train Loss: 0.5301
Epoch 1 Step 601 Train Loss: 0.6467
Epoch 1 Step 651 Train Loss: 0.6034
Epoch 1 Step 701 Train Loss: 0.5336
Epoch 1 Step 751 Train Loss: 0.5433
Epoch 1 Step 801 Train Loss: 0.4478
Epoch 1 Step 851 Train Loss: 0.4778
Epoch 1 Step 901 Train Loss: 0.4667
Epoch 1: Train Overall MSE: 0.0054 Validation Overall MSE: 0.0114. 
Train Top 20 DE MSE: 0.0383 Validation Top 20 DE MSE: 0.4283. 
Epoch 2 Step 1 Train Loss: 0.5030
Epoch 2 Step 51 Train Loss: 0.5307
Epoch 2 Step 101 Train Loss: 0.4312
Epoch 2 Step 151 Train Loss: 0.3853
Epoch 2 Step 201 Train Loss: 0.5761
Epoch 2 Step 251 Train Loss: 0.4733
Epoch 2 Step 301 Train Loss: 0.4977
Epoch 2 Step 351 Train Loss: 0.3774
Epoch 2 Step 401 Train Loss: 0.5687
Epoch 2 Step 451 Train Loss: 0.5661
Epoch 2 Step 501 Train Loss: 0.5347
Epoch 2 Step 551 Train Loss: 0.5721
Epoch 2 Step 601 Train Loss: 0.5700
Epoch 2 Step 651 Train Loss: 0.5221
Epoch 2 Step 701 Train Loss: 0.5371
Epoch 2 Step 751 Train Loss: 0.4503
Epoch 2 Step 801 Train Loss: 0.5163
Epoch 2 Step 851 Train Loss: 0.5561
Epoch 2 Step 901 Train Loss: 0.4836
Epoch 2: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0469 Validation Top 20 DE MSE: 0.0925. 
Epoch 3 Step 1 Train Loss: 0.4184
Epoch 3 Step 51 Train Loss: 0.5346
Epoch 3 Step 101 Train Loss: 0.5458
Epoch 3 Step 151 Train Loss: 0.4441
Epoch 3 Step 201 Train Loss: 0.4623
Epoch 3 Step 251 Train Loss: 0.5344
Epoch 3 Step 301 Train Loss: 0.4372
Epoch 3 Step 351 Train Loss: 0.5470
Epoch 3 Step 401 Train Loss: 0.4908
Epoch 3 Step 451 Train Loss: 0.5545
Epoch 3 Step 501 Train Loss: 0.6960
Epoch 3 Step 551 Train Loss: 0.4265
Epoch 3 Step 601 Train Loss: 0.4966
Epoch 3 Step 651 Train Loss: 0.5079
Epoch 3 Step 701 Train Loss: 0.4570
Epoch 3 Step 751 Train Loss: 0.5271
Epoch 3 Step 801 Train Loss: 0.9335
Epoch 3 Step 851 Train Loss: 0.4566
Epoch 3 Step 901 Train Loss: 0.5286
Epoch 3: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0065. 
Train Top 20 DE MSE: 0.0159 Validation Top 20 DE MSE: 0.3428. 
Epoch 4 Step 1 Train Loss: 0.5527
Epoch 4 Step 51 Train Loss: 0.5292
Epoch 4 Step 101 Train Loss: 0.6081
Epoch 4 Step 151 Train Loss: 0.3878
Epoch 4 Step 201 Train Loss: 0.4537
Epoch 4 Step 251 Train Loss: 0.6027
Epoch 4 Step 301 Train Loss: 0.5295
Epoch 4 Step 351 Train Loss: 0.5933
Epoch 4 Step 401 Train Loss: 0.5821
Epoch 4 Step 451 Train Loss: 0.5438
Epoch 4 Step 501 Train Loss: 0.4719
Epoch 4 Step 551 Train Loss: 0.5411
Epoch 4 Step 601 Train Loss: 0.6259
Epoch 4 Step 651 Train Loss: 0.4665
Epoch 4 Step 701 Train Loss: 0.6143
Epoch 4 Step 751 Train Loss: 0.5127
Epoch 4 Step 801 Train Loss: 0.4951
Epoch 4 Step 851 Train Loss: 0.5982
Epoch 4 Step 901 Train Loss: 0.4366
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0071. 
Train Top 20 DE MSE: 0.0181 Validation Top 20 DE MSE: 0.3843. 
Epoch 5 Step 1 Train Loss: 0.5589
Epoch 5 Step 51 Train Loss: 0.6716
Epoch 5 Step 101 Train Loss: 0.6490
Epoch 5 Step 151 Train Loss: 0.5033
Epoch 5 Step 201 Train Loss: 0.4437
Epoch 5 Step 251 Train Loss: 0.5855
Epoch 5 Step 301 Train Loss: 0.4360
Epoch 5 Step 351 Train Loss: 0.4795
Epoch 5 Step 401 Train Loss: 0.3892
Epoch 5 Step 451 Train Loss: 0.4472
Epoch 5 Step 501 Train Loss: 0.4783
Epoch 5 Step 551 Train Loss: 0.5680
Epoch 5 Step 601 Train Loss: 0.5855
Epoch 5 Step 651 Train Loss: 0.5211
Epoch 5 Step 701 Train Loss: 0.4877
Epoch 5 Step 751 Train Loss: 0.5386
Epoch 5 Step 801 Train Loss: 0.5187
Epoch 5 Step 851 Train Loss: 0.5894
Epoch 5 Step 901 Train Loss: 0.6028
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0072. 
Train Top 20 DE MSE: 0.0123 Validation Top 20 DE MSE: 0.3914. 
Epoch 6 Step 1 Train Loss: 0.5012
Epoch 6 Step 51 Train Loss: 0.4841
Epoch 6 Step 101 Train Loss: 0.5523
Epoch 6 Step 151 Train Loss: 0.5438
Epoch 6 Step 201 Train Loss: 0.5588
Epoch 6 Step 251 Train Loss: 0.5281
Epoch 6 Step 301 Train Loss: 0.4469
Epoch 6 Step 351 Train Loss: 0.6253
Epoch 6 Step 401 Train Loss: 0.5434
Epoch 6 Step 451 Train Loss: 0.6178
Epoch 6 Step 501 Train Loss: 0.5875
Epoch 6 Step 551 Train Loss: 0.4946
Epoch 6 Step 601 Train Loss: 0.5565
Epoch 6 Step 651 Train Loss: 0.5420
Epoch 6 Step 701 Train Loss: 0.4408
Epoch 6 Step 751 Train Loss: 0.6626
Epoch 6 Step 801 Train Loss: 0.4516
Epoch 6 Step 851 Train Loss: 0.5009
Epoch 6 Step 901 Train Loss: 0.4427
Epoch 6: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0077. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.4194. 
Epoch 7 Step 1 Train Loss: 0.4966
Epoch 7 Step 51 Train Loss: 0.4476
Epoch 7 Step 101 Train Loss: 0.3977
Epoch 7 Step 151 Train Loss: 0.4287
Epoch 7 Step 201 Train Loss: 0.5806
Epoch 7 Step 251 Train Loss: 0.4597
Epoch 7 Step 301 Train Loss: 0.4461
Epoch 7 Step 351 Train Loss: 0.5625
Epoch 7 Step 401 Train Loss: 0.5489
Epoch 7 Step 451 Train Loss: 0.5403
Epoch 7 Step 501 Train Loss: 0.4770
Epoch 7 Step 551 Train Loss: 0.4822
Epoch 7 Step 601 Train Loss: 0.5668
Epoch 7 Step 651 Train Loss: 0.6999
Epoch 7 Step 701 Train Loss: 0.5625
Epoch 7 Step 751 Train Loss: 0.5330
Epoch 7 Step 801 Train Loss: 0.7515
Epoch 7 Step 851 Train Loss: 0.5606
Epoch 7 Step 901 Train Loss: 0.5842
Epoch 7: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.4200. 
Epoch 8 Step 1 Train Loss: 0.4099
Epoch 8 Step 51 Train Loss: 0.5238
Epoch 8 Step 101 Train Loss: 0.3889
Epoch 8 Step 151 Train Loss: 0.5394
Epoch 8 Step 201 Train Loss: 0.4997
Epoch 8 Step 251 Train Loss: 0.4409
Epoch 8 Step 301 Train Loss: 0.8012
Epoch 8 Step 351 Train Loss: 0.4390
Epoch 8 Step 401 Train Loss: 0.6412
Epoch 8 Step 451 Train Loss: 0.5455
Epoch 8 Step 501 Train Loss: 0.4604
Epoch 8 Step 551 Train Loss: 0.4981
Epoch 8 Step 601 Train Loss: 0.4236
Epoch 8 Step 651 Train Loss: 0.4001
Epoch 8 Step 701 Train Loss: 0.5997
Epoch 8 Step 751 Train Loss: 0.4508
Epoch 8 Step 801 Train Loss: 0.5247
Epoch 8 Step 851 Train Loss: 0.5920
Epoch 8 Step 901 Train Loss: 0.6981
Epoch 8: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.4184. 
Epoch 9 Step 1 Train Loss: 0.4608
Epoch 9 Step 51 Train Loss: 0.4473
Epoch 9 Step 101 Train Loss: 0.5292
Epoch 9 Step 151 Train Loss: 0.4663
Epoch 9 Step 201 Train Loss: 0.4756
Epoch 9 Step 251 Train Loss: 0.5630
Epoch 9 Step 301 Train Loss: 0.4816
Epoch 9 Step 351 Train Loss: 0.4413
Epoch 9 Step 401 Train Loss: 0.5572
Epoch 9 Step 451 Train Loss: 0.7616
Epoch 9 Step 501 Train Loss: 0.5030
Epoch 9 Step 551 Train Loss: 0.5431
Epoch 9 Step 601 Train Loss: 0.4125
Epoch 9 Step 651 Train Loss: 0.6501
Epoch 9 Step 701 Train Loss: 0.6004
Epoch 9 Step 751 Train Loss: 0.4927
Epoch 9 Step 801 Train Loss: 0.6298
Epoch 9 Step 851 Train Loss: 0.5399
Epoch 9 Step 901 Train Loss: 0.4012
Epoch 9: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0077. 
Train Top 20 DE MSE: 0.0103 Validation Top 20 DE MSE: 0.4208. 
Epoch 10 Step 1 Train Loss: 0.5075
Epoch 10 Step 51 Train Loss: 0.4658
Epoch 10 Step 101 Train Loss: 0.6345
Epoch 10 Step 151 Train Loss: 0.5469
Epoch 10 Step 201 Train Loss: 0.4555
Epoch 10 Step 251 Train Loss: 0.7584
Epoch 10 Step 301 Train Loss: 0.4723
Epoch 10 Step 351 Train Loss: 0.6457
Epoch 10 Step 401 Train Loss: 0.5682
Epoch 10 Step 451 Train Loss: 0.5112
Epoch 10 Step 501 Train Loss: 0.5058
Epoch 10 Step 551 Train Loss: 0.6101
Epoch 10 Step 601 Train Loss: 0.4026
Epoch 10 Step 651 Train Loss: 0.4927
Epoch 10 Step 701 Train Loss: 0.5258
Epoch 10 Step 751 Train Loss: 0.6424
Epoch 10 Step 801 Train Loss: 0.5261
Epoch 10 Step 851 Train Loss: 0.5404
Epoch 10 Step 901 Train Loss: 0.6271
Epoch 10: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0124 Validation Top 20 DE MSE: 0.4025. 
Epoch 11 Step 1 Train Loss: 0.5031
Epoch 11 Step 51 Train Loss: 0.4451
Epoch 11 Step 101 Train Loss: 0.4312
Epoch 11 Step 151 Train Loss: 0.6120
Epoch 11 Step 201 Train Loss: 0.4732
Epoch 11 Step 251 Train Loss: 0.5714
Epoch 11 Step 301 Train Loss: 0.4550
Epoch 11 Step 351 Train Loss: 0.5045
Epoch 11 Step 401 Train Loss: 0.6041
Epoch 11 Step 451 Train Loss: 0.4460
Epoch 11 Step 501 Train Loss: 0.5038
Epoch 11 Step 551 Train Loss: 0.6078
Epoch 11 Step 601 Train Loss: 0.4313
Epoch 11 Step 651 Train Loss: 0.5443
Epoch 11 Step 701 Train Loss: 0.5100
Epoch 11 Step 751 Train Loss: 0.4027
Epoch 11 Step 801 Train Loss: 0.4134
Epoch 11 Step 851 Train Loss: 0.6139
Epoch 11 Step 901 Train Loss: 0.4134
Epoch 11: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.4185. 
Epoch 12 Step 1 Train Loss: 0.5126
Epoch 12 Step 51 Train Loss: 0.4443
Epoch 12 Step 101 Train Loss: 0.5058
Epoch 12 Step 151 Train Loss: 0.4328
Epoch 12 Step 201 Train Loss: 0.6587
Epoch 12 Step 251 Train Loss: 0.5531
Epoch 12 Step 301 Train Loss: 0.6118
Epoch 12 Step 351 Train Loss: 0.6548
Epoch 12 Step 401 Train Loss: 0.5310
Epoch 12 Step 451 Train Loss: 0.5524
Epoch 12 Step 501 Train Loss: 0.5094
Epoch 12 Step 551 Train Loss: 0.5788
Epoch 12 Step 601 Train Loss: 0.5090
Epoch 12 Step 651 Train Loss: 0.7157
Epoch 12 Step 701 Train Loss: 0.4233
Epoch 12 Step 751 Train Loss: 0.6739
Epoch 12 Step 801 Train Loss: 0.4874
Epoch 12 Step 851 Train Loss: 0.5082
Epoch 12 Step 901 Train Loss: 0.4534
Epoch 12: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0074. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.4057. 
Epoch 13 Step 1 Train Loss: 0.5582
Epoch 13 Step 51 Train Loss: 0.4681
Epoch 13 Step 101 Train Loss: 0.7228
Epoch 13 Step 151 Train Loss: 0.6807
Epoch 13 Step 201 Train Loss: 0.4555
Epoch 13 Step 251 Train Loss: 0.5117
Epoch 13 Step 301 Train Loss: 0.4363
Epoch 13 Step 351 Train Loss: 0.6030
Epoch 13 Step 401 Train Loss: 0.4191
Epoch 13 Step 451 Train Loss: 0.4216
Epoch 13 Step 501 Train Loss: 0.5463
Epoch 13 Step 551 Train Loss: 0.4690
Epoch 13 Step 601 Train Loss: 0.4622
Epoch 13 Step 651 Train Loss: 0.4680
Epoch 13 Step 701 Train Loss: 0.5621
Epoch 13 Step 751 Train Loss: 0.5435
Epoch 13 Step 801 Train Loss: 0.5684
Epoch 13 Step 851 Train Loss: 0.5499
Epoch 13 Step 901 Train Loss: 0.7623
Epoch 13: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0103 Validation Top 20 DE MSE: 0.4185. 
Epoch 14 Step 1 Train Loss: 0.4890
Epoch 14 Step 51 Train Loss: 0.4790
Epoch 14 Step 101 Train Loss: 0.4198
Epoch 14 Step 151 Train Loss: 0.5384
Epoch 14 Step 201 Train Loss: 0.4467
Epoch 14 Step 251 Train Loss: 0.5030
Epoch 14 Step 301 Train Loss: 0.5793
Epoch 14 Step 351 Train Loss: 0.5160
Epoch 14 Step 401 Train Loss: 0.4741
Epoch 14 Step 451 Train Loss: 0.5841
Epoch 14 Step 501 Train Loss: 0.5832
Epoch 14 Step 551 Train Loss: 0.3951
Epoch 14 Step 601 Train Loss: 0.6598
Epoch 14 Step 651 Train Loss: 0.4670
Epoch 14 Step 701 Train Loss: 0.4414
Epoch 14 Step 751 Train Loss: 0.6064
Epoch 14 Step 801 Train Loss: 0.6033
Epoch 14 Step 851 Train Loss: 0.4802
Epoch 14 Step 901 Train Loss: 0.4674
Epoch 14: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0074. 
Train Top 20 DE MSE: 0.0117 Validation Top 20 DE MSE: 0.4077. 
Epoch 15 Step 1 Train Loss: 0.5252
Epoch 15 Step 51 Train Loss: 0.5282
Epoch 15 Step 101 Train Loss: 0.4429
Epoch 15 Step 151 Train Loss: 0.5742
Epoch 15 Step 201 Train Loss: 0.5604
Epoch 15 Step 251 Train Loss: 0.5420
Epoch 15 Step 301 Train Loss: 0.5446
Epoch 15 Step 351 Train Loss: 0.5562
Epoch 15 Step 401 Train Loss: 0.4877
Epoch 15 Step 451 Train Loss: 0.4580
Epoch 15 Step 501 Train Loss: 0.6519
Epoch 15 Step 551 Train Loss: 0.5160
Epoch 15 Step 601 Train Loss: 0.6542
Epoch 15 Step 651 Train Loss: 0.4936
Epoch 15 Step 701 Train Loss: 0.4713
Epoch 15 Step 751 Train Loss: 0.5629
Epoch 15 Step 801 Train Loss: 0.3839
Epoch 15 Step 851 Train Loss: 0.5355
Epoch 15 Step 901 Train Loss: 0.4413
Epoch 15: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.4180. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1367
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0026108145
test_unseen_single_pearson: 0.9952110939180957
test_unseen_single_mse_de: 0.13667782
test_unseen_single_pearson_de: 0.9605933182937574
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.026706741665611083
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.48333333333333334
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8916666666666666
test_unseen_single_mse_top20_de_non_dropout: 0.1366778183615461
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.007 MB of 0.028 MB uploadedwandb: / 0.007 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÇ‚ñÖ‚ñá‚ñÇ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.13668
wandb:                                              test_de_pearson 0.96059
wandb:               test_frac_opposite_direction_top20_non_dropout 0.48333
wandb:                          test_frac_sigma_below_1_non_dropout 0.89167
wandb:                                                     test_mse 0.00261
wandb:                                test_mse_top20_de_non_dropout 0.13668
wandb:                                                 test_pearson 0.99521
wandb:                                           test_pearson_delta 0.02671
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.48333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.89167
wandb:                                       test_unseen_single_mse 0.00261
wandb:                                    test_unseen_single_mse_de 0.13668
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.13668
wandb:                                   test_unseen_single_pearson 0.99521
wandb:                                test_unseen_single_pearson_de 0.96059
wandb:                             test_unseen_single_pearson_delta 0.02671
wandb:                                                 train_de_mse 0.01082
wandb:                                             train_de_pearson 0.99678
wandb:                                                    train_mse 0.0007
wandb:                                                train_pearson 0.99888
wandb:                                                training_loss 0.69407
wandb:                                                   val_de_mse 0.418
wandb:                                               val_de_pearson 0.88565
wandb:                                                      val_mse 0.00762
wandb:                                                  val_pearson 0.9866
wandb: 
wandb: üöÄ View run scbert_Dixit_combined_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/fd5fvig3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_183543-fd5fvig3/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_185312-jurenhcj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_combined_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/jurenhcj
wandb: WARNING Serializing object of type ndarray that is 8024128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5384
Epoch 1 Step 51 Train Loss: 0.5044
Epoch 1 Step 101 Train Loss: 0.5093
Epoch 1 Step 151 Train Loss: 0.6527
Epoch 1 Step 201 Train Loss: 0.7056
Epoch 1 Step 251 Train Loss: 0.6737
Epoch 1 Step 301 Train Loss: 0.5536
Epoch 1 Step 351 Train Loss: 0.8012
Epoch 1 Step 401 Train Loss: 0.5462
Epoch 1 Step 451 Train Loss: 0.4831
Epoch 1 Step 501 Train Loss: 0.5117
Epoch 1 Step 551 Train Loss: 0.7486
Epoch 1 Step 601 Train Loss: 0.4750
Epoch 1 Step 651 Train Loss: 0.6391
Epoch 1 Step 701 Train Loss: 0.4901
Epoch 1 Step 751 Train Loss: 0.5268
Epoch 1 Step 801 Train Loss: 0.5662
Epoch 1 Step 851 Train Loss: 0.4254
Epoch 1 Step 901 Train Loss: 0.6568
Epoch 1 Step 951 Train Loss: 0.5875
Epoch 1: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0543 Validation Top 20 DE MSE: 0.0142. 
Epoch 2 Step 1 Train Loss: 0.4865
Epoch 2 Step 51 Train Loss: 0.5035
Epoch 2 Step 101 Train Loss: 0.4964
Epoch 2 Step 151 Train Loss: 0.5475
Epoch 2 Step 201 Train Loss: 0.5237
Epoch 2 Step 251 Train Loss: 0.4481
Epoch 2 Step 301 Train Loss: 0.4736
Epoch 2 Step 351 Train Loss: 0.5312
Epoch 2 Step 401 Train Loss: 0.4534
Epoch 2 Step 451 Train Loss: 0.4791
Epoch 2 Step 501 Train Loss: 0.5310
Epoch 2 Step 551 Train Loss: 0.5477
Epoch 2 Step 601 Train Loss: 0.4494
Epoch 2 Step 651 Train Loss: 0.5342
Epoch 2 Step 701 Train Loss: 0.5362
Epoch 2 Step 751 Train Loss: 0.4150
Epoch 2 Step 801 Train Loss: 0.4870
Epoch 2 Step 851 Train Loss: 0.4625
Epoch 2 Step 901 Train Loss: 0.4251
Epoch 2 Step 951 Train Loss: 0.5501
Epoch 2: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0232 Validation Top 20 DE MSE: 0.0054. 
Epoch 3 Step 1 Train Loss: 0.4147
Epoch 3 Step 51 Train Loss: 0.5322
Epoch 3 Step 101 Train Loss: 0.6678
Epoch 3 Step 151 Train Loss: 0.4445
Epoch 3 Step 201 Train Loss: 0.5235
Epoch 3 Step 251 Train Loss: 0.6607
Epoch 3 Step 301 Train Loss: 0.6884
Epoch 3 Step 351 Train Loss: 0.4508
Epoch 3 Step 401 Train Loss: 0.5393
Epoch 3 Step 451 Train Loss: 0.5508
Epoch 3 Step 501 Train Loss: 0.4773
Epoch 3 Step 551 Train Loss: 0.4911
Epoch 3 Step 601 Train Loss: 0.5358
Epoch 3 Step 651 Train Loss: 0.7869
Epoch 3 Step 701 Train Loss: 0.6156
Epoch 3 Step 751 Train Loss: 0.5167
Epoch 3 Step 801 Train Loss: 0.4804
Epoch 3 Step 851 Train Loss: 0.7253
Epoch 3 Step 901 Train Loss: 0.5721
Epoch 3 Step 951 Train Loss: 0.4879
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0077 Validation Top 20 DE MSE: 0.0070. 
Epoch 4 Step 1 Train Loss: 0.5217
Epoch 4 Step 51 Train Loss: 0.4487
Epoch 4 Step 101 Train Loss: 0.5418
Epoch 4 Step 151 Train Loss: 0.4596
Epoch 4 Step 201 Train Loss: 0.5793
Epoch 4 Step 251 Train Loss: 0.4616
Epoch 4 Step 301 Train Loss: 0.5657
Epoch 4 Step 351 Train Loss: 0.4942
Epoch 4 Step 401 Train Loss: 0.4800
Epoch 4 Step 451 Train Loss: 0.5544
Epoch 4 Step 501 Train Loss: 0.5099
Epoch 4 Step 551 Train Loss: 0.4883
Epoch 4 Step 601 Train Loss: 0.4773
Epoch 4 Step 651 Train Loss: 0.4417
Epoch 4 Step 701 Train Loss: 0.5967
Epoch 4 Step 751 Train Loss: 0.5232
Epoch 4 Step 801 Train Loss: 0.6640
Epoch 4 Step 851 Train Loss: 0.7326
Epoch 4 Step 901 Train Loss: 0.4792
Epoch 4 Step 951 Train Loss: 0.5000
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.0111. 
Epoch 5 Step 1 Train Loss: 0.4732
Epoch 5 Step 51 Train Loss: 0.5464
Epoch 5 Step 101 Train Loss: 0.4432
Epoch 5 Step 151 Train Loss: 0.5090
Epoch 5 Step 201 Train Loss: 0.5106
Epoch 5 Step 251 Train Loss: 0.4736
Epoch 5 Step 301 Train Loss: 0.4938
Epoch 5 Step 351 Train Loss: 0.6348
Epoch 5 Step 401 Train Loss: 0.3924
Epoch 5 Step 451 Train Loss: 0.5569
Epoch 5 Step 501 Train Loss: 0.4721
Epoch 5 Step 551 Train Loss: 0.5298
Epoch 5 Step 601 Train Loss: 0.4547
Epoch 5 Step 651 Train Loss: 0.5618
Epoch 5 Step 701 Train Loss: 0.7064
Epoch 5 Step 751 Train Loss: 0.4768
Epoch 5 Step 801 Train Loss: 0.3832
Epoch 5 Step 851 Train Loss: 0.6575
Epoch 5 Step 901 Train Loss: 0.6500
Epoch 5 Step 951 Train Loss: 0.6014
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0149. 
Epoch 6 Step 1 Train Loss: 0.4712
Epoch 6 Step 51 Train Loss: 0.7532
Epoch 6 Step 101 Train Loss: 0.5648
Epoch 6 Step 151 Train Loss: 0.4933
Epoch 6 Step 201 Train Loss: 0.3949
Epoch 6 Step 251 Train Loss: 0.6620
Epoch 6 Step 301 Train Loss: 0.4452
Epoch 6 Step 351 Train Loss: 0.4410
Epoch 6 Step 401 Train Loss: 0.5962
Epoch 6 Step 451 Train Loss: 0.5606
Epoch 6 Step 501 Train Loss: 0.4233
Epoch 6 Step 551 Train Loss: 0.5714
Epoch 6 Step 601 Train Loss: 0.7030
Epoch 6 Step 651 Train Loss: 0.6687
Epoch 6 Step 701 Train Loss: 0.5330
Epoch 6 Step 751 Train Loss: 0.5810
Epoch 6 Step 801 Train Loss: 0.5453
Epoch 6 Step 851 Train Loss: 0.5505
Epoch 6 Step 901 Train Loss: 0.4243
Epoch 6 Step 951 Train Loss: 0.4528
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.0121. 
Epoch 7 Step 1 Train Loss: 0.6086
Epoch 7 Step 51 Train Loss: 0.4476
Epoch 7 Step 101 Train Loss: 0.5071
Epoch 7 Step 151 Train Loss: 0.4833
Epoch 7 Step 201 Train Loss: 0.5662
Epoch 7 Step 251 Train Loss: 0.4699
Epoch 7 Step 301 Train Loss: 0.5777
Epoch 7 Step 351 Train Loss: 0.4596
Epoch 7 Step 401 Train Loss: 0.4616
Epoch 7 Step 451 Train Loss: 0.4210
Epoch 7 Step 501 Train Loss: 0.4702
Epoch 7 Step 551 Train Loss: 0.5801
Epoch 7 Step 601 Train Loss: 0.4683
Epoch 7 Step 651 Train Loss: 0.4624
Epoch 7 Step 701 Train Loss: 0.5149
Epoch 7 Step 751 Train Loss: 0.5151
Epoch 7 Step 801 Train Loss: 0.5807
Epoch 7 Step 851 Train Loss: 0.5123
Epoch 7 Step 901 Train Loss: 0.5437
Epoch 7 Step 951 Train Loss: 0.4345
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.0155. 
Epoch 8 Step 1 Train Loss: 0.5916
Epoch 8 Step 51 Train Loss: 0.4116
Epoch 8 Step 101 Train Loss: 0.5164
Epoch 8 Step 151 Train Loss: 0.6163
Epoch 8 Step 201 Train Loss: 0.5161
Epoch 8 Step 251 Train Loss: 0.4782
Epoch 8 Step 301 Train Loss: 0.5802
Epoch 8 Step 351 Train Loss: 0.5192
Epoch 8 Step 401 Train Loss: 0.5091
Epoch 8 Step 451 Train Loss: 0.5212
Epoch 8 Step 501 Train Loss: 0.5646
Epoch 8 Step 551 Train Loss: 0.6113
Epoch 8 Step 601 Train Loss: 0.4457
Epoch 8 Step 651 Train Loss: 0.6922
Epoch 8 Step 701 Train Loss: 0.4467
Epoch 8 Step 751 Train Loss: 0.6612
Epoch 8 Step 801 Train Loss: 0.5318
Epoch 8 Step 851 Train Loss: 0.5569
Epoch 8 Step 901 Train Loss: 0.4469
Epoch 8 Step 951 Train Loss: 0.5599
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0157. 
Epoch 9 Step 1 Train Loss: 0.5161
Epoch 9 Step 51 Train Loss: 0.4775
Epoch 9 Step 101 Train Loss: 0.5110
Epoch 9 Step 151 Train Loss: 0.5141
Epoch 9 Step 201 Train Loss: 0.4300
Epoch 9 Step 251 Train Loss: 0.5201
Epoch 9 Step 301 Train Loss: 0.5295
Epoch 9 Step 351 Train Loss: 0.4584
Epoch 9 Step 401 Train Loss: 0.6405
Epoch 9 Step 451 Train Loss: 0.5777
Epoch 9 Step 501 Train Loss: 0.4787
Epoch 9 Step 551 Train Loss: 0.4746
Epoch 9 Step 601 Train Loss: 0.5551
Epoch 9 Step 651 Train Loss: 0.5334
Epoch 9 Step 701 Train Loss: 0.4305
Epoch 9 Step 751 Train Loss: 0.4943
Epoch 9 Step 801 Train Loss: 0.4536
Epoch 9 Step 851 Train Loss: 0.4935
Epoch 9 Step 901 Train Loss: 0.5635
Epoch 9 Step 951 Train Loss: 0.4801
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.0159. 
Epoch 10 Step 1 Train Loss: 0.6218
Epoch 10 Step 51 Train Loss: 0.4308
Epoch 10 Step 101 Train Loss: 0.4996
Epoch 10 Step 151 Train Loss: 0.4993
Epoch 10 Step 201 Train Loss: 0.4946
Epoch 10 Step 251 Train Loss: 0.4986
Epoch 10 Step 301 Train Loss: 0.7234
Epoch 10 Step 351 Train Loss: 0.4284
Epoch 10 Step 401 Train Loss: 0.5258
Epoch 10 Step 451 Train Loss: 0.5430
Epoch 10 Step 501 Train Loss: 0.6286
Epoch 10 Step 551 Train Loss: 0.5294
Epoch 10 Step 601 Train Loss: 0.4193
Epoch 10 Step 651 Train Loss: 0.5522
Epoch 10 Step 701 Train Loss: 0.5217
Epoch 10 Step 751 Train Loss: 0.5538
Epoch 10 Step 801 Train Loss: 0.5431
Epoch 10 Step 851 Train Loss: 0.5424
Epoch 10 Step 901 Train Loss: 0.5777
Epoch 10 Step 951 Train Loss: 0.5105
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.0166. 
Epoch 11 Step 1 Train Loss: 0.5217
Epoch 11 Step 51 Train Loss: 0.5480
Epoch 11 Step 101 Train Loss: 0.6736
Epoch 11 Step 151 Train Loss: 0.5001
Epoch 11 Step 201 Train Loss: 0.5009
Epoch 11 Step 251 Train Loss: 0.6257
Epoch 11 Step 301 Train Loss: 0.4727
Epoch 11 Step 351 Train Loss: 0.5349
Epoch 11 Step 401 Train Loss: 0.4537
Epoch 11 Step 451 Train Loss: 0.5951
Epoch 11 Step 501 Train Loss: 0.5633
Epoch 11 Step 551 Train Loss: 0.5529
Epoch 11 Step 601 Train Loss: 0.6071
Epoch 11 Step 651 Train Loss: 0.3787
Epoch 11 Step 701 Train Loss: 0.4894
Epoch 11 Step 751 Train Loss: 0.4737
Epoch 11 Step 801 Train Loss: 0.4889
Epoch 11 Step 851 Train Loss: 0.6661
Epoch 11 Step 901 Train Loss: 0.4489
Epoch 11 Step 951 Train Loss: 0.6280
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0167. 
Epoch 12 Step 1 Train Loss: 0.4297
Epoch 12 Step 51 Train Loss: 0.5208
Epoch 12 Step 101 Train Loss: 0.6539
Epoch 12 Step 151 Train Loss: 0.5647
Epoch 12 Step 201 Train Loss: 0.5313
Epoch 12 Step 251 Train Loss: 0.5091
Epoch 12 Step 301 Train Loss: 0.4984
Epoch 12 Step 351 Train Loss: 0.4257
Epoch 12 Step 401 Train Loss: 0.4721
Epoch 12 Step 451 Train Loss: 0.4581
Epoch 12 Step 501 Train Loss: 0.6400
Epoch 12 Step 551 Train Loss: 0.6090
Epoch 12 Step 601 Train Loss: 0.5047
Epoch 12 Step 651 Train Loss: 0.5887
Epoch 12 Step 701 Train Loss: 0.4550
Epoch 12 Step 751 Train Loss: 0.4513
Epoch 12 Step 801 Train Loss: 0.5118
Epoch 12 Step 851 Train Loss: 0.5815
Epoch 12 Step 901 Train Loss: 0.4625
Epoch 12 Step 951 Train Loss: 0.5854
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0132. 
Epoch 13 Step 1 Train Loss: 0.4853
Epoch 13 Step 51 Train Loss: 0.4797
Epoch 13 Step 101 Train Loss: 0.4709
Epoch 13 Step 151 Train Loss: 0.4249
Epoch 13 Step 201 Train Loss: 0.4677
Epoch 13 Step 251 Train Loss: 0.7444
Epoch 13 Step 301 Train Loss: 0.5434
Epoch 13 Step 351 Train Loss: 0.5838
Epoch 13 Step 401 Train Loss: 0.4105
Epoch 13 Step 451 Train Loss: 0.5232
Epoch 13 Step 501 Train Loss: 0.4868
Epoch 13 Step 551 Train Loss: 0.5608
Epoch 13 Step 601 Train Loss: 0.6201
Epoch 13 Step 651 Train Loss: 0.4466
Epoch 13 Step 701 Train Loss: 0.6160
Epoch 13 Step 751 Train Loss: 0.5444
Epoch 13 Step 801 Train Loss: 0.5361
Epoch 13 Step 851 Train Loss: 0.4447
Epoch 13 Step 901 Train Loss: 0.4702
Epoch 13 Step 951 Train Loss: 0.4806
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0161. 
Epoch 14 Step 1 Train Loss: 0.3974
Epoch 14 Step 51 Train Loss: 0.4870
Epoch 14 Step 101 Train Loss: 0.5490
Epoch 14 Step 151 Train Loss: 0.5635
Epoch 14 Step 201 Train Loss: 0.4576
Epoch 14 Step 251 Train Loss: 0.5351
Epoch 14 Step 301 Train Loss: 0.4369
Epoch 14 Step 351 Train Loss: 0.4371
Epoch 14 Step 401 Train Loss: 0.4896
Epoch 14 Step 451 Train Loss: 0.6529
Epoch 14 Step 501 Train Loss: 0.5304
Epoch 14 Step 551 Train Loss: 0.4871
Epoch 14 Step 601 Train Loss: 0.6281
Epoch 14 Step 651 Train Loss: 0.4977
Epoch 14 Step 701 Train Loss: 0.5175
Epoch 14 Step 751 Train Loss: 0.5737
Epoch 14 Step 801 Train Loss: 0.5255
Epoch 14 Step 851 Train Loss: 0.4857
Epoch 14 Step 901 Train Loss: 0.5236
Epoch 14 Step 951 Train Loss: 0.6935
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0174. 
Epoch 15 Step 1 Train Loss: 0.4540
Epoch 15 Step 51 Train Loss: 0.6713
Epoch 15 Step 101 Train Loss: 0.6145
Epoch 15 Step 151 Train Loss: 0.4990
Epoch 15 Step 201 Train Loss: 0.4285
Epoch 15 Step 251 Train Loss: 0.4247
Epoch 15 Step 301 Train Loss: 0.4853
Epoch 15 Step 351 Train Loss: 0.4839
Epoch 15 Step 401 Train Loss: 0.5870
Epoch 15 Step 451 Train Loss: 0.4900
Epoch 15 Step 501 Train Loss: 0.4727
Epoch 15 Step 551 Train Loss: 0.5665
Epoch 15 Step 601 Train Loss: 0.5808
Epoch 15 Step 651 Train Loss: 0.5225
Epoch 15 Step 701 Train Loss: 0.4601
Epoch 15 Step 751 Train Loss: 0.4390
Epoch 15 Step 801 Train Loss: 0.5264
Epoch 15 Step 851 Train Loss: 0.4325
Epoch 15 Step 901 Train Loss: 0.4624
Epoch 15 Step 951 Train Loss: 0.6430
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0073 Validation Top 20 DE MSE: 0.0117. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1184
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0025885252
test_unseen_single_pearson: 0.9953762588935712
test_unseen_single_mse_de: 0.118449025
test_unseen_single_pearson_de: 0.9645809215771187
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3469635604258318
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8750000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.11844901925568078
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.001 MB of 0.028 MB uploadedwandb: / 0.028 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñá
wandb:                                                   val_de_mse ‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñÖ
wandb:                                               val_de_pearson ‚ñÇ‚ñà‚ñá‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÑ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.11845
wandb:                                              test_de_pearson 0.96458
wandb:               test_frac_opposite_direction_top20_non_dropout 0.33333
wandb:                          test_frac_sigma_below_1_non_dropout 0.875
wandb:                                                     test_mse 0.00259
wandb:                                test_mse_top20_de_non_dropout 0.11845
wandb:                                                 test_pearson 0.99538
wandb:                                           test_pearson_delta 0.34696
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.33333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.875
wandb:                                       test_unseen_single_mse 0.00259
wandb:                                    test_unseen_single_mse_de 0.11845
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.11845
wandb:                                   test_unseen_single_pearson 0.99538
wandb:                                test_unseen_single_pearson_de 0.96458
wandb:                             test_unseen_single_pearson_delta 0.34696
wandb:                                                 train_de_mse 0.0073
wandb:                                             train_de_pearson 0.99776
wandb:                                                    train_mse 0.00059
wandb:                                                train_pearson 0.99905
wandb:                                                training_loss 0.59473
wandb:                                                   val_de_mse 0.0117
wandb:                                               val_de_pearson 0.99732
wandb:                                                      val_mse 0.00042
wandb:                                                  val_pearson 0.99922
wandb: 
wandb: üöÄ View run scbert_Dixit_combined_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/jurenhcj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_185312-jurenhcj/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_191144-lfbmc4t8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_combined_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/lfbmc4t8
wandb: WARNING Serializing object of type ndarray that is 8024128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5645
Epoch 1 Step 51 Train Loss: 0.6382
Epoch 1 Step 101 Train Loss: 0.5632
Epoch 1 Step 151 Train Loss: 0.5508
Epoch 1 Step 201 Train Loss: 0.5061
Epoch 1 Step 251 Train Loss: 0.5037
Epoch 1 Step 301 Train Loss: 0.4856
Epoch 1 Step 351 Train Loss: 0.5175
Epoch 1 Step 401 Train Loss: 0.5931
Epoch 1 Step 451 Train Loss: 0.5349
Epoch 1 Step 501 Train Loss: 0.4790
Epoch 1 Step 551 Train Loss: 0.5201
Epoch 1 Step 601 Train Loss: 0.6835
Epoch 1 Step 651 Train Loss: 0.4240
Epoch 1 Step 701 Train Loss: 0.6379
Epoch 1 Step 751 Train Loss: 0.5599
Epoch 1 Step 801 Train Loss: 0.5184
Epoch 1 Step 851 Train Loss: 0.5920
Epoch 1 Step 901 Train Loss: 0.5287
Epoch 1 Step 951 Train Loss: 0.6389
Epoch 1 Step 1001 Train Loss: 0.4951
Epoch 1 Step 1051 Train Loss: 0.5377
Epoch 1 Step 1101 Train Loss: 0.4235
Epoch 1: Train Overall MSE: 0.0055 Validation Overall MSE: 0.0069. 
Train Top 20 DE MSE: 0.2635 Validation Top 20 DE MSE: 0.3339. 
Epoch 2 Step 1 Train Loss: 0.5239
Epoch 2 Step 51 Train Loss: 0.4871
Epoch 2 Step 101 Train Loss: 0.4797
Epoch 2 Step 151 Train Loss: 0.5426
Epoch 2 Step 201 Train Loss: 0.4969
Epoch 2 Step 251 Train Loss: 0.5436
Epoch 2 Step 301 Train Loss: 0.5671
Epoch 2 Step 351 Train Loss: 0.4509
Epoch 2 Step 401 Train Loss: 0.4723
Epoch 2 Step 451 Train Loss: 0.6277
Epoch 2 Step 501 Train Loss: 0.5332
Epoch 2 Step 551 Train Loss: 0.4685
Epoch 2 Step 601 Train Loss: 0.5277
Epoch 2 Step 651 Train Loss: 0.4758
Epoch 2 Step 701 Train Loss: 0.6097
Epoch 2 Step 751 Train Loss: 0.4998
Epoch 2 Step 801 Train Loss: 0.5053
Epoch 2 Step 851 Train Loss: 0.5506
Epoch 2 Step 901 Train Loss: 0.4716
Epoch 2 Step 951 Train Loss: 0.4312
Epoch 2 Step 1001 Train Loss: 0.5996
Epoch 2 Step 1051 Train Loss: 0.4978
Epoch 2 Step 1101 Train Loss: 0.4710
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0575 Validation Top 20 DE MSE: 0.0206. 
Epoch 3 Step 1 Train Loss: 0.4895
Epoch 3 Step 51 Train Loss: 0.4625
Epoch 3 Step 101 Train Loss: 0.4938
Epoch 3 Step 151 Train Loss: 0.4134
Epoch 3 Step 201 Train Loss: 0.5614
Epoch 3 Step 251 Train Loss: 0.4305
Epoch 3 Step 301 Train Loss: 0.4872
Epoch 3 Step 351 Train Loss: 0.5792
Epoch 3 Step 401 Train Loss: 0.5118
Epoch 3 Step 451 Train Loss: 0.6854
Epoch 3 Step 501 Train Loss: 0.4298
Epoch 3 Step 551 Train Loss: 0.4605
Epoch 3 Step 601 Train Loss: 0.4826
Epoch 3 Step 651 Train Loss: 0.5921
Epoch 3 Step 701 Train Loss: 0.4180
Epoch 3 Step 751 Train Loss: 0.5613
Epoch 3 Step 801 Train Loss: 0.6195
Epoch 3 Step 851 Train Loss: 0.5717
Epoch 3 Step 901 Train Loss: 0.4510
Epoch 3 Step 951 Train Loss: 0.4545
Epoch 3 Step 1001 Train Loss: 0.4486
Epoch 3 Step 1051 Train Loss: 0.5110
Epoch 3 Step 1101 Train Loss: 0.4591
Epoch 3: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0060. 
Train Top 20 DE MSE: 0.1080 Validation Top 20 DE MSE: 0.3124. 
Epoch 4 Step 1 Train Loss: 0.5423
Epoch 4 Step 51 Train Loss: 0.4563
Epoch 4 Step 101 Train Loss: 0.5613
Epoch 4 Step 151 Train Loss: 0.4707
Epoch 4 Step 201 Train Loss: 0.4857
Epoch 4 Step 251 Train Loss: 0.5766
Epoch 4 Step 301 Train Loss: 0.6018
Epoch 4 Step 351 Train Loss: 0.5648
Epoch 4 Step 401 Train Loss: 0.4839
Epoch 4 Step 451 Train Loss: 0.5666
Epoch 4 Step 501 Train Loss: 0.6322
Epoch 4 Step 551 Train Loss: 0.6820
Epoch 4 Step 601 Train Loss: 0.4710
Epoch 4 Step 651 Train Loss: 0.4519
Epoch 4 Step 701 Train Loss: 0.4840
Epoch 4 Step 751 Train Loss: 0.5240
Epoch 4 Step 801 Train Loss: 0.5090
Epoch 4 Step 851 Train Loss: 0.5274
Epoch 4 Step 901 Train Loss: 0.4784
Epoch 4 Step 951 Train Loss: 0.4977
Epoch 4 Step 1001 Train Loss: 0.6838
Epoch 4 Step 1051 Train Loss: 0.5333
Epoch 4 Step 1101 Train Loss: 0.4023
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0321 Validation Top 20 DE MSE: 0.0439. 
Epoch 5 Step 1 Train Loss: 0.5256
Epoch 5 Step 51 Train Loss: 0.3961
Epoch 5 Step 101 Train Loss: 0.4355
Epoch 5 Step 151 Train Loss: 0.5014
Epoch 5 Step 201 Train Loss: 0.5276
Epoch 5 Step 251 Train Loss: 0.5531
Epoch 5 Step 301 Train Loss: 0.5932
Epoch 5 Step 351 Train Loss: 0.4895
Epoch 5 Step 401 Train Loss: 0.4955
Epoch 5 Step 451 Train Loss: 0.5455
Epoch 5 Step 501 Train Loss: 0.4711
Epoch 5 Step 551 Train Loss: 0.5822
Epoch 5 Step 601 Train Loss: 0.5226
Epoch 5 Step 651 Train Loss: 0.4180
Epoch 5 Step 701 Train Loss: 0.6298
Epoch 5 Step 751 Train Loss: 0.5899
Epoch 5 Step 801 Train Loss: 0.4014
Epoch 5 Step 851 Train Loss: 0.5392
Epoch 5 Step 901 Train Loss: 0.4830
Epoch 5 Step 951 Train Loss: 0.6902
Epoch 5 Step 1001 Train Loss: 0.4713
Epoch 5 Step 1051 Train Loss: 0.4765
Epoch 5 Step 1101 Train Loss: 0.4714
Epoch 5: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0078 Validation Top 20 DE MSE: 0.0089. 
Epoch 6 Step 1 Train Loss: 0.5308
Epoch 6 Step 51 Train Loss: 0.4951
Epoch 6 Step 101 Train Loss: 0.5169
Epoch 6 Step 151 Train Loss: 0.4540
Epoch 6 Step 201 Train Loss: 0.5236
Epoch 6 Step 251 Train Loss: 0.4209
Epoch 6 Step 301 Train Loss: 0.5127
Epoch 6 Step 351 Train Loss: 0.5280
Epoch 6 Step 401 Train Loss: 0.5353
Epoch 6 Step 451 Train Loss: 0.4922
Epoch 6 Step 501 Train Loss: 0.5719
Epoch 6 Step 551 Train Loss: 0.5432
Epoch 6 Step 601 Train Loss: 0.5409
Epoch 6 Step 651 Train Loss: 0.7528
Epoch 6 Step 701 Train Loss: 0.5313
Epoch 6 Step 751 Train Loss: 0.4984
Epoch 6 Step 801 Train Loss: 0.5025
Epoch 6 Step 851 Train Loss: 0.4408
Epoch 6 Step 901 Train Loss: 0.4358
Epoch 6 Step 951 Train Loss: 0.5526
Epoch 6 Step 1001 Train Loss: 0.4104
Epoch 6 Step 1051 Train Loss: 0.4958
Epoch 6 Step 1101 Train Loss: 0.5716
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.0137. 
Epoch 7 Step 1 Train Loss: 0.5104
Epoch 7 Step 51 Train Loss: 0.5125
Epoch 7 Step 101 Train Loss: 0.4611
Epoch 7 Step 151 Train Loss: 0.4846
Epoch 7 Step 201 Train Loss: 0.6523
Epoch 7 Step 251 Train Loss: 0.6686
Epoch 7 Step 301 Train Loss: 0.5087
Epoch 7 Step 351 Train Loss: 0.4821
Epoch 7 Step 401 Train Loss: 0.5467
Epoch 7 Step 451 Train Loss: 0.5188
Epoch 7 Step 501 Train Loss: 0.5092
Epoch 7 Step 551 Train Loss: 0.5174
Epoch 7 Step 601 Train Loss: 0.4945
Epoch 7 Step 651 Train Loss: 0.4154
Epoch 7 Step 701 Train Loss: 0.5329
Epoch 7 Step 751 Train Loss: 0.7537
Epoch 7 Step 801 Train Loss: 0.4716
Epoch 7 Step 851 Train Loss: 0.5803
Epoch 7 Step 901 Train Loss: 0.5708
Epoch 7 Step 951 Train Loss: 0.6683
Epoch 7 Step 1001 Train Loss: 0.4274
Epoch 7 Step 1051 Train Loss: 0.5095
Epoch 7 Step 1101 Train Loss: 0.5233
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0124 Validation Top 20 DE MSE: 0.0138. 
Epoch 8 Step 1 Train Loss: 0.5402
Epoch 8 Step 51 Train Loss: 0.4868
Epoch 8 Step 101 Train Loss: 0.4112
Epoch 8 Step 151 Train Loss: 0.5564
Epoch 8 Step 201 Train Loss: 0.6521
Epoch 8 Step 251 Train Loss: 0.5159
Epoch 8 Step 301 Train Loss: 0.5775
Epoch 8 Step 351 Train Loss: 0.4559
Epoch 8 Step 401 Train Loss: 0.5043
Epoch 8 Step 451 Train Loss: 0.5028
Epoch 8 Step 501 Train Loss: 0.4836
Epoch 8 Step 551 Train Loss: 0.4180
Epoch 8 Step 601 Train Loss: 0.5148
Epoch 8 Step 651 Train Loss: 0.5676
Epoch 8 Step 701 Train Loss: 0.5536
Epoch 8 Step 751 Train Loss: 0.5598
Epoch 8 Step 801 Train Loss: 0.3938
Epoch 8 Step 851 Train Loss: 0.5507
Epoch 8 Step 901 Train Loss: 0.5298
Epoch 8 Step 951 Train Loss: 0.5313
Epoch 8 Step 1001 Train Loss: 0.6790
Epoch 8 Step 1051 Train Loss: 0.4529
Epoch 8 Step 1101 Train Loss: 0.4639
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.0116. 
Epoch 9 Step 1 Train Loss: 0.5283
Epoch 9 Step 51 Train Loss: 0.5241
Epoch 9 Step 101 Train Loss: 0.5491
Epoch 9 Step 151 Train Loss: 0.4726
Epoch 9 Step 201 Train Loss: 0.4198
Epoch 9 Step 251 Train Loss: 0.5135
Epoch 9 Step 301 Train Loss: 0.5646
Epoch 9 Step 351 Train Loss: 0.5429
Epoch 9 Step 401 Train Loss: 0.4303
Epoch 9 Step 451 Train Loss: 0.5004
Epoch 9 Step 501 Train Loss: 0.5093
Epoch 9 Step 551 Train Loss: 0.4513
Epoch 9 Step 601 Train Loss: 0.4575
Epoch 9 Step 651 Train Loss: 0.5007
Epoch 9 Step 701 Train Loss: 0.5747
Epoch 9 Step 751 Train Loss: 0.4618
Epoch 9 Step 801 Train Loss: 0.4782
Epoch 9 Step 851 Train Loss: 0.5117
Epoch 9 Step 901 Train Loss: 0.5575
Epoch 9 Step 951 Train Loss: 0.7760
Epoch 9 Step 1001 Train Loss: 0.5330
Epoch 9 Step 1051 Train Loss: 0.5543
Epoch 9 Step 1101 Train Loss: 0.5417
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0107. 
Epoch 10 Step 1 Train Loss: 0.4505
Epoch 10 Step 51 Train Loss: 0.4971
Epoch 10 Step 101 Train Loss: 0.5111
Epoch 10 Step 151 Train Loss: 0.5542
Epoch 10 Step 201 Train Loss: 0.6490
Epoch 10 Step 251 Train Loss: 0.4513
Epoch 10 Step 301 Train Loss: 0.5070
Epoch 10 Step 351 Train Loss: 0.4923
Epoch 10 Step 401 Train Loss: 0.5514
Epoch 10 Step 451 Train Loss: 0.5722
Epoch 10 Step 501 Train Loss: 0.4038
Epoch 10 Step 551 Train Loss: 0.5026
Epoch 10 Step 601 Train Loss: 0.5899
Epoch 10 Step 651 Train Loss: 0.4995
Epoch 10 Step 701 Train Loss: 0.5558
Epoch 10 Step 751 Train Loss: 0.4443
Epoch 10 Step 801 Train Loss: 0.5291
Epoch 10 Step 851 Train Loss: 0.5912
Epoch 10 Step 901 Train Loss: 0.5986
Epoch 10 Step 951 Train Loss: 0.6185
Epoch 10 Step 1001 Train Loss: 0.4724
Epoch 10 Step 1051 Train Loss: 0.5377
Epoch 10 Step 1101 Train Loss: 0.5330
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0154. 
Epoch 11 Step 1 Train Loss: 0.5544
Epoch 11 Step 51 Train Loss: 0.4384
Epoch 11 Step 101 Train Loss: 0.5671
Epoch 11 Step 151 Train Loss: 0.4488
Epoch 11 Step 201 Train Loss: 0.5214
Epoch 11 Step 251 Train Loss: 0.5128
Epoch 11 Step 301 Train Loss: 0.5390
Epoch 11 Step 351 Train Loss: 0.5467
Epoch 11 Step 401 Train Loss: 0.5004
Epoch 11 Step 451 Train Loss: 0.6411
Epoch 11 Step 501 Train Loss: 0.5892
Epoch 11 Step 551 Train Loss: 0.8077
Epoch 11 Step 601 Train Loss: 0.3930
Epoch 11 Step 651 Train Loss: 0.6303
Epoch 11 Step 701 Train Loss: 0.5235
Epoch 11 Step 751 Train Loss: 0.4864
Epoch 11 Step 801 Train Loss: 0.5298
Epoch 11 Step 851 Train Loss: 0.5305
Epoch 11 Step 901 Train Loss: 0.6180
Epoch 11 Step 951 Train Loss: 0.4727
Epoch 11 Step 1001 Train Loss: 0.4967
Epoch 11 Step 1051 Train Loss: 0.4844
Epoch 11 Step 1101 Train Loss: 0.4558
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.0088. 
Epoch 12 Step 1 Train Loss: 0.5209
Epoch 12 Step 51 Train Loss: 0.4855
Epoch 12 Step 101 Train Loss: 0.4567
Epoch 12 Step 151 Train Loss: 0.5274
Epoch 12 Step 201 Train Loss: 0.3813
Epoch 12 Step 251 Train Loss: 0.6714
Epoch 12 Step 301 Train Loss: 0.4576
Epoch 12 Step 351 Train Loss: 0.5071
Epoch 12 Step 401 Train Loss: 0.4454
Epoch 12 Step 451 Train Loss: 0.4857
Epoch 12 Step 501 Train Loss: 0.4358
Epoch 12 Step 551 Train Loss: 0.4960
Epoch 12 Step 601 Train Loss: 0.4308
Epoch 12 Step 651 Train Loss: 0.4657
Epoch 12 Step 701 Train Loss: 0.5759
Epoch 12 Step 751 Train Loss: 0.4981
Epoch 12 Step 801 Train Loss: 0.5220
Epoch 12 Step 851 Train Loss: 0.4662
Epoch 12 Step 901 Train Loss: 0.4918
Epoch 12 Step 951 Train Loss: 0.4799
Epoch 12 Step 1001 Train Loss: 0.5496
Epoch 12 Step 1051 Train Loss: 0.6445
Epoch 12 Step 1101 Train Loss: 0.4528
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.0089. 
Epoch 13 Step 1 Train Loss: 0.5457
Epoch 13 Step 51 Train Loss: 0.6112
Epoch 13 Step 101 Train Loss: 0.5299
Epoch 13 Step 151 Train Loss: 0.4949
Epoch 13 Step 201 Train Loss: 0.4456
Epoch 13 Step 251 Train Loss: 0.5532
Epoch 13 Step 301 Train Loss: 0.5277
Epoch 13 Step 351 Train Loss: 0.5604
Epoch 13 Step 401 Train Loss: 0.4656
Epoch 13 Step 451 Train Loss: 0.4735
Epoch 13 Step 501 Train Loss: 0.4536
Epoch 13 Step 551 Train Loss: 0.4637
Epoch 13 Step 601 Train Loss: 0.4656
Epoch 13 Step 651 Train Loss: 0.6302
Epoch 13 Step 701 Train Loss: 0.5022
Epoch 13 Step 751 Train Loss: 0.4570
Epoch 13 Step 801 Train Loss: 0.4766
Epoch 13 Step 851 Train Loss: 0.5209
Epoch 13 Step 901 Train Loss: 0.4954
Epoch 13 Step 951 Train Loss: 0.4300
Epoch 13 Step 1001 Train Loss: 0.4627
Epoch 13 Step 1051 Train Loss: 0.4912
Epoch 13 Step 1101 Train Loss: 0.3953
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0077 Validation Top 20 DE MSE: 0.0125. 
Epoch 14 Step 1 Train Loss: 0.5919
Epoch 14 Step 51 Train Loss: 0.5394
Epoch 14 Step 101 Train Loss: 0.6339
Epoch 14 Step 151 Train Loss: 0.3931
Epoch 14 Step 201 Train Loss: 0.4660
Epoch 14 Step 251 Train Loss: 0.4566
Epoch 14 Step 301 Train Loss: 0.4689
Epoch 14 Step 351 Train Loss: 0.4951
Epoch 14 Step 401 Train Loss: 0.4043
Epoch 14 Step 451 Train Loss: 0.6958
Epoch 14 Step 501 Train Loss: 0.4788
Epoch 14 Step 551 Train Loss: 0.5329
Epoch 14 Step 601 Train Loss: 0.5064
Epoch 14 Step 651 Train Loss: 0.6566
Epoch 14 Step 701 Train Loss: 0.5577
Epoch 14 Step 751 Train Loss: 0.5089
Epoch 14 Step 801 Train Loss: 0.6332
Epoch 14 Step 851 Train Loss: 0.4210
Epoch 14 Step 901 Train Loss: 0.4171
Epoch 14 Step 951 Train Loss: 0.5360
Epoch 14 Step 1001 Train Loss: 0.3735
Epoch 14 Step 1051 Train Loss: 0.5019
Epoch 14 Step 1101 Train Loss: 0.6544
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0104. 
Epoch 15 Step 1 Train Loss: 0.4906
Epoch 15 Step 51 Train Loss: 0.5216
Epoch 15 Step 101 Train Loss: 0.5159
Epoch 15 Step 151 Train Loss: 0.4768
Epoch 15 Step 201 Train Loss: 0.5436
Epoch 15 Step 251 Train Loss: 0.5428
Epoch 15 Step 301 Train Loss: 0.4908
Epoch 15 Step 351 Train Loss: 0.4829
Epoch 15 Step 401 Train Loss: 0.5396
Epoch 15 Step 451 Train Loss: 0.5614
Epoch 15 Step 501 Train Loss: 0.5136
Epoch 15 Step 551 Train Loss: 0.4492
Epoch 15 Step 601 Train Loss: 0.3890
Epoch 15 Step 651 Train Loss: 0.5559
Epoch 15 Step 701 Train Loss: 0.4832
Epoch 15 Step 751 Train Loss: 0.5494
Epoch 15 Step 801 Train Loss: 0.6474
Epoch 15 Step 851 Train Loss: 0.4959
Epoch 15 Step 901 Train Loss: 0.4913
Epoch 15 Step 951 Train Loss: 0.5266
Epoch 15 Step 1001 Train Loss: 0.4000
Epoch 15 Step 1051 Train Loss: 0.4779
Epoch 15 Step 1101 Train Loss: 0.5402
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0086 Validation Top 20 DE MSE: 0.0176. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0206
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0007637761
test_unseen_single_pearson: 0.9987012856215371
test_unseen_single_mse_de: 0.020605436
test_unseen_single_pearson_de: 0.9940534093473944
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8510930169403762
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.016666666666666666
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9500000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.0204621312646296
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.030 MB uploadedwandb: | 0.001 MB of 0.030 MB uploadedwandb: / 0.007 MB of 0.030 MB uploadedwandb: - 0.014 MB of 0.030 MB uploadedwandb: \ 0.014 MB of 0.030 MB uploadedwandb: | 0.014 MB of 0.030 MB uploadedwandb: / 0.030 MB of 0.030 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñà‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.02061
wandb:                                              test_de_pearson 0.99405
wandb:               test_frac_opposite_direction_top20_non_dropout 0.01667
wandb:                          test_frac_sigma_below_1_non_dropout 0.95
wandb:                                                     test_mse 0.00076
wandb:                                test_mse_top20_de_non_dropout 0.02046
wandb:                                                 test_pearson 0.9987
wandb:                                           test_pearson_delta 0.85109
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.01667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.95
wandb:                                       test_unseen_single_mse 0.00076
wandb:                                    test_unseen_single_mse_de 0.02061
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.02046
wandb:                                   test_unseen_single_pearson 0.9987
wandb:                                test_unseen_single_pearson_de 0.99405
wandb:                             test_unseen_single_pearson_delta 0.85109
wandb:                                                 train_de_mse 0.00859
wandb:                                             train_de_pearson 0.99743
wandb:                                                    train_mse 0.00049
wandb:                                                train_pearson 0.9992
wandb:                                                training_loss 0.59341
wandb:                                                   val_de_mse 0.01757
wandb:                                               val_de_pearson 0.99493
wandb:                                                      val_mse 0.00071
wandb:                                                  val_pearson 0.99877
wandb: 
wandb: üöÄ View run scbert_Dixit_combined_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/lfbmc4t8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_191144-lfbmc4t8/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_193231-39aw2kjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_combined_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/39aw2kjp
wandb: WARNING Serializing object of type ndarray that is 8024128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5665
Epoch 1 Step 51 Train Loss: 0.5399
Epoch 1 Step 101 Train Loss: 0.4814
Epoch 1 Step 151 Train Loss: 0.5366
Epoch 1 Step 201 Train Loss: 0.5895
Epoch 1 Step 251 Train Loss: 0.6332
Epoch 1 Step 301 Train Loss: 0.5360
Epoch 1 Step 351 Train Loss: 0.5109
Epoch 1 Step 401 Train Loss: 0.6048
Epoch 1 Step 451 Train Loss: 0.5394
Epoch 1 Step 501 Train Loss: 0.5421
Epoch 1 Step 551 Train Loss: 0.4477
Epoch 1 Step 601 Train Loss: 0.4677
Epoch 1 Step 651 Train Loss: 0.5935
Epoch 1 Step 701 Train Loss: 0.5771
Epoch 1 Step 751 Train Loss: 0.5889
Epoch 1 Step 801 Train Loss: 0.7231
Epoch 1 Step 851 Train Loss: 0.4357
Epoch 1 Step 901 Train Loss: 0.4319
Epoch 1 Step 951 Train Loss: 0.4551
Epoch 1 Step 1001 Train Loss: 0.5129
Epoch 1 Step 1051 Train Loss: 0.5168
Epoch 1 Step 1101 Train Loss: 0.4931
Epoch 1: Train Overall MSE: 0.0045 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.2064 Validation Top 20 DE MSE: 0.0833. 
Epoch 2 Step 1 Train Loss: 0.5899
Epoch 2 Step 51 Train Loss: 0.4972
Epoch 2 Step 101 Train Loss: 0.6704
Epoch 2 Step 151 Train Loss: 0.5421
Epoch 2 Step 201 Train Loss: 0.5258
Epoch 2 Step 251 Train Loss: 0.5294
Epoch 2 Step 301 Train Loss: 0.4755
Epoch 2 Step 351 Train Loss: 0.5320
Epoch 2 Step 401 Train Loss: 0.5711
Epoch 2 Step 451 Train Loss: 0.4741
Epoch 2 Step 501 Train Loss: 0.4960
Epoch 2 Step 551 Train Loss: 0.4583
Epoch 2 Step 601 Train Loss: 0.6387
Epoch 2 Step 651 Train Loss: 0.5154
Epoch 2 Step 701 Train Loss: 0.5305
Epoch 2 Step 751 Train Loss: 0.4780
Epoch 2 Step 801 Train Loss: 0.5768
Epoch 2 Step 851 Train Loss: 0.5616
Epoch 2 Step 901 Train Loss: 0.6538
Epoch 2 Step 951 Train Loss: 0.4491
Epoch 2 Step 1001 Train Loss: 0.5198
Epoch 2 Step 1051 Train Loss: 0.4903
Epoch 2 Step 1101 Train Loss: 0.4555
Epoch 2: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0211. 
Epoch 3 Step 1 Train Loss: 0.4665
Epoch 3 Step 51 Train Loss: 0.5306
Epoch 3 Step 101 Train Loss: 0.4726
Epoch 3 Step 151 Train Loss: 0.4860
Epoch 3 Step 201 Train Loss: 0.5130
Epoch 3 Step 251 Train Loss: 0.4064
Epoch 3 Step 301 Train Loss: 0.6716
Epoch 3 Step 351 Train Loss: 0.4859
Epoch 3 Step 401 Train Loss: 0.5142
Epoch 3 Step 451 Train Loss: 0.5068
Epoch 3 Step 501 Train Loss: 0.4448
Epoch 3 Step 551 Train Loss: 0.5746
Epoch 3 Step 601 Train Loss: 0.4313
Epoch 3 Step 651 Train Loss: 0.4563
Epoch 3 Step 701 Train Loss: 0.4597
Epoch 3 Step 751 Train Loss: 0.4083
Epoch 3 Step 801 Train Loss: 0.4763
Epoch 3 Step 851 Train Loss: 0.6886
Epoch 3 Step 901 Train Loss: 0.4100
Epoch 3 Step 951 Train Loss: 0.5095
Epoch 3 Step 1001 Train Loss: 0.5692
Epoch 3 Step 1051 Train Loss: 0.4987
Epoch 3 Step 1101 Train Loss: 0.4743
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.0074. 
Epoch 4 Step 1 Train Loss: 0.5122
Epoch 4 Step 51 Train Loss: 0.4558
Epoch 4 Step 101 Train Loss: 0.6154
Epoch 4 Step 151 Train Loss: 0.6287
Epoch 4 Step 201 Train Loss: 0.5212
Epoch 4 Step 251 Train Loss: 0.5976
Epoch 4 Step 301 Train Loss: 0.4072
Epoch 4 Step 351 Train Loss: 0.4829
Epoch 4 Step 401 Train Loss: 0.5327
Epoch 4 Step 451 Train Loss: 0.5498
Epoch 4 Step 501 Train Loss: 0.5298
Epoch 4 Step 551 Train Loss: 0.5017
Epoch 4 Step 601 Train Loss: 0.4754
Epoch 4 Step 651 Train Loss: 0.4925
Epoch 4 Step 701 Train Loss: 0.5428
Epoch 4 Step 751 Train Loss: 0.5263
Epoch 4 Step 801 Train Loss: 0.6445
Epoch 4 Step 851 Train Loss: 0.6891
Epoch 4 Step 901 Train Loss: 0.5148
Epoch 4 Step 951 Train Loss: 0.5015
Epoch 4 Step 1001 Train Loss: 0.5085
Epoch 4 Step 1051 Train Loss: 0.5113
Epoch 4 Step 1101 Train Loss: 0.4447
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0063 Validation Top 20 DE MSE: 0.0133. 
Epoch 5 Step 1 Train Loss: 0.6998
Epoch 5 Step 51 Train Loss: 0.5503
Epoch 5 Step 101 Train Loss: 0.4864
Epoch 5 Step 151 Train Loss: 0.5310
Epoch 5 Step 201 Train Loss: 0.4839
Epoch 5 Step 251 Train Loss: 0.4668
Epoch 5 Step 301 Train Loss: 0.4769
Epoch 5 Step 351 Train Loss: 0.4839
Epoch 5 Step 401 Train Loss: 0.4766
Epoch 5 Step 451 Train Loss: 0.5625
Epoch 5 Step 501 Train Loss: 0.5884
Epoch 5 Step 551 Train Loss: 0.4999
Epoch 5 Step 601 Train Loss: 0.8086
Epoch 5 Step 651 Train Loss: 0.5569
Epoch 5 Step 701 Train Loss: 0.4341
Epoch 5 Step 751 Train Loss: 0.4043
Epoch 5 Step 801 Train Loss: 0.5757
Epoch 5 Step 851 Train Loss: 0.4789
Epoch 5 Step 901 Train Loss: 0.4663
Epoch 5 Step 951 Train Loss: 0.6166
Epoch 5 Step 1001 Train Loss: 0.4685
Epoch 5 Step 1051 Train Loss: 0.6138
Epoch 5 Step 1101 Train Loss: 0.4768
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0173. 
Epoch 6 Step 1 Train Loss: 0.5096
Epoch 6 Step 51 Train Loss: 0.6076
Epoch 6 Step 101 Train Loss: 0.6228
Epoch 6 Step 151 Train Loss: 0.5567
Epoch 6 Step 201 Train Loss: 0.4320
Epoch 6 Step 251 Train Loss: 0.4638
Epoch 6 Step 301 Train Loss: 0.4337
Epoch 6 Step 351 Train Loss: 0.4322
Epoch 6 Step 401 Train Loss: 0.6695
Epoch 6 Step 451 Train Loss: 0.6818
Epoch 6 Step 501 Train Loss: 0.4983
Epoch 6 Step 551 Train Loss: 0.5689
Epoch 6 Step 601 Train Loss: 0.4457
Epoch 6 Step 651 Train Loss: 0.5235
Epoch 6 Step 701 Train Loss: 0.4957
Epoch 6 Step 751 Train Loss: 0.6652
Epoch 6 Step 801 Train Loss: 0.4611
Epoch 6 Step 851 Train Loss: 0.4341
Epoch 6 Step 901 Train Loss: 0.4983
Epoch 6 Step 951 Train Loss: 0.5000
Epoch 6 Step 1001 Train Loss: 0.4018
Epoch 6 Step 1051 Train Loss: 0.5283
Epoch 6 Step 1101 Train Loss: 0.6250
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0589. 
Epoch 7 Step 1 Train Loss: 0.4849
Epoch 7 Step 51 Train Loss: 0.4743
Epoch 7 Step 101 Train Loss: 0.4849
Epoch 7 Step 151 Train Loss: 0.5659
Epoch 7 Step 201 Train Loss: 0.5253
Epoch 7 Step 251 Train Loss: 0.5149
Epoch 7 Step 301 Train Loss: 0.4972
Epoch 7 Step 351 Train Loss: 0.4508
Epoch 7 Step 401 Train Loss: 0.4626
Epoch 7 Step 451 Train Loss: 0.4444
Epoch 7 Step 501 Train Loss: 0.5388
Epoch 7 Step 551 Train Loss: 0.4588
Epoch 7 Step 601 Train Loss: 0.5518
Epoch 7 Step 651 Train Loss: 0.4332
Epoch 7 Step 701 Train Loss: 0.4263
Epoch 7 Step 751 Train Loss: 0.4341
Epoch 7 Step 801 Train Loss: 0.4732
Epoch 7 Step 851 Train Loss: 0.5402
Epoch 7 Step 901 Train Loss: 0.4779
Epoch 7 Step 951 Train Loss: 0.4978
Epoch 7 Step 1001 Train Loss: 0.5121
Epoch 7 Step 1051 Train Loss: 0.5863
Epoch 7 Step 1101 Train Loss: 0.5089
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.0288. 
Epoch 8 Step 1 Train Loss: 0.3890
Epoch 8 Step 51 Train Loss: 0.5832
Epoch 8 Step 101 Train Loss: 0.5184
Epoch 8 Step 151 Train Loss: 0.5813
Epoch 8 Step 201 Train Loss: 0.5053
Epoch 8 Step 251 Train Loss: 0.5740
Epoch 8 Step 301 Train Loss: 0.6143
Epoch 8 Step 351 Train Loss: 0.5449
Epoch 8 Step 401 Train Loss: 0.5626
Epoch 8 Step 451 Train Loss: 0.6308
Epoch 8 Step 501 Train Loss: 0.5653
Epoch 8 Step 551 Train Loss: 0.4482
Epoch 8 Step 601 Train Loss: 0.5153
Epoch 8 Step 651 Train Loss: 0.5095
Epoch 8 Step 701 Train Loss: 0.4198
Epoch 8 Step 751 Train Loss: 0.4570
Epoch 8 Step 801 Train Loss: 0.4928
Epoch 8 Step 851 Train Loss: 0.6526
Epoch 8 Step 901 Train Loss: 0.4565
Epoch 8 Step 951 Train Loss: 0.4100
Epoch 8 Step 1001 Train Loss: 0.5647
Epoch 8 Step 1051 Train Loss: 0.4861
Epoch 8 Step 1101 Train Loss: 0.4165
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.0543. 
Epoch 9 Step 1 Train Loss: 0.4046
Epoch 9 Step 51 Train Loss: 0.4876
Epoch 9 Step 101 Train Loss: 0.4663
Epoch 9 Step 151 Train Loss: 0.5777
Epoch 9 Step 201 Train Loss: 0.6779
Epoch 9 Step 251 Train Loss: 0.4905
Epoch 9 Step 301 Train Loss: 0.4938
Epoch 9 Step 351 Train Loss: 0.6221
Epoch 9 Step 401 Train Loss: 0.4189
Epoch 9 Step 451 Train Loss: 0.4890
Epoch 9 Step 501 Train Loss: 0.4939
Epoch 9 Step 551 Train Loss: 0.5359
Epoch 9 Step 601 Train Loss: 0.6146
Epoch 9 Step 651 Train Loss: 0.6117
Epoch 9 Step 701 Train Loss: 0.7632
Epoch 9 Step 751 Train Loss: 0.4045
Epoch 9 Step 801 Train Loss: 0.5267
Epoch 9 Step 851 Train Loss: 0.4603
Epoch 9 Step 901 Train Loss: 0.4347
Epoch 9 Step 951 Train Loss: 0.7160
Epoch 9 Step 1001 Train Loss: 0.6104
Epoch 9 Step 1051 Train Loss: 0.4880
Epoch 9 Step 1101 Train Loss: 0.5065
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0712. 
Epoch 10 Step 1 Train Loss: 0.5208
Epoch 10 Step 51 Train Loss: 0.5407
Epoch 10 Step 101 Train Loss: 0.5018
Epoch 10 Step 151 Train Loss: 0.4403
Epoch 10 Step 201 Train Loss: 0.5123
Epoch 10 Step 251 Train Loss: 0.5538
Epoch 10 Step 301 Train Loss: 0.5809
Epoch 10 Step 351 Train Loss: 0.5530
Epoch 10 Step 401 Train Loss: 0.4977
Epoch 10 Step 451 Train Loss: 0.5657
Epoch 10 Step 501 Train Loss: 0.4196
Epoch 10 Step 551 Train Loss: 0.4045
Epoch 10 Step 601 Train Loss: 0.6879
Epoch 10 Step 651 Train Loss: 0.4581
Epoch 10 Step 701 Train Loss: 0.4262
Epoch 10 Step 751 Train Loss: 0.5160
Epoch 10 Step 801 Train Loss: 0.6893
Epoch 10 Step 851 Train Loss: 0.6696
Epoch 10 Step 901 Train Loss: 0.5544
Epoch 10 Step 951 Train Loss: 0.5403
Epoch 10 Step 1001 Train Loss: 0.5838
Epoch 10 Step 1051 Train Loss: 0.5928
Epoch 10 Step 1101 Train Loss: 0.4813
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0334. 
Epoch 11 Step 1 Train Loss: 0.5399
Epoch 11 Step 51 Train Loss: 0.4173
Epoch 11 Step 101 Train Loss: 0.5480
Epoch 11 Step 151 Train Loss: 0.5102
Epoch 11 Step 201 Train Loss: 0.5997
Epoch 11 Step 251 Train Loss: 0.3922
Epoch 11 Step 301 Train Loss: 0.4250
Epoch 11 Step 351 Train Loss: 0.4406
Epoch 11 Step 401 Train Loss: 0.5632
Epoch 11 Step 451 Train Loss: 0.4752
Epoch 11 Step 501 Train Loss: 0.4876
Epoch 11 Step 551 Train Loss: 0.5208
Epoch 11 Step 601 Train Loss: 0.5486
Epoch 11 Step 651 Train Loss: 0.5817
Epoch 11 Step 701 Train Loss: 0.5508
Epoch 11 Step 751 Train Loss: 0.4764
Epoch 11 Step 801 Train Loss: 0.5216
Epoch 11 Step 851 Train Loss: 0.4038
Epoch 11 Step 901 Train Loss: 0.5631
Epoch 11 Step 951 Train Loss: 0.4572
Epoch 11 Step 1001 Train Loss: 0.5672
Epoch 11 Step 1051 Train Loss: 0.4604
Epoch 11 Step 1101 Train Loss: 0.5533
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0343. 
Epoch 12 Step 1 Train Loss: 0.6992
Epoch 12 Step 51 Train Loss: 0.4892
Epoch 12 Step 101 Train Loss: 0.5608
Epoch 12 Step 151 Train Loss: 0.4836
Epoch 12 Step 201 Train Loss: 0.4487
Epoch 12 Step 251 Train Loss: 0.6108
Epoch 12 Step 301 Train Loss: 0.4811
Epoch 12 Step 351 Train Loss: 0.4276
Epoch 12 Step 401 Train Loss: 0.4794
Epoch 12 Step 451 Train Loss: 0.4859
Epoch 12 Step 501 Train Loss: 0.5867
Epoch 12 Step 551 Train Loss: 0.7106
Epoch 12 Step 601 Train Loss: 0.4993
Epoch 12 Step 651 Train Loss: 0.5584
Epoch 12 Step 701 Train Loss: 0.4967
Epoch 12 Step 751 Train Loss: 0.4126
Epoch 12 Step 801 Train Loss: 0.5769
Epoch 12 Step 851 Train Loss: 0.7529
Epoch 12 Step 901 Train Loss: 0.4520
Epoch 12 Step 951 Train Loss: 0.4900
Epoch 12 Step 1001 Train Loss: 0.4843
Epoch 12 Step 1051 Train Loss: 0.5377
Epoch 12 Step 1101 Train Loss: 0.5310
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0685. 
Epoch 13 Step 1 Train Loss: 0.6735
Epoch 13 Step 51 Train Loss: 0.5169
Epoch 13 Step 101 Train Loss: 0.5125
Epoch 13 Step 151 Train Loss: 0.4281
Epoch 13 Step 201 Train Loss: 0.5010
Epoch 13 Step 251 Train Loss: 0.5174
Epoch 13 Step 301 Train Loss: 0.5400
Epoch 13 Step 351 Train Loss: 0.5709
Epoch 13 Step 401 Train Loss: 0.4751
Epoch 13 Step 451 Train Loss: 0.4425
Epoch 13 Step 501 Train Loss: 0.4177
Epoch 13 Step 551 Train Loss: 0.5568
Epoch 13 Step 601 Train Loss: 0.5103
Epoch 13 Step 651 Train Loss: 0.5208
Epoch 13 Step 701 Train Loss: 0.4746
Epoch 13 Step 751 Train Loss: 0.5621
Epoch 13 Step 801 Train Loss: 0.4674
Epoch 13 Step 851 Train Loss: 0.5430
Epoch 13 Step 901 Train Loss: 0.5247
Epoch 13 Step 951 Train Loss: 0.5183
Epoch 13 Step 1001 Train Loss: 0.4874
Epoch 13 Step 1051 Train Loss: 0.7022
Epoch 13 Step 1101 Train Loss: 0.5287
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0069 Validation Top 20 DE MSE: 0.0232. 
Epoch 14 Step 1 Train Loss: 0.4810
Epoch 14 Step 51 Train Loss: 0.5178
Epoch 14 Step 101 Train Loss: 0.5965
Epoch 14 Step 151 Train Loss: 0.4955
Epoch 14 Step 201 Train Loss: 0.5586
Epoch 14 Step 251 Train Loss: 0.5187
Epoch 14 Step 301 Train Loss: 0.4462
Epoch 14 Step 351 Train Loss: 0.5195
Epoch 14 Step 401 Train Loss: 0.5548
Epoch 14 Step 451 Train Loss: 0.4639
Epoch 14 Step 501 Train Loss: 0.5732
Epoch 14 Step 551 Train Loss: 0.4560
Epoch 14 Step 601 Train Loss: 0.4709
Epoch 14 Step 651 Train Loss: 0.4699
Epoch 14 Step 701 Train Loss: 0.4006
Epoch 14 Step 751 Train Loss: 0.5000
Epoch 14 Step 801 Train Loss: 0.5237
Epoch 14 Step 851 Train Loss: 0.4829
Epoch 14 Step 901 Train Loss: 0.5262
Epoch 14 Step 951 Train Loss: 0.5377
Epoch 14 Step 1001 Train Loss: 0.4540
Epoch 14 Step 1051 Train Loss: 0.5969
Epoch 14 Step 1101 Train Loss: 0.4234
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0525. 
Epoch 15 Step 1 Train Loss: 0.5014
Epoch 15 Step 51 Train Loss: 0.5796
Epoch 15 Step 101 Train Loss: 0.4709
Epoch 15 Step 151 Train Loss: 0.4824
Epoch 15 Step 201 Train Loss: 0.4367
Epoch 15 Step 251 Train Loss: 0.4844
Epoch 15 Step 301 Train Loss: 0.4915
Epoch 15 Step 351 Train Loss: 0.4944
Epoch 15 Step 401 Train Loss: 0.4599
Epoch 15 Step 451 Train Loss: 0.5485
Epoch 15 Step 501 Train Loss: 0.6868
Epoch 15 Step 551 Train Loss: 0.5622
Epoch 15 Step 601 Train Loss: 0.4441
Epoch 15 Step 651 Train Loss: 0.5977
Epoch 15 Step 701 Train Loss: 0.5738
Epoch 15 Step 751 Train Loss: 0.6230
Epoch 15 Step 801 Train Loss: 0.5225
Epoch 15 Step 851 Train Loss: 0.6009
Epoch 15 Step 901 Train Loss: 0.4772
Epoch 15 Step 951 Train Loss: 0.4226
Epoch 15 Step 1001 Train Loss: 0.5417
Epoch 15 Step 1051 Train Loss: 0.4438
Epoch 15 Step 1101 Train Loss: 0.5077
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0751. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0441
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0013034394
test_unseen_single_pearson: 0.9977285465569993
test_unseen_single_mse_de: 0.044118047
test_unseen_single_pearson_de: 0.9891454312009347
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.6115734115118814
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.11666666666666665
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9750000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.044118045261582285
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.030 MB uploadedwandb: | 0.025 MB of 0.030 MB uploadedwandb: / 0.030 MB of 0.030 MB uploadedwandb: - 0.030 MB of 0.030 MB uploadedwandb: \ 0.030 MB of 0.030 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÅ
wandb:                                                   val_de_mse ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÖ‚ñá
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÑ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñà‚ñÜ‚ñÖ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.04412
wandb:                                              test_de_pearson 0.98915
wandb:               test_frac_opposite_direction_top20_non_dropout 0.11667
wandb:                          test_frac_sigma_below_1_non_dropout 0.975
wandb:                                                     test_mse 0.0013
wandb:                                test_mse_top20_de_non_dropout 0.04412
wandb:                                                 test_pearson 0.99773
wandb:                                           test_pearson_delta 0.61157
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.11667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.975
wandb:                                       test_unseen_single_mse 0.0013
wandb:                                    test_unseen_single_mse_de 0.04412
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.04412
wandb:                                   test_unseen_single_pearson 0.99773
wandb:                                test_unseen_single_pearson_de 0.98915
wandb:                             test_unseen_single_pearson_delta 0.61157
wandb:                                                 train_de_mse 0.00898
wandb:                                             train_de_pearson 0.99728
wandb:                                                    train_mse 0.00053
wandb:                                                train_pearson 0.99912
wandb:                                                training_loss 0.59581
wandb:                                                   val_de_mse 0.07506
wandb:                                               val_de_pearson 0.98263
wandb:                                                      val_mse 0.00162
wandb:                                                  val_pearson 0.9972
wandb: 
wandb: üöÄ View run scbert_Dixit_combined_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/39aw2kjp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_193231-39aw2kjp/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_195329-l9blqu1a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396858_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/l9blqu1a
wandb: WARNING Serializing object of type ndarray that is 8012928 bytes
  0%|                                                                                       | 0/3576 [00:00<?, ?it/s]  0%|‚ñè                                                                              | 7/3576 [00:00<00:52, 68.27it/s]  0%|‚ñé                                                                             | 16/3576 [00:00<00:47, 75.72it/s]  1%|‚ñå                                                                             | 24/3576 [00:00<00:46, 76.71it/s]  1%|‚ñã                                                                             | 34/3576 [00:00<00:41, 85.48it/s]  1%|‚ñâ                                                                             | 43/3576 [00:00<00:41, 84.19it/s]  2%|‚ñà‚ñè                                                                            | 54/3576 [00:00<00:39, 89.65it/s]  2%|‚ñà‚ñç                                                                            | 64/3576 [00:00<00:38, 90.76it/s]  2%|‚ñà‚ñå                                                                            | 74/3576 [00:00<00:38, 91.26it/s]  2%|‚ñà‚ñä                                                                            | 84/3576 [00:00<00:38, 90.95it/s]  3%|‚ñà‚ñà                                                                            | 94/3576 [00:01<00:38, 90.40it/s]  3%|‚ñà‚ñà‚ñè                                                                          | 104/3576 [00:01<00:38, 90.52it/s]  3%|‚ñà‚ñà‚ñç                                                                          | 114/3576 [00:01<00:37, 91.20it/s]  3%|‚ñà‚ñà‚ñã                                                                          | 124/3576 [00:01<00:37, 91.28it/s]  4%|‚ñà‚ñà‚ñâ                                                                          | 134/3576 [00:01<00:37, 91.31it/s]  4%|‚ñà‚ñà‚ñà                                                                          | 144/3576 [00:01<00:37, 90.88it/s]  4%|‚ñà‚ñà‚ñà‚ñé                                                                         | 154/3576 [00:01<00:37, 90.56it/s]  5%|‚ñà‚ñà‚ñà‚ñå                                                                         | 164/3576 [00:01<00:37, 90.71it/s]  5%|‚ñà‚ñà‚ñà‚ñã                                                                         | 174/3576 [00:01<00:37, 91.33it/s]  5%|‚ñà‚ñà‚ñà‚ñâ                                                                         | 184/3576 [00:02<00:37, 91.55it/s]  5%|‚ñà‚ñà‚ñà‚ñà‚ñè                                                                        | 194/3576 [00:02<00:36, 91.42it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 204/3576 [00:02<00:37, 90.98it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñå                                                                        | 214/3576 [00:02<00:37, 90.75it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                        | 224/3576 [00:02<00:38, 88.10it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà                                                                        | 235/3576 [00:02<00:36, 91.62it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                       | 245/3576 [00:02<00:36, 91.34it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                       | 255/3576 [00:02<00:37, 88.17it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                       | 265/3576 [00:02<00:36, 91.24it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                       | 275/3576 [00:03<00:37, 87.16it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                      | 286/3576 [00:03<00:36, 90.90it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                      | 296/3576 [00:03<00:36, 91.05it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 306/3576 [00:03<00:35, 91.06it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                      | 316/3576 [00:03<00:35, 91.21it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                      | 326/3576 [00:03<00:35, 91.32it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                     | 336/3576 [00:03<00:35, 91.41it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                     | 346/3576 [00:03<00:35, 91.13it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                     | 356/3576 [00:03<00:35, 91.19it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                     | 366/3576 [00:04<00:35, 90.89it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                     | 376/3576 [00:04<00:35, 91.01it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                    | 386/3576 [00:04<00:36, 88.06it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                    | 396/3576 [00:04<00:34, 91.02it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                    | 406/3576 [00:04<00:34, 91.02it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                    | 416/3576 [00:04<00:34, 90.79it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                   | 426/3576 [00:04<00:34, 90.90it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                   | 436/3576 [00:04<00:34, 90.58it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                   | 446/3576 [00:04<00:34, 90.32it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                   | 456/3576 [00:05<00:34, 90.43it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                   | 466/3576 [00:05<00:34, 91.00it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                  | 476/3576 [00:05<00:34, 91.14it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                  | 486/3576 [00:05<00:33, 91.11it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                  | 496/3576 [00:05<00:34, 88.75it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                  | 505/3576 [00:05<00:38, 79.04it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 514/3576 [00:05<00:41, 73.18it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                 | 522/3576 [00:05<00:43, 69.58it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                 | 530/3576 [00:06<00:45, 67.65it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                 | 538/3576 [00:06<00:43, 69.35it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                 | 547/3576 [00:06<00:41, 73.85it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                 | 557/3576 [00:06<00:37, 80.25it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                | 566/3576 [00:06<00:37, 79.41it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                | 575/3576 [00:06<00:37, 81.11it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                | 585/3576 [00:06<00:34, 86.33it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                | 594/3576 [00:06<00:34, 86.95it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                | 604/3576 [00:06<00:34, 85.75it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                               | 614/3576 [00:07<00:33, 89.71it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                               | 624/3576 [00:07<00:32, 89.78it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                               | 634/3576 [00:07<00:32, 90.12it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                               | 644/3576 [00:07<00:32, 89.95it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                               | 654/3576 [00:07<00:32, 90.18it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                              | 664/3576 [00:07<00:32, 89.61it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                              | 673/3576 [00:07<00:32, 89.51it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                              | 682/3576 [00:07<00:32, 89.38it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                              | 691/3576 [00:07<00:32, 89.47it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                              | 700/3576 [00:07<00:33, 85.86it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                             | 710/3576 [00:08<00:32, 89.53it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                             | 719/3576 [00:08<00:32, 89.10it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             | 729/3576 [00:08<00:31, 89.38it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                             | 738/3576 [00:08<00:32, 88.58it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                             | 748/3576 [00:08<00:31, 88.79it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                            | 758/3576 [00:08<00:31, 89.29it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                            | 768/3576 [00:08<00:32, 86.83it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                            | 779/3576 [00:08<00:30, 90.93it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                            | 789/3576 [00:08<00:30, 90.70it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                           | 799/3576 [00:09<00:30, 90.18it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                           | 809/3576 [00:09<00:31, 87.99it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                           | 819/3576 [00:09<00:30, 90.66it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                           | 829/3576 [00:09<00:30, 90.55it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 839/3576 [00:09<00:31, 87.85it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                          | 849/3576 [00:09<00:30, 90.45it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                          | 859/3576 [00:09<00:30, 89.57it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 868/3576 [00:09<00:31, 86.19it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                          | 878/3576 [00:09<00:31, 86.80it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                          | 887/3576 [00:10<00:30, 87.06it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                         | 897/3576 [00:10<00:29, 90.62it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                         | 907/3576 [00:10<00:29, 90.60it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                         | 917/3576 [00:10<00:29, 90.77it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                         | 927/3576 [00:10<00:29, 90.60it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                        | 937/3576 [00:10<00:30, 87.92it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                        | 947/3576 [00:10<00:28, 90.81it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                        | 957/3576 [00:10<00:28, 90.69it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                        | 967/3576 [00:10<00:28, 90.45it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                        | 977/3576 [00:11<00:29, 87.57it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                       | 987/3576 [00:11<00:28, 89.76it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                       | 997/3576 [00:11<00:28, 89.19it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                      | 1006/3576 [00:11<00:28, 88.91it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 1015/3576 [00:11<00:28, 88.73it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                      | 1024/3576 [00:11<00:28, 88.94it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                      | 1033/3576 [00:11<00:29, 86.02it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                     | 1043/3576 [00:11<00:28, 89.54it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                     | 1052/3576 [00:11<00:28, 89.34it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                     | 1062/3576 [00:12<00:27, 90.00it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                     | 1072/3576 [00:12<00:28, 89.35it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                     | 1081/3576 [00:12<00:27, 89.17it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 1090/3576 [00:12<00:27, 89.31it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 1099/3576 [00:12<00:27, 89.39it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                    | 1108/3576 [00:12<00:29, 82.32it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                    | 1117/3576 [00:12<00:29, 82.37it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                    | 1126/3576 [00:12<00:31, 76.67it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                    | 1134/3576 [00:12<00:31, 76.74it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                   | 1143/3576 [00:13<00:30, 79.21it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                   | 1151/3576 [00:13<00:30, 78.95it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                   | 1160/3576 [00:13<00:30, 79.38it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                   | 1169/3576 [00:13<00:30, 80.00it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                   | 1178/3576 [00:13<00:30, 77.62it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                  | 1188/3576 [00:13<00:29, 81.60it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                  | 1197/3576 [00:13<00:29, 81.62it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                  | 1206/3576 [00:13<00:29, 81.48it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                  | 1215/3576 [00:13<00:29, 81.40it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                  | 1224/3576 [00:14<00:28, 81.26it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                 | 1233/3576 [00:14<00:28, 81.03it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                 | 1242/3576 [00:14<00:28, 81.19it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                 | 1251/3576 [00:14<00:28, 80.95it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                 | 1260/3576 [00:14<00:31, 74.14it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                 | 1268/3576 [00:14<00:32, 71.19it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                | 1278/3576 [00:14<00:29, 77.93it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                | 1288/3576 [00:14<00:27, 83.17it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                | 1299/3576 [00:14<00:25, 88.80it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                | 1309/3576 [00:15<00:25, 89.20it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                | 1319/3576 [00:15<00:24, 91.76it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                               | 1329/3576 [00:15<00:24, 92.10it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                               | 1339/3576 [00:15<00:23, 93.90it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                               | 1349/3576 [00:15<00:23, 94.22it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                               | 1359/3576 [00:15<00:23, 95.02it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 1370/3576 [00:15<00:23, 95.43it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                              | 1380/3576 [00:15<00:22, 95.79it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 1390/3576 [00:15<00:23, 92.30it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 1401/3576 [00:16<00:23, 91.25it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                              | 1411/3576 [00:16<00:23, 93.26it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                             | 1422/3576 [00:16<00:22, 95.71it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                             | 1432/3576 [00:16<00:22, 96.01it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                             | 1443/3576 [00:16<00:21, 97.38it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                             | 1453/3576 [00:16<00:22, 93.55it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                             | 1463/3576 [00:16<00:23, 89.15it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                            | 1473/3576 [00:16<00:22, 91.80it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                            | 1483/3576 [00:16<00:22, 93.52it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                            | 1494/3576 [00:17<00:21, 94.84it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                            | 1505/3576 [00:17<00:21, 98.12it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                           | 1515/3576 [00:17<00:21, 96.03it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                           | 1526/3576 [00:17<00:21, 96.43it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                           | 1536/3576 [00:17<00:21, 97.04it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                           | 1547/3576 [00:17<00:21, 96.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                           | 1557/3576 [00:17<00:20, 96.44it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                          | 1568/3576 [00:17<00:20, 97.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                          | 1578/3576 [00:17<00:20, 96.98it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                          | 1588/3576 [00:17<00:20, 97.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                          | 1598/3576 [00:18<00:20, 96.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                         | 1608/3576 [00:18<00:20, 96.53it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                         | 1620/3576 [00:18<00:19, 100.45it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 1631/3576 [00:18<00:20, 96.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                         | 1642/3576 [00:18<00:19, 97.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 1653/3576 [00:18<00:19, 97.73it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                        | 1663/3576 [00:18<00:19, 96.29it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                        | 1674/3576 [00:18<00:19, 99.49it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 1685/3576 [00:18<00:19, 99.18it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1695/3576 [00:19<00:19, 94.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 1705/3576 [00:19<00:20, 92.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1715/3576 [00:19<00:21, 84.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                       | 1724/3576 [00:19<00:21, 85.08it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                       | 1734/3576 [00:19<00:21, 86.91it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                       | 1744/3576 [00:19<00:20, 88.36it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 1755/3576 [00:19<00:20, 90.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                      | 1765/3576 [00:19<00:19, 90.88it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                      | 1775/3576 [00:20<00:19, 92.33it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                      | 1785/3576 [00:20<00:19, 89.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 1795/3576 [00:20<00:19, 89.11it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 1804/3576 [00:20<00:20, 86.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 1813/3576 [00:20<00:21, 83.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 1822/3576 [00:20<00:21, 82.10it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 1831/3576 [00:20<00:21, 80.78it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 1840/3576 [00:20<00:21, 79.91it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                    | 1848/3576 [00:20<00:21, 79.44it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 1856/3576 [00:21<00:22, 76.84it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 1864/3576 [00:21<00:22, 77.19it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                    | 1873/3576 [00:21<00:21, 80.26it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                    | 1882/3576 [00:21<00:22, 76.79it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 1891/3576 [00:21<00:21, 77.37it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 1900/3576 [00:21<00:21, 77.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 1909/3576 [00:21<00:20, 81.00it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 1918/3576 [00:21<00:20, 80.19it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 1927/3576 [00:21<00:21, 77.49it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 1935/3576 [00:22<00:21, 77.93it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 1944/3576 [00:22<00:20, 80.86it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 1953/3576 [00:22<00:20, 80.98it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 1962/3576 [00:22<00:19, 80.90it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 1971/3576 [00:22<00:20, 80.07it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 1980/3576 [00:22<00:19, 79.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 1988/3576 [00:22<00:19, 79.82it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 1996/3576 [00:22<00:20, 78.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 2004/3576 [00:22<00:19, 79.20it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 2012/3576 [00:22<00:19, 79.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 2020/3576 [00:23<00:20, 76.82it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 2029/3576 [00:23<00:19, 80.58it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 2038/3576 [00:23<00:18, 83.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 2048/3576 [00:23<00:17, 87.16it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 2058/3576 [00:23<00:16, 90.16it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 2068/3576 [00:23<00:16, 92.20it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 2078/3576 [00:23<00:16, 92.08it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 2088/3576 [00:23<00:15, 93.57it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 2099/3576 [00:23<00:15, 96.70it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 2109/3576 [00:24<00:15, 94.49it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 2120/3576 [00:24<00:15, 97.05it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 2131/3576 [00:24<00:14, 99.25it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 2141/3576 [00:24<00:14, 96.91it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 2152/3576 [00:24<00:14, 97.47it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 2163/3576 [00:24<00:14, 97.45it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 2174/3576 [00:24<00:14, 98.72it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 2185/3576 [00:24<00:14, 95.69it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 2195/3576 [00:24<00:14, 94.02it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 2205/3576 [00:25<00:15, 90.11it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 2215/3576 [00:25<00:15, 87.30it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 2224/3576 [00:25<00:15, 85.48it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 2233/3576 [00:25<00:16, 81.32it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 2243/3576 [00:25<00:15, 83.80it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 2252/3576 [00:25<00:15, 83.12it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 2261/3576 [00:25<00:15, 82.32it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 2270/3576 [00:25<00:15, 82.36it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 2279/3576 [00:25<00:16, 79.06it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 2289/3576 [00:26<00:15, 82.34it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 2298/3576 [00:26<00:15, 82.43it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 2307/3576 [00:26<00:15, 79.95it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 2317/3576 [00:26<00:15, 83.05it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 2326/3576 [00:26<00:15, 82.12it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 2335/3576 [00:26<00:15, 81.65it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 2344/3576 [00:26<00:15, 78.23it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 2353/3576 [00:26<00:15, 80.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 2362/3576 [00:26<00:14, 81.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 2371/3576 [00:27<00:14, 81.70it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 2380/3576 [00:27<00:14, 82.13it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 2389/3576 [00:27<00:14, 79.28it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 2399/3576 [00:27<00:14, 80.17it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 2408/3576 [00:27<00:14, 79.56it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 2420/3576 [00:27<00:13, 87.02it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 2431/3576 [00:27<00:12, 92.27it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 2441/3576 [00:27<00:12, 93.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 2451/3576 [00:28<00:12, 92.08it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 2461/3576 [00:28<00:11, 93.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 2471/3576 [00:28<00:11, 95.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 2482/3576 [00:28<00:11, 97.15it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 2493/3576 [00:28<00:10, 98.64it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 2503/3576 [00:28<00:10, 98.93it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2514/3576 [00:28<00:10, 100.30it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2525/3576 [00:28<00:10, 99.84it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2536/3576 [00:28<00:10, 97.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2547/3576 [00:28<00:10, 100.70it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2558/3576 [00:29<00:09, 102.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 2569/3576 [00:29<00:10, 99.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2580/3576 [00:29<00:09, 99.64it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2591/3576 [00:29<00:09, 99.64it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2602/3576 [00:29<00:09, 98.84it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2612/3576 [00:29<00:10, 96.33it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2624/3576 [00:29<00:09, 102.90it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2635/3576 [00:29<00:09, 98.63it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2645/3576 [00:29<00:09, 98.25it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2655/3576 [00:30<00:09, 97.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2665/3576 [00:30<00:09, 94.27it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2676/3576 [00:30<00:09, 97.44it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2687/3576 [00:30<00:09, 97.32it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2697/3576 [00:30<00:09, 97.65it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2707/3576 [00:30<00:08, 97.26it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2718/3576 [00:30<00:08, 98.00it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2728/3576 [00:30<00:08, 97.80it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2739/3576 [00:30<00:08, 99.64it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2749/3576 [00:31<00:08, 99.35it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2759/3576 [00:31<00:08, 97.30it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2770/3576 [00:31<00:07, 100.81it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2781/3576 [00:31<00:07, 100.02it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2792/3576 [00:31<00:07, 100.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2803/3576 [00:31<00:07, 101.98it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2814/3576 [00:31<00:07, 99.31it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2825/3576 [00:31<00:07, 98.81it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2836/3576 [00:31<00:07, 99.49it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2846/3576 [00:31<00:07, 99.04it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2856/3576 [00:32<00:07, 98.55it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2866/3576 [00:32<00:07, 98.84it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2876/3576 [00:32<00:07, 98.32it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2886/3576 [00:32<00:07, 95.98it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2897/3576 [00:32<00:06, 97.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2907/3576 [00:32<00:06, 97.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2917/3576 [00:32<00:06, 98.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2928/3576 [00:32<00:06, 97.22it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2939/3576 [00:32<00:06, 98.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2951/3576 [00:33<00:06, 101.96it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2962/3576 [00:33<00:06, 101.83it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2973/3576 [00:33<00:05, 101.04it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2984/3576 [00:33<00:06, 97.58it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2995/3576 [00:33<00:05, 99.82it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3006/3576 [00:33<00:05, 98.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 3017/3576 [00:33<00:05, 101.09it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3028/3576 [00:33<00:05, 102.91it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3039/3576 [00:33<00:05, 99.99it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 3050/3576 [00:34<00:05, 102.74it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 3061/3576 [00:34<00:05, 101.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3072/3576 [00:34<00:04, 101.09it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 3083/3576 [00:34<00:05, 97.73it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3094/3576 [00:34<00:04, 99.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 3105/3576 [00:34<00:04, 100.75it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3116/3576 [00:34<00:04, 99.29it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3127/3576 [00:34<00:04, 101.85it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3138/3576 [00:34<00:04, 99.29it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3148/3576 [00:35<00:04, 99.34it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3158/3576 [00:35<00:04, 96.66it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3171/3576 [00:35<00:03, 103.27it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3182/3576 [00:35<00:03, 101.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3193/3576 [00:35<00:03, 98.93it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3204/3576 [00:35<00:03, 100.96it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3215/3576 [00:35<00:03, 98.58it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3226/3576 [00:35<00:03, 95.71it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3239/3576 [00:35<00:03, 99.99it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3250/3576 [00:36<00:03, 98.53it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3261/3576 [00:36<00:03, 99.54it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3271/3576 [00:36<00:03, 99.26it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3282/3576 [00:36<00:02, 99.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3293/3576 [00:36<00:02, 97.88it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3304/3576 [00:36<00:02, 99.45it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3314/3576 [00:36<00:02, 97.20it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3324/3576 [00:36<00:02, 96.62it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3334/3576 [00:36<00:02, 97.24it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3344/3576 [00:37<00:02, 97.08it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3354/3576 [00:37<00:02, 97.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3365/3576 [00:37<00:02, 98.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3375/3576 [00:37<00:02, 97.92it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3386/3576 [00:37<00:01, 99.41it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3396/3576 [00:37<00:01, 98.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3406/3576 [00:37<00:01, 98.63it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3418/3576 [00:37<00:01, 99.60it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3429/3576 [00:37<00:01, 98.57it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3439/3576 [00:37<00:01, 98.51it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3450/3576 [00:38<00:01, 97.21it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3461/3576 [00:38<00:01, 97.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3472/3576 [00:38<00:01, 97.36it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3482/3576 [00:38<00:00, 96.91it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3493/3576 [00:38<00:00, 97.07it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3504/3576 [00:38<00:00, 99.34it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3514/3576 [00:38<00:00, 95.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3525/3576 [00:38<00:00, 95.20it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3536/3576 [00:38<00:00, 96.15it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3547/3576 [00:39<00:00, 98.24it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3557/3576 [00:39<00:00, 97.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3567/3576 [00:39<00:00, 95.56it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3576/3576 [00:39<00:00, 90.79it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.3749
Epoch 1 Step 51 Train Loss: 0.4254
Epoch 1 Step 101 Train Loss: 0.4213
Epoch 1 Step 151 Train Loss: 0.3802
Epoch 1 Step 201 Train Loss: 0.4737
Epoch 1 Step 251 Train Loss: 0.4208
Epoch 1 Step 301 Train Loss: 0.4373
Epoch 1 Step 351 Train Loss: 0.4009
Epoch 1 Step 401 Train Loss: 0.4193
Epoch 1 Step 451 Train Loss: 0.4253
Epoch 1 Step 501 Train Loss: 0.4726
Epoch 1 Step 551 Train Loss: 0.4709
Epoch 1 Step 601 Train Loss: 0.4520
Epoch 1 Step 651 Train Loss: 0.5514
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0053 Validation Top 20 DE MSE: 0.0025. 
Epoch 2 Step 1 Train Loss: 0.5444
Epoch 2 Step 51 Train Loss: 0.4452
Epoch 2 Step 101 Train Loss: 0.4661
Epoch 2 Step 151 Train Loss: 0.3887
Epoch 2 Step 201 Train Loss: 0.4535
Epoch 2 Step 251 Train Loss: 0.3958
Epoch 2 Step 301 Train Loss: 0.3932
Epoch 2 Step 351 Train Loss: 0.5112
Epoch 2 Step 401 Train Loss: 0.3860
Epoch 2 Step 451 Train Loss: 0.3598
Epoch 2 Step 501 Train Loss: 0.3758
Epoch 2 Step 551 Train Loss: 0.4505
Epoch 2 Step 601 Train Loss: 0.4304
Epoch 2 Step 651 Train Loss: 0.4397
Epoch 2: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0026. 
Epoch 3 Step 1 Train Loss: 0.4045
Epoch 3 Step 51 Train Loss: 0.3911
Epoch 3 Step 101 Train Loss: 0.3442
Epoch 3 Step 151 Train Loss: 0.4135
Epoch 3 Step 201 Train Loss: 0.3902
Epoch 3 Step 251 Train Loss: 0.3899
Epoch 3 Step 301 Train Loss: 0.3878
Epoch 3 Step 351 Train Loss: 0.5757
Epoch 3 Step 401 Train Loss: 0.4100
Epoch 3 Step 451 Train Loss: 0.3141
Epoch 3 Step 501 Train Loss: 0.3517
Epoch 3 Step 551 Train Loss: 0.3455
Epoch 3 Step 601 Train Loss: 0.4451
Epoch 3 Step 651 Train Loss: 0.4069
Epoch 3: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0020. 
Epoch 4 Step 1 Train Loss: 0.3707
Epoch 4 Step 51 Train Loss: 0.4205
Epoch 4 Step 101 Train Loss: 0.4281
Epoch 4 Step 151 Train Loss: 0.4530
Epoch 4 Step 201 Train Loss: 0.4787
Epoch 4 Step 251 Train Loss: 0.3791
Epoch 4 Step 301 Train Loss: 0.3633
Epoch 4 Step 351 Train Loss: 0.3750
Epoch 4 Step 401 Train Loss: 0.3832
Epoch 4 Step 451 Train Loss: 0.3955
Epoch 4 Step 501 Train Loss: 0.3140
Epoch 4 Step 551 Train Loss: 0.3602
Epoch 4 Step 601 Train Loss: 0.5473
Epoch 4 Step 651 Train Loss: 0.3304
Epoch 4: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0028. 
Epoch 5 Step 1 Train Loss: 0.3770
Epoch 5 Step 51 Train Loss: 0.4019
Epoch 5 Step 101 Train Loss: 0.4585
Epoch 5 Step 151 Train Loss: 0.3416
Epoch 5 Step 201 Train Loss: 0.3983
Epoch 5 Step 251 Train Loss: 0.3506
Epoch 5 Step 301 Train Loss: 0.3571
Epoch 5 Step 351 Train Loss: 0.3570
Epoch 5 Step 401 Train Loss: 0.4007
Epoch 5 Step 451 Train Loss: 0.3763
Epoch 5 Step 501 Train Loss: 0.3491
Epoch 5 Step 551 Train Loss: 0.4135
Epoch 5 Step 601 Train Loss: 0.4302
Epoch 5 Step 651 Train Loss: 0.3846
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0019. 
Epoch 6 Step 1 Train Loss: 0.3607
Epoch 6 Step 51 Train Loss: 0.3739
Epoch 6 Step 101 Train Loss: 0.3578
Epoch 6 Step 151 Train Loss: 0.3045
Epoch 6 Step 201 Train Loss: 0.3874
Epoch 6 Step 251 Train Loss: 0.4147
Epoch 6 Step 301 Train Loss: 0.3899
Epoch 6 Step 351 Train Loss: 0.3879
Epoch 6 Step 401 Train Loss: 0.3420
Epoch 6 Step 451 Train Loss: 0.3664
Epoch 6 Step 501 Train Loss: 0.3666
Epoch 6 Step 551 Train Loss: 0.4074
Epoch 6 Step 601 Train Loss: 0.4808
Epoch 6 Step 651 Train Loss: 0.3562
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0024. 
Epoch 7 Step 1 Train Loss: 0.4231
Epoch 7 Step 51 Train Loss: 0.3475
Epoch 7 Step 101 Train Loss: 0.3065
Epoch 7 Step 151 Train Loss: 0.3956
Epoch 7 Step 201 Train Loss: 0.4679
Epoch 7 Step 251 Train Loss: 0.4284
Epoch 7 Step 301 Train Loss: 0.3760
Epoch 7 Step 351 Train Loss: 0.4308
Epoch 7 Step 401 Train Loss: 0.3026
Epoch 7 Step 451 Train Loss: 0.4082
Epoch 7 Step 501 Train Loss: 0.3150
Epoch 7 Step 551 Train Loss: 0.4050
Epoch 7 Step 601 Train Loss: 0.3358
Epoch 7 Step 651 Train Loss: 0.3754
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0024. 
Epoch 8 Step 1 Train Loss: 0.4557
Epoch 8 Step 51 Train Loss: 0.3275
Epoch 8 Step 101 Train Loss: 0.4423
Epoch 8 Step 151 Train Loss: 0.3703
Epoch 8 Step 201 Train Loss: 0.3711
Epoch 8 Step 251 Train Loss: 0.4095
Epoch 8 Step 301 Train Loss: 0.3638
Epoch 8 Step 351 Train Loss: 0.4142
Epoch 8 Step 401 Train Loss: 0.4203
Epoch 8 Step 451 Train Loss: 0.3326
Epoch 8 Step 501 Train Loss: 0.4391
Epoch 8 Step 551 Train Loss: 0.4447
Epoch 8 Step 601 Train Loss: 0.3567
Epoch 8 Step 651 Train Loss: 0.3656
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0026. 
Epoch 9 Step 1 Train Loss: 0.4477
Epoch 9 Step 51 Train Loss: 0.3710
Epoch 9 Step 101 Train Loss: 0.4208
Epoch 9 Step 151 Train Loss: 0.4326
Epoch 9 Step 201 Train Loss: 0.3457
Epoch 9 Step 251 Train Loss: 0.3680
Epoch 9 Step 301 Train Loss: 0.3899
Epoch 9 Step 351 Train Loss: 0.4774
Epoch 9 Step 401 Train Loss: 0.4116
Epoch 9 Step 451 Train Loss: 0.3921
Epoch 9 Step 501 Train Loss: 0.3691
Epoch 9 Step 551 Train Loss: 0.3897
Epoch 9 Step 601 Train Loss: 0.3855
Epoch 9 Step 651 Train Loss: 0.3275
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0024. 
Epoch 10 Step 1 Train Loss: 0.4401
Epoch 10 Step 51 Train Loss: 0.4160
Epoch 10 Step 101 Train Loss: 0.3410
Epoch 10 Step 151 Train Loss: 0.4372
Epoch 10 Step 201 Train Loss: 0.4637
Epoch 10 Step 251 Train Loss: 0.4539
Epoch 10 Step 301 Train Loss: 0.3812
Epoch 10 Step 351 Train Loss: 0.3289
Epoch 10 Step 401 Train Loss: 0.3315
Epoch 10 Step 451 Train Loss: 0.3312
Epoch 10 Step 501 Train Loss: 0.3809
Epoch 10 Step 551 Train Loss: 0.4402
Epoch 10 Step 601 Train Loss: 0.4856
Epoch 10 Step 651 Train Loss: 0.3341
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0023. 
Epoch 11 Step 1 Train Loss: 0.4026
Epoch 11 Step 51 Train Loss: 0.4070
Epoch 11 Step 101 Train Loss: 0.3617
Epoch 11 Step 151 Train Loss: 0.4606
Epoch 11 Step 201 Train Loss: 0.4695
Epoch 11 Step 251 Train Loss: 0.4360
Epoch 11 Step 301 Train Loss: 0.3377
Epoch 11 Step 351 Train Loss: 0.3671
Epoch 11 Step 401 Train Loss: 0.3653
Epoch 11 Step 451 Train Loss: 0.3688
Epoch 11 Step 501 Train Loss: 0.3479
Epoch 11 Step 551 Train Loss: 0.3414
Epoch 11 Step 601 Train Loss: 0.4953
Epoch 11 Step 651 Train Loss: 0.3002
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0023. 
Epoch 12 Step 1 Train Loss: 0.5723
Epoch 12 Step 51 Train Loss: 0.3604
Epoch 12 Step 101 Train Loss: 0.2973
Epoch 12 Step 151 Train Loss: 0.5054
Epoch 12 Step 201 Train Loss: 0.5304
Epoch 12 Step 251 Train Loss: 0.3618
Epoch 12 Step 301 Train Loss: 0.4973
Epoch 12 Step 351 Train Loss: 0.3276
Epoch 12 Step 401 Train Loss: 0.3983
Epoch 12 Step 451 Train Loss: 0.3506
Epoch 12 Step 501 Train Loss: 0.3167
Epoch 12 Step 551 Train Loss: 0.3570
Epoch 12 Step 601 Train Loss: 0.3272
Epoch 12 Step 651 Train Loss: 0.4381
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0023. 
Epoch 13 Step 1 Train Loss: 0.4048
Epoch 13 Step 51 Train Loss: 0.3799
Epoch 13 Step 101 Train Loss: 0.3555
Epoch 13 Step 151 Train Loss: 0.4078
Epoch 13 Step 201 Train Loss: 0.3566
Epoch 13 Step 251 Train Loss: 0.4236
Epoch 13 Step 301 Train Loss: 0.3671
Epoch 13 Step 351 Train Loss: 0.3682
Epoch 13 Step 401 Train Loss: 0.3813
Epoch 13 Step 451 Train Loss: 0.4626
Epoch 13 Step 501 Train Loss: 0.4697
Epoch 13 Step 551 Train Loss: 0.3971
Epoch 13 Step 601 Train Loss: 0.3360
Epoch 13 Step 651 Train Loss: 0.4090
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0021. 
Epoch 14 Step 1 Train Loss: 0.4283
Epoch 14 Step 51 Train Loss: 0.4287
Epoch 14 Step 101 Train Loss: 0.5038
Epoch 14 Step 151 Train Loss: 0.3242
Epoch 14 Step 201 Train Loss: 0.4773
Epoch 14 Step 251 Train Loss: 0.4230
Epoch 14 Step 301 Train Loss: 0.4211
Epoch 14 Step 351 Train Loss: 0.3464
Epoch 14 Step 401 Train Loss: 0.4261
Epoch 14 Step 451 Train Loss: 0.3848
Epoch 14 Step 501 Train Loss: 0.3004
Epoch 14 Step 551 Train Loss: 0.3106
Epoch 14 Step 601 Train Loss: 0.3080
Epoch 14 Step 651 Train Loss: 0.2734
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0024. 
Epoch 15 Step 1 Train Loss: 0.4406
Epoch 15 Step 51 Train Loss: 0.3288
Epoch 15 Step 101 Train Loss: 0.3497
Epoch 15 Step 151 Train Loss: 0.3721
Epoch 15 Step 201 Train Loss: 0.4121
Epoch 15 Step 251 Train Loss: 0.3170
Epoch 15 Step 301 Train Loss: 0.4397
Epoch 15 Step 351 Train Loss: 0.3995
Epoch 15 Step 401 Train Loss: 0.3450
Epoch 15 Step 451 Train Loss: 0.2890
Epoch 15 Step 501 Train Loss: 0.3888
Epoch 15 Step 551 Train Loss: 0.3499
Epoch 15 Step 601 Train Loss: 0.3476
Epoch 15 Step 651 Train Loss: 0.3907
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0026. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0025
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00021902336
test_unseen_single_pearson: 0.9995845639878732
test_unseen_single_mse_de: 0.0024799255
test_unseen_single_pearson_de: 0.9996569220184277
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.44420812281287114
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.13333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002907225601025172
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.003 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÑ
wandb:                                                   val_de_mse ‚ñÜ‚ñÜ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñá
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÉ
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00248
wandb:                                              test_de_pearson 0.99966
wandb:               test_frac_opposite_direction_top20_non_dropout 0.13333
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00022
wandb:                                test_mse_top20_de_non_dropout 0.00291
wandb:                                                 test_pearson 0.99958
wandb:                                           test_pearson_delta 0.44421
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.13333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00022
wandb:                                    test_unseen_single_mse_de 0.00248
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00291
wandb:                                   test_unseen_single_pearson 0.99958
wandb:                                test_unseen_single_pearson_de 0.99966
wandb:                             test_unseen_single_pearson_delta 0.44421
wandb:                                                 train_de_mse 0.00374
wandb:                                             train_de_pearson 0.99946
wandb:                                                    train_mse 0.00016
wandb:                                                train_pearson 0.99972
wandb:                                                training_loss 0.43723
wandb:                                                   val_de_mse 0.00263
wandb:                                               val_de_pearson 0.99966
wandb:                                                      val_mse 0.00027
wandb:                                                  val_pearson 0.99949
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396858_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/l9blqu1a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_195329-l9blqu1a/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_200806-3nisdt9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396858_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/3nisdt9j
wandb: WARNING Serializing object of type ndarray that is 8012928 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4848
Epoch 1 Step 51 Train Loss: 0.3729
Epoch 1 Step 101 Train Loss: 0.4809
Epoch 1 Step 151 Train Loss: 0.3737
Epoch 1 Step 201 Train Loss: 0.3541
Epoch 1 Step 251 Train Loss: 0.3418
Epoch 1 Step 301 Train Loss: 0.4583
Epoch 1 Step 351 Train Loss: 0.4405
Epoch 1 Step 401 Train Loss: 0.3933
Epoch 1 Step 451 Train Loss: 0.4322
Epoch 1 Step 501 Train Loss: 0.4812
Epoch 1 Step 551 Train Loss: 0.4708
Epoch 1 Step 601 Train Loss: 0.6029
Epoch 1: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0033. 
Epoch 2 Step 1 Train Loss: 0.4446
Epoch 2 Step 51 Train Loss: 0.5900
Epoch 2 Step 101 Train Loss: 0.4931
Epoch 2 Step 151 Train Loss: 0.3702
Epoch 2 Step 201 Train Loss: 0.4920
Epoch 2 Step 251 Train Loss: 0.4047
Epoch 2 Step 301 Train Loss: 0.4437
Epoch 2 Step 351 Train Loss: 0.4017
Epoch 2 Step 401 Train Loss: 0.4361
Epoch 2 Step 451 Train Loss: 0.3777
Epoch 2 Step 501 Train Loss: 0.4628
Epoch 2 Step 551 Train Loss: 0.5331
Epoch 2 Step 601 Train Loss: 0.4194
Epoch 2: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0034. 
Epoch 3 Step 1 Train Loss: 0.5015
Epoch 3 Step 51 Train Loss: 0.3851
Epoch 3 Step 101 Train Loss: 0.3589
Epoch 3 Step 151 Train Loss: 0.3716
Epoch 3 Step 201 Train Loss: 0.3863
Epoch 3 Step 251 Train Loss: 0.3140
Epoch 3 Step 301 Train Loss: 0.4497
Epoch 3 Step 351 Train Loss: 0.4062
Epoch 3 Step 401 Train Loss: 0.4060
Epoch 3 Step 451 Train Loss: 0.3766
Epoch 3 Step 501 Train Loss: 0.3160
Epoch 3 Step 551 Train Loss: 0.5224
Epoch 3 Step 601 Train Loss: 0.3812
Epoch 3: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0038. 
Epoch 4 Step 1 Train Loss: 0.3726
Epoch 4 Step 51 Train Loss: 0.4218
Epoch 4 Step 101 Train Loss: 0.3960
Epoch 4 Step 151 Train Loss: 0.4543
Epoch 4 Step 201 Train Loss: 0.4498
Epoch 4 Step 251 Train Loss: 0.4125
Epoch 4 Step 301 Train Loss: 0.3362
Epoch 4 Step 351 Train Loss: 0.3933
Epoch 4 Step 401 Train Loss: 0.5091
Epoch 4 Step 451 Train Loss: 0.4062
Epoch 4 Step 501 Train Loss: 0.4126
Epoch 4 Step 551 Train Loss: 0.3927
Epoch 4 Step 601 Train Loss: 0.3441
Epoch 4: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0031. 
Epoch 5 Step 1 Train Loss: 0.5069
Epoch 5 Step 51 Train Loss: 0.4326
Epoch 5 Step 101 Train Loss: 0.4125
Epoch 5 Step 151 Train Loss: 0.5450
Epoch 5 Step 201 Train Loss: 0.3860
Epoch 5 Step 251 Train Loss: 0.3732
Epoch 5 Step 301 Train Loss: 0.3634
Epoch 5 Step 351 Train Loss: 0.4461
Epoch 5 Step 401 Train Loss: 0.4694
Epoch 5 Step 451 Train Loss: 0.4652
Epoch 5 Step 501 Train Loss: 0.3444
Epoch 5 Step 551 Train Loss: 0.4195
Epoch 5 Step 601 Train Loss: 0.3423
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0033. 
Epoch 6 Step 1 Train Loss: 0.4475
Epoch 6 Step 51 Train Loss: 0.3249
Epoch 6 Step 101 Train Loss: 0.3550
Epoch 6 Step 151 Train Loss: 0.3444
Epoch 6 Step 201 Train Loss: 0.4023
Epoch 6 Step 251 Train Loss: 0.4159
Epoch 6 Step 301 Train Loss: 0.4588
Epoch 6 Step 351 Train Loss: 0.3587
Epoch 6 Step 401 Train Loss: 0.4200
Epoch 6 Step 451 Train Loss: 0.4377
Epoch 6 Step 501 Train Loss: 0.3931
Epoch 6 Step 551 Train Loss: 0.3974
Epoch 6 Step 601 Train Loss: 0.3588
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Epoch 7 Step 1 Train Loss: 0.4751
Epoch 7 Step 51 Train Loss: 0.4237
Epoch 7 Step 101 Train Loss: 0.3278
Epoch 7 Step 151 Train Loss: 0.3518
Epoch 7 Step 201 Train Loss: 0.3455
Epoch 7 Step 251 Train Loss: 0.5117
Epoch 7 Step 301 Train Loss: 0.4580
Epoch 7 Step 351 Train Loss: 0.3406
Epoch 7 Step 401 Train Loss: 0.3588
Epoch 7 Step 451 Train Loss: 0.4303
Epoch 7 Step 501 Train Loss: 0.5161
Epoch 7 Step 551 Train Loss: 0.4849
Epoch 7 Step 601 Train Loss: 0.4404
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0032. 
Epoch 8 Step 1 Train Loss: 0.4366
Epoch 8 Step 51 Train Loss: 0.3100
Epoch 8 Step 101 Train Loss: 0.3747
Epoch 8 Step 151 Train Loss: 0.3187
Epoch 8 Step 201 Train Loss: 0.4208
Epoch 8 Step 251 Train Loss: 0.3828
Epoch 8 Step 301 Train Loss: 0.3596
Epoch 8 Step 351 Train Loss: 0.4034
Epoch 8 Step 401 Train Loss: 0.3670
Epoch 8 Step 451 Train Loss: 0.3685
Epoch 8 Step 501 Train Loss: 0.4386
Epoch 8 Step 551 Train Loss: 0.3639
Epoch 8 Step 601 Train Loss: 0.4050
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0033. 
Epoch 9 Step 1 Train Loss: 0.4418
Epoch 9 Step 51 Train Loss: 0.3792
Epoch 9 Step 101 Train Loss: 0.3651
Epoch 9 Step 151 Train Loss: 0.3511
Epoch 9 Step 201 Train Loss: 0.4678
Epoch 9 Step 251 Train Loss: 0.3445
Epoch 9 Step 301 Train Loss: 0.5332
Epoch 9 Step 351 Train Loss: 0.4579
Epoch 9 Step 401 Train Loss: 0.3991
Epoch 9 Step 451 Train Loss: 0.3972
Epoch 9 Step 501 Train Loss: 0.4115
Epoch 9 Step 551 Train Loss: 0.4243
Epoch 9 Step 601 Train Loss: 0.3962
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Epoch 10 Step 1 Train Loss: 0.3761
Epoch 10 Step 51 Train Loss: 0.3774
Epoch 10 Step 101 Train Loss: 0.3998
Epoch 10 Step 151 Train Loss: 0.3690
Epoch 10 Step 201 Train Loss: 0.5115
Epoch 10 Step 251 Train Loss: 0.4211
Epoch 10 Step 301 Train Loss: 0.3664
Epoch 10 Step 351 Train Loss: 0.3731
Epoch 10 Step 401 Train Loss: 0.3598
Epoch 10 Step 451 Train Loss: 0.3606
Epoch 10 Step 501 Train Loss: 0.4131
Epoch 10 Step 551 Train Loss: 0.4241
Epoch 10 Step 601 Train Loss: 0.3160
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Epoch 11 Step 1 Train Loss: 0.3177
Epoch 11 Step 51 Train Loss: 0.4464
Epoch 11 Step 101 Train Loss: 0.3552
Epoch 11 Step 151 Train Loss: 0.3996
Epoch 11 Step 201 Train Loss: 0.2830
Epoch 11 Step 251 Train Loss: 0.3788
Epoch 11 Step 301 Train Loss: 0.4821
Epoch 11 Step 351 Train Loss: 0.4375
Epoch 11 Step 401 Train Loss: 0.4540
Epoch 11 Step 451 Train Loss: 0.3600
Epoch 11 Step 501 Train Loss: 0.3867
Epoch 11 Step 551 Train Loss: 0.3552
Epoch 11 Step 601 Train Loss: 0.3389
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Epoch 12 Step 1 Train Loss: 0.3016
Epoch 12 Step 51 Train Loss: 0.3263
Epoch 12 Step 101 Train Loss: 0.4403
Epoch 12 Step 151 Train Loss: 0.4539
Epoch 12 Step 201 Train Loss: 0.4144
Epoch 12 Step 251 Train Loss: 0.3801
Epoch 12 Step 301 Train Loss: 0.4289
Epoch 12 Step 351 Train Loss: 0.4045
Epoch 12 Step 401 Train Loss: 0.5315
Epoch 12 Step 451 Train Loss: 0.3181
Epoch 12 Step 501 Train Loss: 0.3731
Epoch 12 Step 551 Train Loss: 0.4380
Epoch 12 Step 601 Train Loss: 0.3696
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Epoch 13 Step 1 Train Loss: 0.4820
Epoch 13 Step 51 Train Loss: 0.3872
Epoch 13 Step 101 Train Loss: 0.3811
Epoch 13 Step 151 Train Loss: 0.4574
Epoch 13 Step 201 Train Loss: 0.3894
Epoch 13 Step 251 Train Loss: 0.4553
Epoch 13 Step 301 Train Loss: 0.4126
Epoch 13 Step 351 Train Loss: 0.3414
Epoch 13 Step 401 Train Loss: 0.4790
Epoch 13 Step 451 Train Loss: 0.4252
Epoch 13 Step 501 Train Loss: 0.3974
Epoch 13 Step 551 Train Loss: 0.3062
Epoch 13 Step 601 Train Loss: 0.4491
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Epoch 14 Step 1 Train Loss: 0.3600
Epoch 14 Step 51 Train Loss: 0.3598
Epoch 14 Step 101 Train Loss: 0.4624
Epoch 14 Step 151 Train Loss: 0.3817
Epoch 14 Step 201 Train Loss: 0.3956
Epoch 14 Step 251 Train Loss: 0.3936
Epoch 14 Step 301 Train Loss: 0.4170
Epoch 14 Step 351 Train Loss: 0.3718
Epoch 14 Step 401 Train Loss: 0.3901
Epoch 14 Step 451 Train Loss: 0.3899
Epoch 14 Step 501 Train Loss: 0.4242
Epoch 14 Step 551 Train Loss: 0.3572
Epoch 14 Step 601 Train Loss: 0.5217
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0033. 
Epoch 15 Step 1 Train Loss: 0.3594
Epoch 15 Step 51 Train Loss: 0.5220
Epoch 15 Step 101 Train Loss: 0.3936
Epoch 15 Step 151 Train Loss: 0.3982
Epoch 15 Step 201 Train Loss: 0.4106
Epoch 15 Step 251 Train Loss: 0.4516
Epoch 15 Step 301 Train Loss: 0.4709
Epoch 15 Step 351 Train Loss: 0.3736
Epoch 15 Step 401 Train Loss: 0.3473
Epoch 15 Step 451 Train Loss: 0.3130
Epoch 15 Step 501 Train Loss: 0.5097
Epoch 15 Step 551 Train Loss: 0.5167
Epoch 15 Step 601 Train Loss: 0.3859
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0034. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0100
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00039602097
test_unseen_single_pearson: 0.999270305580577
test_unseen_single_mse_de: 0.0099836355
test_unseen_single_pearson_de: 0.9987028881075442
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.37937768694055646
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.11666666666666665
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.010036298537207422
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.007 MB of 0.024 MB uploadedwandb: / 0.013 MB of 0.024 MB uploadedwandb: - 0.023 MB of 0.024 MB uploadedwandb: \ 0.023 MB of 0.024 MB uploadedwandb: | 0.023 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÖ
wandb:                                                   val_de_mse ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ
wandb:                                               val_de_pearson ‚ñà‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00998
wandb:                                              test_de_pearson 0.9987
wandb:               test_frac_opposite_direction_top20_non_dropout 0.11667
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.0004
wandb:                                test_mse_top20_de_non_dropout 0.01004
wandb:                                                 test_pearson 0.99927
wandb:                                           test_pearson_delta 0.37938
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.11667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.0004
wandb:                                    test_unseen_single_mse_de 0.00998
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01004
wandb:                                   test_unseen_single_pearson 0.99927
wandb:                                test_unseen_single_pearson_de 0.9987
wandb:                             test_unseen_single_pearson_delta 0.37938
wandb:                                                 train_de_mse 0.00148
wandb:                                             train_de_pearson 0.9998
wandb:                                                    train_mse 0.00017
wandb:                                                train_pearson 0.99972
wandb:                                                training_loss 0.36437
wandb:                                                   val_de_mse 0.00341
wandb:                                               val_de_pearson 0.9995
wandb:                                                      val_mse 0.00022
wandb:                                                  val_pearson 0.99961
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396858_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/3nisdt9j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_200806-3nisdt9j/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_202032-04yfwmr0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396858_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/04yfwmr0
wandb: WARNING Serializing object of type ndarray that is 8012928 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4718
Epoch 1 Step 51 Train Loss: 0.5008
Epoch 1 Step 101 Train Loss: 0.4766
Epoch 1 Step 151 Train Loss: 0.5752
Epoch 1 Step 201 Train Loss: 0.5183
Epoch 1 Step 251 Train Loss: 0.4502
Epoch 1 Step 301 Train Loss: 0.5217
Epoch 1 Step 351 Train Loss: 0.4023
Epoch 1 Step 401 Train Loss: 0.4639
Epoch 1 Step 451 Train Loss: 0.5725
Epoch 1 Step 501 Train Loss: 0.4643
Epoch 1: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0052 Validation Top 20 DE MSE: 0.0043. 
Epoch 2 Step 1 Train Loss: 0.5023
Epoch 2 Step 51 Train Loss: 0.4395
Epoch 2 Step 101 Train Loss: 0.4853
Epoch 2 Step 151 Train Loss: 0.3835
Epoch 2 Step 201 Train Loss: 0.5263
Epoch 2 Step 251 Train Loss: 0.6321
Epoch 2 Step 301 Train Loss: 0.4876
Epoch 2 Step 351 Train Loss: 0.5390
Epoch 2 Step 401 Train Loss: 0.4705
Epoch 2 Step 451 Train Loss: 0.5367
Epoch 2 Step 501 Train Loss: 0.3907
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0057. 
Epoch 3 Step 1 Train Loss: 0.5315
Epoch 3 Step 51 Train Loss: 0.5935
Epoch 3 Step 101 Train Loss: 0.5664
Epoch 3 Step 151 Train Loss: 0.3943
Epoch 3 Step 201 Train Loss: 0.3688
Epoch 3 Step 251 Train Loss: 0.2894
Epoch 3 Step 301 Train Loss: 0.4355
Epoch 3 Step 351 Train Loss: 0.4251
Epoch 3 Step 401 Train Loss: 0.3395
Epoch 3 Step 451 Train Loss: 0.4215
Epoch 3 Step 501 Train Loss: 0.4238
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0037. 
Epoch 4 Step 1 Train Loss: 0.4031
Epoch 4 Step 51 Train Loss: 0.4440
Epoch 4 Step 101 Train Loss: 0.4164
Epoch 4 Step 151 Train Loss: 0.4880
Epoch 4 Step 201 Train Loss: 0.3988
Epoch 4 Step 251 Train Loss: 0.3898
Epoch 4 Step 301 Train Loss: 0.3944
Epoch 4 Step 351 Train Loss: 0.4354
Epoch 4 Step 401 Train Loss: 0.5203
Epoch 4 Step 451 Train Loss: 0.3589
Epoch 4 Step 501 Train Loss: 0.3915
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0051 Validation Top 20 DE MSE: 0.0049. 
Epoch 5 Step 1 Train Loss: 0.4647
Epoch 5 Step 51 Train Loss: 0.4227
Epoch 5 Step 101 Train Loss: 0.4635
Epoch 5 Step 151 Train Loss: 0.6048
Epoch 5 Step 201 Train Loss: 0.3963
Epoch 5 Step 251 Train Loss: 0.4261
Epoch 5 Step 301 Train Loss: 0.4569
Epoch 5 Step 351 Train Loss: 0.4178
Epoch 5 Step 401 Train Loss: 0.3942
Epoch 5 Step 451 Train Loss: 0.4967
Epoch 5 Step 501 Train Loss: 0.4857
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0020. 
Epoch 6 Step 1 Train Loss: 0.3949
Epoch 6 Step 51 Train Loss: 0.4953
Epoch 6 Step 101 Train Loss: 0.3898
Epoch 6 Step 151 Train Loss: 0.3218
Epoch 6 Step 201 Train Loss: 0.4811
Epoch 6 Step 251 Train Loss: 0.4952
Epoch 6 Step 301 Train Loss: 0.4358
Epoch 6 Step 351 Train Loss: 0.3266
Epoch 6 Step 401 Train Loss: 0.3707
Epoch 6 Step 451 Train Loss: 0.4168
Epoch 6 Step 501 Train Loss: 0.4500
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0029. 
Epoch 7 Step 1 Train Loss: 0.5275
Epoch 7 Step 51 Train Loss: 0.4100
Epoch 7 Step 101 Train Loss: 0.4885
Epoch 7 Step 151 Train Loss: 0.4940
Epoch 7 Step 201 Train Loss: 0.4496
Epoch 7 Step 251 Train Loss: 0.4764
Epoch 7 Step 301 Train Loss: 0.4812
Epoch 7 Step 351 Train Loss: 0.4734
Epoch 7 Step 401 Train Loss: 0.3911
Epoch 7 Step 451 Train Loss: 0.4387
Epoch 7 Step 501 Train Loss: 0.4757
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0028. 
Epoch 8 Step 1 Train Loss: 0.3502
Epoch 8 Step 51 Train Loss: 0.3778
Epoch 8 Step 101 Train Loss: 0.4604
Epoch 8 Step 151 Train Loss: 0.4398
Epoch 8 Step 201 Train Loss: 0.3858
Epoch 8 Step 251 Train Loss: 0.4369
Epoch 8 Step 301 Train Loss: 0.5683
Epoch 8 Step 351 Train Loss: 0.4620
Epoch 8 Step 401 Train Loss: 0.5052
Epoch 8 Step 451 Train Loss: 0.4151
Epoch 8 Step 501 Train Loss: 0.4145
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0024. 
Epoch 9 Step 1 Train Loss: 0.3658
Epoch 9 Step 51 Train Loss: 0.4339
Epoch 9 Step 101 Train Loss: 0.3897
Epoch 9 Step 151 Train Loss: 0.3469
Epoch 9 Step 201 Train Loss: 0.4056
Epoch 9 Step 251 Train Loss: 0.4358
Epoch 9 Step 301 Train Loss: 0.5047
Epoch 9 Step 351 Train Loss: 0.6562
Epoch 9 Step 401 Train Loss: 0.3771
Epoch 9 Step 451 Train Loss: 0.4491
Epoch 9 Step 501 Train Loss: 0.5339
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0024. 
Epoch 10 Step 1 Train Loss: 0.4071
Epoch 10 Step 51 Train Loss: 0.4502
Epoch 10 Step 101 Train Loss: 0.4686
Epoch 10 Step 151 Train Loss: 0.5066
Epoch 10 Step 201 Train Loss: 0.3757
Epoch 10 Step 251 Train Loss: 0.3678
Epoch 10 Step 301 Train Loss: 0.4182
Epoch 10 Step 351 Train Loss: 0.4434
Epoch 10 Step 401 Train Loss: 0.4888
Epoch 10 Step 451 Train Loss: 0.4623
Epoch 10 Step 501 Train Loss: 0.3923
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0024. 
Epoch 11 Step 1 Train Loss: 0.4198
Epoch 11 Step 51 Train Loss: 0.4702
Epoch 11 Step 101 Train Loss: 0.4867
Epoch 11 Step 151 Train Loss: 0.5222
Epoch 11 Step 201 Train Loss: 0.4133
Epoch 11 Step 251 Train Loss: 0.4394
Epoch 11 Step 301 Train Loss: 0.3638
Epoch 11 Step 351 Train Loss: 0.4307
Epoch 11 Step 401 Train Loss: 0.4678
Epoch 11 Step 451 Train Loss: 0.4260
Epoch 11 Step 501 Train Loss: 0.5529
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0022. 
Epoch 12 Step 1 Train Loss: 0.4683
Epoch 12 Step 51 Train Loss: 0.4114
Epoch 12 Step 101 Train Loss: 0.5562
Epoch 12 Step 151 Train Loss: 0.3328
Epoch 12 Step 201 Train Loss: 0.4278
Epoch 12 Step 251 Train Loss: 0.4365
Epoch 12 Step 301 Train Loss: 0.3612
Epoch 12 Step 351 Train Loss: 0.3858
Epoch 12 Step 401 Train Loss: 0.3973
Epoch 12 Step 451 Train Loss: 0.4503
Epoch 12 Step 501 Train Loss: 0.4529
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0023. 
Epoch 13 Step 1 Train Loss: 0.4441
Epoch 13 Step 51 Train Loss: 0.5497
Epoch 13 Step 101 Train Loss: 0.3890
Epoch 13 Step 151 Train Loss: 0.4142
Epoch 13 Step 201 Train Loss: 0.5269
Epoch 13 Step 251 Train Loss: 0.3934
Epoch 13 Step 301 Train Loss: 0.3735
Epoch 13 Step 351 Train Loss: 0.4135
Epoch 13 Step 401 Train Loss: 0.4303
Epoch 13 Step 451 Train Loss: 0.4104
Epoch 13 Step 501 Train Loss: 0.4431
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0023. 
Epoch 14 Step 1 Train Loss: 0.3929
Epoch 14 Step 51 Train Loss: 0.4956
Epoch 14 Step 101 Train Loss: 0.4170
Epoch 14 Step 151 Train Loss: 0.4052
Epoch 14 Step 201 Train Loss: 0.3970
Epoch 14 Step 251 Train Loss: 0.4227
Epoch 14 Step 301 Train Loss: 0.3977
Epoch 14 Step 351 Train Loss: 0.3733
Epoch 14 Step 401 Train Loss: 0.3856
Epoch 14 Step 451 Train Loss: 0.4890
Epoch 14 Step 501 Train Loss: 0.3984
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0023. 
Epoch 15 Step 1 Train Loss: 0.4478
Epoch 15 Step 51 Train Loss: 0.4329
Epoch 15 Step 101 Train Loss: 0.4913
Epoch 15 Step 151 Train Loss: 0.5064
Epoch 15 Step 201 Train Loss: 0.4195
Epoch 15 Step 251 Train Loss: 0.5313
Epoch 15 Step 301 Train Loss: 0.4194
Epoch 15 Step 351 Train Loss: 0.4161
Epoch 15 Step 401 Train Loss: 0.4093
Epoch 15 Step 451 Train Loss: 0.4685
Epoch 15 Step 501 Train Loss: 0.3936
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0023. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0025
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00044693798
test_unseen_single_pearson: 0.9991862899245502
test_unseen_single_mse_de: 0.0024712412
test_unseen_single_pearson_de: 0.9995208707229879
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.48785719458655086
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.05000000000000001
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002471241079505245
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.001 MB of 0.023 MB uploadedwandb: / 0.001 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñá‚ñÑ‚ñÅ‚ñá‚ñà‚ñá‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñá‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÜ
wandb:                                                   val_de_mse ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÖ‚ñÅ‚ñÜ‚ñÇ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00247
wandb:                                              test_de_pearson 0.99952
wandb:               test_frac_opposite_direction_top20_non_dropout 0.05
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00045
wandb:                                test_mse_top20_de_non_dropout 0.00247
wandb:                                                 test_pearson 0.99919
wandb:                                           test_pearson_delta 0.48786
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.05
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00045
wandb:                                    test_unseen_single_mse_de 0.00247
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00247
wandb:                                   test_unseen_single_pearson 0.99919
wandb:                                test_unseen_single_pearson_de 0.99952
wandb:                             test_unseen_single_pearson_delta 0.48786
wandb:                                                 train_de_mse 0.00248
wandb:                                             train_de_pearson 0.9997
wandb:                                                    train_mse 0.00024
wandb:                                                train_pearson 0.99958
wandb:                                                training_loss 0.49376
wandb:                                                   val_de_mse 0.00231
wandb:                                               val_de_pearson 0.99959
wandb:                                                      val_mse 0.0003
wandb:                                                  val_pearson 0.99947
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396858_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/04yfwmr0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_202032-04yfwmr0/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_203045-6hg9rjsj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396858_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/6hg9rjsj
wandb: WARNING Serializing object of type ndarray that is 8012928 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4051
Epoch 1 Step 51 Train Loss: 0.4021
Epoch 1 Step 101 Train Loss: 0.4347
Epoch 1 Step 151 Train Loss: 0.3917
Epoch 1 Step 201 Train Loss: 0.4130
Epoch 1 Step 251 Train Loss: 0.4797
Epoch 1 Step 301 Train Loss: 0.3553
Epoch 1 Step 351 Train Loss: 0.3782
Epoch 1 Step 401 Train Loss: 0.4637
Epoch 1 Step 451 Train Loss: 0.5097
Epoch 1 Step 501 Train Loss: 0.4116
Epoch 1 Step 551 Train Loss: 0.4700
Epoch 1 Step 601 Train Loss: 0.4117
Epoch 1: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0021. 
Epoch 2 Step 1 Train Loss: 0.4024
Epoch 2 Step 51 Train Loss: 0.4638
Epoch 2 Step 101 Train Loss: 0.4623
Epoch 2 Step 151 Train Loss: 0.4592
Epoch 2 Step 201 Train Loss: 0.4651
Epoch 2 Step 251 Train Loss: 0.3800
Epoch 2 Step 301 Train Loss: 0.4303
Epoch 2 Step 351 Train Loss: 0.4051
Epoch 2 Step 401 Train Loss: 0.4902
Epoch 2 Step 451 Train Loss: 0.4226
Epoch 2 Step 501 Train Loss: 0.4986
Epoch 2 Step 551 Train Loss: 0.3863
Epoch 2 Step 601 Train Loss: 0.4961
Epoch 2: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0011. 
Epoch 3 Step 1 Train Loss: 0.3984
Epoch 3 Step 51 Train Loss: 0.3491
Epoch 3 Step 101 Train Loss: 0.3819
Epoch 3 Step 151 Train Loss: 0.4493
Epoch 3 Step 201 Train Loss: 0.3924
Epoch 3 Step 251 Train Loss: 0.4560
Epoch 3 Step 301 Train Loss: 0.4561
Epoch 3 Step 351 Train Loss: 0.3619
Epoch 3 Step 401 Train Loss: 0.3966
Epoch 3 Step 451 Train Loss: 0.3808
Epoch 3 Step 501 Train Loss: 0.4146
Epoch 3 Step 551 Train Loss: 0.3813
Epoch 3 Step 601 Train Loss: 0.5151
Epoch 3: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0009. 
Epoch 4 Step 1 Train Loss: 0.4258
Epoch 4 Step 51 Train Loss: 0.4190
Epoch 4 Step 101 Train Loss: 0.5573
Epoch 4 Step 151 Train Loss: 0.3332
Epoch 4 Step 201 Train Loss: 0.4467
Epoch 4 Step 251 Train Loss: 0.3340
Epoch 4 Step 301 Train Loss: 0.4611
Epoch 4 Step 351 Train Loss: 0.4001
Epoch 4 Step 401 Train Loss: 0.4005
Epoch 4 Step 451 Train Loss: 0.5266
Epoch 4 Step 501 Train Loss: 0.3527
Epoch 4 Step 551 Train Loss: 0.3595
Epoch 4 Step 601 Train Loss: 0.4126
Epoch 4: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0077 Validation Top 20 DE MSE: 0.0044. 
Epoch 5 Step 1 Train Loss: 0.3873
Epoch 5 Step 51 Train Loss: 0.4200
Epoch 5 Step 101 Train Loss: 0.4260
Epoch 5 Step 151 Train Loss: 0.3939
Epoch 5 Step 201 Train Loss: 0.4160
Epoch 5 Step 251 Train Loss: 0.3868
Epoch 5 Step 301 Train Loss: 0.4523
Epoch 5 Step 351 Train Loss: 0.3637
Epoch 5 Step 401 Train Loss: 0.4008
Epoch 5 Step 451 Train Loss: 0.3507
Epoch 5 Step 501 Train Loss: 0.4844
Epoch 5 Step 551 Train Loss: 0.3920
Epoch 5 Step 601 Train Loss: 0.4672
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0009. 
Epoch 6 Step 1 Train Loss: 0.4472
Epoch 6 Step 51 Train Loss: 0.4000
Epoch 6 Step 101 Train Loss: 0.4916
Epoch 6 Step 151 Train Loss: 0.4003
Epoch 6 Step 201 Train Loss: 0.3924
Epoch 6 Step 251 Train Loss: 0.4391
Epoch 6 Step 301 Train Loss: 0.4038
Epoch 6 Step 351 Train Loss: 0.4272
Epoch 6 Step 401 Train Loss: 0.4054
Epoch 6 Step 451 Train Loss: 0.3835
Epoch 6 Step 501 Train Loss: 0.4594
Epoch 6 Step 551 Train Loss: 0.3736
Epoch 6 Step 601 Train Loss: 0.4238
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0017. 
Epoch 7 Step 1 Train Loss: 0.4869
Epoch 7 Step 51 Train Loss: 0.3922
Epoch 7 Step 101 Train Loss: 0.4604
Epoch 7 Step 151 Train Loss: 0.3660
Epoch 7 Step 201 Train Loss: 0.4279
Epoch 7 Step 251 Train Loss: 0.3555
Epoch 7 Step 301 Train Loss: 0.3388
Epoch 7 Step 351 Train Loss: 0.3982
Epoch 7 Step 401 Train Loss: 0.3749
Epoch 7 Step 451 Train Loss: 0.4201
Epoch 7 Step 501 Train Loss: 0.4198
Epoch 7 Step 551 Train Loss: 0.4424
Epoch 7 Step 601 Train Loss: 0.3920
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0009. 
Epoch 8 Step 1 Train Loss: 0.3358
Epoch 8 Step 51 Train Loss: 0.3594
Epoch 8 Step 101 Train Loss: 0.4310
Epoch 8 Step 151 Train Loss: 0.3142
Epoch 8 Step 201 Train Loss: 0.3705
Epoch 8 Step 251 Train Loss: 0.3842
Epoch 8 Step 301 Train Loss: 0.3909
Epoch 8 Step 351 Train Loss: 0.3234
Epoch 8 Step 401 Train Loss: 0.4197
Epoch 8 Step 451 Train Loss: 0.3750
Epoch 8 Step 501 Train Loss: 0.3206
Epoch 8 Step 551 Train Loss: 0.4772
Epoch 8 Step 601 Train Loss: 0.4068
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0008. 
Epoch 9 Step 1 Train Loss: 0.3183
Epoch 9 Step 51 Train Loss: 0.4085
Epoch 9 Step 101 Train Loss: 0.4120
Epoch 9 Step 151 Train Loss: 0.3576
Epoch 9 Step 201 Train Loss: 0.4182
Epoch 9 Step 251 Train Loss: 0.4354
Epoch 9 Step 301 Train Loss: 0.3904
Epoch 9 Step 351 Train Loss: 0.5216
Epoch 9 Step 401 Train Loss: 0.4020
Epoch 9 Step 451 Train Loss: 0.3720
Epoch 9 Step 501 Train Loss: 0.4521
Epoch 9 Step 551 Train Loss: 0.4199
Epoch 9 Step 601 Train Loss: 0.3152
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0009. 
Epoch 10 Step 1 Train Loss: 0.5127
Epoch 10 Step 51 Train Loss: 0.3651
Epoch 10 Step 101 Train Loss: 0.4163
Epoch 10 Step 151 Train Loss: 0.3709
Epoch 10 Step 201 Train Loss: 0.3388
Epoch 10 Step 251 Train Loss: 0.5788
Epoch 10 Step 301 Train Loss: 0.6566
Epoch 10 Step 351 Train Loss: 0.3834
Epoch 10 Step 401 Train Loss: 0.4533
Epoch 10 Step 451 Train Loss: 0.4052
Epoch 10 Step 501 Train Loss: 0.4921
Epoch 10 Step 551 Train Loss: 0.3944
Epoch 10 Step 601 Train Loss: 0.3907
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0009. 
Epoch 11 Step 1 Train Loss: 0.5036
Epoch 11 Step 51 Train Loss: 0.4434
Epoch 11 Step 101 Train Loss: 0.5492
Epoch 11 Step 151 Train Loss: 0.3947
Epoch 11 Step 201 Train Loss: 0.4531
Epoch 11 Step 251 Train Loss: 0.4802
Epoch 11 Step 301 Train Loss: 0.5009
Epoch 11 Step 351 Train Loss: 0.4450
Epoch 11 Step 401 Train Loss: 0.5811
Epoch 11 Step 451 Train Loss: 0.4197
Epoch 11 Step 501 Train Loss: 0.4710
Epoch 11 Step 551 Train Loss: 0.4595
Epoch 11 Step 601 Train Loss: 0.3706
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0008. 
Epoch 12 Step 1 Train Loss: 0.4226
Epoch 12 Step 51 Train Loss: 0.4633
Epoch 12 Step 101 Train Loss: 0.4298
Epoch 12 Step 151 Train Loss: 0.3677
Epoch 12 Step 201 Train Loss: 0.4093
Epoch 12 Step 251 Train Loss: 0.4121
Epoch 12 Step 301 Train Loss: 0.4160
Epoch 12 Step 351 Train Loss: 0.3164
Epoch 12 Step 401 Train Loss: 0.3589
Epoch 12 Step 451 Train Loss: 0.3481
Epoch 12 Step 501 Train Loss: 0.5213
Epoch 12 Step 551 Train Loss: 0.3163
Epoch 12 Step 601 Train Loss: 0.3520
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0008. 
Epoch 13 Step 1 Train Loss: 0.3831
Epoch 13 Step 51 Train Loss: 0.4462
Epoch 13 Step 101 Train Loss: 0.3787
Epoch 13 Step 151 Train Loss: 0.4091
Epoch 13 Step 201 Train Loss: 0.4804
Epoch 13 Step 251 Train Loss: 0.4200
Epoch 13 Step 301 Train Loss: 0.4116
Epoch 13 Step 351 Train Loss: 0.3832
Epoch 13 Step 401 Train Loss: 0.3500
Epoch 13 Step 451 Train Loss: 0.4354
Epoch 13 Step 501 Train Loss: 0.3942
Epoch 13 Step 551 Train Loss: 0.3895
Epoch 13 Step 601 Train Loss: 0.3169
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0009. 
Epoch 14 Step 1 Train Loss: 0.4756
Epoch 14 Step 51 Train Loss: 0.5005
Epoch 14 Step 101 Train Loss: 0.5161
Epoch 14 Step 151 Train Loss: 0.3753
Epoch 14 Step 201 Train Loss: 0.3787
Epoch 14 Step 251 Train Loss: 0.4994
Epoch 14 Step 301 Train Loss: 0.3359
Epoch 14 Step 351 Train Loss: 0.3265
Epoch 14 Step 401 Train Loss: 0.3947
Epoch 14 Step 451 Train Loss: 0.3237
Epoch 14 Step 501 Train Loss: 0.3935
Epoch 14 Step 551 Train Loss: 0.4829
Epoch 14 Step 601 Train Loss: 0.4737
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0008. 
Epoch 15 Step 1 Train Loss: 0.4020
Epoch 15 Step 51 Train Loss: 0.3567
Epoch 15 Step 101 Train Loss: 0.3613
Epoch 15 Step 151 Train Loss: 0.4095
Epoch 15 Step 201 Train Loss: 0.3436
Epoch 15 Step 251 Train Loss: 0.4250
Epoch 15 Step 301 Train Loss: 0.4140
Epoch 15 Step 351 Train Loss: 0.3794
Epoch 15 Step 401 Train Loss: 0.5285
Epoch 15 Step 451 Train Loss: 0.3882
Epoch 15 Step 501 Train Loss: 0.5077
Epoch 15 Step 551 Train Loss: 0.4139
Epoch 15 Step 601 Train Loss: 0.3799
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0008. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0013
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00021771435
test_unseen_single_pearson: 0.9995980529653256
test_unseen_single_mse_de: 0.0012882577
test_unseen_single_pearson_de: 0.9998555992194816
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.49255566620834096
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.10000000000000002
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.001634001456882055
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÇ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÅ
wandb:                                                   val_de_mse ‚ñÑ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñà‚ñÅ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00129
wandb:                                              test_de_pearson 0.99986
wandb:               test_frac_opposite_direction_top20_non_dropout 0.1
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00022
wandb:                                test_mse_top20_de_non_dropout 0.00163
wandb:                                                 test_pearson 0.9996
wandb:                                           test_pearson_delta 0.49256
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.1
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00022
wandb:                                    test_unseen_single_mse_de 0.00129
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00163
wandb:                                   test_unseen_single_pearson 0.9996
wandb:                                test_unseen_single_pearson_de 0.99986
wandb:                             test_unseen_single_pearson_delta 0.49256
wandb:                                                 train_de_mse 0.00284
wandb:                                             train_de_pearson 0.99957
wandb:                                                    train_mse 0.00015
wandb:                                                train_pearson 0.99972
wandb:                                                training_loss 0.39384
wandb:                                                   val_de_mse 0.00083
wandb:                                               val_de_pearson 0.9998
wandb:                                                      val_mse 0.00015
wandb:                                                  val_pearson 0.99974
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396858_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/6hg9rjsj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_203045-6hg9rjsj/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_204242-pq0qaw3f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396858_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/pq0qaw3f
wandb: WARNING Serializing object of type ndarray that is 8012928 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4749
Epoch 1 Step 51 Train Loss: 0.4851
Epoch 1 Step 101 Train Loss: 0.4951
Epoch 1 Step 151 Train Loss: 0.4513
Epoch 1 Step 201 Train Loss: 0.3763
Epoch 1 Step 251 Train Loss: 0.4174
Epoch 1 Step 301 Train Loss: 0.4426
Epoch 1 Step 351 Train Loss: 0.4973
Epoch 1 Step 401 Train Loss: 0.3954
Epoch 1 Step 451 Train Loss: 0.3614
Epoch 1 Step 501 Train Loss: 0.4041
Epoch 1: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0056 Validation Top 20 DE MSE: 0.0036. 
Epoch 2 Step 1 Train Loss: 0.4296
Epoch 2 Step 51 Train Loss: 0.4023
Epoch 2 Step 101 Train Loss: 0.4274
Epoch 2 Step 151 Train Loss: 0.3971
Epoch 2 Step 201 Train Loss: 0.4800
Epoch 2 Step 251 Train Loss: 0.3737
Epoch 2 Step 301 Train Loss: 0.4172
Epoch 2 Step 351 Train Loss: 0.4172
Epoch 2 Step 401 Train Loss: 0.4884
Epoch 2 Step 451 Train Loss: 0.4464
Epoch 2 Step 501 Train Loss: 0.3386
Epoch 2: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0046. 
Epoch 3 Step 1 Train Loss: 0.4445
Epoch 3 Step 51 Train Loss: 0.4479
Epoch 3 Step 101 Train Loss: 0.4450
Epoch 3 Step 151 Train Loss: 0.3692
Epoch 3 Step 201 Train Loss: 0.5316
Epoch 3 Step 251 Train Loss: 0.4808
Epoch 3 Step 301 Train Loss: 0.4233
Epoch 3 Step 351 Train Loss: 0.3993
Epoch 3 Step 401 Train Loss: 0.5084
Epoch 3 Step 451 Train Loss: 0.3923
Epoch 3 Step 501 Train Loss: 0.4571
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0056 Validation Top 20 DE MSE: 0.0035. 
Epoch 4 Step 1 Train Loss: 0.3878
Epoch 4 Step 51 Train Loss: 0.5726
Epoch 4 Step 101 Train Loss: 0.4175
Epoch 4 Step 151 Train Loss: 0.4150
Epoch 4 Step 201 Train Loss: 0.4461
Epoch 4 Step 251 Train Loss: 0.4605
Epoch 4 Step 301 Train Loss: 0.3636
Epoch 4 Step 351 Train Loss: 0.4176
Epoch 4 Step 401 Train Loss: 0.4205
Epoch 4 Step 451 Train Loss: 0.4169
Epoch 4 Step 501 Train Loss: 0.4363
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0031. 
Epoch 5 Step 1 Train Loss: 0.4468
Epoch 5 Step 51 Train Loss: 0.4100
Epoch 5 Step 101 Train Loss: 0.3670
Epoch 5 Step 151 Train Loss: 0.5041
Epoch 5 Step 201 Train Loss: 0.3661
Epoch 5 Step 251 Train Loss: 0.4595
Epoch 5 Step 301 Train Loss: 0.3936
Epoch 5 Step 351 Train Loss: 0.4124
Epoch 5 Step 401 Train Loss: 0.5362
Epoch 5 Step 451 Train Loss: 0.4274
Epoch 5 Step 501 Train Loss: 0.4537
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0026. 
Epoch 6 Step 1 Train Loss: 0.4115
Epoch 6 Step 51 Train Loss: 0.4233
Epoch 6 Step 101 Train Loss: 0.3556
Epoch 6 Step 151 Train Loss: 0.3875
Epoch 6 Step 201 Train Loss: 0.5228
Epoch 6 Step 251 Train Loss: 0.4828
Epoch 6 Step 301 Train Loss: 0.3756
Epoch 6 Step 351 Train Loss: 0.4453
Epoch 6 Step 401 Train Loss: 0.4447
Epoch 6 Step 451 Train Loss: 0.4947
Epoch 6 Step 501 Train Loss: 0.4007
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0025. 
Epoch 7 Step 1 Train Loss: 0.4902
Epoch 7 Step 51 Train Loss: 0.4322
Epoch 7 Step 101 Train Loss: 0.5103
Epoch 7 Step 151 Train Loss: 0.3883
Epoch 7 Step 201 Train Loss: 0.4703
Epoch 7 Step 251 Train Loss: 0.5288
Epoch 7 Step 301 Train Loss: 0.4160
Epoch 7 Step 351 Train Loss: 0.5169
Epoch 7 Step 401 Train Loss: 0.5440
Epoch 7 Step 451 Train Loss: 0.3503
Epoch 7 Step 501 Train Loss: 0.3441
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0027. 
Epoch 8 Step 1 Train Loss: 0.4493
Epoch 8 Step 51 Train Loss: 0.3582
Epoch 8 Step 101 Train Loss: 0.5867
Epoch 8 Step 151 Train Loss: 0.4529
Epoch 8 Step 201 Train Loss: 0.4109
Epoch 8 Step 251 Train Loss: 0.3504
Epoch 8 Step 301 Train Loss: 0.4382
Epoch 8 Step 351 Train Loss: 0.4081
Epoch 8 Step 401 Train Loss: 0.4723
Epoch 8 Step 451 Train Loss: 0.4650
Epoch 8 Step 501 Train Loss: 0.4362
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0025. 
Epoch 9 Step 1 Train Loss: 0.3962
Epoch 9 Step 51 Train Loss: 0.4478
Epoch 9 Step 101 Train Loss: 0.3688
Epoch 9 Step 151 Train Loss: 0.4203
Epoch 9 Step 201 Train Loss: 0.3704
Epoch 9 Step 251 Train Loss: 0.4455
Epoch 9 Step 301 Train Loss: 0.3765
Epoch 9 Step 351 Train Loss: 0.4928
Epoch 9 Step 401 Train Loss: 0.4796
Epoch 9 Step 451 Train Loss: 0.4432
Epoch 9 Step 501 Train Loss: 0.3911
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0024. 
Epoch 10 Step 1 Train Loss: 0.5110
Epoch 10 Step 51 Train Loss: 0.4403
Epoch 10 Step 101 Train Loss: 0.4403
Epoch 10 Step 151 Train Loss: 0.3721
Epoch 10 Step 201 Train Loss: 0.3660
Epoch 10 Step 251 Train Loss: 0.4754
Epoch 10 Step 301 Train Loss: 0.4173
Epoch 10 Step 351 Train Loss: 0.4240
Epoch 10 Step 401 Train Loss: 0.4263
Epoch 10 Step 451 Train Loss: 0.3583
Epoch 10 Step 501 Train Loss: 0.3744
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0024. 
Epoch 11 Step 1 Train Loss: 0.3938
Epoch 11 Step 51 Train Loss: 0.3603
Epoch 11 Step 101 Train Loss: 0.3156
Epoch 11 Step 151 Train Loss: 0.4539
Epoch 11 Step 201 Train Loss: 0.5174
Epoch 11 Step 251 Train Loss: 0.4145
Epoch 11 Step 301 Train Loss: 0.3568
Epoch 11 Step 351 Train Loss: 0.3775
Epoch 11 Step 401 Train Loss: 0.4690
Epoch 11 Step 451 Train Loss: 0.5165
Epoch 11 Step 501 Train Loss: 0.4268
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0024. 
Epoch 12 Step 1 Train Loss: 0.3917
Epoch 12 Step 51 Train Loss: 0.5131
Epoch 12 Step 101 Train Loss: 0.4738
Epoch 12 Step 151 Train Loss: 0.3746
Epoch 12 Step 201 Train Loss: 0.4282
Epoch 12 Step 251 Train Loss: 0.4799
Epoch 12 Step 301 Train Loss: 0.3793
Epoch 12 Step 351 Train Loss: 0.4288
Epoch 12 Step 401 Train Loss: 0.3981
Epoch 12 Step 451 Train Loss: 0.4713
Epoch 12 Step 501 Train Loss: 0.4681
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0025. 
Epoch 13 Step 1 Train Loss: 0.4103
Epoch 13 Step 51 Train Loss: 0.5390
Epoch 13 Step 101 Train Loss: 0.3449
Epoch 13 Step 151 Train Loss: 0.6146
Epoch 13 Step 201 Train Loss: 0.4396
Epoch 13 Step 251 Train Loss: 0.4301
Epoch 13 Step 301 Train Loss: 0.4528
Epoch 13 Step 351 Train Loss: 0.4047
Epoch 13 Step 401 Train Loss: 0.3976
Epoch 13 Step 451 Train Loss: 0.4167
Epoch 13 Step 501 Train Loss: 0.4033
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0024. 
Epoch 14 Step 1 Train Loss: 0.3365
Epoch 14 Step 51 Train Loss: 0.4550
Epoch 14 Step 101 Train Loss: 0.3701
Epoch 14 Step 151 Train Loss: 0.3911
Epoch 14 Step 201 Train Loss: 0.4264
Epoch 14 Step 251 Train Loss: 0.3865
Epoch 14 Step 301 Train Loss: 0.3642
Epoch 14 Step 351 Train Loss: 0.4105
Epoch 14 Step 401 Train Loss: 0.6040
Epoch 14 Step 451 Train Loss: 0.5505
Epoch 14 Step 501 Train Loss: 0.5667
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0025. 
Epoch 15 Step 1 Train Loss: 0.4095
Epoch 15 Step 51 Train Loss: 0.3671
Epoch 15 Step 101 Train Loss: 0.3834
Epoch 15 Step 151 Train Loss: 0.4141
Epoch 15 Step 201 Train Loss: 0.3637
Epoch 15 Step 251 Train Loss: 0.4871
Epoch 15 Step 301 Train Loss: 0.3340
Epoch 15 Step 351 Train Loss: 0.3926
Epoch 15 Step 401 Train Loss: 0.3738
Epoch 15 Step 451 Train Loss: 0.3736
Epoch 15 Step 501 Train Loss: 0.4210
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0025. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0020
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00030004626
test_unseen_single_pearson: 0.9994689608964639
test_unseen_single_mse_de: 0.002008946
test_unseen_single_pearson_de: 0.999635229956787
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5275349717351449
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.016666666666666666
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002112070435752352
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñá‚ñá‚ñÇ‚ñÉ‚ñÜ
wandb:                                                   val_de_mse ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÉ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00201
wandb:                                              test_de_pearson 0.99964
wandb:               test_frac_opposite_direction_top20_non_dropout 0.01667
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.0003
wandb:                                test_mse_top20_de_non_dropout 0.00211
wandb:                                                 test_pearson 0.99947
wandb:                                           test_pearson_delta 0.52753
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.01667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.0003
wandb:                                    test_unseen_single_mse_de 0.00201
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00211
wandb:                                   test_unseen_single_pearson 0.99947
wandb:                                test_unseen_single_pearson_de 0.99964
wandb:                             test_unseen_single_pearson_delta 0.52753
wandb:                                                 train_de_mse 0.00185
wandb:                                             train_de_pearson 0.99971
wandb:                                                    train_mse 0.00021
wandb:                                                train_pearson 0.99963
wandb:                                                training_loss 0.34844
wandb:                                                   val_de_mse 0.00248
wandb:                                               val_de_pearson 0.99958
wandb:                                                      val_mse 0.00024
wandb:                                                  val_pearson 0.99958
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396858_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/pq0qaw3f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_204242-pq0qaw3f/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_205404-bimsxhrz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396861_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/bimsxhrz
wandb: WARNING Serializing object of type ndarray that is 8011328 bytes
  0%|                                                                                       | 0/3671 [00:00<?, ?it/s]  0%|‚ñè                                                                              | 8/3671 [00:00<00:48, 75.16it/s]  0%|‚ñé                                                                             | 16/3671 [00:00<00:47, 77.29it/s]  1%|‚ñå                                                                             | 24/3671 [00:00<00:47, 76.82it/s]  1%|‚ñã                                                                             | 35/3671 [00:00<00:41, 86.72it/s]  1%|‚ñâ                                                                             | 45/3671 [00:00<00:40, 88.46it/s]  1%|‚ñà‚ñè                                                                            | 54/3671 [00:00<00:42, 86.11it/s]  2%|‚ñà‚ñç                                                                            | 65/3671 [00:00<00:39, 90.74it/s]  2%|‚ñà‚ñå                                                                            | 75/3671 [00:00<00:39, 90.17it/s]  2%|‚ñà‚ñä                                                                            | 85/3671 [00:00<00:39, 90.99it/s]  3%|‚ñà‚ñà                                                                            | 95/3671 [00:01<00:39, 90.71it/s]  3%|‚ñà‚ñà‚ñè                                                                          | 105/3671 [00:01<00:40, 87.56it/s]  3%|‚ñà‚ñà‚ñç                                                                          | 114/3671 [00:01<00:40, 87.71it/s]  3%|‚ñà‚ñà‚ñå                                                                          | 124/3671 [00:01<00:39, 90.01it/s]  4%|‚ñà‚ñà‚ñä                                                                          | 134/3671 [00:01<00:38, 90.79it/s]  4%|‚ñà‚ñà‚ñà                                                                          | 144/3671 [00:01<00:38, 91.14it/s]  4%|‚ñà‚ñà‚ñà‚ñè                                                                         | 154/3671 [00:01<00:39, 88.07it/s]  4%|‚ñà‚ñà‚ñà‚ñç                                                                         | 164/3671 [00:01<00:39, 89.34it/s]  5%|‚ñà‚ñà‚ñà‚ñã                                                                         | 174/3671 [00:01<00:37, 92.11it/s]  5%|‚ñà‚ñà‚ñà‚ñä                                                                         | 184/3671 [00:02<00:37, 92.24it/s]  5%|‚ñà‚ñà‚ñà‚ñà                                                                         | 194/3671 [00:02<00:37, 91.73it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñé                                                                        | 204/3671 [00:02<00:37, 91.64it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 214/3671 [00:02<00:37, 91.82it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñã                                                                        | 224/3671 [00:02<00:37, 91.37it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                        | 234/3671 [00:02<00:38, 88.55it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà                                                                        | 244/3671 [00:02<00:38, 88.55it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                       | 255/3671 [00:02<00:37, 91.97it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                       | 265/3671 [00:02<00:38, 89.15it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                       | 275/3671 [00:03<00:37, 89.49it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                       | 284/3671 [00:03<00:38, 88.62it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                      | 294/3671 [00:03<00:37, 90.49it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                      | 304/3671 [00:03<00:37, 89.53it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 314/3671 [00:03<00:37, 90.49it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                      | 324/3671 [00:03<00:38, 87.92it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                      | 335/3671 [00:03<00:36, 91.66it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                     | 345/3671 [00:03<00:37, 88.39it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                     | 355/3671 [00:03<00:37, 89.31it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                     | 366/3671 [00:04<00:35, 92.42it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                     | 376/3671 [00:04<00:36, 91.33it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                     | 386/3671 [00:04<00:36, 88.80it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                    | 396/3671 [00:04<00:35, 91.45it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                    | 406/3671 [00:04<00:35, 91.06it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                    | 416/3671 [00:04<00:36, 90.25it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                    | 426/3671 [00:04<00:36, 89.06it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                   | 436/3671 [00:04<00:36, 89.72it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                   | 446/3671 [00:04<00:35, 89.65it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                   | 456/3671 [00:05<00:35, 90.24it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                   | 466/3671 [00:05<00:35, 89.95it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                   | 476/3671 [00:05<00:35, 89.78it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                  | 486/3671 [00:05<00:35, 90.86it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                  | 496/3671 [00:05<00:35, 90.62it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                  | 506/3671 [00:05<00:34, 91.23it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                  | 516/3671 [00:05<00:34, 91.20it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 526/3671 [00:05<00:34, 91.00it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                 | 536/3671 [00:05<00:34, 91.07it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                 | 546/3671 [00:06<00:34, 90.15it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                 | 556/3671 [00:06<00:34, 90.43it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                 | 566/3671 [00:06<00:36, 85.71it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                 | 576/3671 [00:06<00:34, 89.50it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                | 586/3671 [00:06<00:33, 91.64it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                | 596/3671 [00:06<00:34, 89.67it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                | 606/3671 [00:06<00:33, 90.17it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                | 616/3671 [00:06<00:33, 90.17it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                               | 626/3671 [00:06<00:33, 90.59it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                               | 636/3671 [00:07<00:34, 88.67it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                               | 646/3671 [00:07<00:33, 91.39it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                               | 656/3671 [00:07<00:32, 91.88it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                               | 666/3671 [00:07<00:33, 89.04it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                              | 676/3671 [00:07<00:32, 91.97it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                              | 686/3671 [00:07<00:32, 92.05it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                              | 696/3671 [00:07<00:32, 90.82it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                              | 706/3671 [00:07<00:32, 91.18it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                              | 716/3671 [00:07<00:32, 90.84it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                             | 726/3671 [00:08<00:33, 87.61it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                             | 737/3671 [00:08<00:32, 89.23it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             | 747/3671 [00:08<00:32, 90.25it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                             | 758/3671 [00:08<00:30, 94.08it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                             | 768/3671 [00:08<00:31, 91.32it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                            | 779/3671 [00:08<00:30, 94.30it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                            | 789/3671 [00:08<00:30, 93.83it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                            | 799/3671 [00:08<00:30, 93.40it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                            | 809/3671 [00:08<00:30, 93.17it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                           | 819/3671 [00:09<00:31, 90.29it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                           | 830/3671 [00:09<00:31, 90.62it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                           | 841/3671 [00:09<00:31, 90.86it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                           | 852/3671 [00:09<00:30, 93.64it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 862/3671 [00:09<00:30, 93.45it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                          | 872/3671 [00:09<00:30, 92.82it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                          | 882/3671 [00:09<00:31, 89.58it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 893/3671 [00:09<00:30, 92.40it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                          | 903/3671 [00:09<00:30, 91.46it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                         | 913/3671 [00:10<00:30, 91.90it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                         | 923/3671 [00:10<00:31, 88.44it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                         | 932/3671 [00:10<00:31, 87.30it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                         | 941/3671 [00:10<00:31, 86.80it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                         | 950/3671 [00:10<00:32, 84.28it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                        | 960/3671 [00:10<00:31, 86.89it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                        | 969/3671 [00:10<00:32, 83.80it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                        | 979/3671 [00:10<00:30, 87.63it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                        | 988/3671 [00:10<00:31, 84.88it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                        | 998/3671 [00:11<00:31, 85.76it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                       | 1007/3671 [00:11<00:31, 85.83it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                       | 1017/3671 [00:11<00:30, 86.77it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                      | 1026/3671 [00:11<00:30, 86.23it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                      | 1036/3671 [00:11<00:30, 86.57it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                      | 1046/3671 [00:11<00:30, 86.24it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                      | 1056/3671 [00:11<00:29, 88.81it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                      | 1065/3671 [00:11<00:29, 88.04it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                     | 1074/3671 [00:11<00:29, 87.55it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 1083/3671 [00:12<00:29, 87.29it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                     | 1092/3671 [00:12<00:29, 86.83it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                     | 1101/3671 [00:12<00:29, 87.42it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                     | 1110/3671 [00:12<00:30, 84.79it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 1119/3671 [00:12<00:29, 85.37it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 1128/3671 [00:12<00:29, 86.41it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                    | 1137/3671 [00:12<00:29, 86.45it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                    | 1147/3671 [00:12<00:29, 86.62it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                    | 1156/3671 [00:12<00:29, 86.63it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                    | 1165/3671 [00:13<00:29, 86.39it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                   | 1175/3671 [00:13<00:28, 86.71it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                   | 1185/3671 [00:13<00:27, 89.57it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                   | 1194/3671 [00:13<00:27, 88.84it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                   | 1203/3671 [00:13<00:28, 85.70it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                   | 1213/3671 [00:13<00:28, 86.99it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                  | 1223/3671 [00:13<00:27, 90.33it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 1233/3671 [00:13<00:27, 87.51it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                  | 1242/3671 [00:13<00:27, 88.19it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                  | 1252/3671 [00:14<00:26, 91.42it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                 | 1262/3671 [00:14<00:27, 88.40it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                 | 1271/3671 [00:14<00:27, 88.34it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                 | 1281/3671 [00:14<00:26, 91.45it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                 | 1291/3671 [00:14<00:26, 90.51it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                 | 1301/3671 [00:14<00:27, 87.38it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                | 1311/3671 [00:14<00:26, 90.38it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                | 1321/3671 [00:14<00:27, 84.59it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                | 1332/3671 [00:14<00:25, 91.15it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                | 1342/3671 [00:15<00:25, 90.33it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                | 1352/3671 [00:15<00:25, 90.38it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                               | 1362/3671 [00:15<00:25, 90.06it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                               | 1372/3671 [00:15<00:26, 87.07it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                               | 1381/3671 [00:15<00:26, 87.45it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                               | 1391/3671 [00:15<00:25, 90.88it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 1401/3671 [00:15<00:25, 90.17it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                              | 1411/3671 [00:15<00:25, 89.51it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                              | 1420/3671 [00:15<00:25, 89.06it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 1429/3671 [00:15<00:25, 88.96it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 1438/3671 [00:16<00:25, 88.95it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                              | 1447/3671 [00:16<00:25, 86.20it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                             | 1456/3671 [00:16<00:25, 86.44it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                             | 1465/3671 [00:16<00:25, 86.39it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                             | 1474/3671 [00:16<00:26, 83.56it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                             | 1484/3671 [00:16<00:26, 84.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                             | 1493/3671 [00:16<00:26, 81.71it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                             | 1502/3671 [00:16<00:26, 83.25it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                            | 1513/3671 [00:16<00:24, 88.35it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                            | 1524/3671 [00:17<00:23, 92.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                            | 1535/3671 [00:17<00:22, 94.20it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                            | 1547/3671 [00:17<00:22, 96.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                           | 1557/3671 [00:17<00:21, 96.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                           | 1568/3671 [00:17<00:22, 95.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                           | 1579/3671 [00:17<00:21, 97.70it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                           | 1589/3671 [00:17<00:21, 96.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                           | 1599/3671 [00:17<00:21, 96.50it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                          | 1609/3671 [00:17<00:21, 95.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                          | 1619/3671 [00:18<00:21, 94.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                          | 1629/3671 [00:18<00:21, 93.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                          | 1639/3671 [00:18<00:21, 93.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                         | 1649/3671 [00:18<00:21, 93.25it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                         | 1659/3671 [00:18<00:21, 93.48it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                         | 1670/3671 [00:18<00:20, 95.91it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                         | 1680/3671 [00:18<00:20, 96.43it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                         | 1690/3671 [00:18<00:21, 93.27it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 1700/3671 [00:18<00:20, 94.63it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                        | 1710/3671 [00:19<00:21, 92.02it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                        | 1720/3671 [00:19<00:21, 91.67it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 1730/3671 [00:19<00:20, 93.30it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1740/3671 [00:19<00:20, 93.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 1750/3671 [00:19<00:20, 93.36it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1760/3671 [00:19<00:21, 88.50it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                       | 1770/3671 [00:19<00:20, 91.24it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                       | 1780/3671 [00:19<00:22, 84.88it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                       | 1789/3671 [00:19<00:23, 81.61it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                      | 1798/3671 [00:20<00:22, 82.67it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                      | 1807/3671 [00:20<00:22, 83.96it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                      | 1816/3671 [00:20<00:22, 80.88it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                      | 1825/3671 [00:20<00:22, 81.58it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                      | 1834/3671 [00:20<00:22, 81.51it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 1843/3671 [00:20<00:21, 83.22it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 1852/3671 [00:20<00:22, 82.33it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 1861/3671 [00:20<00:22, 82.23it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 1870/3671 [00:20<00:23, 77.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 1879/3671 [00:21<00:23, 75.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                     | 1887/3671 [00:21<00:23, 75.14it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 1895/3671 [00:21<00:24, 73.31it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 1904/3671 [00:21<00:24, 71.83it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 1912/3671 [00:21<00:24, 72.90it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                    | 1920/3671 [00:21<00:24, 70.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                    | 1929/3671 [00:21<00:23, 75.29it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 1937/3671 [00:21<00:24, 71.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 1945/3671 [00:22<00:23, 72.29it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 1954/3671 [00:22<00:23, 72.96it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 1962/3671 [00:22<00:23, 73.67it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 1970/3671 [00:22<00:22, 75.21it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 1978/3671 [00:22<00:22, 76.40it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 1986/3671 [00:22<00:21, 77.35it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 1994/3671 [00:22<00:21, 77.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 2003/3671 [00:22<00:21, 79.03it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 2011/3671 [00:22<00:20, 79.23it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 2020/3671 [00:22<00:20, 79.64it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 2029/3671 [00:23<00:20, 80.23it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 2038/3671 [00:23<00:20, 78.20it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 2048/3671 [00:23<00:19, 82.17it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 2058/3671 [00:23<00:20, 80.64it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 2067/3671 [00:23<00:19, 82.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 2076/3671 [00:23<00:19, 83.84it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 2085/3671 [00:23<00:18, 85.51it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 2094/3671 [00:23<00:19, 82.14it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 2103/3671 [00:23<00:18, 83.53it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 2113/3671 [00:24<00:17, 86.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 2122/3671 [00:24<00:18, 84.02it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 2131/3671 [00:24<00:18, 85.24it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 2140/3671 [00:24<00:18, 84.05it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 2149/3671 [00:24<00:20, 75.68it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 2159/3671 [00:24<00:18, 80.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 2168/3671 [00:24<00:18, 81.79it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 2177/3671 [00:24<00:19, 77.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 2186/3671 [00:24<00:18, 80.45it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 2198/3671 [00:25<00:16, 89.04it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 2208/3671 [00:25<00:16, 89.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 2218/3671 [00:25<00:15, 91.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 2229/3671 [00:25<00:15, 94.32it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 2239/3671 [00:25<00:15, 95.28it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 2249/3671 [00:25<00:14, 96.61it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 2260/3671 [00:25<00:14, 100.43it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 2271/3671 [00:25<00:13, 100.18it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 2282/3671 [00:25<00:13, 100.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 2293/3671 [00:26<00:14, 97.45it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 2304/3671 [00:26<00:13, 98.71it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 2314/3671 [00:26<00:13, 98.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 2326/3671 [00:26<00:13, 99.77it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 2336/3671 [00:26<00:13, 99.56it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 2346/3671 [00:26<00:13, 99.51it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 2358/3671 [00:26<00:13, 100.03it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 2369/3671 [00:26<00:13, 99.52it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 2380/3671 [00:26<00:12, 100.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 2391/3671 [00:27<00:12, 99.95it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 2402/3671 [00:27<00:12, 97.80it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 2414/3671 [00:27<00:12, 100.26it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 2426/3671 [00:27<00:12, 103.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 2437/3671 [00:27<00:12, 100.10it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 2449/3671 [00:27<00:12, 100.59it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 2460/3671 [00:28<00:26, 45.84it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 2468/3671 [00:28<00:32, 36.63it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 2496/3671 [00:28<00:20, 56.69it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 2504/3671 [00:28<00:21, 54.82it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 2511/3671 [00:29<00:24, 46.69it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2587/3671 [00:29<00:07, 152.53it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2614/3671 [00:29<00:08, 123.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2635/3671 [00:29<00:09, 110.65it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 2653/3671 [00:30<00:10, 100.37it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 2668/3671 [00:30<00:10, 93.97it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2681/3671 [00:30<00:11, 87.45it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2692/3671 [00:30<00:11, 81.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2702/3671 [00:30<00:12, 76.42it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2711/3671 [00:31<00:13, 73.66it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2719/3671 [00:31<00:13, 71.49it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2727/3671 [00:31<00:13, 68.62it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2735/3671 [00:31<00:13, 68.46it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2742/3671 [00:31<00:13, 67.42it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2749/3671 [00:31<00:13, 66.28it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2756/3671 [00:31<00:13, 65.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2763/3671 [00:31<00:13, 66.28it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2771/3671 [00:31<00:13, 68.21it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2782/3671 [00:32<00:11, 77.95it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2793/3671 [00:32<00:10, 85.08it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2804/3671 [00:32<00:09, 90.11it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2815/3671 [00:32<00:09, 93.60it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2827/3671 [00:32<00:08, 98.60it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2837/3671 [00:32<00:08, 98.68it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2847/3671 [00:32<00:08, 96.89it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2858/3671 [00:32<00:08, 98.03it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2869/3671 [00:32<00:08, 98.96it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2880/3671 [00:32<00:07, 100.19it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2891/3671 [00:33<00:07, 97.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2903/3671 [00:33<00:07, 102.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2914/3671 [00:33<00:07, 101.05it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2925/3671 [00:33<00:07, 101.61it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2936/3671 [00:33<00:07, 103.90it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2947/3671 [00:33<00:07, 94.50it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2957/3671 [00:33<00:08, 86.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2966/3671 [00:33<00:08, 87.60it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2975/3671 [00:34<00:08, 84.97it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2984/3671 [00:34<00:08, 83.17it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2993/3671 [00:34<00:08, 79.14it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3002/3671 [00:34<00:08, 81.92it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 3011/3671 [00:34<00:08, 81.17it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3020/3671 [00:34<00:08, 80.82it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3029/3671 [00:34<00:08, 77.55it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 3037/3671 [00:34<00:08, 77.61it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 3046/3671 [00:34<00:08, 77.62it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3054/3671 [00:35<00:07, 77.82it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3063/3671 [00:35<00:07, 78.18it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 3072/3671 [00:35<00:07, 80.96it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 3081/3671 [00:35<00:07, 77.60it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3090/3671 [00:35<00:07, 80.47it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3099/3671 [00:35<00:07, 80.02it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 3108/3671 [00:35<00:07, 79.97it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3117/3671 [00:35<00:06, 79.63it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 3126/3671 [00:35<00:06, 79.67it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 3134/3671 [00:36<00:06, 77.35it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3142/3671 [00:36<00:06, 77.99it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 3150/3671 [00:36<00:06, 78.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3158/3671 [00:36<00:06, 78.01it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 3168/3671 [00:36<00:06, 81.61it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3177/3671 [00:36<00:06, 81.06it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3186/3671 [00:36<00:06, 80.39it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3195/3671 [00:36<00:05, 80.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 3204/3671 [00:36<00:05, 80.42it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3213/3671 [00:37<00:05, 80.41it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3222/3671 [00:37<00:05, 80.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3231/3671 [00:37<00:05, 79.57it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3239/3671 [00:37<00:05, 79.03it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3247/3671 [00:37<00:05, 76.59it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3256/3671 [00:37<00:05, 79.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3265/3671 [00:37<00:05, 78.99it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3273/3671 [00:37<00:05, 76.54it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3282/3671 [00:37<00:05, 77.47it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3291/3671 [00:38<00:04, 80.62it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3300/3671 [00:38<00:04, 77.88it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3309/3671 [00:38<00:04, 81.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3318/3671 [00:38<00:04, 77.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3327/3671 [00:38<00:04, 80.93it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3336/3671 [00:38<00:04, 80.58it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3345/3671 [00:38<00:04, 80.01it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3354/3671 [00:38<00:04, 76.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3362/3671 [00:38<00:03, 77.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3371/3671 [00:39<00:03, 80.76it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3380/3671 [00:39<00:03, 77.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3388/3671 [00:39<00:03, 78.02it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3397/3671 [00:39<00:03, 79.02it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3406/3671 [00:39<00:03, 79.42it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3415/3671 [00:39<00:03, 79.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3424/3671 [00:39<00:03, 79.14it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3434/3671 [00:39<00:02, 82.33it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3443/3671 [00:39<00:02, 76.10it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3453/3671 [00:40<00:02, 82.33it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3462/3671 [00:40<00:02, 78.66it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3472/3671 [00:40<00:02, 79.21it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3481/3671 [00:40<00:02, 81.87it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3490/3671 [00:40<00:02, 81.39it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3499/3671 [00:40<00:02, 78.44it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3508/3671 [00:40<00:02, 78.65it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3516/3671 [00:40<00:01, 78.85it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3526/3671 [00:40<00:01, 81.86it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3535/3671 [00:41<00:01, 81.52it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3544/3671 [00:41<00:01, 81.11it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3553/3671 [00:41<00:01, 81.07it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3562/3671 [00:41<00:01, 81.14it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3571/3671 [00:41<00:01, 81.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3580/3671 [00:41<00:01, 78.01it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3589/3671 [00:41<00:01, 81.06it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3598/3671 [00:41<00:00, 80.83it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3607/3671 [00:41<00:00, 80.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3616/3671 [00:42<00:00, 80.65it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3625/3671 [00:42<00:00, 77.92it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3633/3671 [00:42<00:00, 77.90it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3642/3671 [00:42<00:00, 76.64it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3651/3671 [00:42<00:00, 78.33it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3661/3671 [00:42<00:00, 81.74it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3670/3671 [00:42<00:00, 81.80it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3671/3671 [00:42<00:00, 85.78it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.7439
Epoch 1 Step 51 Train Loss: 0.4586
Epoch 1 Step 101 Train Loss: 0.5047
Epoch 1 Step 151 Train Loss: 0.5235
Epoch 1 Step 201 Train Loss: 0.6409
Epoch 1 Step 251 Train Loss: 0.4671
Epoch 1 Step 301 Train Loss: 0.4436
Epoch 1 Step 351 Train Loss: 0.4846
Epoch 1 Step 401 Train Loss: 0.4279
Epoch 1: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0046. 
Epoch 2 Step 1 Train Loss: 0.5288
Epoch 2 Step 51 Train Loss: 0.6244
Epoch 2 Step 101 Train Loss: 0.4657
Epoch 2 Step 151 Train Loss: 0.4519
Epoch 2 Step 201 Train Loss: 0.4470
Epoch 2 Step 251 Train Loss: 0.4219
Epoch 2 Step 301 Train Loss: 0.5915
Epoch 2 Step 351 Train Loss: 0.5095
Epoch 2 Step 401 Train Loss: 0.5369
Epoch 2: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0040. 
Epoch 3 Step 1 Train Loss: 0.5227
Epoch 3 Step 51 Train Loss: 0.4816
Epoch 3 Step 101 Train Loss: 0.4833
Epoch 3 Step 151 Train Loss: 0.5421
Epoch 3 Step 201 Train Loss: 0.3770
Epoch 3 Step 251 Train Loss: 0.4672
Epoch 3 Step 301 Train Loss: 0.6105
Epoch 3 Step 351 Train Loss: 0.5137
Epoch 3 Step 401 Train Loss: 0.6139
Epoch 3: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0041. 
Epoch 4 Step 1 Train Loss: 0.4705
Epoch 4 Step 51 Train Loss: 0.4323
Epoch 4 Step 101 Train Loss: 0.3945
Epoch 4 Step 151 Train Loss: 0.5614
Epoch 4 Step 201 Train Loss: 0.4603
Epoch 4 Step 251 Train Loss: 0.5323
Epoch 4 Step 301 Train Loss: 0.4427
Epoch 4 Step 351 Train Loss: 0.5311
Epoch 4 Step 401 Train Loss: 0.5529
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0038. 
Epoch 5 Step 1 Train Loss: 0.4798
Epoch 5 Step 51 Train Loss: 0.4278
Epoch 5 Step 101 Train Loss: 0.5023
Epoch 5 Step 151 Train Loss: 0.5900
Epoch 5 Step 201 Train Loss: 0.7687
Epoch 5 Step 251 Train Loss: 0.4990
Epoch 5 Step 301 Train Loss: 0.7043
Epoch 5 Step 351 Train Loss: 0.4795
Epoch 5 Step 401 Train Loss: 0.3726
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0034. 
Epoch 6 Step 1 Train Loss: 0.5121
Epoch 6 Step 51 Train Loss: 0.5501
Epoch 6 Step 101 Train Loss: 0.3912
Epoch 6 Step 151 Train Loss: 0.4240
Epoch 6 Step 201 Train Loss: 0.4010
Epoch 6 Step 251 Train Loss: 0.5267
Epoch 6 Step 301 Train Loss: 0.4376
Epoch 6 Step 351 Train Loss: 0.5990
Epoch 6 Step 401 Train Loss: 0.7296
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0046. 
Epoch 7 Step 1 Train Loss: 0.4909
Epoch 7 Step 51 Train Loss: 0.5056
Epoch 7 Step 101 Train Loss: 0.4597
Epoch 7 Step 151 Train Loss: 0.4132
Epoch 7 Step 201 Train Loss: 0.3965
Epoch 7 Step 251 Train Loss: 0.5172
Epoch 7 Step 301 Train Loss: 0.4172
Epoch 7 Step 351 Train Loss: 0.6188
Epoch 7 Step 401 Train Loss: 0.4526
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0042. 
Epoch 8 Step 1 Train Loss: 0.6170
Epoch 8 Step 51 Train Loss: 0.5225
Epoch 8 Step 101 Train Loss: 0.5624
Epoch 8 Step 151 Train Loss: 0.4184
Epoch 8 Step 201 Train Loss: 0.4022
Epoch 8 Step 251 Train Loss: 0.5857
Epoch 8 Step 301 Train Loss: 0.5800
Epoch 8 Step 351 Train Loss: 0.4808
Epoch 8 Step 401 Train Loss: 0.5035
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0040. 
Epoch 9 Step 1 Train Loss: 0.5583
Epoch 9 Step 51 Train Loss: 0.4765
Epoch 9 Step 101 Train Loss: 0.4729
Epoch 9 Step 151 Train Loss: 0.4194
Epoch 9 Step 201 Train Loss: 0.5385
Epoch 9 Step 251 Train Loss: 0.4338
Epoch 9 Step 301 Train Loss: 0.5069
Epoch 9 Step 351 Train Loss: 0.4470
Epoch 9 Step 401 Train Loss: 0.4960
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0037. 
Epoch 10 Step 1 Train Loss: 0.5626
Epoch 10 Step 51 Train Loss: 0.4523
Epoch 10 Step 101 Train Loss: 0.5231
Epoch 10 Step 151 Train Loss: 0.5945
Epoch 10 Step 201 Train Loss: 0.4380
Epoch 10 Step 251 Train Loss: 0.4440
Epoch 10 Step 301 Train Loss: 0.5030
Epoch 10 Step 351 Train Loss: 0.5061
Epoch 10 Step 401 Train Loss: 0.4050
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0037. 
Epoch 11 Step 1 Train Loss: 0.5219
Epoch 11 Step 51 Train Loss: 0.6152
Epoch 11 Step 101 Train Loss: 0.5713
Epoch 11 Step 151 Train Loss: 0.5590
Epoch 11 Step 201 Train Loss: 0.4447
Epoch 11 Step 251 Train Loss: 0.4481
Epoch 11 Step 301 Train Loss: 0.3454
Epoch 11 Step 351 Train Loss: 0.4802
Epoch 11 Step 401 Train Loss: 0.5966
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0036. 
Epoch 12 Step 1 Train Loss: 0.4420
Epoch 12 Step 51 Train Loss: 0.5370
Epoch 12 Step 101 Train Loss: 0.5130
Epoch 12 Step 151 Train Loss: 0.5648
Epoch 12 Step 201 Train Loss: 0.5099
Epoch 12 Step 251 Train Loss: 0.5967
Epoch 12 Step 301 Train Loss: 0.5973
Epoch 12 Step 351 Train Loss: 0.6139
Epoch 12 Step 401 Train Loss: 0.4743
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0036. 
Epoch 13 Step 1 Train Loss: 0.7023
Epoch 13 Step 51 Train Loss: 0.5397
Epoch 13 Step 101 Train Loss: 0.4562
Epoch 13 Step 151 Train Loss: 0.4800
Epoch 13 Step 201 Train Loss: 0.4935
Epoch 13 Step 251 Train Loss: 0.4890
Epoch 13 Step 301 Train Loss: 0.4353
Epoch 13 Step 351 Train Loss: 0.5892
Epoch 13 Step 401 Train Loss: 0.4756
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0035. 
Epoch 14 Step 1 Train Loss: 0.5721
Epoch 14 Step 51 Train Loss: 0.5202
Epoch 14 Step 101 Train Loss: 0.4770
Epoch 14 Step 151 Train Loss: 0.3743
Epoch 14 Step 201 Train Loss: 0.5159
Epoch 14 Step 251 Train Loss: 0.4494
Epoch 14 Step 301 Train Loss: 0.4507
Epoch 14 Step 351 Train Loss: 0.4555
Epoch 14 Step 401 Train Loss: 0.5004
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0036. 
Epoch 15 Step 1 Train Loss: 0.4346
Epoch 15 Step 51 Train Loss: 0.5404
Epoch 15 Step 101 Train Loss: 0.4830
Epoch 15 Step 151 Train Loss: 0.5044
Epoch 15 Step 201 Train Loss: 0.5002
Epoch 15 Step 251 Train Loss: 0.5411
Epoch 15 Step 301 Train Loss: 0.6152
Epoch 15 Step 351 Train Loss: 0.4518
Epoch 15 Step 401 Train Loss: 0.4438
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0037. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0027
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00040135236
test_unseen_single_pearson: 0.9993435042569517
test_unseen_single_mse_de: 0.002742867
test_unseen_single_pearson_de: 0.999261804714371
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5321828373009718
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.075
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.003145329414618049
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.001 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00274
wandb:                                              test_de_pearson 0.99926
wandb:               test_frac_opposite_direction_top20_non_dropout 0.075
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.0004
wandb:                                test_mse_top20_de_non_dropout 0.00315
wandb:                                                 test_pearson 0.99934
wandb:                                           test_pearson_delta 0.53218
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.075
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.0004
wandb:                                    test_unseen_single_mse_de 0.00274
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00315
wandb:                                   test_unseen_single_pearson 0.99934
wandb:                                test_unseen_single_pearson_de 0.99926
wandb:                             test_unseen_single_pearson_delta 0.53218
wandb:                                                 train_de_mse 0.0025
wandb:                                             train_de_pearson 0.9995
wandb:                                                    train_mse 0.00036
wandb:                                                train_pearson 0.99942
wandb:                                                training_loss 0.40481
wandb:                                                   val_de_mse 0.00365
wandb:                                               val_de_pearson 0.99906
wandb:                                                      val_mse 0.00025
wandb:                                                  val_pearson 0.9996
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396861_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/bimsxhrz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_205404-bimsxhrz/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_210447-c870bx4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396861_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/c870bx4s
wandb: WARNING Serializing object of type ndarray that is 8011328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4292
Epoch 1 Step 51 Train Loss: 0.4591
Epoch 1 Step 101 Train Loss: 0.6338
Epoch 1 Step 151 Train Loss: 0.6921
Epoch 1 Step 201 Train Loss: 0.4828
Epoch 1 Step 251 Train Loss: 0.4406
Epoch 1 Step 301 Train Loss: 0.4989
Epoch 1 Step 351 Train Loss: 0.5161
Epoch 1 Step 401 Train Loss: 0.5401
Epoch 1 Step 451 Train Loss: 0.4961
Epoch 1: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0013. 
Epoch 2 Step 1 Train Loss: 0.6701
Epoch 2 Step 51 Train Loss: 0.5179
Epoch 2 Step 101 Train Loss: 0.5084
Epoch 2 Step 151 Train Loss: 0.4338
Epoch 2 Step 201 Train Loss: 0.6043
Epoch 2 Step 251 Train Loss: 0.4578
Epoch 2 Step 301 Train Loss: 0.4474
Epoch 2 Step 351 Train Loss: 0.5078
Epoch 2 Step 401 Train Loss: 0.4624
Epoch 2 Step 451 Train Loss: 0.5155
Epoch 2: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0008. 
Epoch 3 Step 1 Train Loss: 0.4757
Epoch 3 Step 51 Train Loss: 0.4480
Epoch 3 Step 101 Train Loss: 0.4167
Epoch 3 Step 151 Train Loss: 0.5289
Epoch 3 Step 201 Train Loss: 0.5272
Epoch 3 Step 251 Train Loss: 0.4988
Epoch 3 Step 301 Train Loss: 0.5287
Epoch 3 Step 351 Train Loss: 0.5412
Epoch 3 Step 401 Train Loss: 0.4595
Epoch 3 Step 451 Train Loss: 0.4583
Epoch 3: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0007. 
Epoch 4 Step 1 Train Loss: 0.5721
Epoch 4 Step 51 Train Loss: 0.4017
Epoch 4 Step 101 Train Loss: 0.5216
Epoch 4 Step 151 Train Loss: 0.5505
Epoch 4 Step 201 Train Loss: 0.6007
Epoch 4 Step 251 Train Loss: 0.3954
Epoch 4 Step 301 Train Loss: 0.5841
Epoch 4 Step 351 Train Loss: 0.4551
Epoch 4 Step 401 Train Loss: 0.5132
Epoch 4 Step 451 Train Loss: 0.3785
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0009. 
Epoch 5 Step 1 Train Loss: 0.5865
Epoch 5 Step 51 Train Loss: 0.4628
Epoch 5 Step 101 Train Loss: 0.4910
Epoch 5 Step 151 Train Loss: 0.5041
Epoch 5 Step 201 Train Loss: 0.5007
Epoch 5 Step 251 Train Loss: 0.5300
Epoch 5 Step 301 Train Loss: 0.5151
Epoch 5 Step 351 Train Loss: 0.5031
Epoch 5 Step 401 Train Loss: 0.5591
Epoch 5 Step 451 Train Loss: 0.4291
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0011. 
Epoch 6 Step 1 Train Loss: 0.4824
Epoch 6 Step 51 Train Loss: 0.4577
Epoch 6 Step 101 Train Loss: 0.4362
Epoch 6 Step 151 Train Loss: 0.4567
Epoch 6 Step 201 Train Loss: 0.4935
Epoch 6 Step 251 Train Loss: 0.5400
Epoch 6 Step 301 Train Loss: 0.5046
Epoch 6 Step 351 Train Loss: 0.4503
Epoch 6 Step 401 Train Loss: 0.4338
Epoch 6 Step 451 Train Loss: 0.4437
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0013. 
Epoch 7 Step 1 Train Loss: 0.4321
Epoch 7 Step 51 Train Loss: 0.5411
Epoch 7 Step 101 Train Loss: 0.5113
Epoch 7 Step 151 Train Loss: 0.5015
Epoch 7 Step 201 Train Loss: 0.4918
Epoch 7 Step 251 Train Loss: 0.5357
Epoch 7 Step 301 Train Loss: 0.4126
Epoch 7 Step 351 Train Loss: 0.4406
Epoch 7 Step 401 Train Loss: 0.4877
Epoch 7 Step 451 Train Loss: 0.6696
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0009. 
Epoch 8 Step 1 Train Loss: 0.5228
Epoch 8 Step 51 Train Loss: 0.5504
Epoch 8 Step 101 Train Loss: 0.5369
Epoch 8 Step 151 Train Loss: 0.4707
Epoch 8 Step 201 Train Loss: 0.4196
Epoch 8 Step 251 Train Loss: 0.5073
Epoch 8 Step 301 Train Loss: 0.4481
Epoch 8 Step 351 Train Loss: 0.4671
Epoch 8 Step 401 Train Loss: 0.4219
Epoch 8 Step 451 Train Loss: 0.4051
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0011. 
Epoch 9 Step 1 Train Loss: 0.3820
Epoch 9 Step 51 Train Loss: 0.4914
Epoch 9 Step 101 Train Loss: 0.5310
Epoch 9 Step 151 Train Loss: 0.4544
Epoch 9 Step 201 Train Loss: 0.5892
Epoch 9 Step 251 Train Loss: 0.5171
Epoch 9 Step 301 Train Loss: 0.3923
Epoch 9 Step 351 Train Loss: 0.3949
Epoch 9 Step 401 Train Loss: 0.4649
Epoch 9 Step 451 Train Loss: 0.5578
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0011. 
Epoch 10 Step 1 Train Loss: 0.4320
Epoch 10 Step 51 Train Loss: 0.4964
Epoch 10 Step 101 Train Loss: 0.4275
Epoch 10 Step 151 Train Loss: 0.4829
Epoch 10 Step 201 Train Loss: 0.4581
Epoch 10 Step 251 Train Loss: 0.4239
Epoch 10 Step 301 Train Loss: 0.5167
Epoch 10 Step 351 Train Loss: 0.5915
Epoch 10 Step 401 Train Loss: 0.5274
Epoch 10 Step 451 Train Loss: 0.4535
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0010. 
Epoch 11 Step 1 Train Loss: 0.4965
Epoch 11 Step 51 Train Loss: 0.4433
Epoch 11 Step 101 Train Loss: 0.4827
Epoch 11 Step 151 Train Loss: 0.4318
Epoch 11 Step 201 Train Loss: 0.4833
Epoch 11 Step 251 Train Loss: 0.5202
Epoch 11 Step 301 Train Loss: 0.5276
Epoch 11 Step 351 Train Loss: 0.3494
Epoch 11 Step 401 Train Loss: 0.5761
Epoch 11 Step 451 Train Loss: 0.4735
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0011. 
Epoch 12 Step 1 Train Loss: 0.5524
Epoch 12 Step 51 Train Loss: 0.4313
Epoch 12 Step 101 Train Loss: 0.5190
Epoch 12 Step 151 Train Loss: 0.5484
Epoch 12 Step 201 Train Loss: 0.4502
Epoch 12 Step 251 Train Loss: 0.4013
Epoch 12 Step 301 Train Loss: 0.5601
Epoch 12 Step 351 Train Loss: 0.5374
Epoch 12 Step 401 Train Loss: 0.4924
Epoch 12 Step 451 Train Loss: 0.4682
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0009. 
Epoch 13 Step 1 Train Loss: 0.4580
Epoch 13 Step 51 Train Loss: 0.3531
Epoch 13 Step 101 Train Loss: 0.4810
Epoch 13 Step 151 Train Loss: 0.5089
Epoch 13 Step 201 Train Loss: 0.4795
Epoch 13 Step 251 Train Loss: 0.4458
Epoch 13 Step 301 Train Loss: 0.5152
Epoch 13 Step 351 Train Loss: 0.4588
Epoch 13 Step 401 Train Loss: 0.4244
Epoch 13 Step 451 Train Loss: 0.5495
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0010. 
Epoch 14 Step 1 Train Loss: 0.4349
Epoch 14 Step 51 Train Loss: 0.5049
Epoch 14 Step 101 Train Loss: 0.5125
Epoch 14 Step 151 Train Loss: 0.4709
Epoch 14 Step 201 Train Loss: 0.4467
Epoch 14 Step 251 Train Loss: 0.4506
Epoch 14 Step 301 Train Loss: 0.4526
Epoch 14 Step 351 Train Loss: 0.4557
Epoch 14 Step 401 Train Loss: 0.4599
Epoch 14 Step 451 Train Loss: 0.6475
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0009. 
Epoch 15 Step 1 Train Loss: 0.4925
Epoch 15 Step 51 Train Loss: 0.4876
Epoch 15 Step 101 Train Loss: 0.4724
Epoch 15 Step 151 Train Loss: 0.4798
Epoch 15 Step 201 Train Loss: 0.4300
Epoch 15 Step 251 Train Loss: 0.4782
Epoch 15 Step 301 Train Loss: 0.5649
Epoch 15 Step 351 Train Loss: 0.6374
Epoch 15 Step 401 Train Loss: 0.4202
Epoch 15 Step 451 Train Loss: 0.4749
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0010. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0019
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.000444077
test_unseen_single_pearson: 0.9992796360684217
test_unseen_single_mse_de: 0.001867418
test_unseen_single_pearson_de: 0.9995660943511758
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5413288810566077
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.025
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9875
test_unseen_single_mse_top20_de_non_dropout: 0.0023107096910728654
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.008 MB of 0.023 MB uploadedwandb: / 0.014 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñà‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá
wandb:                                                training_loss ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00187
wandb:                                              test_de_pearson 0.99957
wandb:               test_frac_opposite_direction_top20_non_dropout 0.025
wandb:                          test_frac_sigma_below_1_non_dropout 0.9875
wandb:                                                     test_mse 0.00044
wandb:                                test_mse_top20_de_non_dropout 0.00231
wandb:                                                 test_pearson 0.99928
wandb:                                           test_pearson_delta 0.54133
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.025
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9875
wandb:                                       test_unseen_single_mse 0.00044
wandb:                                    test_unseen_single_mse_de 0.00187
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00231
wandb:                                   test_unseen_single_pearson 0.99928
wandb:                                test_unseen_single_pearson_de 0.99957
wandb:                             test_unseen_single_pearson_delta 0.54133
wandb:                                                 train_de_mse 0.00166
wandb:                                             train_de_pearson 0.99952
wandb:                                                    train_mse 0.00039
wandb:                                                train_pearson 0.99938
wandb:                                                training_loss 0.5073
wandb:                                                   val_de_mse 0.00095
wandb:                                               val_de_pearson 0.99981
wandb:                                                      val_mse 0.00047
wandb:                                                  val_pearson 0.99923
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396861_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/c870bx4s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_210447-c870bx4s/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_211431-zm6ajyzl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396861_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zm6ajyzl
wandb: WARNING Serializing object of type ndarray that is 8011328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5498
Epoch 1 Step 51 Train Loss: 0.6026
Epoch 1 Step 101 Train Loss: 0.5088
Epoch 1 Step 151 Train Loss: 0.4588
Epoch 1 Step 201 Train Loss: 0.4980
Epoch 1 Step 251 Train Loss: 0.4678
Epoch 1 Step 301 Train Loss: 0.4015
Epoch 1 Step 351 Train Loss: 0.5371
Epoch 1 Step 401 Train Loss: 0.4667
Epoch 1: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0043. 
Epoch 2 Step 1 Train Loss: 0.5352
Epoch 2 Step 51 Train Loss: 0.5266
Epoch 2 Step 101 Train Loss: 0.5522
Epoch 2 Step 151 Train Loss: 0.4961
Epoch 2 Step 201 Train Loss: 0.5178
Epoch 2 Step 251 Train Loss: 0.5688
Epoch 2 Step 301 Train Loss: 0.4788
Epoch 2 Step 351 Train Loss: 0.4442
Epoch 2 Step 401 Train Loss: 0.5431
Epoch 2: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0036. 
Epoch 3 Step 1 Train Loss: 0.4902
Epoch 3 Step 51 Train Loss: 0.3863
Epoch 3 Step 101 Train Loss: 0.4130
Epoch 3 Step 151 Train Loss: 0.4757
Epoch 3 Step 201 Train Loss: 0.4630
Epoch 3 Step 251 Train Loss: 0.4728
Epoch 3 Step 301 Train Loss: 0.5350
Epoch 3 Step 351 Train Loss: 0.5068
Epoch 3 Step 401 Train Loss: 0.4814
Epoch 3: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0030. 
Epoch 4 Step 1 Train Loss: 0.4271
Epoch 4 Step 51 Train Loss: 0.5082
Epoch 4 Step 101 Train Loss: 0.4955
Epoch 4 Step 151 Train Loss: 0.5996
Epoch 4 Step 201 Train Loss: 0.4149
Epoch 4 Step 251 Train Loss: 0.4413
Epoch 4 Step 301 Train Loss: 0.4485
Epoch 4 Step 351 Train Loss: 0.5656
Epoch 4 Step 401 Train Loss: 0.5017
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0029. 
Epoch 5 Step 1 Train Loss: 0.4258
Epoch 5 Step 51 Train Loss: 0.4599
Epoch 5 Step 101 Train Loss: 0.3669
Epoch 5 Step 151 Train Loss: 0.5351
Epoch 5 Step 201 Train Loss: 0.4830
Epoch 5 Step 251 Train Loss: 0.6191
Epoch 5 Step 301 Train Loss: 0.6074
Epoch 5 Step 351 Train Loss: 0.5292
Epoch 5 Step 401 Train Loss: 0.5052
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0046. 
Epoch 6 Step 1 Train Loss: 0.5424
Epoch 6 Step 51 Train Loss: 0.5813
Epoch 6 Step 101 Train Loss: 0.5912
Epoch 6 Step 151 Train Loss: 0.5448
Epoch 6 Step 201 Train Loss: 0.5451
Epoch 6 Step 251 Train Loss: 0.6068
Epoch 6 Step 301 Train Loss: 0.5877
Epoch 6 Step 351 Train Loss: 0.7358
Epoch 6 Step 401 Train Loss: 0.4164
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0023. 
Epoch 7 Step 1 Train Loss: 0.4925
Epoch 7 Step 51 Train Loss: 0.5031
Epoch 7 Step 101 Train Loss: 0.4286
Epoch 7 Step 151 Train Loss: 0.5233
Epoch 7 Step 201 Train Loss: 0.5110
Epoch 7 Step 251 Train Loss: 0.4783
Epoch 7 Step 301 Train Loss: 0.3988
Epoch 7 Step 351 Train Loss: 0.4889
Epoch 7 Step 401 Train Loss: 0.5176
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0023. 
Epoch 8 Step 1 Train Loss: 0.4478
Epoch 8 Step 51 Train Loss: 0.8650
Epoch 8 Step 101 Train Loss: 0.4249
Epoch 8 Step 151 Train Loss: 0.4442
Epoch 8 Step 201 Train Loss: 0.3885
Epoch 8 Step 251 Train Loss: 0.7354
Epoch 8 Step 301 Train Loss: 0.6371
Epoch 8 Step 351 Train Loss: 0.4928
Epoch 8 Step 401 Train Loss: 0.4042
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0026. 
Epoch 9 Step 1 Train Loss: 0.5201
Epoch 9 Step 51 Train Loss: 0.4797
Epoch 9 Step 101 Train Loss: 0.4727
Epoch 9 Step 151 Train Loss: 0.4873
Epoch 9 Step 201 Train Loss: 0.4707
Epoch 9 Step 251 Train Loss: 0.4326
Epoch 9 Step 301 Train Loss: 0.4912
Epoch 9 Step 351 Train Loss: 0.4193
Epoch 9 Step 401 Train Loss: 0.4740
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0020. 
Epoch 10 Step 1 Train Loss: 0.4618
Epoch 10 Step 51 Train Loss: 0.4392
Epoch 10 Step 101 Train Loss: 0.5270
Epoch 10 Step 151 Train Loss: 0.4710
Epoch 10 Step 201 Train Loss: 0.5310
Epoch 10 Step 251 Train Loss: 0.5177
Epoch 10 Step 301 Train Loss: 0.5496
Epoch 10 Step 351 Train Loss: 0.5672
Epoch 10 Step 401 Train Loss: 0.4729
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0026. 
Epoch 11 Step 1 Train Loss: 0.4207
Epoch 11 Step 51 Train Loss: 0.4288
Epoch 11 Step 101 Train Loss: 0.5087
Epoch 11 Step 151 Train Loss: 0.5136
Epoch 11 Step 201 Train Loss: 0.3688
Epoch 11 Step 251 Train Loss: 0.5383
Epoch 11 Step 301 Train Loss: 0.6432
Epoch 11 Step 351 Train Loss: 0.6051
Epoch 11 Step 401 Train Loss: 0.4479
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0021. 
Epoch 12 Step 1 Train Loss: 0.4873
Epoch 12 Step 51 Train Loss: 0.4533
Epoch 12 Step 101 Train Loss: 0.4445
Epoch 12 Step 151 Train Loss: 0.5282
Epoch 12 Step 201 Train Loss: 0.4175
Epoch 12 Step 251 Train Loss: 0.5883
Epoch 12 Step 301 Train Loss: 0.4314
Epoch 12 Step 351 Train Loss: 0.5174
Epoch 12 Step 401 Train Loss: 0.4248
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0020. 
Epoch 13 Step 1 Train Loss: 0.4207
Epoch 13 Step 51 Train Loss: 0.4454
Epoch 13 Step 101 Train Loss: 0.6698
Epoch 13 Step 151 Train Loss: 0.4749
Epoch 13 Step 201 Train Loss: 0.4552
Epoch 13 Step 251 Train Loss: 0.5090
Epoch 13 Step 301 Train Loss: 0.4465
Epoch 13 Step 351 Train Loss: 0.4688
Epoch 13 Step 401 Train Loss: 0.4615
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0023. 
Epoch 14 Step 1 Train Loss: 0.5454
Epoch 14 Step 51 Train Loss: 0.4595
Epoch 14 Step 101 Train Loss: 0.5205
Epoch 14 Step 151 Train Loss: 0.4411
Epoch 14 Step 201 Train Loss: 0.5186
Epoch 14 Step 251 Train Loss: 0.7060
Epoch 14 Step 301 Train Loss: 0.5119
Epoch 14 Step 351 Train Loss: 0.4955
Epoch 14 Step 401 Train Loss: 0.3880
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0023. 
Epoch 15 Step 1 Train Loss: 0.4923
Epoch 15 Step 51 Train Loss: 0.5665
Epoch 15 Step 101 Train Loss: 0.5470
Epoch 15 Step 151 Train Loss: 0.5202
Epoch 15 Step 201 Train Loss: 0.5955
Epoch 15 Step 251 Train Loss: 0.4619
Epoch 15 Step 301 Train Loss: 0.4323
Epoch 15 Step 351 Train Loss: 0.5472
Epoch 15 Step 401 Train Loss: 0.5015
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0026. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0018
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00045763515
test_unseen_single_pearson: 0.9992733245176817
test_unseen_single_mse_de: 0.0018030871
test_unseen_single_pearson_de: 0.9993707427901449
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5710617427187973
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.05
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.0018030870137062595
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.003 MB of 0.022 MB uploadedwandb: / 0.021 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:                                               val_de_pearson ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñÜ
wandb:                                                      val_mse ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.0018
wandb:                                              test_de_pearson 0.99937
wandb:               test_frac_opposite_direction_top20_non_dropout 0.05
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00046
wandb:                                test_mse_top20_de_non_dropout 0.0018
wandb:                                                 test_pearson 0.99927
wandb:                                           test_pearson_delta 0.57106
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.05
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00046
wandb:                                    test_unseen_single_mse_de 0.0018
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0018
wandb:                                   test_unseen_single_pearson 0.99927
wandb:                                test_unseen_single_pearson_de 0.99937
wandb:                             test_unseen_single_pearson_delta 0.57106
wandb:                                                 train_de_mse 0.00199
wandb:                                             train_de_pearson 0.99948
wandb:                                                    train_mse 0.00043
wandb:                                                train_pearson 0.9993
wandb:                                                training_loss 0.52977
wandb:                                                   val_de_mse 0.00259
wandb:                                               val_de_pearson 0.99968
wandb:                                                      val_mse 0.00061
wandb:                                                  val_pearson 0.99903
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396861_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/zm6ajyzl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_211431-zm6ajyzl/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_212422-99jdjqrv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396861_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/99jdjqrv
wandb: WARNING Serializing object of type ndarray that is 8011328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5251
Epoch 1 Step 51 Train Loss: 0.5028
Epoch 1 Step 101 Train Loss: 0.6249
Epoch 1 Step 151 Train Loss: 0.5160
Epoch 1 Step 201 Train Loss: 0.5993
Epoch 1 Step 251 Train Loss: 0.4379
Epoch 1 Step 301 Train Loss: 0.4283
Epoch 1 Step 351 Train Loss: 0.5629
Epoch 1 Step 401 Train Loss: 0.4790
Epoch 1: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0007. 
Epoch 2 Step 1 Train Loss: 0.5717
Epoch 2 Step 51 Train Loss: 0.4809
Epoch 2 Step 101 Train Loss: 0.4170
Epoch 2 Step 151 Train Loss: 0.4975
Epoch 2 Step 201 Train Loss: 0.4688
Epoch 2 Step 251 Train Loss: 0.6920
Epoch 2 Step 301 Train Loss: 0.4074
Epoch 2 Step 351 Train Loss: 0.4998
Epoch 2 Step 401 Train Loss: 0.4421
Epoch 2: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0051 Validation Top 20 DE MSE: 0.0014. 
Epoch 3 Step 1 Train Loss: 0.5862
Epoch 3 Step 51 Train Loss: 0.4331
Epoch 3 Step 101 Train Loss: 0.4496
Epoch 3 Step 151 Train Loss: 0.5743
Epoch 3 Step 201 Train Loss: 0.5370
Epoch 3 Step 251 Train Loss: 0.5740
Epoch 3 Step 301 Train Loss: 0.5722
Epoch 3 Step 351 Train Loss: 0.4354
Epoch 3 Step 401 Train Loss: 0.4700
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0008. 
Epoch 4 Step 1 Train Loss: 0.4642
Epoch 4 Step 51 Train Loss: 0.6010
Epoch 4 Step 101 Train Loss: 0.4123
Epoch 4 Step 151 Train Loss: 0.4057
Epoch 4 Step 201 Train Loss: 0.5279
Epoch 4 Step 251 Train Loss: 0.4945
Epoch 4 Step 301 Train Loss: 0.4780
Epoch 4 Step 351 Train Loss: 0.4611
Epoch 4 Step 401 Train Loss: 0.4391
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0008. 
Epoch 5 Step 1 Train Loss: 0.4737
Epoch 5 Step 51 Train Loss: 0.5199
Epoch 5 Step 101 Train Loss: 0.4300
Epoch 5 Step 151 Train Loss: 0.4948
Epoch 5 Step 201 Train Loss: 0.5614
Epoch 5 Step 251 Train Loss: 0.4863
Epoch 5 Step 301 Train Loss: 0.5482
Epoch 5 Step 351 Train Loss: 0.4183
Epoch 5 Step 401 Train Loss: 0.4718
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0008. 
Epoch 6 Step 1 Train Loss: 0.4580
Epoch 6 Step 51 Train Loss: 0.4392
Epoch 6 Step 101 Train Loss: 0.4482
Epoch 6 Step 151 Train Loss: 0.5139
Epoch 6 Step 201 Train Loss: 0.5746
Epoch 6 Step 251 Train Loss: 0.5142
Epoch 6 Step 301 Train Loss: 0.4622
Epoch 6 Step 351 Train Loss: 0.4623
Epoch 6 Step 401 Train Loss: 0.4655
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0007. 
Epoch 7 Step 1 Train Loss: 0.7907
Epoch 7 Step 51 Train Loss: 0.5192
Epoch 7 Step 101 Train Loss: 0.4556
Epoch 7 Step 151 Train Loss: 0.4602
Epoch 7 Step 201 Train Loss: 0.4565
Epoch 7 Step 251 Train Loss: 0.5180
Epoch 7 Step 301 Train Loss: 0.4452
Epoch 7 Step 351 Train Loss: 0.5369
Epoch 7 Step 401 Train Loss: 0.4242
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0007. 
Epoch 8 Step 1 Train Loss: 0.5528
Epoch 8 Step 51 Train Loss: 0.4312
Epoch 8 Step 101 Train Loss: 0.4113
Epoch 8 Step 151 Train Loss: 0.4227
Epoch 8 Step 201 Train Loss: 0.5918
Epoch 8 Step 251 Train Loss: 0.5039
Epoch 8 Step 301 Train Loss: 0.4808
Epoch 8 Step 351 Train Loss: 0.5277
Epoch 8 Step 401 Train Loss: 0.4353
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0010. 
Epoch 9 Step 1 Train Loss: 0.4496
Epoch 9 Step 51 Train Loss: 0.6554
Epoch 9 Step 101 Train Loss: 0.4347
Epoch 9 Step 151 Train Loss: 0.4960
Epoch 9 Step 201 Train Loss: 0.4382
Epoch 9 Step 251 Train Loss: 0.5257
Epoch 9 Step 301 Train Loss: 0.5317
Epoch 9 Step 351 Train Loss: 0.5602
Epoch 9 Step 401 Train Loss: 0.5107
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0008. 
Epoch 10 Step 1 Train Loss: 0.4424
Epoch 10 Step 51 Train Loss: 0.5971
Epoch 10 Step 101 Train Loss: 0.5230
Epoch 10 Step 151 Train Loss: 0.6570
Epoch 10 Step 201 Train Loss: 0.5062
Epoch 10 Step 251 Train Loss: 0.5160
Epoch 10 Step 301 Train Loss: 0.4010
Epoch 10 Step 351 Train Loss: 0.4012
Epoch 10 Step 401 Train Loss: 0.5193
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0008. 
Epoch 11 Step 1 Train Loss: 0.4964
Epoch 11 Step 51 Train Loss: 0.5125
Epoch 11 Step 101 Train Loss: 0.4489
Epoch 11 Step 151 Train Loss: 0.4702
Epoch 11 Step 201 Train Loss: 0.4767
Epoch 11 Step 251 Train Loss: 0.5912
Epoch 11 Step 301 Train Loss: 0.5947
Epoch 11 Step 351 Train Loss: 0.4915
Epoch 11 Step 401 Train Loss: 0.4709
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0008. 
Epoch 12 Step 1 Train Loss: 0.4821
Epoch 12 Step 51 Train Loss: 0.5128
Epoch 12 Step 101 Train Loss: 0.4943
Epoch 12 Step 151 Train Loss: 0.4707
Epoch 12 Step 201 Train Loss: 0.5234
Epoch 12 Step 251 Train Loss: 0.4917
Epoch 12 Step 301 Train Loss: 0.4254
Epoch 12 Step 351 Train Loss: 0.4793
Epoch 12 Step 401 Train Loss: 0.4219
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0008. 
Epoch 13 Step 1 Train Loss: 0.5711
Epoch 13 Step 51 Train Loss: 0.5040
Epoch 13 Step 101 Train Loss: 0.4442
Epoch 13 Step 151 Train Loss: 0.4305
Epoch 13 Step 201 Train Loss: 0.5249
Epoch 13 Step 251 Train Loss: 0.5491
Epoch 13 Step 301 Train Loss: 0.5480
Epoch 13 Step 351 Train Loss: 0.4350
Epoch 13 Step 401 Train Loss: 0.3559
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0009. 
Epoch 14 Step 1 Train Loss: 0.4253
Epoch 14 Step 51 Train Loss: 0.4701
Epoch 14 Step 101 Train Loss: 0.4376
Epoch 14 Step 151 Train Loss: 0.4971
Epoch 14 Step 201 Train Loss: 0.4426
Epoch 14 Step 251 Train Loss: 0.6147
Epoch 14 Step 301 Train Loss: 0.4496
Epoch 14 Step 351 Train Loss: 0.5475
Epoch 14 Step 401 Train Loss: 0.4122
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0009. 
Epoch 15 Step 1 Train Loss: 0.5218
Epoch 15 Step 51 Train Loss: 0.5413
Epoch 15 Step 101 Train Loss: 0.6002
Epoch 15 Step 151 Train Loss: 0.4904
Epoch 15 Step 201 Train Loss: 0.4358
Epoch 15 Step 251 Train Loss: 0.4648
Epoch 15 Step 301 Train Loss: 0.5144
Epoch 15 Step 351 Train Loss: 0.4921
Epoch 15 Step 401 Train Loss: 0.4837
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0009. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0019
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.000364909
test_unseen_single_pearson: 0.9994037538638445
test_unseen_single_mse_de: 0.0019230924
test_unseen_single_pearson_de: 0.999511880309611
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5767397265782775
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.037500000000000006
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002231196276783289
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.001 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ
wandb:                                               val_de_pearson ‚ñá‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00192
wandb:                                              test_de_pearson 0.99951
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0375
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00036
wandb:                                test_mse_top20_de_non_dropout 0.00223
wandb:                                                 test_pearson 0.9994
wandb:                                           test_pearson_delta 0.57674
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00036
wandb:                                    test_unseen_single_mse_de 0.00192
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00223
wandb:                                   test_unseen_single_pearson 0.9994
wandb:                                test_unseen_single_pearson_de 0.99951
wandb:                             test_unseen_single_pearson_delta 0.57674
wandb:                                                 train_de_mse 0.00206
wandb:                                             train_de_pearson 0.99952
wandb:                                                    train_mse 0.00028
wandb:                                                train_pearson 0.99954
wandb:                                                training_loss 0.4512
wandb:                                                   val_de_mse 0.0009
wandb:                                               val_de_pearson 0.99981
wandb:                                                      val_mse 0.00045
wandb:                                                  val_pearson 0.99926
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396861_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/99jdjqrv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_212422-99jdjqrv/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241028_213328-x3o9h518
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_Dixit_GSM2396861_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/x3o9h518
wandb: WARNING Serializing object of type ndarray that is 8011328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5273
Epoch 1 Step 51 Train Loss: 0.4232
Epoch 1 Step 101 Train Loss: 0.5682
Epoch 1 Step 151 Train Loss: 0.5824
Epoch 1 Step 201 Train Loss: 0.4011
Epoch 1 Step 251 Train Loss: 0.5466
Epoch 1 Step 301 Train Loss: 0.3808
Epoch 1 Step 351 Train Loss: 0.4904
Epoch 1: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0088. 
Epoch 2 Step 1 Train Loss: 0.4402
Epoch 2 Step 51 Train Loss: 0.5752
Epoch 2 Step 101 Train Loss: 0.5938
Epoch 2 Step 151 Train Loss: 0.7477
Epoch 2 Step 201 Train Loss: 0.5137
Epoch 2 Step 251 Train Loss: 0.5339
Epoch 2 Step 301 Train Loss: 0.5888
Epoch 2 Step 351 Train Loss: 0.4725
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0028. 
Epoch 3 Step 1 Train Loss: 0.4632
Epoch 3 Step 51 Train Loss: 0.4727
Epoch 3 Step 101 Train Loss: 0.5457
Epoch 3 Step 151 Train Loss: 0.5176
Epoch 3 Step 201 Train Loss: 0.6919
Epoch 3 Step 251 Train Loss: 0.5781
Epoch 3 Step 301 Train Loss: 0.5577
Epoch 3 Step 351 Train Loss: 0.5303
Epoch 3: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0036. 
Epoch 4 Step 1 Train Loss: 0.4542
Epoch 4 Step 51 Train Loss: 0.4415
Epoch 4 Step 101 Train Loss: 0.4383
Epoch 4 Step 151 Train Loss: 0.5944
Epoch 4 Step 201 Train Loss: 0.4395
Epoch 4 Step 251 Train Loss: 0.5654
Epoch 4 Step 301 Train Loss: 0.5501
Epoch 4 Step 351 Train Loss: 0.4333
Epoch 4: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0046. 
Epoch 5 Step 1 Train Loss: 0.4170
Epoch 5 Step 51 Train Loss: 0.4503
Epoch 5 Step 101 Train Loss: 0.4775
Epoch 5 Step 151 Train Loss: 0.4906
Epoch 5 Step 201 Train Loss: 0.7074
Epoch 5 Step 251 Train Loss: 0.4572
Epoch 5 Step 301 Train Loss: 0.5295
Epoch 5 Step 351 Train Loss: 0.5616
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0035. 
Epoch 6 Step 1 Train Loss: 0.6427
Epoch 6 Step 51 Train Loss: 0.5254
Epoch 6 Step 101 Train Loss: 0.5333
Epoch 6 Step 151 Train Loss: 0.5177
Epoch 6 Step 201 Train Loss: 0.5363
Epoch 6 Step 251 Train Loss: 0.5159
Epoch 6 Step 301 Train Loss: 0.5404
Epoch 6 Step 351 Train Loss: 0.4655
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0040. 
Epoch 7 Step 1 Train Loss: 0.5965
Epoch 7 Step 51 Train Loss: 0.5006
Epoch 7 Step 101 Train Loss: 0.6125
Epoch 7 Step 151 Train Loss: 0.4587
Epoch 7 Step 201 Train Loss: 0.4882
Epoch 7 Step 251 Train Loss: 0.5709
Epoch 7 Step 301 Train Loss: 0.5412
Epoch 7 Step 351 Train Loss: 0.6043
Epoch 7: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0039. 
Epoch 8 Step 1 Train Loss: 0.5302
Epoch 8 Step 51 Train Loss: 0.5040
Epoch 8 Step 101 Train Loss: 0.4682
Epoch 8 Step 151 Train Loss: 0.5045
Epoch 8 Step 201 Train Loss: 0.6479
Epoch 8 Step 251 Train Loss: 0.4855
Epoch 8 Step 301 Train Loss: 0.6272
Epoch 8 Step 351 Train Loss: 0.4991
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0038. 
Epoch 9 Step 1 Train Loss: 0.4602
Epoch 9 Step 51 Train Loss: 0.4970
Epoch 9 Step 101 Train Loss: 0.7625
Epoch 9 Step 151 Train Loss: 0.4187
Epoch 9 Step 201 Train Loss: 0.4511
Epoch 9 Step 251 Train Loss: 0.5884
Epoch 9 Step 301 Train Loss: 0.4418
Epoch 9 Step 351 Train Loss: 0.5329
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0038. 
Epoch 10 Step 1 Train Loss: 0.4502
Epoch 10 Step 51 Train Loss: 0.4798
Epoch 10 Step 101 Train Loss: 0.4788
Epoch 10 Step 151 Train Loss: 0.5883
Epoch 10 Step 201 Train Loss: 0.5208
Epoch 10 Step 251 Train Loss: 0.5131
Epoch 10 Step 301 Train Loss: 0.5076
Epoch 10 Step 351 Train Loss: 0.4846
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0038. 
Epoch 11 Step 1 Train Loss: 0.5333
Epoch 11 Step 51 Train Loss: 0.4993
Epoch 11 Step 101 Train Loss: 0.4898
Epoch 11 Step 151 Train Loss: 0.5550
Epoch 11 Step 201 Train Loss: 0.4865
Epoch 11 Step 251 Train Loss: 0.8812
Epoch 11 Step 301 Train Loss: 0.4729
Epoch 11 Step 351 Train Loss: 0.5840
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0038. 
Epoch 12 Step 1 Train Loss: 0.4553
Epoch 12 Step 51 Train Loss: 0.5525
Epoch 12 Step 101 Train Loss: 0.4059
Epoch 12 Step 151 Train Loss: 0.5035
Epoch 12 Step 201 Train Loss: 0.5331
Epoch 12 Step 251 Train Loss: 0.5452
Epoch 12 Step 301 Train Loss: 0.4693
Epoch 12 Step 351 Train Loss: 0.4512
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0038. 
Epoch 13 Step 1 Train Loss: 0.4835
Epoch 13 Step 51 Train Loss: 0.4249
Epoch 13 Step 101 Train Loss: 0.6226
Epoch 13 Step 151 Train Loss: 0.4488
Epoch 13 Step 201 Train Loss: 0.5524
Epoch 13 Step 251 Train Loss: 0.5159
Epoch 13 Step 301 Train Loss: 0.6137
Epoch 13 Step 351 Train Loss: 0.5455
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0037. 
Epoch 14 Step 1 Train Loss: 0.3897
Epoch 14 Step 51 Train Loss: 0.3951
Epoch 14 Step 101 Train Loss: 0.4661
Epoch 14 Step 151 Train Loss: 0.5373
Epoch 14 Step 201 Train Loss: 0.5107
Epoch 14 Step 251 Train Loss: 0.6223
Epoch 14 Step 301 Train Loss: 0.4490
Epoch 14 Step 351 Train Loss: 0.5206
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0037. 
Epoch 15 Step 1 Train Loss: 0.4916
Epoch 15 Step 51 Train Loss: 0.6711
Epoch 15 Step 101 Train Loss: 0.5916
Epoch 15 Step 151 Train Loss: 0.4850
Epoch 15 Step 201 Train Loss: 0.5439
Epoch 15 Step 251 Train Loss: 0.5919
Epoch 15 Step 301 Train Loss: 0.4374
Epoch 15 Step 351 Train Loss: 0.5207
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0037. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0022
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00051573745
test_unseen_single_pearson: 0.9991576624517607
test_unseen_single_mse_de: 0.0022371518
test_unseen_single_pearson_de: 0.9993749264714131
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5094466422316162
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.05
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.0025005532380433532
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.016 MB of 0.022 MB uploadedwandb: - 0.016 MB of 0.022 MB uploadedwandb: \ 0.016 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                training_loss ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñá‚ñà‚ñÑ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00224
wandb:                                              test_de_pearson 0.99937
wandb:               test_frac_opposite_direction_top20_non_dropout 0.05
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00052
wandb:                                test_mse_top20_de_non_dropout 0.0025
wandb:                                                 test_pearson 0.99916
wandb:                                           test_pearson_delta 0.50945
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.05
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00052
wandb:                                    test_unseen_single_mse_de 0.00224
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0025
wandb:                                   test_unseen_single_pearson 0.99916
wandb:                                test_unseen_single_pearson_de 0.99937
wandb:                             test_unseen_single_pearson_delta 0.50945
wandb:                                                 train_de_mse 0.00179
wandb:                                             train_de_pearson 0.99955
wandb:                                                    train_mse 0.00051
wandb:                                                train_pearson 0.99918
wandb:                                                training_loss 0.4275
wandb:                                                   val_de_mse 0.00367
wandb:                                               val_de_pearson 0.99898
wandb:                                                      val_mse 0.00053
wandb:                                                  val_pearson 0.99913
wandb: 
wandb: üöÄ View run scbert_Dixit_GSM2396861_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/x3o9h518
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241028_213328-x3o9h518/logs
