Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
['LYL1+IER5L' 'IER5L+ctrl' 'KIAA1804+ctrl']
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:9
combo_seen1:52
combo_seen2:17
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241029_081039-dn3wblvv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_NormanWeissman2019_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/dn3wblvv
wandb: WARNING Serializing object of type ndarray that is 8059328 bytes
  0%|                                                                                       | 0/3397 [00:00<?, ?it/s]  0%|‚ñè                                                                              | 6/3397 [00:00<01:00, 55.94it/s]  0%|‚ñé                                                                             | 15/3397 [00:00<00:47, 71.67it/s]  1%|‚ñå                                                                             | 23/3397 [00:00<00:48, 68.86it/s]  1%|‚ñä                                                                             | 34/3397 [00:00<00:41, 80.92it/s]  1%|‚ñâ                                                                             | 43/3397 [00:00<00:43, 77.29it/s]  2%|‚ñà‚ñè                                                                            | 53/3397 [00:00<00:40, 83.22it/s]  2%|‚ñà‚ñç                                                                            | 62/3397 [00:00<00:39, 83.95it/s]  2%|‚ñà‚ñã                                                                            | 71/3397 [00:00<00:39, 83.99it/s]  2%|‚ñà‚ñä                                                                            | 80/3397 [00:00<00:39, 84.53it/s]  3%|‚ñà‚ñà                                                                            | 89/3397 [00:01<00:39, 84.81it/s]  3%|‚ñà‚ñà‚ñé                                                                           | 98/3397 [00:01<00:38, 84.69it/s]  3%|‚ñà‚ñà‚ñç                                                                          | 107/3397 [00:01<00:38, 85.37it/s]  3%|‚ñà‚ñà‚ñã                                                                          | 116/3397 [00:01<00:38, 85.25it/s]  4%|‚ñà‚ñà‚ñä                                                                          | 125/3397 [00:01<00:38, 85.60it/s]  4%|‚ñà‚ñà‚ñà                                                                          | 134/3397 [00:01<00:37, 86.20it/s]  4%|‚ñà‚ñà‚ñà‚ñè                                                                         | 143/3397 [00:01<00:37, 86.07it/s]  4%|‚ñà‚ñà‚ñà‚ñç                                                                         | 152/3397 [00:01<00:39, 82.83it/s]  5%|‚ñà‚ñà‚ñà‚ñã                                                                         | 161/3397 [00:01<00:39, 82.87it/s]  5%|‚ñà‚ñà‚ñà‚ñä                                                                         | 170/3397 [00:02<00:39, 82.15it/s]  5%|‚ñà‚ñà‚ñà‚ñà                                                                         | 179/3397 [00:02<00:40, 79.24it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñè                                                                        | 187/3397 [00:02<00:42, 76.29it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñç                                                                        | 195/3397 [00:02<00:41, 76.26it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñå                                                                        | 203/3397 [00:02<00:44, 72.13it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                        | 211/3397 [00:02<00:46, 69.10it/s]  6%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                        | 219/3397 [00:02<00:44, 71.23it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                       | 227/3397 [00:02<00:49, 64.57it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                       | 236/3397 [00:03<00:45, 69.08it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                       | 244/3397 [00:03<00:44, 70.43it/s]  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                       | 252/3397 [00:03<00:45, 69.43it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                       | 260/3397 [00:03<00:45, 68.58it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                       | 269/3397 [00:03<00:43, 72.07it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                      | 277/3397 [00:03<00:43, 71.53it/s]  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                      | 285/3397 [00:03<00:43, 71.98it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                      | 293/3397 [00:03<00:41, 73.98it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                      | 301/3397 [00:03<00:42, 72.84it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                      | 309/3397 [00:04<00:42, 72.58it/s]  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                     | 317/3397 [00:04<00:41, 73.79it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                     | 325/3397 [00:04<00:41, 73.90it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                     | 333/3397 [00:04<00:42, 72.04it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                     | 341/3397 [00:04<00:41, 73.25it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                     | 349/3397 [00:04<00:41, 74.05it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                     | 357/3397 [00:04<00:41, 72.76it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                    | 365/3397 [00:04<00:41, 73.88it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                    | 373/3397 [00:04<00:41, 73.48it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                    | 381/3397 [00:05<00:41, 71.85it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                    | 389/3397 [00:05<00:42, 71.14it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                    | 398/3397 [00:05<00:41, 72.36it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                   | 406/3397 [00:05<00:40, 73.23it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                   | 414/3397 [00:05<00:40, 73.04it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                   | 423/3397 [00:05<00:40, 73.50it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                   | 431/3397 [00:05<00:39, 74.76it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                   | 439/3397 [00:05<00:39, 74.79it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                  | 447/3397 [00:05<00:39, 75.43it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                  | 455/3397 [00:06<00:40, 73.41it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                  | 463/3397 [00:06<00:40, 72.03it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                  | 472/3397 [00:06<00:39, 74.48it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                  | 480/3397 [00:06<00:40, 71.40it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 488/3397 [00:06<00:40, 71.12it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                 | 496/3397 [00:06<00:39, 73.06it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                 | 504/3397 [00:06<00:40, 71.57it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                 | 512/3397 [00:06<00:40, 71.65it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                 | 520/3397 [00:06<00:40, 70.65it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                 | 528/3397 [00:07<00:41, 68.91it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                | 536/3397 [00:07<00:40, 70.26it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                | 544/3397 [00:07<00:41, 68.50it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                | 551/3397 [00:07<00:42, 66.90it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                | 559/3397 [00:07<00:41, 68.36it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                | 566/3397 [00:07<00:41, 67.78it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                | 573/3397 [00:07<00:43, 64.87it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                               | 580/3397 [00:07<00:42, 65.88it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                               | 588/3397 [00:07<00:42, 66.73it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                               | 595/3397 [00:08<00:42, 66.07it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                               | 602/3397 [00:08<00:43, 63.64it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                               | 609/3397 [00:08<00:44, 61.99it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                               | 617/3397 [00:08<00:42, 65.16it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                              | 624/3397 [00:08<00:44, 62.19it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                              | 631/3397 [00:08<00:44, 61.73it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                              | 639/3397 [00:08<00:41, 66.43it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                              | 646/3397 [00:08<00:43, 62.68it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                              | 653/3397 [00:09<00:45, 60.82it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                              | 661/3397 [00:09<00:42, 64.98it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                             | 668/3397 [00:09<00:42, 64.61it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                             | 675/3397 [00:09<00:44, 61.30it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                             | 682/3397 [00:09<00:43, 62.07it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                             | 690/3397 [00:09<00:41, 65.20it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                             | 697/3397 [00:09<00:42, 63.25it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                             | 704/3397 [00:09<00:42, 63.40it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                             | 711/3397 [00:09<00:42, 63.64it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                            | 718/3397 [00:10<00:41, 64.38it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                            | 725/3397 [00:10<00:43, 62.12it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                            | 732/3397 [00:10<00:42, 62.68it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                            | 740/3397 [00:10<00:41, 64.44it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                            | 747/3397 [00:10<00:42, 61.70it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                            | 754/3397 [00:10<00:43, 60.69it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                           | 762/3397 [00:10<00:40, 64.31it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                           | 769/3397 [00:10<00:42, 61.72it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                           | 776/3397 [00:10<00:43, 59.68it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                           | 783/3397 [00:11<00:42, 62.23it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                           | 791/3397 [00:11<00:40, 64.24it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 798/3397 [00:11<00:42, 61.25it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                          | 805/3397 [00:11<00:42, 61.11it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                          | 813/3397 [00:11<00:40, 63.10it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                          | 820/3397 [00:11<00:40, 63.29it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                          | 827/3397 [00:11<00:42, 61.13it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                          | 835/3397 [00:11<00:40, 63.97it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                          | 842/3397 [00:12<00:39, 64.34it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                         | 849/3397 [00:12<00:42, 60.06it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                         | 856/3397 [00:12<00:42, 60.11it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                         | 864/3397 [00:12<00:39, 64.13it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                         | 871/3397 [00:12<00:41, 60.26it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                         | 878/3397 [00:12<00:42, 58.94it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                         | 886/3397 [00:12<00:41, 61.14it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                        | 893/3397 [00:12<00:40, 62.53it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                        | 900/3397 [00:12<00:42, 59.38it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                        | 907/3397 [00:13<00:41, 60.25it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                        | 915/3397 [00:13<00:38, 64.32it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                        | 922/3397 [00:13<00:41, 59.38it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                        | 929/3397 [00:13<00:41, 58.96it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                       | 937/3397 [00:13<00:38, 63.43it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                       | 944/3397 [00:13<00:39, 61.89it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                       | 951/3397 [00:13<00:40, 60.18it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                       | 958/3397 [00:13<00:38, 62.59it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                       | 966/3397 [00:14<00:39, 61.09it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                       | 973/3397 [00:14<00:39, 62.11it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                      | 980/3397 [00:14<00:39, 61.45it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                      | 987/3397 [00:14<00:38, 62.90it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 994/3397 [00:14<00:39, 60.28it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 1001/3397 [00:14<00:40, 58.92it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                     | 1008/3397 [00:14<00:39, 61.14it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                     | 1015/3397 [00:14<00:38, 62.45it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                     | 1022/3397 [00:14<00:40, 59.24it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                     | 1028/3397 [00:15<00:41, 57.78it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 1036/3397 [00:15<00:38, 62.05it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                    | 1043/3397 [00:15<00:38, 61.32it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                    | 1050/3397 [00:15<00:39, 59.91it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                    | 1057/3397 [00:15<00:38, 60.44it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                    | 1064/3397 [00:15<00:37, 61.61it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                    | 1071/3397 [00:15<00:38, 60.48it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                    | 1078/3397 [00:15<00:38, 59.64it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                   | 1086/3397 [00:16<00:36, 63.15it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                   | 1093/3397 [00:16<00:36, 62.62it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                   | 1100/3397 [00:16<00:38, 60.16it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                   | 1107/3397 [00:16<00:37, 60.46it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                   | 1115/3397 [00:16<00:35, 63.61it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                   | 1122/3397 [00:16<00:37, 60.12it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                  | 1129/3397 [00:16<00:38, 59.47it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                  | 1136/3397 [00:16<00:36, 61.49it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 1143/3397 [00:16<00:36, 61.02it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                  | 1150/3397 [00:17<00:37, 59.38it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                  | 1156/3397 [00:17<00:38, 58.36it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                  | 1163/3397 [00:17<00:36, 60.89it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                 | 1170/3397 [00:17<00:36, 60.55it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                 | 1177/3397 [00:17<00:39, 56.83it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                 | 1186/3397 [00:17<00:34, 64.03it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                 | 1193/3397 [00:17<00:34, 64.32it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                 | 1200/3397 [00:17<00:36, 60.36it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                 | 1207/3397 [00:18<00:36, 60.54it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                | 1215/3397 [00:18<00:34, 64.02it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                | 1222/3397 [00:18<00:37, 58.50it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                | 1228/3397 [00:18<00:38, 56.72it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                | 1236/3397 [00:18<00:35, 61.56it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                | 1243/3397 [00:18<00:35, 60.76it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                | 1250/3397 [00:18<00:36, 58.65it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                | 1256/3397 [00:18<00:37, 57.20it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                               | 1263/3397 [00:18<00:36, 59.26it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                               | 1270/3397 [00:19<00:35, 60.36it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                               | 1277/3397 [00:19<00:36, 58.01it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                               | 1285/3397 [00:19<00:34, 61.66it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                               | 1292/3397 [00:19<00:33, 62.76it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                               | 1299/3397 [00:19<00:34, 60.34it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                              | 1306/3397 [00:19<00:35, 58.43it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                              | 1314/3397 [00:19<00:32, 63.52it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 1321/3397 [00:19<00:34, 60.43it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                              | 1328/3397 [00:20<00:35, 58.53it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                              | 1334/3397 [00:20<00:35, 58.69it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                              | 1341/3397 [00:20<00:33, 61.67it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                             | 1348/3397 [00:20<00:32, 62.24it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                             | 1355/3397 [00:20<00:44, 46.09it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                             | 1363/3397 [00:20<00:38, 53.51it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                             | 1370/3397 [00:20<00:36, 55.73it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                             | 1377/3397 [00:20<00:37, 54.19it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                             | 1383/3397 [00:21<00:37, 54.30it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                             | 1391/3397 [00:21<00:35, 56.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                            | 1399/3397 [00:21<00:35, 56.69it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                            | 1405/3397 [00:21<00:36, 54.83it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                            | 1413/3397 [00:21<00:35, 55.61it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                            | 1421/3397 [00:21<00:33, 59.44it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                            | 1428/3397 [00:21<00:34, 56.63it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                            | 1435/3397 [00:21<00:33, 58.00it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                           | 1441/3397 [00:22<00:34, 56.59it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                           | 1447/3397 [00:22<00:35, 54.92it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                           | 1453/3397 [00:22<00:35, 54.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                           | 1460/3397 [00:22<00:35, 54.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                           | 1467/3397 [00:22<00:34, 56.58it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                           | 1473/3397 [00:22<00:34, 55.14it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                           | 1479/3397 [00:22<00:35, 54.67it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                          | 1485/3397 [00:22<00:35, 54.28it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                          | 1491/3397 [00:22<00:34, 54.83it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                          | 1497/3397 [00:23<00:35, 53.92it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                          | 1503/3397 [00:23<00:35, 53.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                          | 1509/3397 [00:23<00:36, 51.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                          | 1516/3397 [00:23<00:34, 54.03it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                          | 1522/3397 [00:23<00:38, 49.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                         | 1529/3397 [00:23<00:35, 52.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                         | 1535/3397 [00:23<00:35, 53.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                         | 1541/3397 [00:23<00:34, 53.70it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                         | 1547/3397 [00:24<00:35, 52.40it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 1553/3397 [00:24<00:35, 52.35it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                         | 1559/3397 [00:24<00:34, 53.87it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                         | 1565/3397 [00:24<00:35, 51.55it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 1571/3397 [00:24<00:35, 50.76it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                        | 1577/3397 [00:24<00:35, 50.92it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                        | 1583/3397 [00:24<00:35, 50.54it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                        | 1589/3397 [00:24<00:35, 50.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                        | 1595/3397 [00:25<00:36, 49.70it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 1600/3397 [00:25<00:36, 49.25it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                        | 1606/3397 [00:25<00:35, 50.14it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1612/3397 [00:25<00:35, 50.31it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 1618/3397 [00:25<00:35, 49.82it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                       | 1623/3397 [00:25<00:35, 49.45it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 1629/3397 [00:25<00:35, 50.18it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                       | 1635/3397 [00:25<00:35, 49.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                       | 1641/3397 [00:25<00:35, 49.74it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                       | 1646/3397 [00:26<00:35, 49.36it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                       | 1651/3397 [00:26<00:35, 49.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                       | 1657/3397 [00:26<00:34, 50.07it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                      | 1663/3397 [00:26<00:33, 52.35it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 1669/3397 [00:26<00:36, 47.63it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                      | 1674/3397 [00:26<00:36, 47.46it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                      | 1680/3397 [00:26<00:35, 48.87it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                      | 1685/3397 [00:26<00:35, 48.38it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                      | 1690/3397 [00:26<00:35, 48.64it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                      | 1695/3397 [00:27<00:34, 48.71it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                      | 1701/3397 [00:27<00:34, 49.38it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                     | 1706/3397 [00:27<00:34, 49.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                     | 1712/3397 [00:27<00:34, 49.40it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                     | 1717/3397 [00:27<00:34, 49.21it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 1722/3397 [00:27<00:33, 49.29it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 1727/3397 [00:27<00:33, 49.43it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                     | 1732/3397 [00:27<00:34, 48.86it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 1738/3397 [00:27<00:33, 49.29it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                     | 1743/3397 [00:28<00:35, 46.51it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 1749/3397 [00:28<00:33, 49.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                    | 1757/3397 [00:28<00:29, 55.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 1764/3397 [00:28<00:28, 57.75it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 1771/3397 [00:28<00:27, 59.07it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                    | 1778/3397 [00:28<00:26, 60.14it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                    | 1785/3397 [00:28<00:26, 60.96it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                    | 1792/3397 [00:28<00:26, 61.71it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 1799/3397 [00:28<00:25, 61.97it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 1806/3397 [00:29<00:25, 62.14it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 1813/3397 [00:29<00:25, 62.35it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 1820/3397 [00:29<00:25, 62.33it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 1827/3397 [00:29<00:25, 62.44it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 1834/3397 [00:29<00:24, 62.68it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 1841/3397 [00:29<00:24, 62.78it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 1848/3397 [00:29<00:24, 63.26it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 1855/3397 [00:29<00:24, 63.70it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 1862/3397 [00:29<00:24, 63.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 1869/3397 [00:30<00:23, 63.97it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 1876/3397 [00:30<00:23, 64.23it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 1883/3397 [00:30<00:23, 64.47it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 1890/3397 [00:30<00:23, 64.10it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 1897/3397 [00:30<00:23, 64.17it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 1904/3397 [00:30<00:23, 64.03it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 1911/3397 [00:30<00:23, 64.27it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 1918/3397 [00:30<00:22, 64.71it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 1925/3397 [00:30<00:22, 64.58it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 1932/3397 [00:31<00:22, 64.92it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 1939/3397 [00:31<00:22, 64.73it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 1946/3397 [00:31<00:22, 64.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 1953/3397 [00:31<00:23, 61.92it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 1961/3397 [00:31<00:22, 65.25it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 1968/3397 [00:31<00:21, 65.19it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 1975/3397 [00:31<00:21, 64.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 1982/3397 [00:31<00:21, 65.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 1989/3397 [00:31<00:22, 62.51it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 1997/3397 [00:32<00:21, 65.96it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 2004/3397 [00:32<00:21, 65.74it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 2011/3397 [00:32<00:21, 65.73it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 2018/3397 [00:32<00:21, 65.62it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 2025/3397 [00:32<00:21, 65.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 2032/3397 [00:32<00:21, 62.84it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 2040/3397 [00:32<00:21, 63.30it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 2047/3397 [00:32<00:21, 63.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 2055/3397 [00:32<00:20, 66.80it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 2062/3397 [00:33<00:20, 63.97it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 2070/3397 [00:33<00:19, 66.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 2077/3397 [00:33<00:19, 66.63it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 2084/3397 [00:33<00:19, 66.72it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 2091/3397 [00:33<00:19, 66.14it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 2098/3397 [00:33<00:19, 65.75it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 2105/3397 [00:33<00:20, 63.18it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 2112/3397 [00:33<00:20, 63.87it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 2120/3397 [00:33<00:19, 66.70it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 2127/3397 [00:33<00:19, 66.28it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 2134/3397 [00:34<00:18, 66.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 2141/3397 [00:34<00:18, 66.16it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 2148/3397 [00:34<00:18, 65.89it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 2155/3397 [00:34<00:18, 65.88it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 2162/3397 [00:34<00:18, 66.07it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 2169/3397 [00:34<00:18, 65.94it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 2176/3397 [00:34<00:18, 66.10it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 2183/3397 [00:34<00:18, 66.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 2190/3397 [00:34<00:18, 66.33it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 2197/3397 [00:35<00:18, 65.97it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 2204/3397 [00:35<00:18, 66.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 2211/3397 [00:35<00:17, 66.37it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 2218/3397 [00:35<00:17, 66.23it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 2225/3397 [00:35<00:17, 66.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 2232/3397 [00:35<00:17, 66.28it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 2239/3397 [00:35<00:17, 66.42it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 2246/3397 [00:35<00:17, 65.94it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 2253/3397 [00:35<00:18, 63.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 2261/3397 [00:36<00:16, 67.13it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 2268/3397 [00:36<00:17, 63.92it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 2276/3397 [00:36<00:17, 64.68it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 2284/3397 [00:36<00:16, 67.84it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 2291/3397 [00:36<00:16, 67.34it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 2298/3397 [00:36<00:16, 66.84it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 2305/3397 [00:36<00:16, 66.90it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 2312/3397 [00:36<00:16, 66.86it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 2319/3397 [00:36<00:16, 66.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 2326/3397 [00:37<00:16, 66.29it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 2333/3397 [00:37<00:16, 66.38it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 2340/3397 [00:37<00:15, 66.42it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 2347/3397 [00:37<00:15, 66.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 2354/3397 [00:37<00:16, 63.54it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 2362/3397 [00:37<00:16, 64.55it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 2370/3397 [00:37<00:15, 67.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 2377/3397 [00:37<00:15, 67.27it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 2384/3397 [00:37<00:15, 67.12it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2391/3397 [00:37<00:15, 66.58it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2398/3397 [00:38<00:15, 66.06it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2405/3397 [00:38<00:15, 66.05it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2412/3397 [00:38<00:15, 63.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2419/3397 [00:38<00:15, 63.46it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2426/3397 [00:38<00:15, 64.34it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2433/3397 [00:38<00:14, 65.06it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 2441/3397 [00:38<00:14, 67.99it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2448/3397 [00:38<00:14, 67.21it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2455/3397 [00:38<00:14, 67.09it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2462/3397 [00:39<00:13, 67.08it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 2469/3397 [00:39<00:13, 66.43it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 2476/3397 [00:39<00:13, 66.35it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2483/3397 [00:39<00:13, 66.45it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2490/3397 [00:39<00:13, 66.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2497/3397 [00:39<00:13, 65.93it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2504/3397 [00:39<00:13, 65.94it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2511/3397 [00:39<00:13, 66.28it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2518/3397 [00:39<00:13, 66.15it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2525/3397 [00:40<00:13, 66.05it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2532/3397 [00:40<00:13, 66.35it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2539/3397 [00:40<00:12, 66.44it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2546/3397 [00:40<00:12, 65.98it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2553/3397 [00:40<00:13, 63.47it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2561/3397 [00:40<00:12, 67.12it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2568/3397 [00:40<00:12, 63.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2576/3397 [00:40<00:12, 64.58it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2584/3397 [00:40<00:12, 67.74it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2591/3397 [00:41<00:11, 67.18it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2598/3397 [00:41<00:11, 66.59it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2605/3397 [00:41<00:11, 66.67it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2612/3397 [00:41<00:12, 63.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2620/3397 [00:41<00:11, 66.82it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2627/3397 [00:41<00:11, 66.68it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2634/3397 [00:41<00:11, 66.64it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2641/3397 [00:41<00:11, 66.46it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2648/3397 [00:41<00:11, 63.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2656/3397 [00:42<00:11, 67.14it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2663/3397 [00:42<00:10, 67.08it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2670/3397 [00:42<00:10, 66.57it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2677/3397 [00:42<00:10, 66.64it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2684/3397 [00:42<00:10, 66.78it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2691/3397 [00:42<00:10, 66.57it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2698/3397 [00:42<00:10, 66.30it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2705/3397 [00:42<00:10, 63.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2712/3397 [00:42<00:10, 64.36it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2720/3397 [00:42<00:10, 67.31it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2727/3397 [00:43<00:10, 66.98it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2734/3397 [00:43<00:09, 66.86it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2741/3397 [00:43<00:09, 66.55it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2748/3397 [00:43<00:09, 66.22it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2755/3397 [00:43<00:09, 66.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2762/3397 [00:43<00:09, 64.96it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2770/3397 [00:43<00:09, 68.80it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2778/3397 [00:43<00:09, 68.00it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2787/3397 [00:43<00:08, 70.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2796/3397 [00:44<00:08, 73.16it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2804/3397 [00:44<00:08, 70.99it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2812/3397 [00:44<00:08, 71.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2821/3397 [00:44<00:07, 72.22it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2830/3397 [00:44<00:07, 75.31it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2838/3397 [00:44<00:07, 74.96it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2846/3397 [00:44<00:07, 74.27it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2854/3397 [00:44<00:07, 71.63it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2862/3397 [00:44<00:07, 72.29it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2871/3397 [00:45<00:07, 74.79it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2879/3397 [00:45<00:07, 72.34it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2888/3397 [00:45<00:06, 75.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2896/3397 [00:45<00:06, 73.61it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2904/3397 [00:45<00:06, 71.34it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2912/3397 [00:45<00:06, 72.48it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2921/3397 [00:45<00:06, 72.72it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2929/3397 [00:45<00:06, 73.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2937/3397 [00:45<00:06, 73.60it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2946/3397 [00:46<00:05, 75.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2954/3397 [00:46<00:06, 72.77it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2962/3397 [00:46<00:05, 73.32it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2970/3397 [00:46<00:05, 72.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2979/3397 [00:46<00:05, 73.69it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2988/3397 [00:46<00:05, 76.60it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2996/3397 [00:46<00:05, 72.96it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3004/3397 [00:46<00:05, 73.59it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3012/3397 [00:47<00:05, 73.71it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3021/3397 [00:47<00:04, 75.71it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3029/3397 [00:47<00:04, 75.39it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3037/3397 [00:47<00:04, 72.50it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3046/3397 [00:47<00:04, 75.25it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3054/3397 [00:47<00:04, 72.40it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3062/3397 [00:47<00:04, 73.11it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3071/3397 [00:47<00:04, 75.67it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3079/3397 [00:47<00:04, 72.67it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3087/3397 [00:48<00:04, 72.82it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3096/3397 [00:48<00:04, 75.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3104/3397 [00:48<00:04, 72.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3112/3397 [00:48<00:03, 73.26it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3120/3397 [00:48<00:03, 73.08it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3129/3397 [00:48<00:03, 73.71it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3137/3397 [00:48<00:03, 74.07it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3146/3397 [00:48<00:03, 76.47it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3154/3397 [00:48<00:03, 73.41it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3162/3397 [00:49<00:03, 73.58it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3171/3397 [00:49<00:02, 75.67it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3179/3397 [00:49<00:02, 75.06it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3187/3397 [00:49<00:02, 71.80it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3196/3397 [00:49<00:02, 74.50it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3204/3397 [00:49<00:02, 72.02it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3212/3397 [00:49<00:02, 73.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3220/3397 [00:49<00:02, 63.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3227/3397 [00:50<00:02, 59.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3234/3397 [00:50<00:02, 55.93it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3240/3397 [00:50<00:02, 53.17it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3246/3397 [00:50<00:02, 51.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3252/3397 [00:50<00:02, 51.65it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3258/3397 [00:50<00:02, 51.28it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3264/3397 [00:50<00:02, 51.32it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3270/3397 [00:50<00:02, 50.55it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3276/3397 [00:51<00:02, 48.27it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3283/3397 [00:51<00:02, 51.94it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3289/3397 [00:51<00:02, 52.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3295/3397 [00:51<00:01, 51.24it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3301/3397 [00:51<00:01, 51.39it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3307/3397 [00:51<00:01, 50.92it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3313/3397 [00:51<00:01, 50.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3319/3397 [00:51<00:01, 49.61it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3325/3397 [00:51<00:01, 50.56it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3331/3397 [00:52<00:01, 50.23it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3337/3397 [00:52<00:01, 50.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3343/3397 [00:52<00:01, 50.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3349/3397 [00:52<00:00, 50.80it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3357/3397 [00:52<00:00, 56.75it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3364/3397 [00:52<00:00, 58.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3372/3397 [00:52<00:00, 61.91it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3379/3397 [00:52<00:00, 64.01it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3387/3397 [00:53<00:00, 66.72it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3395/3397 [00:53<00:00, 69.34it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3397/3397 [00:53<00:00, 63.93it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.3616
Epoch 1 Step 51 Train Loss: 0.4583
Epoch 1 Step 101 Train Loss: 0.4594
Epoch 1 Step 151 Train Loss: 0.4282
Epoch 1 Step 201 Train Loss: 0.4376
Epoch 1 Step 251 Train Loss: 0.4275
Epoch 1 Step 301 Train Loss: 0.4158
Epoch 1 Step 351 Train Loss: 0.4170
Epoch 1 Step 401 Train Loss: 0.4486
Epoch 1 Step 451 Train Loss: 0.5075
Epoch 1 Step 501 Train Loss: 0.4366
Epoch 1 Step 551 Train Loss: 0.4546
Epoch 1 Step 601 Train Loss: 0.4452
Epoch 1 Step 651 Train Loss: 0.5027
Epoch 1 Step 701 Train Loss: 0.4290
Epoch 1 Step 751 Train Loss: 0.4312
Epoch 1 Step 801 Train Loss: 0.4309
Epoch 1 Step 851 Train Loss: 0.4295
Epoch 1 Step 901 Train Loss: 0.4361
Epoch 1 Step 951 Train Loss: 0.4628
Epoch 1 Step 1001 Train Loss: 0.4722
Epoch 1 Step 1051 Train Loss: 0.4038
Epoch 1 Step 1101 Train Loss: 0.3994
Epoch 1 Step 1151 Train Loss: 0.3839
Epoch 1 Step 1201 Train Loss: 0.4927
Epoch 1 Step 1251 Train Loss: 0.4161
Epoch 1 Step 1301 Train Loss: 0.4066
Epoch 1 Step 1351 Train Loss: 0.4195
Epoch 1 Step 1401 Train Loss: 0.4259
Epoch 1 Step 1451 Train Loss: 0.4390
Epoch 1 Step 1501 Train Loss: 0.3845
Epoch 1 Step 1551 Train Loss: 0.5024
Epoch 1 Step 1601 Train Loss: 0.4574
Epoch 1 Step 1651 Train Loss: 0.4282
Epoch 1 Step 1701 Train Loss: 0.4276
Epoch 1 Step 1751 Train Loss: 0.4449
Epoch 1 Step 1801 Train Loss: 0.4482
Epoch 1 Step 1851 Train Loss: 0.4297
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.1292 Validation Top 20 DE MSE: 0.1534. 
Epoch 2 Step 1 Train Loss: 0.4286
Epoch 2 Step 51 Train Loss: 0.4263
Epoch 2 Step 101 Train Loss: 0.4801
Epoch 2 Step 151 Train Loss: 0.4339
Epoch 2 Step 201 Train Loss: 0.4067
Epoch 2 Step 251 Train Loss: 0.4017
Epoch 2 Step 301 Train Loss: 0.4430
Epoch 2 Step 351 Train Loss: 0.4476
Epoch 2 Step 401 Train Loss: 0.4605
Epoch 2 Step 451 Train Loss: 0.4277
Epoch 2 Step 501 Train Loss: 0.4002
Epoch 2 Step 551 Train Loss: 0.4537
Epoch 2 Step 601 Train Loss: 0.4796
Epoch 2 Step 651 Train Loss: 0.4604
Epoch 2 Step 701 Train Loss: 0.4944
Epoch 2 Step 751 Train Loss: 0.5025
Epoch 2 Step 801 Train Loss: 0.4742
Epoch 2 Step 851 Train Loss: 0.4825
Epoch 2 Step 901 Train Loss: 0.5200
Epoch 2 Step 951 Train Loss: 0.4860
Epoch 2 Step 1001 Train Loss: 0.4635
Epoch 2 Step 1051 Train Loss: 0.4131
Epoch 2 Step 1101 Train Loss: 0.4094
Epoch 2 Step 1151 Train Loss: 0.4487
Epoch 2 Step 1201 Train Loss: 0.4112
Epoch 2 Step 1251 Train Loss: 0.4540
Epoch 2 Step 1301 Train Loss: 0.3883
Epoch 2 Step 1351 Train Loss: 0.4811
Epoch 2 Step 1401 Train Loss: 0.4529
Epoch 2 Step 1451 Train Loss: 0.4190
Epoch 2 Step 1501 Train Loss: 0.4518
Epoch 2 Step 1551 Train Loss: 0.4416
Epoch 2 Step 1601 Train Loss: 0.4613
Epoch 2 Step 1651 Train Loss: 0.4560
Epoch 2 Step 1701 Train Loss: 0.5315
Epoch 2 Step 1751 Train Loss: 0.4580
Epoch 2 Step 1801 Train Loss: 0.4270
Epoch 2 Step 1851 Train Loss: 0.3862
Epoch 2: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.1307 Validation Top 20 DE MSE: 0.1457. 
Epoch 3 Step 1 Train Loss: 0.4780
Epoch 3 Step 51 Train Loss: 0.4220
Epoch 3 Step 101 Train Loss: 0.4938
Epoch 3 Step 151 Train Loss: 0.4416
Epoch 3 Step 201 Train Loss: 0.4091
Epoch 3 Step 251 Train Loss: 0.4188
Epoch 3 Step 301 Train Loss: 0.4493
Epoch 3 Step 351 Train Loss: 0.4446
Epoch 3 Step 401 Train Loss: 0.4552
Epoch 3 Step 451 Train Loss: 0.5023
Epoch 3 Step 501 Train Loss: 0.4408
Epoch 3 Step 551 Train Loss: 0.4278
Epoch 3 Step 601 Train Loss: 0.4677
Epoch 3 Step 651 Train Loss: 0.4792
Epoch 3 Step 701 Train Loss: 0.4321
Epoch 3 Step 751 Train Loss: 0.4483
Epoch 3 Step 801 Train Loss: 0.4473
Epoch 3 Step 851 Train Loss: 0.4293
Epoch 3 Step 901 Train Loss: 0.4427
Epoch 3 Step 951 Train Loss: 0.4184
Epoch 3 Step 1001 Train Loss: 0.4580
Epoch 3 Step 1051 Train Loss: 0.4135
Epoch 3 Step 1101 Train Loss: 0.4477
Epoch 3 Step 1151 Train Loss: 0.4190
Epoch 3 Step 1201 Train Loss: 0.4284
Epoch 3 Step 1251 Train Loss: 0.4364
Epoch 3 Step 1301 Train Loss: 0.4642
Epoch 3 Step 1351 Train Loss: 0.4464
Epoch 3 Step 1401 Train Loss: 0.4460
Epoch 3 Step 1451 Train Loss: 0.5107
Epoch 3 Step 1501 Train Loss: 0.3839
Epoch 3 Step 1551 Train Loss: 0.4698
Epoch 3 Step 1601 Train Loss: 0.4501
Epoch 3 Step 1651 Train Loss: 0.4256
Epoch 3 Step 1701 Train Loss: 0.4669
Epoch 3 Step 1751 Train Loss: 0.4826
Epoch 3 Step 1801 Train Loss: 0.4120
Epoch 3 Step 1851 Train Loss: 0.3910
Epoch 3: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.1038 Validation Top 20 DE MSE: 0.1451. 
Epoch 4 Step 1 Train Loss: 0.4526
Epoch 4 Step 51 Train Loss: 0.5439
Epoch 4 Step 101 Train Loss: 0.4897
Epoch 4 Step 151 Train Loss: 0.4459
Epoch 4 Step 201 Train Loss: 0.4481
Epoch 4 Step 251 Train Loss: 0.4976
Epoch 4 Step 301 Train Loss: 0.4680
Epoch 4 Step 351 Train Loss: 0.4404
Epoch 4 Step 401 Train Loss: 0.4457
Epoch 4 Step 451 Train Loss: 0.5006
Epoch 4 Step 501 Train Loss: 0.4382
Epoch 4 Step 551 Train Loss: 0.4552
Epoch 4 Step 601 Train Loss: 0.4083
Epoch 4 Step 651 Train Loss: 0.4677
Epoch 4 Step 701 Train Loss: 0.3983
Epoch 4 Step 751 Train Loss: 0.3981
Epoch 4 Step 801 Train Loss: 0.4679
Epoch 4 Step 851 Train Loss: 0.4924
Epoch 4 Step 901 Train Loss: 0.4690
Epoch 4 Step 951 Train Loss: 0.4671
Epoch 4 Step 1001 Train Loss: 0.5095
Epoch 4 Step 1051 Train Loss: 0.4691
Epoch 4 Step 1101 Train Loss: 0.5220
Epoch 4 Step 1151 Train Loss: 0.4520
Epoch 4 Step 1201 Train Loss: 0.4739
Epoch 4 Step 1251 Train Loss: 0.4607
Epoch 4 Step 1301 Train Loss: 0.4706
Epoch 4 Step 1351 Train Loss: 0.4368
Epoch 4 Step 1401 Train Loss: 0.4571
Epoch 4 Step 1451 Train Loss: 0.4597
Epoch 4 Step 1501 Train Loss: 0.5115
Epoch 4 Step 1551 Train Loss: 0.5196
Epoch 4 Step 1601 Train Loss: 0.4404
Epoch 4 Step 1651 Train Loss: 0.4350
Epoch 4 Step 1701 Train Loss: 0.4875
Epoch 4 Step 1751 Train Loss: 0.4511
Epoch 4 Step 1801 Train Loss: 0.4391
Epoch 4 Step 1851 Train Loss: 0.4828
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0932 Validation Top 20 DE MSE: 0.1317. 
Epoch 5 Step 1 Train Loss: 0.4667
Epoch 5 Step 51 Train Loss: 0.5384
Epoch 5 Step 101 Train Loss: 0.4714
Epoch 5 Step 151 Train Loss: 0.4327
Epoch 5 Step 201 Train Loss: 0.4466
Epoch 5 Step 251 Train Loss: 0.4223
Epoch 5 Step 301 Train Loss: 0.4387
Epoch 5 Step 351 Train Loss: 0.4473
Epoch 5 Step 401 Train Loss: 0.5507
Epoch 5 Step 451 Train Loss: 0.4555
Epoch 5 Step 501 Train Loss: 0.5375
Epoch 5 Step 551 Train Loss: 0.4545
Epoch 5 Step 601 Train Loss: 0.5160
Epoch 5 Step 651 Train Loss: 0.4881
Epoch 5 Step 701 Train Loss: 0.4743
Epoch 5 Step 751 Train Loss: 0.4618
Epoch 5 Step 801 Train Loss: 0.5689
Epoch 5 Step 851 Train Loss: 0.5088
Epoch 5 Step 901 Train Loss: 0.5318
Epoch 5 Step 951 Train Loss: 0.4633
Epoch 5 Step 1001 Train Loss: 0.4526
Epoch 5 Step 1051 Train Loss: 0.4845
Epoch 5 Step 1101 Train Loss: 0.4188
Epoch 5 Step 1151 Train Loss: 0.4151
Epoch 5 Step 1201 Train Loss: 0.4322
Epoch 5 Step 1251 Train Loss: 0.5259
Epoch 5 Step 1301 Train Loss: 0.4336
Epoch 5 Step 1351 Train Loss: 0.4844
Epoch 5 Step 1401 Train Loss: 0.4356
Epoch 5 Step 1451 Train Loss: 0.4357
Epoch 5 Step 1501 Train Loss: 0.4747
Epoch 5 Step 1551 Train Loss: 0.4922
Epoch 5 Step 1601 Train Loss: 0.4632
Epoch 5 Step 1651 Train Loss: 0.4734
Epoch 5 Step 1701 Train Loss: 0.4215
Epoch 5 Step 1751 Train Loss: 0.4865
Epoch 5 Step 1801 Train Loss: 0.4476
Epoch 5 Step 1851 Train Loss: 0.4391
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0921 Validation Top 20 DE MSE: 0.1342. 
Epoch 6 Step 1 Train Loss: 0.4446
Epoch 6 Step 51 Train Loss: 0.4481
Epoch 6 Step 101 Train Loss: 0.4767
Epoch 6 Step 151 Train Loss: 0.4572
Epoch 6 Step 201 Train Loss: 0.5727
Epoch 6 Step 251 Train Loss: 0.4347
Epoch 6 Step 301 Train Loss: 0.4550
Epoch 6 Step 351 Train Loss: 0.4496
Epoch 6 Step 401 Train Loss: 0.4585
Epoch 6 Step 451 Train Loss: 0.4748
Epoch 6 Step 501 Train Loss: 0.4988
Epoch 6 Step 551 Train Loss: 0.4481
Epoch 6 Step 601 Train Loss: 0.4977
Epoch 6 Step 651 Train Loss: 0.4351
Epoch 6 Step 701 Train Loss: 0.4965
Epoch 6 Step 751 Train Loss: 0.4866
Epoch 6 Step 801 Train Loss: 0.4907
Epoch 6 Step 851 Train Loss: 0.4579
Epoch 6 Step 901 Train Loss: 0.4049
Epoch 6 Step 951 Train Loss: 0.5181
Epoch 6 Step 1001 Train Loss: 0.4905
Epoch 6 Step 1051 Train Loss: 0.4523
Epoch 6 Step 1101 Train Loss: 0.5048
Epoch 6 Step 1151 Train Loss: 0.4267
Epoch 6 Step 1201 Train Loss: 0.4884
Epoch 6 Step 1251 Train Loss: 0.4843
Epoch 6 Step 1301 Train Loss: 0.5386
Epoch 6 Step 1351 Train Loss: 0.4350
Epoch 6 Step 1401 Train Loss: 0.5771
Epoch 6 Step 1451 Train Loss: 0.4731
Epoch 6 Step 1501 Train Loss: 0.5319
Epoch 6 Step 1551 Train Loss: 0.4156
Epoch 6 Step 1601 Train Loss: 0.4157
Epoch 6 Step 1651 Train Loss: 0.4486
Epoch 6 Step 1701 Train Loss: 0.4626
Epoch 6 Step 1751 Train Loss: 0.4966
Epoch 6 Step 1801 Train Loss: 0.5417
Epoch 6 Step 1851 Train Loss: 0.5310
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0938 Validation Top 20 DE MSE: 0.1336. 
Epoch 7 Step 1 Train Loss: 0.4412
Epoch 7 Step 51 Train Loss: 0.4284
Epoch 7 Step 101 Train Loss: 0.4324
Epoch 7 Step 151 Train Loss: 0.5132
Epoch 7 Step 201 Train Loss: 0.4794
Epoch 7 Step 251 Train Loss: 0.4262
Epoch 7 Step 301 Train Loss: 0.4028
Epoch 7 Step 351 Train Loss: 0.5121
Epoch 7 Step 401 Train Loss: 0.4541
Epoch 7 Step 451 Train Loss: 0.4757
Epoch 7 Step 501 Train Loss: 0.4606
Epoch 7 Step 551 Train Loss: 0.4959
Epoch 7 Step 601 Train Loss: 0.4479
Epoch 7 Step 651 Train Loss: 0.4787
Epoch 7 Step 701 Train Loss: 0.5241
Epoch 7 Step 751 Train Loss: 0.4129
Epoch 7 Step 801 Train Loss: 0.4261
Epoch 7 Step 851 Train Loss: 0.4663
Epoch 7 Step 901 Train Loss: 0.4009
Epoch 7 Step 951 Train Loss: 0.4018
Epoch 7 Step 1001 Train Loss: 0.4848
Epoch 7 Step 1051 Train Loss: 0.5278
Epoch 7 Step 1101 Train Loss: 0.5077
Epoch 7 Step 1151 Train Loss: 0.4700
Epoch 7 Step 1201 Train Loss: 0.4480
Epoch 7 Step 1251 Train Loss: 0.4988
Epoch 7 Step 1301 Train Loss: 0.5312
Epoch 7 Step 1351 Train Loss: 0.4725
Epoch 7 Step 1401 Train Loss: 0.4258
Epoch 7 Step 1451 Train Loss: 0.4922
Epoch 7 Step 1501 Train Loss: 0.4703
Epoch 7 Step 1551 Train Loss: 0.5306
Epoch 7 Step 1601 Train Loss: 0.4131
Epoch 7 Step 1651 Train Loss: 0.4681
Epoch 7 Step 1701 Train Loss: 0.4366
Epoch 7 Step 1751 Train Loss: 0.4532
Epoch 7 Step 1801 Train Loss: 0.4853
Epoch 7 Step 1851 Train Loss: 0.4811
Epoch 7: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0937 Validation Top 20 DE MSE: 0.1338. 
Epoch 8 Step 1 Train Loss: 0.5042
Epoch 8 Step 51 Train Loss: 0.4847
Epoch 8 Step 101 Train Loss: 0.4581
Epoch 8 Step 151 Train Loss: 0.4183
Epoch 8 Step 201 Train Loss: 0.4398
Epoch 8 Step 251 Train Loss: 0.4557
Epoch 8 Step 301 Train Loss: 0.4521
Epoch 8 Step 351 Train Loss: 0.4304
Epoch 8 Step 401 Train Loss: 0.4361
Epoch 8 Step 451 Train Loss: 0.4536
Epoch 8 Step 501 Train Loss: 0.4538
Epoch 8 Step 551 Train Loss: 0.4641
Epoch 8 Step 601 Train Loss: 0.4157
Epoch 8 Step 651 Train Loss: 0.4483
Epoch 8 Step 701 Train Loss: 0.4360
Epoch 8 Step 751 Train Loss: 0.4555
Epoch 8 Step 801 Train Loss: 0.4927
Epoch 8 Step 851 Train Loss: 0.4947
Epoch 8 Step 901 Train Loss: 0.4640
Epoch 8 Step 951 Train Loss: 0.4849
Epoch 8 Step 1001 Train Loss: 0.5445
Epoch 8 Step 1051 Train Loss: 0.4623
Epoch 8 Step 1101 Train Loss: 0.4984
Epoch 8 Step 1151 Train Loss: 0.4765
Epoch 8 Step 1201 Train Loss: 0.4667
Epoch 8 Step 1251 Train Loss: 0.4447
Epoch 8 Step 1301 Train Loss: 0.4484
Epoch 8 Step 1351 Train Loss: 0.4277
Epoch 8 Step 1401 Train Loss: 0.4918
Epoch 8 Step 1451 Train Loss: 0.4865
Epoch 8 Step 1501 Train Loss: 0.4978
Epoch 8 Step 1551 Train Loss: 0.4818
Epoch 8 Step 1601 Train Loss: 0.4890
Epoch 8 Step 1651 Train Loss: 0.4520
Epoch 8 Step 1701 Train Loss: 0.4269
Epoch 8 Step 1751 Train Loss: 0.4512
Epoch 8 Step 1801 Train Loss: 0.4666
Epoch 8 Step 1851 Train Loss: 0.4643
Epoch 8: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0904 Validation Top 20 DE MSE: 0.1321. 
Epoch 9 Step 1 Train Loss: 0.4751
Epoch 9 Step 51 Train Loss: 0.4483
Epoch 9 Step 101 Train Loss: 0.4790
Epoch 9 Step 151 Train Loss: 0.4389
Epoch 9 Step 201 Train Loss: 0.4486
Epoch 9 Step 251 Train Loss: 0.4339
Epoch 9 Step 301 Train Loss: 0.4761
Epoch 9 Step 351 Train Loss: 0.4593
Epoch 9 Step 401 Train Loss: 0.4314
Epoch 9 Step 451 Train Loss: 0.4945
Epoch 9 Step 501 Train Loss: 0.4696
Epoch 9 Step 551 Train Loss: 0.4842
Epoch 9 Step 601 Train Loss: 0.5333
Epoch 9 Step 651 Train Loss: 0.4770
Epoch 9 Step 701 Train Loss: 0.4888
Epoch 9 Step 751 Train Loss: 0.4763
Epoch 9 Step 801 Train Loss: 0.5319
Epoch 9 Step 851 Train Loss: 0.4955
Epoch 9 Step 901 Train Loss: 0.4611
Epoch 9 Step 951 Train Loss: 0.5010
Epoch 9 Step 1001 Train Loss: 0.4694
Epoch 9 Step 1051 Train Loss: 0.4644
Epoch 9 Step 1101 Train Loss: 0.4527
Epoch 9 Step 1151 Train Loss: 0.4249
Epoch 9 Step 1201 Train Loss: 0.4450
Epoch 9 Step 1251 Train Loss: 0.4620
Epoch 9 Step 1301 Train Loss: 0.4569
Epoch 9 Step 1351 Train Loss: 0.4584
Epoch 9 Step 1401 Train Loss: 0.4826
Epoch 9 Step 1451 Train Loss: 0.4645
Epoch 9 Step 1501 Train Loss: 0.5109
Epoch 9 Step 1551 Train Loss: 0.4303
Epoch 9 Step 1601 Train Loss: 0.4298
Epoch 9 Step 1651 Train Loss: 0.5003
Epoch 9 Step 1701 Train Loss: 0.4556
Epoch 9 Step 1751 Train Loss: 0.4292
Epoch 9 Step 1801 Train Loss: 0.5293
Epoch 9 Step 1851 Train Loss: 0.5045
Epoch 9: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0926 Validation Top 20 DE MSE: 0.1336. 
Epoch 10 Step 1 Train Loss: 0.5156
Epoch 10 Step 51 Train Loss: 0.4818
Epoch 10 Step 101 Train Loss: 0.4959
Epoch 10 Step 151 Train Loss: 0.4923
Epoch 10 Step 201 Train Loss: 0.4689
Epoch 10 Step 251 Train Loss: 0.4976
Epoch 10 Step 301 Train Loss: 0.4751
Epoch 10 Step 351 Train Loss: 0.4670
Epoch 10 Step 401 Train Loss: 0.5019
Epoch 10 Step 451 Train Loss: 0.4795
Epoch 10 Step 501 Train Loss: 0.4704
Epoch 10 Step 551 Train Loss: 0.5639
Epoch 10 Step 601 Train Loss: 0.4929
Epoch 10 Step 651 Train Loss: 0.4301
Epoch 10 Step 701 Train Loss: 0.5074
Epoch 10 Step 751 Train Loss: 0.4977
Epoch 10 Step 801 Train Loss: 0.4443
Epoch 10 Step 851 Train Loss: 0.4220
Epoch 10 Step 901 Train Loss: 0.4810
Epoch 10 Step 951 Train Loss: 0.4507
Epoch 10 Step 1001 Train Loss: 0.4647
Epoch 10 Step 1051 Train Loss: 0.4397
Epoch 10 Step 1101 Train Loss: 0.4477
Epoch 10 Step 1151 Train Loss: 0.4587
Epoch 10 Step 1201 Train Loss: 0.4624
Epoch 10 Step 1251 Train Loss: 0.4804
Epoch 10 Step 1301 Train Loss: 0.4392
Epoch 10 Step 1351 Train Loss: 0.4857
Epoch 10 Step 1401 Train Loss: 0.4447
Epoch 10 Step 1451 Train Loss: 0.5324
Epoch 10 Step 1501 Train Loss: 0.4155
Epoch 10 Step 1551 Train Loss: 0.4643
Epoch 10 Step 1601 Train Loss: 0.4564
Epoch 10 Step 1651 Train Loss: 0.4787
Epoch 10 Step 1701 Train Loss: 0.4626
Epoch 10 Step 1751 Train Loss: 0.4573
Epoch 10 Step 1801 Train Loss: 0.5094
Epoch 10 Step 1851 Train Loss: 0.5120
Epoch 10: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0952 Validation Top 20 DE MSE: 0.1356. 
Epoch 11 Step 1 Train Loss: 0.4651
Epoch 11 Step 51 Train Loss: 0.4336
Epoch 11 Step 101 Train Loss: 0.4855
Epoch 11 Step 151 Train Loss: 0.4426
Epoch 11 Step 201 Train Loss: 0.5050
Epoch 11 Step 251 Train Loss: 0.4966
Epoch 11 Step 301 Train Loss: 0.4633
Epoch 11 Step 351 Train Loss: 0.4705
Epoch 11 Step 401 Train Loss: 0.4663
Epoch 11 Step 451 Train Loss: 0.4323
Epoch 11 Step 501 Train Loss: 0.4787
Epoch 11 Step 551 Train Loss: 0.4518
Epoch 11 Step 601 Train Loss: 0.4511
Epoch 11 Step 651 Train Loss: 0.4428
Epoch 11 Step 701 Train Loss: 0.4742
Epoch 11 Step 751 Train Loss: 0.5305
Epoch 11 Step 801 Train Loss: 0.4293
Epoch 11 Step 851 Train Loss: 0.4927
Epoch 11 Step 901 Train Loss: 0.5362
Epoch 11 Step 951 Train Loss: 0.4525
Epoch 11 Step 1001 Train Loss: 0.4397
Epoch 11 Step 1051 Train Loss: 0.4269
Epoch 11 Step 1101 Train Loss: 0.4797
Epoch 11 Step 1151 Train Loss: 0.4887
Epoch 11 Step 1201 Train Loss: 0.4709
Epoch 11 Step 1251 Train Loss: 0.4599
Epoch 11 Step 1301 Train Loss: 0.5027
Epoch 11 Step 1351 Train Loss: 0.5109
Epoch 11 Step 1401 Train Loss: 0.4314
Epoch 11 Step 1451 Train Loss: 0.4223
Epoch 11 Step 1501 Train Loss: 0.4649
Epoch 11 Step 1551 Train Loss: 0.4883
Epoch 11 Step 1601 Train Loss: 0.4760
Epoch 11 Step 1651 Train Loss: 0.4681
Epoch 11 Step 1701 Train Loss: 0.4573
Epoch 11 Step 1751 Train Loss: 0.4895
Epoch 11 Step 1801 Train Loss: 0.4171
Epoch 11 Step 1851 Train Loss: 0.4280
Epoch 11: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0930 Validation Top 20 DE MSE: 0.1348. 
Epoch 12 Step 1 Train Loss: 0.4799
Epoch 12 Step 51 Train Loss: 0.4532
Epoch 12 Step 101 Train Loss: 0.4905
Epoch 12 Step 151 Train Loss: 0.4480
Epoch 12 Step 201 Train Loss: 0.4365
Epoch 12 Step 251 Train Loss: 0.4604
Epoch 12 Step 301 Train Loss: 0.4588
Epoch 12 Step 351 Train Loss: 0.4769
Epoch 12 Step 401 Train Loss: 0.4651
Epoch 12 Step 451 Train Loss: 0.4906
Epoch 12 Step 501 Train Loss: 0.4792
Epoch 12 Step 551 Train Loss: 0.4849
Epoch 12 Step 601 Train Loss: 0.4490
Epoch 12 Step 651 Train Loss: 0.4758
Epoch 12 Step 701 Train Loss: 0.4598
Epoch 12 Step 751 Train Loss: 0.5160
Epoch 12 Step 801 Train Loss: 0.4636
Epoch 12 Step 851 Train Loss: 0.4990
Epoch 12 Step 901 Train Loss: 0.4517
Epoch 12 Step 951 Train Loss: 0.4998
Epoch 12 Step 1001 Train Loss: 0.4655
Epoch 12 Step 1051 Train Loss: 0.4833
Epoch 12 Step 1101 Train Loss: 0.4797
Epoch 12 Step 1151 Train Loss: 0.4701
Epoch 12 Step 1201 Train Loss: 0.4503
Epoch 12 Step 1251 Train Loss: 0.4692
Epoch 12 Step 1301 Train Loss: 0.5114
Epoch 12 Step 1351 Train Loss: 0.4127
Epoch 12 Step 1401 Train Loss: 0.4416
Epoch 12 Step 1451 Train Loss: 0.5204
Epoch 12 Step 1501 Train Loss: 0.4644
Epoch 12 Step 1551 Train Loss: 0.4640
Epoch 12 Step 1601 Train Loss: 0.4689
Epoch 12 Step 1651 Train Loss: 0.4816
Epoch 12 Step 1701 Train Loss: 0.4871
Epoch 12 Step 1751 Train Loss: 0.4680
Epoch 12 Step 1801 Train Loss: 0.5405
Epoch 12 Step 1851 Train Loss: 0.4670
Epoch 12: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0891 Validation Top 20 DE MSE: 0.1304. 
Epoch 13 Step 1 Train Loss: 0.4631
Epoch 13 Step 51 Train Loss: 0.4651
Epoch 13 Step 101 Train Loss: 0.5047
Epoch 13 Step 151 Train Loss: 0.4953
Epoch 13 Step 201 Train Loss: 0.5620
Epoch 13 Step 251 Train Loss: 0.4545
Epoch 13 Step 301 Train Loss: 0.4601
Epoch 13 Step 351 Train Loss: 0.4794
Epoch 13 Step 401 Train Loss: 0.4645
Epoch 13 Step 451 Train Loss: 0.4516
Epoch 13 Step 501 Train Loss: 0.5165
Epoch 13 Step 551 Train Loss: 0.4789
Epoch 13 Step 601 Train Loss: 0.4691
Epoch 13 Step 651 Train Loss: 0.5883
Epoch 13 Step 701 Train Loss: 0.4404
Epoch 13 Step 751 Train Loss: 0.4590
Epoch 13 Step 801 Train Loss: 0.5606
Epoch 13 Step 851 Train Loss: 0.5330
Epoch 13 Step 901 Train Loss: 0.4151
Epoch 13 Step 951 Train Loss: 0.4839
Epoch 13 Step 1001 Train Loss: 0.4828
Epoch 13 Step 1051 Train Loss: 0.4796
Epoch 13 Step 1101 Train Loss: 0.4796
Epoch 13 Step 1151 Train Loss: 0.4950
Epoch 13 Step 1201 Train Loss: 0.5506
Epoch 13 Step 1251 Train Loss: 0.4619
Epoch 13 Step 1301 Train Loss: 0.4279
Epoch 13 Step 1351 Train Loss: 0.4874
Epoch 13 Step 1401 Train Loss: 0.4270
Epoch 13 Step 1451 Train Loss: 0.4286
Epoch 13 Step 1501 Train Loss: 0.4502
Epoch 13 Step 1551 Train Loss: 0.5045
Epoch 13 Step 1601 Train Loss: 0.4812
Epoch 13 Step 1651 Train Loss: 0.4613
Epoch 13 Step 1701 Train Loss: 0.4876
Epoch 13 Step 1751 Train Loss: 0.4642
Epoch 13 Step 1801 Train Loss: 0.4669
Epoch 13 Step 1851 Train Loss: 0.4336
Epoch 13: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0966 Validation Top 20 DE MSE: 0.1355. 
Epoch 14 Step 1 Train Loss: 0.4624
Epoch 14 Step 51 Train Loss: 0.4366
Epoch 14 Step 101 Train Loss: 0.5029
Epoch 14 Step 151 Train Loss: 0.4445
Epoch 14 Step 201 Train Loss: 0.5149
Epoch 14 Step 251 Train Loss: 0.4537
Epoch 14 Step 301 Train Loss: 0.4445
Epoch 14 Step 351 Train Loss: 0.5043
Epoch 14 Step 401 Train Loss: 0.4536
Epoch 14 Step 451 Train Loss: 0.4694
Epoch 14 Step 501 Train Loss: 0.4465
Epoch 14 Step 551 Train Loss: 0.4299
Epoch 14 Step 601 Train Loss: 0.4416
Epoch 14 Step 651 Train Loss: 0.4220
Epoch 14 Step 701 Train Loss: 0.5226
Epoch 14 Step 751 Train Loss: 0.4788
Epoch 14 Step 801 Train Loss: 0.4800
Epoch 14 Step 851 Train Loss: 0.4814
Epoch 14 Step 901 Train Loss: 0.4890
Epoch 14 Step 951 Train Loss: 0.4390
Epoch 14 Step 1001 Train Loss: 0.4379
Epoch 14 Step 1051 Train Loss: 0.4559
Epoch 14 Step 1101 Train Loss: 0.4763
Epoch 14 Step 1151 Train Loss: 0.4170
Epoch 14 Step 1201 Train Loss: 0.4740
Epoch 14 Step 1251 Train Loss: 0.4952
Epoch 14 Step 1301 Train Loss: 0.4467
Epoch 14 Step 1351 Train Loss: 0.4229
Epoch 14 Step 1401 Train Loss: 0.4508
Epoch 14 Step 1451 Train Loss: 0.4630
Epoch 14 Step 1501 Train Loss: 0.5211
Epoch 14 Step 1551 Train Loss: 0.4525
Epoch 14 Step 1601 Train Loss: 0.4847
Epoch 14 Step 1651 Train Loss: 0.4329
Epoch 14 Step 1701 Train Loss: 0.4565
Epoch 14 Step 1751 Train Loss: 0.4563
Epoch 14 Step 1801 Train Loss: 0.4610
Epoch 14 Step 1851 Train Loss: 0.4875
Epoch 14: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0872 Validation Top 20 DE MSE: 0.1296. 
Epoch 15 Step 1 Train Loss: 0.4308
Epoch 15 Step 51 Train Loss: 0.4503
Epoch 15 Step 101 Train Loss: 0.4740
Epoch 15 Step 151 Train Loss: 0.5263
Epoch 15 Step 201 Train Loss: 0.5222
Epoch 15 Step 251 Train Loss: 0.5025
Epoch 15 Step 301 Train Loss: 0.4156
Epoch 15 Step 351 Train Loss: 0.4621
Epoch 15 Step 401 Train Loss: 0.4539
Epoch 15 Step 451 Train Loss: 0.5225
Epoch 15 Step 501 Train Loss: 0.5070
Epoch 15 Step 551 Train Loss: 0.4462
Epoch 15 Step 601 Train Loss: 0.4568
Epoch 15 Step 651 Train Loss: 0.4339
Epoch 15 Step 701 Train Loss: 0.4696
Epoch 15 Step 751 Train Loss: 0.4626
Epoch 15 Step 801 Train Loss: 0.4534
Epoch 15 Step 851 Train Loss: 0.4255
Epoch 15 Step 901 Train Loss: 0.4885
Epoch 15 Step 951 Train Loss: 0.4684
Epoch 15 Step 1001 Train Loss: 0.4838
Epoch 15 Step 1051 Train Loss: 0.4512
Epoch 15 Step 1101 Train Loss: 0.4322
Epoch 15 Step 1151 Train Loss: 0.4839
Epoch 15 Step 1201 Train Loss: 0.4716
Epoch 15 Step 1251 Train Loss: 0.5093
Epoch 15 Step 1301 Train Loss: 0.4455
Epoch 15 Step 1351 Train Loss: 0.5171
Epoch 15 Step 1401 Train Loss: 0.4896
Epoch 15 Step 1451 Train Loss: 0.5291
Epoch 15 Step 1501 Train Loss: 0.4874
Epoch 15 Step 1551 Train Loss: 0.5346
Epoch 15 Step 1601 Train Loss: 0.5387
Epoch 15 Step 1651 Train Loss: 0.4542
Epoch 15 Step 1701 Train Loss: 0.4147
Epoch 15 Step 1751 Train Loss: 0.4500
Epoch 15 Step 1801 Train Loss: 0.5455
Epoch 15 Step 1851 Train Loss: 0.4533
Epoch 15: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0873 Validation Top 20 DE MSE: 0.1290. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1425
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0039013391
test_combo_seen0_pearson: 0.9909404228001356
test_combo_seen0_mse_de: 0.12127842
test_combo_seen0_pearson_de: 0.6416607541206576
test_combo_seen1_mse: 0.0044755307
test_combo_seen1_pearson: 0.9898136574668233
test_combo_seen1_mse_de: 0.1553352
test_combo_seen1_pearson_de: 0.7689414576879151
test_combo_seen2_mse: 0.0034922962
test_combo_seen2_pearson: 0.9922890090102879
test_combo_seen2_mse_de: 0.12711416
test_combo_seen2_pearson_de: 0.9499773791585888
test_unseen_single_mse: 0.0023609344
test_unseen_single_pearson: 0.9945411024229623
test_unseen_single_mse_de: 0.13471143
test_unseen_single_pearson_de: 0.9124551436186209
test_combo_seen0_pearson_delta: 0.6086220460401678
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.08888888888888889
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.9055555555555556
test_combo_seen0_mse_top20_de_non_dropout: 0.16156366
test_combo_seen1_pearson_delta: 0.5750553198697597
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.12884615384615383
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8596153846153847
test_combo_seen1_mse_top20_de_non_dropout: 0.19569741
test_combo_seen2_pearson_delta: 0.602822514209911
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.11764705882352941
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.9382352941176471
test_combo_seen2_mse_top20_de_non_dropout: 0.1347459
test_unseen_single_pearson_delta: 0.43409098680146685
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.24444444444444444
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9129629629629631
test_unseen_single_mse_top20_de_non_dropout: 0.15222009
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:                                                    train_mse ‚ñÑ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                train_pearson ‚ñÖ‚ñÅ‚ñÖ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ
wandb:                                                   val_de_mse ‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                                      val_mse ‚ñÑ‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ
wandb:                                                  val_pearson ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.08889
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.90556
wandb:                                         test_combo_seen0_mse 0.0039
wandb:                                      test_combo_seen0_mse_de 0.12128
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.16156
wandb:                                     test_combo_seen0_pearson 0.99094
wandb:                                  test_combo_seen0_pearson_de 0.64166
wandb:                               test_combo_seen0_pearson_delta 0.60862
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.12885
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.85962
wandb:                                         test_combo_seen1_mse 0.00448
wandb:                                      test_combo_seen1_mse_de 0.15534
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.1957
wandb:                                     test_combo_seen1_pearson 0.98981
wandb:                                  test_combo_seen1_pearson_de 0.76894
wandb:                               test_combo_seen1_pearson_delta 0.57506
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.11765
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.93824
wandb:                                         test_combo_seen2_mse 0.00349
wandb:                                      test_combo_seen2_mse_de 0.12711
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.13475
wandb:                                     test_combo_seen2_pearson 0.99229
wandb:                                  test_combo_seen2_pearson_de 0.94998
wandb:                               test_combo_seen2_pearson_delta 0.60282
wandb:                                                  test_de_mse 0.14254
wandb:                                              test_de_pearson 0.82425
wandb:               test_frac_opposite_direction_top20_non_dropout 0.15333
wandb:                          test_frac_sigma_below_1_non_dropout 0.89
wandb:                                                     test_mse 0.00372
wandb:                                test_mse_top20_de_non_dropout 0.17172
wandb:                                                 test_pearson 0.99153
wandb:                                           test_pearson_delta 0.54618
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.24444
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.91296
wandb:                                       test_unseen_single_mse 0.00236
wandb:                                    test_unseen_single_mse_de 0.13471
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15222
wandb:                                   test_unseen_single_pearson 0.99454
wandb:                                test_unseen_single_pearson_de 0.91246
wandb:                             test_unseen_single_pearson_delta 0.43409
wandb:                                                 train_de_mse 0.08735
wandb:                                             train_de_pearson 0.91881
wandb:                                                    train_mse 0.00206
wandb:                                                train_pearson 0.99541
wandb:                                                training_loss 0.52202
wandb:                                                   val_de_mse 0.12901
wandb:                                               val_de_pearson 0.82569
wandb:                                                      val_mse 0.00342
wandb:                                                  val_pearson 0.99224
wandb: 
wandb: üöÄ View run scbert_NormanWeissman2019_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/dn3wblvv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_081039-dn3wblvv/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:7
combo_seen1:51
combo_seen2:18
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241029_091953-v5kgxi7i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_NormanWeissman2019_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/v5kgxi7i
wandb: WARNING Serializing object of type ndarray that is 8059328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4675
Epoch 1 Step 51 Train Loss: 0.4585
Epoch 1 Step 101 Train Loss: 0.4835
Epoch 1 Step 151 Train Loss: 0.4966
Epoch 1 Step 201 Train Loss: 0.4940
Epoch 1 Step 251 Train Loss: 0.4729
Epoch 1 Step 301 Train Loss: 0.4817
Epoch 1 Step 351 Train Loss: 0.4879
Epoch 1 Step 401 Train Loss: 0.4225
Epoch 1 Step 451 Train Loss: 0.4145
Epoch 1 Step 501 Train Loss: 0.5053
Epoch 1 Step 551 Train Loss: 0.4228
Epoch 1 Step 601 Train Loss: 0.4407
Epoch 1 Step 651 Train Loss: 0.4207
Epoch 1 Step 701 Train Loss: 0.4529
Epoch 1 Step 751 Train Loss: 0.4717
Epoch 1 Step 801 Train Loss: 0.4940
Epoch 1 Step 851 Train Loss: 0.4577
Epoch 1 Step 901 Train Loss: 0.4131
Epoch 1 Step 951 Train Loss: 0.4359
Epoch 1 Step 1001 Train Loss: 0.4449
Epoch 1 Step 1051 Train Loss: 0.4689
Epoch 1 Step 1101 Train Loss: 0.4687
Epoch 1 Step 1151 Train Loss: 0.4462
Epoch 1 Step 1201 Train Loss: 0.4566
Epoch 1 Step 1251 Train Loss: 0.5045
Epoch 1 Step 1301 Train Loss: 0.4945
Epoch 1 Step 1351 Train Loss: 0.4497
Epoch 1 Step 1401 Train Loss: 0.4531
Epoch 1 Step 1451 Train Loss: 0.4678
Epoch 1 Step 1501 Train Loss: 0.4197
Epoch 1 Step 1551 Train Loss: 0.3976
Epoch 1 Step 1601 Train Loss: 0.3949
Epoch 1 Step 1651 Train Loss: 0.4342
Epoch 1 Step 1701 Train Loss: 0.4612
Epoch 1 Step 1751 Train Loss: 0.3985
Epoch 1 Step 1801 Train Loss: 0.4570
Epoch 1 Step 1851 Train Loss: 0.4276
Epoch 1: Train Overall MSE: 0.0049 Validation Overall MSE: 0.0055. 
Train Top 20 DE MSE: 0.2160 Validation Top 20 DE MSE: 0.2811. 
Epoch 2 Step 1 Train Loss: 0.4563
Epoch 2 Step 51 Train Loss: 0.4902
Epoch 2 Step 101 Train Loss: 0.4606
Epoch 2 Step 151 Train Loss: 0.4817
Epoch 2 Step 201 Train Loss: 0.5089
Epoch 2 Step 251 Train Loss: 0.4133
Epoch 2 Step 301 Train Loss: 0.4245
Epoch 2 Step 351 Train Loss: 0.4599
Epoch 2 Step 401 Train Loss: 0.4574
Epoch 2 Step 451 Train Loss: 0.4045
Epoch 2 Step 501 Train Loss: 0.4267
Epoch 2 Step 551 Train Loss: 0.4459
Epoch 2 Step 601 Train Loss: 0.4618
Epoch 2 Step 651 Train Loss: 0.4749
Epoch 2 Step 701 Train Loss: 0.4102
Epoch 2 Step 751 Train Loss: 0.4675
Epoch 2 Step 801 Train Loss: 0.4786
Epoch 2 Step 851 Train Loss: 0.5069
Epoch 2 Step 901 Train Loss: 0.4683
Epoch 2 Step 951 Train Loss: 0.4439
Epoch 2 Step 1001 Train Loss: 0.4193
Epoch 2 Step 1051 Train Loss: 0.4494
Epoch 2 Step 1101 Train Loss: 0.4272
Epoch 2 Step 1151 Train Loss: 0.4349
Epoch 2 Step 1201 Train Loss: 0.4638
Epoch 2 Step 1251 Train Loss: 0.4608
Epoch 2 Step 1301 Train Loss: 0.5052
Epoch 2 Step 1351 Train Loss: 0.4326
Epoch 2 Step 1401 Train Loss: 0.4601
Epoch 2 Step 1451 Train Loss: 0.4374
Epoch 2 Step 1501 Train Loss: 0.4792
Epoch 2 Step 1551 Train Loss: 0.3975
Epoch 2 Step 1601 Train Loss: 0.4474
Epoch 2 Step 1651 Train Loss: 0.4499
Epoch 2 Step 1701 Train Loss: 0.4572
Epoch 2 Step 1751 Train Loss: 0.4607
Epoch 2 Step 1801 Train Loss: 0.5093
Epoch 2 Step 1851 Train Loss: 0.4391
Epoch 2: Train Overall MSE: 0.0043 Validation Overall MSE: 0.0046. 
Train Top 20 DE MSE: 0.1096 Validation Top 20 DE MSE: 0.1345. 
Epoch 3 Step 1 Train Loss: 0.4372
Epoch 3 Step 51 Train Loss: 0.4016
Epoch 3 Step 101 Train Loss: 0.4953
Epoch 3 Step 151 Train Loss: 0.4444
Epoch 3 Step 201 Train Loss: 0.5068
Epoch 3 Step 251 Train Loss: 0.4116
Epoch 3 Step 301 Train Loss: 0.4350
Epoch 3 Step 351 Train Loss: 0.4939
Epoch 3 Step 401 Train Loss: 0.4519
Epoch 3 Step 451 Train Loss: 0.4669
Epoch 3 Step 501 Train Loss: 0.4901
Epoch 3 Step 551 Train Loss: 0.4922
Epoch 3 Step 601 Train Loss: 0.4229
Epoch 3 Step 651 Train Loss: 0.4681
Epoch 3 Step 701 Train Loss: 0.4231
Epoch 3 Step 751 Train Loss: 0.4094
Epoch 3 Step 801 Train Loss: 0.4359
Epoch 3 Step 851 Train Loss: 0.4512
Epoch 3 Step 901 Train Loss: 0.4753
Epoch 3 Step 951 Train Loss: 0.4714
Epoch 3 Step 1001 Train Loss: 0.4796
Epoch 3 Step 1051 Train Loss: 0.4397
Epoch 3 Step 1101 Train Loss: 0.4134
Epoch 3 Step 1151 Train Loss: 0.4227
Epoch 3 Step 1201 Train Loss: 0.4737
Epoch 3 Step 1251 Train Loss: 0.4293
Epoch 3 Step 1301 Train Loss: 0.4968
Epoch 3 Step 1351 Train Loss: 0.4531
Epoch 3 Step 1401 Train Loss: 0.4428
Epoch 3 Step 1451 Train Loss: 0.4603
Epoch 3 Step 1501 Train Loss: 0.4344
Epoch 3 Step 1551 Train Loss: 0.4155
Epoch 3 Step 1601 Train Loss: 0.4457
Epoch 3 Step 1651 Train Loss: 0.5275
Epoch 3 Step 1701 Train Loss: 0.4460
Epoch 3 Step 1751 Train Loss: 0.4165
Epoch 3 Step 1801 Train Loss: 0.3761
Epoch 3 Step 1851 Train Loss: 0.4348
Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0038. 
Train Top 20 DE MSE: 0.1138 Validation Top 20 DE MSE: 0.1834. 
Epoch 4 Step 1 Train Loss: 0.4602
Epoch 4 Step 51 Train Loss: 0.4285
Epoch 4 Step 101 Train Loss: 0.4988
Epoch 4 Step 151 Train Loss: 0.4879
Epoch 4 Step 201 Train Loss: 0.4756
Epoch 4 Step 251 Train Loss: 0.4617
Epoch 4 Step 301 Train Loss: 0.4671
Epoch 4 Step 351 Train Loss: 0.4328
Epoch 4 Step 401 Train Loss: 0.4385
Epoch 4 Step 451 Train Loss: 0.4516
Epoch 4 Step 501 Train Loss: 0.4272
Epoch 4 Step 551 Train Loss: 0.5725
Epoch 4 Step 601 Train Loss: 0.4643
Epoch 4 Step 651 Train Loss: 0.4427
Epoch 4 Step 701 Train Loss: 0.4158
Epoch 4 Step 751 Train Loss: 0.4637
Epoch 4 Step 801 Train Loss: 0.4568
Epoch 4 Step 851 Train Loss: 0.4510
Epoch 4 Step 901 Train Loss: 0.4214
Epoch 4 Step 951 Train Loss: 0.4750
Epoch 4 Step 1001 Train Loss: 0.4598
Epoch 4 Step 1051 Train Loss: 0.4678
Epoch 4 Step 1101 Train Loss: 0.4658
Epoch 4 Step 1151 Train Loss: 0.4425
Epoch 4 Step 1201 Train Loss: 0.4599
Epoch 4 Step 1251 Train Loss: 0.4788
Epoch 4 Step 1301 Train Loss: 0.4711
Epoch 4 Step 1351 Train Loss: 0.4442
Epoch 4 Step 1401 Train Loss: 0.4609
Epoch 4 Step 1451 Train Loss: 0.4499
Epoch 4 Step 1501 Train Loss: 0.4924
Epoch 4 Step 1551 Train Loss: 0.4108
Epoch 4 Step 1601 Train Loss: 0.4428
Epoch 4 Step 1651 Train Loss: 0.4468
Epoch 4 Step 1701 Train Loss: 0.4999
Epoch 4 Step 1751 Train Loss: 0.4710
Epoch 4 Step 1801 Train Loss: 0.4609
Epoch 4 Step 1851 Train Loss: 0.4673
Epoch 4: Train Overall MSE: 0.0032 Validation Overall MSE: 0.0043. 
Train Top 20 DE MSE: 0.0870 Validation Top 20 DE MSE: 0.1476. 
Epoch 5 Step 1 Train Loss: 0.4135
Epoch 5 Step 51 Train Loss: 0.4752
Epoch 5 Step 101 Train Loss: 0.4536
Epoch 5 Step 151 Train Loss: 0.5166
Epoch 5 Step 201 Train Loss: 0.4257
Epoch 5 Step 251 Train Loss: 0.4543
Epoch 5 Step 301 Train Loss: 0.4445
Epoch 5 Step 351 Train Loss: 0.4553
Epoch 5 Step 401 Train Loss: 0.4973
Epoch 5 Step 451 Train Loss: 0.4249
Epoch 5 Step 501 Train Loss: 0.4229
Epoch 5 Step 551 Train Loss: 0.4325
Epoch 5 Step 601 Train Loss: 0.4995
Epoch 5 Step 651 Train Loss: 0.4552
Epoch 5 Step 701 Train Loss: 0.5000
Epoch 5 Step 751 Train Loss: 0.4284
Epoch 5 Step 801 Train Loss: 0.4367
Epoch 5 Step 851 Train Loss: 0.4403
Epoch 5 Step 901 Train Loss: 0.4221
Epoch 5 Step 951 Train Loss: 0.4478
Epoch 5 Step 1001 Train Loss: 0.4781
Epoch 5 Step 1051 Train Loss: 0.4663
Epoch 5 Step 1101 Train Loss: 0.4343
Epoch 5 Step 1151 Train Loss: 0.4718
Epoch 5 Step 1201 Train Loss: 0.5008
Epoch 5 Step 1251 Train Loss: 0.4475
Epoch 5 Step 1301 Train Loss: 0.4947
Epoch 5 Step 1351 Train Loss: 0.4850
Epoch 5 Step 1401 Train Loss: 0.4298
Epoch 5 Step 1451 Train Loss: 0.4689
Epoch 5 Step 1501 Train Loss: 0.4660
Epoch 5 Step 1551 Train Loss: 0.4387
Epoch 5 Step 1601 Train Loss: 0.4300
Epoch 5 Step 1651 Train Loss: 0.4405
Epoch 5 Step 1701 Train Loss: 0.4449
Epoch 5 Step 1751 Train Loss: 0.5107
Epoch 5 Step 1801 Train Loss: 0.5048
Epoch 5 Step 1851 Train Loss: 0.4925
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0884 Validation Top 20 DE MSE: 0.1490. 
Epoch 6 Step 1 Train Loss: 0.4450
Epoch 6 Step 51 Train Loss: 0.4864
Epoch 6 Step 101 Train Loss: 0.5650
Epoch 6 Step 151 Train Loss: 0.5178
Epoch 6 Step 201 Train Loss: 0.4608
Epoch 6 Step 251 Train Loss: 0.4786
Epoch 6 Step 301 Train Loss: 0.4715
Epoch 6 Step 351 Train Loss: 0.4897
Epoch 6 Step 401 Train Loss: 0.4217
Epoch 6 Step 451 Train Loss: 0.4932
Epoch 6 Step 501 Train Loss: 0.4531
Epoch 6 Step 551 Train Loss: 0.4288
Epoch 6 Step 601 Train Loss: 0.4840
Epoch 6 Step 651 Train Loss: 0.4691
Epoch 6 Step 701 Train Loss: 0.4861
Epoch 6 Step 751 Train Loss: 0.4087
Epoch 6 Step 801 Train Loss: 0.4652
Epoch 6 Step 851 Train Loss: 0.5170
Epoch 6 Step 901 Train Loss: 0.5520
Epoch 6 Step 951 Train Loss: 0.4509
Epoch 6 Step 1001 Train Loss: 0.5704
Epoch 6 Step 1051 Train Loss: 0.4535
Epoch 6 Step 1101 Train Loss: 0.4756
Epoch 6 Step 1151 Train Loss: 0.4784
Epoch 6 Step 1201 Train Loss: 0.4567
Epoch 6 Step 1251 Train Loss: 0.4450
Epoch 6 Step 1301 Train Loss: 0.5502
Epoch 6 Step 1351 Train Loss: 0.4283
Epoch 6 Step 1401 Train Loss: 0.4640
Epoch 6 Step 1451 Train Loss: 0.4507
Epoch 6 Step 1501 Train Loss: 0.4215
Epoch 6 Step 1551 Train Loss: 0.4879
Epoch 6 Step 1601 Train Loss: 0.5112
Epoch 6 Step 1651 Train Loss: 0.4289
Epoch 6 Step 1701 Train Loss: 0.4736
Epoch 6 Step 1751 Train Loss: 0.5008
Epoch 6 Step 1801 Train Loss: 0.4660
Epoch 6 Step 1851 Train Loss: 0.5298
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0865 Validation Top 20 DE MSE: 0.1555. 
Epoch 7 Step 1 Train Loss: 0.4371
Epoch 7 Step 51 Train Loss: 0.4493
Epoch 7 Step 101 Train Loss: 0.4913
Epoch 7 Step 151 Train Loss: 0.4597
Epoch 7 Step 201 Train Loss: 0.5247
Epoch 7 Step 251 Train Loss: 0.4598
Epoch 7 Step 301 Train Loss: 0.4329
Epoch 7 Step 351 Train Loss: 0.5323
Epoch 7 Step 401 Train Loss: 0.4898
Epoch 7 Step 451 Train Loss: 0.4348
Epoch 7 Step 501 Train Loss: 0.5484
Epoch 7 Step 551 Train Loss: 0.5014
Epoch 7 Step 601 Train Loss: 0.4765
Epoch 7 Step 651 Train Loss: 0.4569
Epoch 7 Step 701 Train Loss: 0.4309
Epoch 7 Step 751 Train Loss: 0.4738
Epoch 7 Step 801 Train Loss: 0.4575
Epoch 7 Step 851 Train Loss: 0.5663
Epoch 7 Step 901 Train Loss: 0.5228
Epoch 7 Step 951 Train Loss: 0.4515
Epoch 7 Step 1001 Train Loss: 0.5203
Epoch 7 Step 1051 Train Loss: 0.4734
Epoch 7 Step 1101 Train Loss: 0.5206
Epoch 7 Step 1151 Train Loss: 0.5004
Epoch 7 Step 1201 Train Loss: 0.4902
Epoch 7 Step 1251 Train Loss: 0.4853
Epoch 7 Step 1301 Train Loss: 0.4795
Epoch 7 Step 1351 Train Loss: 0.5378
Epoch 7 Step 1401 Train Loss: 0.4728
Epoch 7 Step 1451 Train Loss: 0.4765
Epoch 7 Step 1501 Train Loss: 0.4844
Epoch 7 Step 1551 Train Loss: 0.4902
Epoch 7 Step 1601 Train Loss: 0.4983
Epoch 7 Step 1651 Train Loss: 0.4589
Epoch 7 Step 1701 Train Loss: 0.4400
Epoch 7 Step 1751 Train Loss: 0.4316
Epoch 7 Step 1801 Train Loss: 0.5163
Epoch 7 Step 1851 Train Loss: 0.4385
Epoch 7: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0823 Validation Top 20 DE MSE: 0.1494. 
Epoch 8 Step 1 Train Loss: 0.4894
Epoch 8 Step 51 Train Loss: 0.4628
Epoch 8 Step 101 Train Loss: 0.4726
Epoch 8 Step 151 Train Loss: 0.4901
Epoch 8 Step 201 Train Loss: 0.4222
Epoch 8 Step 251 Train Loss: 0.4608
Epoch 8 Step 301 Train Loss: 0.4592
Epoch 8 Step 351 Train Loss: 0.5085
Epoch 8 Step 401 Train Loss: 0.4848
Epoch 8 Step 451 Train Loss: 0.5109
Epoch 8 Step 501 Train Loss: 0.4911
Epoch 8 Step 551 Train Loss: 0.4479
Epoch 8 Step 601 Train Loss: 0.4616
Epoch 8 Step 651 Train Loss: 0.5845
Epoch 8 Step 701 Train Loss: 0.4696
Epoch 8 Step 751 Train Loss: 0.5011
Epoch 8 Step 801 Train Loss: 0.4925
Epoch 8 Step 851 Train Loss: 0.4477
Epoch 8 Step 901 Train Loss: 0.5292
Epoch 8 Step 951 Train Loss: 0.5159
Epoch 8 Step 1001 Train Loss: 0.5077
Epoch 8 Step 1051 Train Loss: 0.4342
Epoch 8 Step 1101 Train Loss: 0.5130
Epoch 8 Step 1151 Train Loss: 0.4639
Epoch 8 Step 1201 Train Loss: 0.4712
Epoch 8 Step 1251 Train Loss: 0.5251
Epoch 8 Step 1301 Train Loss: 0.4893
Epoch 8 Step 1351 Train Loss: 0.4605
Epoch 8 Step 1401 Train Loss: 0.4952
Epoch 8 Step 1451 Train Loss: 0.4982
Epoch 8 Step 1501 Train Loss: 0.4394
Epoch 8 Step 1551 Train Loss: 0.4377
Epoch 8 Step 1601 Train Loss: 0.5913
Epoch 8 Step 1651 Train Loss: 0.4848
Epoch 8 Step 1701 Train Loss: 0.4873
Epoch 8 Step 1751 Train Loss: 0.4695
Epoch 8 Step 1801 Train Loss: 0.4578
Epoch 8 Step 1851 Train Loss: 0.4956
Epoch 8: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0824 Validation Top 20 DE MSE: 0.1498. 
Epoch 9 Step 1 Train Loss: 0.4651
Epoch 9 Step 51 Train Loss: 0.4482
Epoch 9 Step 101 Train Loss: 0.4925
Epoch 9 Step 151 Train Loss: 0.4994
Epoch 9 Step 201 Train Loss: 0.4915
Epoch 9 Step 251 Train Loss: 0.4903
Epoch 9 Step 301 Train Loss: 0.4209
Epoch 9 Step 351 Train Loss: 0.4400
Epoch 9 Step 401 Train Loss: 0.4824
Epoch 9 Step 451 Train Loss: 0.4310
Epoch 9 Step 501 Train Loss: 0.4640
Epoch 9 Step 551 Train Loss: 0.4936
Epoch 9 Step 601 Train Loss: 0.4993
Epoch 9 Step 651 Train Loss: 0.4936
Epoch 9 Step 701 Train Loss: 0.4718
Epoch 9 Step 751 Train Loss: 0.4464
Epoch 9 Step 801 Train Loss: 0.5443
Epoch 9 Step 851 Train Loss: 0.4512
Epoch 9 Step 901 Train Loss: 0.4863
Epoch 9 Step 951 Train Loss: 0.4472
Epoch 9 Step 1001 Train Loss: 0.4651
Epoch 9 Step 1051 Train Loss: 0.4093
Epoch 9 Step 1101 Train Loss: 0.4429
Epoch 9 Step 1151 Train Loss: 0.4700
Epoch 9 Step 1201 Train Loss: 0.4844
Epoch 9 Step 1251 Train Loss: 0.4809
Epoch 9 Step 1301 Train Loss: 0.5285
Epoch 9 Step 1351 Train Loss: 0.5175
Epoch 9 Step 1401 Train Loss: 0.5380
Epoch 9 Step 1451 Train Loss: 0.5018
Epoch 9 Step 1501 Train Loss: 0.5214
Epoch 9 Step 1551 Train Loss: 0.4405
Epoch 9 Step 1601 Train Loss: 0.4881
Epoch 9 Step 1651 Train Loss: 0.4899
Epoch 9 Step 1701 Train Loss: 0.4746
Epoch 9 Step 1751 Train Loss: 0.4624
Epoch 9 Step 1801 Train Loss: 0.4463
Epoch 9 Step 1851 Train Loss: 0.4680
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0787 Validation Top 20 DE MSE: 0.1481. 
Epoch 10 Step 1 Train Loss: 0.4482
Epoch 10 Step 51 Train Loss: 0.4393
Epoch 10 Step 101 Train Loss: 0.5012
Epoch 10 Step 151 Train Loss: 0.5112
Epoch 10 Step 201 Train Loss: 0.5083
Epoch 10 Step 251 Train Loss: 0.4681
Epoch 10 Step 301 Train Loss: 0.4873
Epoch 10 Step 351 Train Loss: 0.4127
Epoch 10 Step 401 Train Loss: 0.4441
Epoch 10 Step 451 Train Loss: 0.5204
Epoch 10 Step 501 Train Loss: 0.4676
Epoch 10 Step 551 Train Loss: 0.5079
Epoch 10 Step 601 Train Loss: 0.5278
Epoch 10 Step 651 Train Loss: 0.4970
Epoch 10 Step 701 Train Loss: 0.5068
Epoch 10 Step 751 Train Loss: 0.4425
Epoch 10 Step 801 Train Loss: 0.4991
Epoch 10 Step 851 Train Loss: 0.4733
Epoch 10 Step 901 Train Loss: 0.5131
Epoch 10 Step 951 Train Loss: 0.4449
Epoch 10 Step 1001 Train Loss: 0.4508
Epoch 10 Step 1051 Train Loss: 0.4882
Epoch 10 Step 1101 Train Loss: 0.4887
Epoch 10 Step 1151 Train Loss: 0.4589
Epoch 10 Step 1201 Train Loss: 0.4705
Epoch 10 Step 1251 Train Loss: 0.4795
Epoch 10 Step 1301 Train Loss: 0.4575
Epoch 10 Step 1351 Train Loss: 0.4518
Epoch 10 Step 1401 Train Loss: 0.4738
Epoch 10 Step 1451 Train Loss: 0.4602
Epoch 10 Step 1501 Train Loss: 0.4558
Epoch 10 Step 1551 Train Loss: 0.4811
Epoch 10 Step 1601 Train Loss: 0.4841
Epoch 10 Step 1651 Train Loss: 0.4719
Epoch 10 Step 1701 Train Loss: 0.4739
Epoch 10 Step 1751 Train Loss: 0.4892
Epoch 10 Step 1801 Train Loss: 0.4470
Epoch 10 Step 1851 Train Loss: 0.5023
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0825 Validation Top 20 DE MSE: 0.1512. 
Epoch 11 Step 1 Train Loss: 0.4675
Epoch 11 Step 51 Train Loss: 0.5108
Epoch 11 Step 101 Train Loss: 0.4973
Epoch 11 Step 151 Train Loss: 0.4801
Epoch 11 Step 201 Train Loss: 0.5868
Epoch 11 Step 251 Train Loss: 0.4775
Epoch 11 Step 301 Train Loss: 0.4392
Epoch 11 Step 351 Train Loss: 0.4756
Epoch 11 Step 401 Train Loss: 0.5443
Epoch 11 Step 451 Train Loss: 0.4936
Epoch 11 Step 501 Train Loss: 0.4639
Epoch 11 Step 551 Train Loss: 0.4947
Epoch 11 Step 601 Train Loss: 0.4947
Epoch 11 Step 651 Train Loss: 0.4560
Epoch 11 Step 701 Train Loss: 0.5265
Epoch 11 Step 751 Train Loss: 0.5304
Epoch 11 Step 801 Train Loss: 0.4510
Epoch 11 Step 851 Train Loss: 0.4845
Epoch 11 Step 901 Train Loss: 0.4837
Epoch 11 Step 951 Train Loss: 0.4403
Epoch 11 Step 1001 Train Loss: 0.4481
Epoch 11 Step 1051 Train Loss: 0.4844
Epoch 11 Step 1101 Train Loss: 0.5124
Epoch 11 Step 1151 Train Loss: 0.4736
Epoch 11 Step 1201 Train Loss: 0.5194
Epoch 11 Step 1251 Train Loss: 0.5639
Epoch 11 Step 1301 Train Loss: 0.4776
Epoch 11 Step 1351 Train Loss: 0.4421
Epoch 11 Step 1401 Train Loss: 0.4751
Epoch 11 Step 1451 Train Loss: 0.4882
Epoch 11 Step 1501 Train Loss: 0.4859
Epoch 11 Step 1551 Train Loss: 0.4425
Epoch 11 Step 1601 Train Loss: 0.4796
Epoch 11 Step 1651 Train Loss: 0.5265
Epoch 11 Step 1701 Train Loss: 0.4593
Epoch 11 Step 1751 Train Loss: 0.4451
Epoch 11 Step 1801 Train Loss: 0.4652
Epoch 11 Step 1851 Train Loss: 0.4253
Epoch 11: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0788 Validation Top 20 DE MSE: 0.1482. 
Epoch 12 Step 1 Train Loss: 0.4881
Epoch 12 Step 51 Train Loss: 0.4754
Epoch 12 Step 101 Train Loss: 0.5126
Epoch 12 Step 151 Train Loss: 0.5027
Epoch 12 Step 201 Train Loss: 0.4327
Epoch 12 Step 251 Train Loss: 0.4477
Epoch 12 Step 301 Train Loss: 0.4672
Epoch 12 Step 351 Train Loss: 0.4852
Epoch 12 Step 401 Train Loss: 0.4238
Epoch 12 Step 451 Train Loss: 0.4862
Epoch 12 Step 501 Train Loss: 0.5153
Epoch 12 Step 551 Train Loss: 0.4534
Epoch 12 Step 601 Train Loss: 0.4346
Epoch 12 Step 651 Train Loss: 0.4801
Epoch 12 Step 701 Train Loss: 0.5225
Epoch 12 Step 751 Train Loss: 0.4620
Epoch 12 Step 801 Train Loss: 0.5321
Epoch 12 Step 851 Train Loss: 0.4442
Epoch 12 Step 901 Train Loss: 0.4925
Epoch 12 Step 951 Train Loss: 0.4923
Epoch 12 Step 1001 Train Loss: 0.4843
Epoch 12 Step 1051 Train Loss: 0.4612
Epoch 12 Step 1101 Train Loss: 0.5414
Epoch 12 Step 1151 Train Loss: 0.5244
Epoch 12 Step 1201 Train Loss: 0.4494
Epoch 12 Step 1251 Train Loss: 0.4844
Epoch 12 Step 1301 Train Loss: 0.4615
Epoch 12 Step 1351 Train Loss: 0.4856
Epoch 12 Step 1401 Train Loss: 0.4387
Epoch 12 Step 1451 Train Loss: 0.4737
Epoch 12 Step 1501 Train Loss: 0.4714
Epoch 12 Step 1551 Train Loss: 0.4671
Epoch 12 Step 1601 Train Loss: 0.4574
Epoch 12 Step 1651 Train Loss: 0.4951
Epoch 12 Step 1701 Train Loss: 0.4854
Epoch 12 Step 1751 Train Loss: 0.5042
Epoch 12 Step 1801 Train Loss: 0.4530
Epoch 12 Step 1851 Train Loss: 0.4554
Epoch 12: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0791 Validation Top 20 DE MSE: 0.1480. 
Epoch 13 Step 1 Train Loss: 0.5168
Epoch 13 Step 51 Train Loss: 0.4588
Epoch 13 Step 101 Train Loss: 0.4799
Epoch 13 Step 151 Train Loss: 0.5163
Epoch 13 Step 201 Train Loss: 0.4793
Epoch 13 Step 251 Train Loss: 0.5281
Epoch 13 Step 301 Train Loss: 0.4462
Epoch 13 Step 351 Train Loss: 0.5032
Epoch 13 Step 401 Train Loss: 0.5306
Epoch 13 Step 451 Train Loss: 0.4651
Epoch 13 Step 501 Train Loss: 0.4864
Epoch 13 Step 551 Train Loss: 0.4748
Epoch 13 Step 601 Train Loss: 0.4532
Epoch 13 Step 651 Train Loss: 0.4780
Epoch 13 Step 701 Train Loss: 0.5194
Epoch 13 Step 751 Train Loss: 0.4791
Epoch 13 Step 801 Train Loss: 0.5369
Epoch 13 Step 851 Train Loss: 0.4529
Epoch 13 Step 901 Train Loss: 0.5054
Epoch 13 Step 951 Train Loss: 0.4302
Epoch 13 Step 1001 Train Loss: 0.4829
Epoch 13 Step 1051 Train Loss: 0.4488
Epoch 13 Step 1101 Train Loss: 0.4869
Epoch 13 Step 1151 Train Loss: 0.4781
Epoch 13 Step 1201 Train Loss: 0.4540
Epoch 13 Step 1251 Train Loss: 0.4724
Epoch 13 Step 1301 Train Loss: 0.4677
Epoch 13 Step 1351 Train Loss: 0.5159
Epoch 13 Step 1401 Train Loss: 0.4814
Epoch 13 Step 1451 Train Loss: 0.4618
Epoch 13 Step 1501 Train Loss: 0.4778
Epoch 13 Step 1551 Train Loss: 0.4565
Epoch 13 Step 1601 Train Loss: 0.4188
Epoch 13 Step 1651 Train Loss: 0.4108
Epoch 13 Step 1701 Train Loss: 0.4254
Epoch 13 Step 1751 Train Loss: 0.4984
Epoch 13 Step 1801 Train Loss: 0.4258
Epoch 13 Step 1851 Train Loss: 0.4943
Epoch 13: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0823 Validation Top 20 DE MSE: 0.1508. 
Epoch 14 Step 1 Train Loss: 0.4494
Epoch 14 Step 51 Train Loss: 0.5090
Epoch 14 Step 101 Train Loss: 0.5329
Epoch 14 Step 151 Train Loss: 0.5126
Epoch 14 Step 201 Train Loss: 0.4837
Epoch 14 Step 251 Train Loss: 0.4862
Epoch 14 Step 301 Train Loss: 0.4799
Epoch 14 Step 351 Train Loss: 0.4976
Epoch 14 Step 401 Train Loss: 0.4516
Epoch 14 Step 451 Train Loss: 0.4566
Epoch 14 Step 501 Train Loss: 0.5148
Epoch 14 Step 551 Train Loss: 0.5266
Epoch 14 Step 601 Train Loss: 0.4899
Epoch 14 Step 651 Train Loss: 0.5435
Epoch 14 Step 701 Train Loss: 0.4479
Epoch 14 Step 751 Train Loss: 0.4588
Epoch 14 Step 801 Train Loss: 0.4881
Epoch 14 Step 851 Train Loss: 0.4664
Epoch 14 Step 901 Train Loss: 0.4826
Epoch 14 Step 951 Train Loss: 0.4513
Epoch 14 Step 1001 Train Loss: 0.4851
Epoch 14 Step 1051 Train Loss: 0.4938
Epoch 14 Step 1101 Train Loss: 0.4879
Epoch 14 Step 1151 Train Loss: 0.4391
Epoch 14 Step 1201 Train Loss: 0.4902
Epoch 14 Step 1251 Train Loss: 0.4584
Epoch 14 Step 1301 Train Loss: 0.5491
Epoch 14 Step 1351 Train Loss: 0.4676
Epoch 14 Step 1401 Train Loss: 0.4496
Epoch 14 Step 1451 Train Loss: 0.5032
Epoch 14 Step 1501 Train Loss: 0.4590
Epoch 14 Step 1551 Train Loss: 0.4991
Epoch 14 Step 1601 Train Loss: 0.5289
Epoch 14 Step 1651 Train Loss: 0.4537
Epoch 14 Step 1701 Train Loss: 0.4673
Epoch 14 Step 1751 Train Loss: 0.5350
Epoch 14 Step 1801 Train Loss: 0.4846
Epoch 14 Step 1851 Train Loss: 0.5040
Epoch 14: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0785 Validation Top 20 DE MSE: 0.1474. 
Epoch 15 Step 1 Train Loss: 0.5168
Epoch 15 Step 51 Train Loss: 0.4789
Epoch 15 Step 101 Train Loss: 0.5017
Epoch 15 Step 151 Train Loss: 0.5254
Epoch 15 Step 201 Train Loss: 0.4239
Epoch 15 Step 251 Train Loss: 0.4345
Epoch 15 Step 301 Train Loss: 0.4660
Epoch 15 Step 351 Train Loss: 0.4976
Epoch 15 Step 401 Train Loss: 0.4125
Epoch 15 Step 451 Train Loss: 0.4948
Epoch 15 Step 501 Train Loss: 0.4804
Epoch 15 Step 551 Train Loss: 0.4920
Epoch 15 Step 601 Train Loss: 0.4987
Epoch 15 Step 651 Train Loss: 0.4709
Epoch 15 Step 701 Train Loss: 0.5359
Epoch 15 Step 751 Train Loss: 0.4986
Epoch 15 Step 801 Train Loss: 0.4770
Epoch 15 Step 851 Train Loss: 0.4842
Epoch 15 Step 901 Train Loss: 0.4829
Epoch 15 Step 951 Train Loss: 0.5647
Epoch 15 Step 1001 Train Loss: 0.4968
Epoch 15 Step 1051 Train Loss: 0.4180
Epoch 15 Step 1101 Train Loss: 0.5217
Epoch 15 Step 1151 Train Loss: 0.4661
Epoch 15 Step 1201 Train Loss: 0.4957
Epoch 15 Step 1251 Train Loss: 0.4704
Epoch 15 Step 1301 Train Loss: 0.4902
Epoch 15 Step 1351 Train Loss: 0.5130
Epoch 15 Step 1401 Train Loss: 0.4454
Epoch 15 Step 1451 Train Loss: 0.4415
Epoch 15 Step 1501 Train Loss: 0.4654
Epoch 15 Step 1551 Train Loss: 0.4398
Epoch 15 Step 1601 Train Loss: 0.5206
Epoch 15 Step 1651 Train Loss: 0.4587
Epoch 15 Step 1701 Train Loss: 0.5144
Epoch 15 Step 1751 Train Loss: 0.4662
Epoch 15 Step 1801 Train Loss: 0.4117
Epoch 15 Step 1851 Train Loss: 0.4519
Epoch 15: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0838 Validation Top 20 DE MSE: 0.1529. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1443
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0048676007
test_combo_seen0_pearson: 0.9887344414618909
test_combo_seen0_mse_de: 0.12825064
test_combo_seen0_pearson_de: 0.8178936388690864
test_combo_seen1_mse: 0.0051409765
test_combo_seen1_pearson: 0.9882611608892558
test_combo_seen1_mse_de: 0.16823597
test_combo_seen1_pearson_de: 0.7888753376688868
test_combo_seen2_mse: 0.0054823076
test_combo_seen2_pearson: 0.9877441264402731
test_combo_seen2_mse_de: 0.10711023
test_combo_seen2_pearson_de: 0.7994877678612796
test_unseen_single_mse: 0.0040354184
test_unseen_single_pearson: 0.9908881288425422
test_unseen_single_mse_de: 0.12820211
test_unseen_single_pearson_de: 0.8924715527411166
test_combo_seen0_pearson_delta: 0.5892841427059636
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.11428571428571431
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.9285714285714286
test_combo_seen0_mse_top20_de_non_dropout: 0.18283223
test_combo_seen1_pearson_delta: 0.5368649487020307
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.14705882352941177
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8568627450980392
test_combo_seen1_mse_top20_de_non_dropout: 0.20002632
test_combo_seen2_pearson_delta: 0.5029914788372309
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.15277777777777776
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.8805555555555554
test_combo_seen2_mse_top20_de_non_dropout: 0.13194522
test_unseen_single_pearson_delta: 0.43834615651527714
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.18518518518518517
test_unseen_single_frac_sigma_below_1_non_dropout: 0.898148148148148
test_unseen_single_mse_top20_de_non_dropout: 0.15123323
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.004 MB of 0.038 MB uploadedwandb: / 0.004 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñá‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÖ‚ñÖ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.11429
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.92857
wandb:                                         test_combo_seen0_mse 0.00487
wandb:                                      test_combo_seen0_mse_de 0.12825
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.18283
wandb:                                     test_combo_seen0_pearson 0.98873
wandb:                                  test_combo_seen0_pearson_de 0.81789
wandb:                               test_combo_seen0_pearson_delta 0.58928
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.14706
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.85686
wandb:                                         test_combo_seen1_mse 0.00514
wandb:                                      test_combo_seen1_mse_de 0.16824
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.20003
wandb:                                     test_combo_seen1_pearson 0.98826
wandb:                                  test_combo_seen1_pearson_de 0.78888
wandb:                               test_combo_seen1_pearson_delta 0.53686
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.15278
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.88056
wandb:                                         test_combo_seen2_mse 0.00548
wandb:                                      test_combo_seen2_mse_de 0.10711
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.13195
wandb:                                     test_combo_seen2_pearson 0.98774
wandb:                                  test_combo_seen2_pearson_de 0.79949
wandb:                               test_combo_seen2_pearson_delta 0.50299
wandb:                                                  test_de_mse 0.14434
wandb:                                              test_de_pearson 0.81986
wandb:               test_frac_opposite_direction_top20_non_dropout 0.15583
wandb:                          test_frac_sigma_below_1_non_dropout 0.8767
wandb:                                                     test_mse 0.00489
wandb:                                test_mse_top20_de_non_dropout 0.17417
wandb:                                                 test_pearson 0.98889
wandb:                                           test_pearson_delta 0.50868
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.18519
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.89815
wandb:                                       test_unseen_single_mse 0.00404
wandb:                                    test_unseen_single_mse_de 0.1282
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15123
wandb:                                   test_unseen_single_pearson 0.99089
wandb:                                test_unseen_single_pearson_de 0.89247
wandb:                             test_unseen_single_pearson_delta 0.43835
wandb:                                                 train_de_mse 0.0838
wandb:                                             train_de_pearson 0.92036
wandb:                                                    train_mse 0.00204
wandb:                                                train_pearson 0.99539
wandb:                                                training_loss 0.45725
wandb:                                                   val_de_mse 0.15295
wandb:                                               val_de_pearson 0.78952
wandb:                                                      val_mse 0.00325
wandb:                                                  val_pearson 0.99262
wandb: 
wandb: üöÄ View run scbert_NormanWeissman2019_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/v5kgxi7i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_091953-v5kgxi7i/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:2
combo_seen1:39
combo_seen2:23
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241029_102537-4ftb82a6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_NormanWeissman2019_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/4ftb82a6
wandb: WARNING Serializing object of type ndarray that is 8059328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5684
Epoch 1 Step 51 Train Loss: 0.4174
Epoch 1 Step 101 Train Loss: 0.4746
Epoch 1 Step 151 Train Loss: 0.4282
Epoch 1 Step 201 Train Loss: 0.4252
Epoch 1 Step 251 Train Loss: 0.4600
Epoch 1 Step 301 Train Loss: 0.5222
Epoch 1 Step 351 Train Loss: 0.4242
Epoch 1 Step 401 Train Loss: 0.4541
Epoch 1 Step 451 Train Loss: 0.4711
Epoch 1 Step 501 Train Loss: 0.4139
Epoch 1 Step 551 Train Loss: 0.4266
Epoch 1 Step 601 Train Loss: 0.4680
Epoch 1 Step 651 Train Loss: 0.4672
Epoch 1 Step 701 Train Loss: 0.4302
Epoch 1 Step 751 Train Loss: 0.4407
Epoch 1 Step 801 Train Loss: 0.4637
Epoch 1 Step 851 Train Loss: 0.5454
Epoch 1 Step 901 Train Loss: 0.4832
Epoch 1 Step 951 Train Loss: 0.4713
Epoch 1 Step 1001 Train Loss: 0.4484
Epoch 1 Step 1051 Train Loss: 0.4784
Epoch 1 Step 1101 Train Loss: 0.5330
Epoch 1 Step 1151 Train Loss: 0.4555
Epoch 1 Step 1201 Train Loss: 0.4449
Epoch 1 Step 1251 Train Loss: 0.4670
Epoch 1 Step 1301 Train Loss: 0.4134
Epoch 1 Step 1351 Train Loss: 0.4562
Epoch 1 Step 1401 Train Loss: 0.4707
Epoch 1 Step 1451 Train Loss: 0.4720
Epoch 1 Step 1501 Train Loss: 0.4192
Epoch 1 Step 1551 Train Loss: 0.4931
Epoch 1 Step 1601 Train Loss: 0.4144
Epoch 1 Step 1651 Train Loss: 0.4361
Epoch 1 Step 1701 Train Loss: 0.4172
Epoch 1 Step 1751 Train Loss: 0.4496
Epoch 1 Step 1801 Train Loss: 0.4527
Epoch 1 Step 1851 Train Loss: 0.4217
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.1211 Validation Top 20 DE MSE: 0.1753. 
Epoch 2 Step 1 Train Loss: 0.4088
Epoch 2 Step 51 Train Loss: 0.4218
Epoch 2 Step 101 Train Loss: 0.4222
Epoch 2 Step 151 Train Loss: 0.4866
Epoch 2 Step 201 Train Loss: 0.4128
Epoch 2 Step 251 Train Loss: 0.4331
Epoch 2 Step 301 Train Loss: 0.4264
Epoch 2 Step 351 Train Loss: 0.4310
Epoch 2 Step 401 Train Loss: 0.4273
Epoch 2 Step 451 Train Loss: 0.4443
Epoch 2 Step 501 Train Loss: 0.4435
Epoch 2 Step 551 Train Loss: 0.4653
Epoch 2 Step 601 Train Loss: 0.4372
Epoch 2 Step 651 Train Loss: 0.4647
Epoch 2 Step 701 Train Loss: 0.4674
Epoch 2 Step 751 Train Loss: 0.4622
Epoch 2 Step 801 Train Loss: 0.4081
Epoch 2 Step 851 Train Loss: 0.4729
Epoch 2 Step 901 Train Loss: 0.4158
Epoch 2 Step 951 Train Loss: 0.4380
Epoch 2 Step 1001 Train Loss: 0.4952
Epoch 2 Step 1051 Train Loss: 0.4246
Epoch 2 Step 1101 Train Loss: 0.4608
Epoch 2 Step 1151 Train Loss: 0.4236
Epoch 2 Step 1201 Train Loss: 0.4942
Epoch 2 Step 1251 Train Loss: 0.4538
Epoch 2 Step 1301 Train Loss: 0.4352
Epoch 2 Step 1351 Train Loss: 0.4307
Epoch 2 Step 1401 Train Loss: 0.4044
Epoch 2 Step 1451 Train Loss: 0.3940
Epoch 2 Step 1501 Train Loss: 0.5110
Epoch 2 Step 1551 Train Loss: 0.4605
Epoch 2 Step 1601 Train Loss: 0.4654
Epoch 2 Step 1651 Train Loss: 0.4537
Epoch 2 Step 1701 Train Loss: 0.4763
Epoch 2 Step 1751 Train Loss: 0.4646
Epoch 2 Step 1801 Train Loss: 0.4875
Epoch 2 Step 1851 Train Loss: 0.4971
Epoch 2: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0991 Validation Top 20 DE MSE: 0.1723. 
Epoch 3 Step 1 Train Loss: 0.4524
Epoch 3 Step 51 Train Loss: 0.4301
Epoch 3 Step 101 Train Loss: 0.4539
Epoch 3 Step 151 Train Loss: 0.4752
Epoch 3 Step 201 Train Loss: 0.4295
Epoch 3 Step 251 Train Loss: 0.4845
Epoch 3 Step 301 Train Loss: 0.4733
Epoch 3 Step 351 Train Loss: 0.4768
Epoch 3 Step 401 Train Loss: 0.4528
Epoch 3 Step 451 Train Loss: 0.5077
Epoch 3 Step 501 Train Loss: 0.4423
Epoch 3 Step 551 Train Loss: 0.4882
Epoch 3 Step 601 Train Loss: 0.4257
Epoch 3 Step 651 Train Loss: 0.4866
Epoch 3 Step 701 Train Loss: 0.4621
Epoch 3 Step 751 Train Loss: 0.4856
Epoch 3 Step 801 Train Loss: 0.5121
Epoch 3 Step 851 Train Loss: 0.5550
Epoch 3 Step 901 Train Loss: 0.5388
Epoch 3 Step 951 Train Loss: 0.4467
Epoch 3 Step 1001 Train Loss: 0.4781
Epoch 3 Step 1051 Train Loss: 0.4684
Epoch 3 Step 1101 Train Loss: 0.4520
Epoch 3 Step 1151 Train Loss: 0.4537
Epoch 3 Step 1201 Train Loss: 0.5168
Epoch 3 Step 1251 Train Loss: 0.4791
Epoch 3 Step 1301 Train Loss: 0.5259
Epoch 3 Step 1351 Train Loss: 0.4468
Epoch 3 Step 1401 Train Loss: 0.4449
Epoch 3 Step 1451 Train Loss: 0.4360
Epoch 3 Step 1501 Train Loss: 0.4674
Epoch 3 Step 1551 Train Loss: 0.4844
Epoch 3 Step 1601 Train Loss: 0.4272
Epoch 3 Step 1651 Train Loss: 0.5131
Epoch 3 Step 1701 Train Loss: 0.4540
Epoch 3 Step 1751 Train Loss: 0.4615
Epoch 3 Step 1801 Train Loss: 0.5503
Epoch 3 Step 1851 Train Loss: 0.5590
Epoch 3: Train Overall MSE: 0.0034 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.1601 Validation Top 20 DE MSE: 0.1737. 
Epoch 4 Step 1 Train Loss: 0.4590
Epoch 4 Step 51 Train Loss: 0.4770
Epoch 4 Step 101 Train Loss: 0.4442
Epoch 4 Step 151 Train Loss: 0.4066
Epoch 4 Step 201 Train Loss: 0.4177
Epoch 4 Step 251 Train Loss: 0.4270
Epoch 4 Step 301 Train Loss: 0.4948
Epoch 4 Step 351 Train Loss: 0.4570
Epoch 4 Step 401 Train Loss: 0.4955
Epoch 4 Step 451 Train Loss: 0.5121
Epoch 4 Step 501 Train Loss: 0.4812
Epoch 4 Step 551 Train Loss: 0.4692
Epoch 4 Step 601 Train Loss: 0.4479
Epoch 4 Step 651 Train Loss: 0.4281
Epoch 4 Step 701 Train Loss: 0.4451
Epoch 4 Step 751 Train Loss: 0.4562
Epoch 4 Step 801 Train Loss: 0.5186
Epoch 4 Step 851 Train Loss: 0.4345
Epoch 4 Step 901 Train Loss: 0.4976
Epoch 4 Step 951 Train Loss: 0.4724
Epoch 4 Step 1001 Train Loss: 0.4366
Epoch 4 Step 1051 Train Loss: 0.4879
Epoch 4 Step 1101 Train Loss: 0.4938
Epoch 4 Step 1151 Train Loss: 0.4410
Epoch 4 Step 1201 Train Loss: 0.5145
Epoch 4 Step 1251 Train Loss: 0.4681
Epoch 4 Step 1301 Train Loss: 0.4575
Epoch 4 Step 1351 Train Loss: 0.4941
Epoch 4 Step 1401 Train Loss: 0.4597
Epoch 4 Step 1451 Train Loss: 0.4632
Epoch 4 Step 1501 Train Loss: 0.4835
Epoch 4 Step 1551 Train Loss: 0.4578
Epoch 4 Step 1601 Train Loss: 0.4524
Epoch 4 Step 1651 Train Loss: 0.4613
Epoch 4 Step 1701 Train Loss: 0.4788
Epoch 4 Step 1751 Train Loss: 0.4606
Epoch 4 Step 1801 Train Loss: 0.5055
Epoch 4 Step 1851 Train Loss: 0.4355
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.1016 Validation Top 20 DE MSE: 0.1707. 
Epoch 5 Step 1 Train Loss: 0.4339
Epoch 5 Step 51 Train Loss: 0.5929
Epoch 5 Step 101 Train Loss: 0.5123
Epoch 5 Step 151 Train Loss: 0.5253
Epoch 5 Step 201 Train Loss: 0.4870
Epoch 5 Step 251 Train Loss: 0.4225
Epoch 5 Step 301 Train Loss: 0.5424
Epoch 5 Step 351 Train Loss: 0.4802
Epoch 5 Step 401 Train Loss: 0.5115
Epoch 5 Step 451 Train Loss: 0.4940
Epoch 5 Step 501 Train Loss: 0.4658
Epoch 5 Step 551 Train Loss: 0.5137
Epoch 5 Step 601 Train Loss: 0.4909
Epoch 5 Step 651 Train Loss: 0.4977
Epoch 5 Step 701 Train Loss: 0.4937
Epoch 5 Step 751 Train Loss: 0.4446
Epoch 5 Step 801 Train Loss: 0.4898
Epoch 5 Step 851 Train Loss: 0.4427
Epoch 5 Step 901 Train Loss: 0.5195
Epoch 5 Step 951 Train Loss: 0.5000
Epoch 5 Step 1001 Train Loss: 0.4962
Epoch 5 Step 1051 Train Loss: 0.4625
Epoch 5 Step 1101 Train Loss: 0.4860
Epoch 5 Step 1151 Train Loss: 0.4566
Epoch 5 Step 1201 Train Loss: 0.5181
Epoch 5 Step 1251 Train Loss: 0.4764
Epoch 5 Step 1301 Train Loss: 0.4574
Epoch 5 Step 1351 Train Loss: 0.4801
Epoch 5 Step 1401 Train Loss: 0.5016
Epoch 5 Step 1451 Train Loss: 0.4299
Epoch 5 Step 1501 Train Loss: 0.4791
Epoch 5 Step 1551 Train Loss: 0.5162
Epoch 5 Step 1601 Train Loss: 0.4439
Epoch 5 Step 1651 Train Loss: 0.4867
Epoch 5 Step 1701 Train Loss: 0.4139
Epoch 5 Step 1751 Train Loss: 0.5079
Epoch 5 Step 1801 Train Loss: 0.4932
Epoch 5 Step 1851 Train Loss: 0.4438
Epoch 5: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0863 Validation Top 20 DE MSE: 0.1669. 
Epoch 6 Step 1 Train Loss: 0.5395
Epoch 6 Step 51 Train Loss: 0.5050
Epoch 6 Step 101 Train Loss: 0.4914
Epoch 6 Step 151 Train Loss: 0.5546
Epoch 6 Step 201 Train Loss: 0.4337
Epoch 6 Step 251 Train Loss: 0.4729
Epoch 6 Step 301 Train Loss: 0.5334
Epoch 6 Step 351 Train Loss: 0.4875
Epoch 6 Step 401 Train Loss: 0.4518
Epoch 6 Step 451 Train Loss: 0.4546
Epoch 6 Step 501 Train Loss: 0.4600
Epoch 6 Step 551 Train Loss: 0.5554
Epoch 6 Step 601 Train Loss: 0.4577
Epoch 6 Step 651 Train Loss: 0.4569
Epoch 6 Step 701 Train Loss: 0.4525
Epoch 6 Step 751 Train Loss: 0.4479
Epoch 6 Step 801 Train Loss: 0.5049
Epoch 6 Step 851 Train Loss: 0.5259
Epoch 6 Step 901 Train Loss: 0.4686
Epoch 6 Step 951 Train Loss: 0.4657
Epoch 6 Step 1001 Train Loss: 0.4318
Epoch 6 Step 1051 Train Loss: 0.4477
Epoch 6 Step 1101 Train Loss: 0.4720
Epoch 6 Step 1151 Train Loss: 0.5873
Epoch 6 Step 1201 Train Loss: 0.4630
Epoch 6 Step 1251 Train Loss: 0.4885
Epoch 6 Step 1301 Train Loss: 0.4374
Epoch 6 Step 1351 Train Loss: 0.5107
Epoch 6 Step 1401 Train Loss: 0.4284
Epoch 6 Step 1451 Train Loss: 0.4769
Epoch 6 Step 1501 Train Loss: 0.4564
Epoch 6 Step 1551 Train Loss: 0.5018
Epoch 6 Step 1601 Train Loss: 0.4447
Epoch 6 Step 1651 Train Loss: 0.5354
Epoch 6 Step 1701 Train Loss: 0.5508
Epoch 6 Step 1751 Train Loss: 0.4933
Epoch 6 Step 1801 Train Loss: 0.5543
Epoch 6 Step 1851 Train Loss: 0.5412
Epoch 6: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0911 Validation Top 20 DE MSE: 0.1763. 
Epoch 7 Step 1 Train Loss: 0.4591
Epoch 7 Step 51 Train Loss: 0.5133
Epoch 7 Step 101 Train Loss: 0.4436
Epoch 7 Step 151 Train Loss: 0.4802
Epoch 7 Step 201 Train Loss: 0.5035
Epoch 7 Step 251 Train Loss: 0.4362
Epoch 7 Step 301 Train Loss: 0.4613
Epoch 7 Step 351 Train Loss: 0.5020
Epoch 7 Step 401 Train Loss: 0.4405
Epoch 7 Step 451 Train Loss: 0.5607
Epoch 7 Step 501 Train Loss: 0.5055
Epoch 7 Step 551 Train Loss: 0.4698
Epoch 7 Step 601 Train Loss: 0.4619
Epoch 7 Step 651 Train Loss: 0.5364
Epoch 7 Step 701 Train Loss: 0.4514
Epoch 7 Step 751 Train Loss: 0.4799
Epoch 7 Step 801 Train Loss: 0.4542
Epoch 7 Step 851 Train Loss: 0.4988
Epoch 7 Step 901 Train Loss: 0.4853
Epoch 7 Step 951 Train Loss: 0.5075
Epoch 7 Step 1001 Train Loss: 0.5759
Epoch 7 Step 1051 Train Loss: 0.5279
Epoch 7 Step 1101 Train Loss: 0.4772
Epoch 7 Step 1151 Train Loss: 0.4879
Epoch 7 Step 1201 Train Loss: 0.5219
Epoch 7 Step 1251 Train Loss: 0.4923
Epoch 7 Step 1301 Train Loss: 0.4900
Epoch 7 Step 1351 Train Loss: 0.4851
Epoch 7 Step 1401 Train Loss: 0.5349
Epoch 7 Step 1451 Train Loss: 0.4533
Epoch 7 Step 1501 Train Loss: 0.5442
Epoch 7 Step 1551 Train Loss: 0.5206
Epoch 7 Step 1601 Train Loss: 0.4597
Epoch 7 Step 1651 Train Loss: 0.4698
Epoch 7 Step 1701 Train Loss: 0.4634
Epoch 7 Step 1751 Train Loss: 0.4856
Epoch 7 Step 1801 Train Loss: 0.5254
Epoch 7 Step 1851 Train Loss: 0.5107
Epoch 7: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0867 Validation Top 20 DE MSE: 0.1692. 
Epoch 8 Step 1 Train Loss: 0.5323
Epoch 8 Step 51 Train Loss: 0.5079
Epoch 8 Step 101 Train Loss: 0.5662
Epoch 8 Step 151 Train Loss: 0.5131
Epoch 8 Step 201 Train Loss: 0.4952
Epoch 8 Step 251 Train Loss: 0.4738
Epoch 8 Step 301 Train Loss: 0.4856
Epoch 8 Step 351 Train Loss: 0.5388
Epoch 8 Step 401 Train Loss: 0.4828
Epoch 8 Step 451 Train Loss: 0.4732
Epoch 8 Step 501 Train Loss: 0.5034
Epoch 8 Step 551 Train Loss: 0.5045
Epoch 8 Step 601 Train Loss: 0.5358
Epoch 8 Step 651 Train Loss: 0.5626
Epoch 8 Step 701 Train Loss: 0.4876
Epoch 8 Step 751 Train Loss: 0.4806
Epoch 8 Step 801 Train Loss: 0.4774
Epoch 8 Step 851 Train Loss: 0.5270
Epoch 8 Step 901 Train Loss: 0.5087
Epoch 8 Step 951 Train Loss: 0.4748
Epoch 8 Step 1001 Train Loss: 0.5194
Epoch 8 Step 1051 Train Loss: 0.4810
Epoch 8 Step 1101 Train Loss: 0.4690
Epoch 8 Step 1151 Train Loss: 0.5159
Epoch 8 Step 1201 Train Loss: 0.4995
Epoch 8 Step 1251 Train Loss: 0.4810
Epoch 8 Step 1301 Train Loss: 0.4753
Epoch 8 Step 1351 Train Loss: 0.5450
Epoch 8 Step 1401 Train Loss: 0.5003
Epoch 8 Step 1451 Train Loss: 0.4741
Epoch 8 Step 1501 Train Loss: 0.5153
Epoch 8 Step 1551 Train Loss: 0.4532
Epoch 8 Step 1601 Train Loss: 0.5425
Epoch 8 Step 1651 Train Loss: 0.5394
Epoch 8 Step 1701 Train Loss: 0.4743
Epoch 8 Step 1751 Train Loss: 0.5547
Epoch 8 Step 1801 Train Loss: 0.4918
Epoch 8 Step 1851 Train Loss: 0.4640
Epoch 8: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0856 Validation Top 20 DE MSE: 0.1697. 
Epoch 9 Step 1 Train Loss: 0.4486
Epoch 9 Step 51 Train Loss: 0.5365
Epoch 9 Step 101 Train Loss: 0.4445
Epoch 9 Step 151 Train Loss: 0.4787
Epoch 9 Step 201 Train Loss: 0.4940
Epoch 9 Step 251 Train Loss: 0.4975
Epoch 9 Step 301 Train Loss: 0.5160
Epoch 9 Step 351 Train Loss: 0.4839
Epoch 9 Step 401 Train Loss: 0.4577
Epoch 9 Step 451 Train Loss: 0.4941
Epoch 9 Step 501 Train Loss: 0.5055
Epoch 9 Step 551 Train Loss: 0.4957
Epoch 9 Step 601 Train Loss: 0.5024
Epoch 9 Step 651 Train Loss: 0.6092
Epoch 9 Step 701 Train Loss: 0.5182
Epoch 9 Step 751 Train Loss: 0.4837
Epoch 9 Step 801 Train Loss: 0.4755
Epoch 9 Step 851 Train Loss: 0.5331
Epoch 9 Step 901 Train Loss: 0.4818
Epoch 9 Step 951 Train Loss: 0.5180
Epoch 9 Step 1001 Train Loss: 0.4880
Epoch 9 Step 1051 Train Loss: 0.4612
Epoch 9 Step 1101 Train Loss: 0.4932
Epoch 9 Step 1151 Train Loss: 0.4886
Epoch 9 Step 1201 Train Loss: 0.5225
Epoch 9 Step 1251 Train Loss: 0.4572
Epoch 9 Step 1301 Train Loss: 0.4414
Epoch 9 Step 1351 Train Loss: 0.5129
Epoch 9 Step 1401 Train Loss: 0.4768
Epoch 9 Step 1451 Train Loss: 0.5304
Epoch 9 Step 1501 Train Loss: 0.5280
Epoch 9 Step 1551 Train Loss: 0.5214
Epoch 9 Step 1601 Train Loss: 0.4513
Epoch 9 Step 1651 Train Loss: 0.4841
Epoch 9 Step 1701 Train Loss: 0.4584
Epoch 9 Step 1751 Train Loss: 0.4571
Epoch 9 Step 1801 Train Loss: 0.5241
Epoch 9 Step 1851 Train Loss: 0.5115
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0886 Validation Top 20 DE MSE: 0.1738. 
Epoch 10 Step 1 Train Loss: 0.5383
Epoch 10 Step 51 Train Loss: 0.4633
Epoch 10 Step 101 Train Loss: 0.5053
Epoch 10 Step 151 Train Loss: 0.5538
Epoch 10 Step 201 Train Loss: 0.5136
Epoch 10 Step 251 Train Loss: 0.4612
Epoch 10 Step 301 Train Loss: 0.5074
Epoch 10 Step 351 Train Loss: 0.4912
Epoch 10 Step 401 Train Loss: 0.5369
Epoch 10 Step 451 Train Loss: 0.4929
Epoch 10 Step 501 Train Loss: 0.4686
Epoch 10 Step 551 Train Loss: 0.5747
Epoch 10 Step 601 Train Loss: 0.4679
Epoch 10 Step 651 Train Loss: 0.4747
Epoch 10 Step 701 Train Loss: 0.5440
Epoch 10 Step 751 Train Loss: 0.5299
Epoch 10 Step 801 Train Loss: 0.4800
Epoch 10 Step 851 Train Loss: 0.4927
Epoch 10 Step 901 Train Loss: 0.4976
Epoch 10 Step 951 Train Loss: 0.4660
Epoch 10 Step 1001 Train Loss: 0.4838
Epoch 10 Step 1051 Train Loss: 0.5107
Epoch 10 Step 1101 Train Loss: 0.4835
Epoch 10 Step 1151 Train Loss: 0.5188
Epoch 10 Step 1201 Train Loss: 0.5733
Epoch 10 Step 1251 Train Loss: 0.5078
Epoch 10 Step 1301 Train Loss: 0.4599
Epoch 10 Step 1351 Train Loss: 0.5013
Epoch 10 Step 1401 Train Loss: 0.5066
Epoch 10 Step 1451 Train Loss: 0.4988
Epoch 10 Step 1501 Train Loss: 0.4357
Epoch 10 Step 1551 Train Loss: 0.4616
Epoch 10 Step 1601 Train Loss: 0.4663
Epoch 10 Step 1651 Train Loss: 0.4626
Epoch 10 Step 1701 Train Loss: 0.5465
Epoch 10 Step 1751 Train Loss: 0.4959
Epoch 10 Step 1801 Train Loss: 0.4201
Epoch 10 Step 1851 Train Loss: 0.5191
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0858 Validation Top 20 DE MSE: 0.1689. 
Epoch 11 Step 1 Train Loss: 0.4794
Epoch 11 Step 51 Train Loss: 0.4482
Epoch 11 Step 101 Train Loss: 0.4704
Epoch 11 Step 151 Train Loss: 0.4625
Epoch 11 Step 201 Train Loss: 0.5318
Epoch 11 Step 251 Train Loss: 0.4760
Epoch 11 Step 301 Train Loss: 0.4622
Epoch 11 Step 351 Train Loss: 0.4979
Epoch 11 Step 401 Train Loss: 0.5462
Epoch 11 Step 451 Train Loss: 0.5011
Epoch 11 Step 501 Train Loss: 0.5191
Epoch 11 Step 551 Train Loss: 0.4895
Epoch 11 Step 601 Train Loss: 0.4792
Epoch 11 Step 651 Train Loss: 0.5204
Epoch 11 Step 701 Train Loss: 0.4826
Epoch 11 Step 751 Train Loss: 0.5019
Epoch 11 Step 801 Train Loss: 0.5248
Epoch 11 Step 851 Train Loss: 0.5584
Epoch 11 Step 901 Train Loss: 0.4819
Epoch 11 Step 951 Train Loss: 0.4716
Epoch 11 Step 1001 Train Loss: 0.5008
Epoch 11 Step 1051 Train Loss: 0.5044
Epoch 11 Step 1101 Train Loss: 0.4485
Epoch 11 Step 1151 Train Loss: 0.4769
Epoch 11 Step 1201 Train Loss: 0.6298
Epoch 11 Step 1251 Train Loss: 0.4889
Epoch 11 Step 1301 Train Loss: 0.4896
Epoch 11 Step 1351 Train Loss: 0.5225
Epoch 11 Step 1401 Train Loss: 0.4834
Epoch 11 Step 1451 Train Loss: 0.4330
Epoch 11 Step 1501 Train Loss: 0.4849
Epoch 11 Step 1551 Train Loss: 0.5464
Epoch 11 Step 1601 Train Loss: 0.4957
Epoch 11 Step 1651 Train Loss: 0.4666
Epoch 11 Step 1701 Train Loss: 0.4855
Epoch 11 Step 1751 Train Loss: 0.4990
Epoch 11 Step 1801 Train Loss: 0.5071
Epoch 11 Step 1851 Train Loss: 0.4774
Epoch 11: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0880 Validation Top 20 DE MSE: 0.1721. 
Epoch 12 Step 1 Train Loss: 0.4672
Epoch 12 Step 51 Train Loss: 0.5011
Epoch 12 Step 101 Train Loss: 0.5185
Epoch 12 Step 151 Train Loss: 0.4659
Epoch 12 Step 201 Train Loss: 0.5234
Epoch 12 Step 251 Train Loss: 0.4492
Epoch 12 Step 301 Train Loss: 0.5867
Epoch 12 Step 351 Train Loss: 0.5119
Epoch 12 Step 401 Train Loss: 0.4713
Epoch 12 Step 451 Train Loss: 0.4958
Epoch 12 Step 501 Train Loss: 0.4798
Epoch 12 Step 551 Train Loss: 0.5495
Epoch 12 Step 601 Train Loss: 0.4973
Epoch 12 Step 651 Train Loss: 0.4985
Epoch 12 Step 701 Train Loss: 0.5066
Epoch 12 Step 751 Train Loss: 0.4484
Epoch 12 Step 801 Train Loss: 0.5093
Epoch 12 Step 851 Train Loss: 0.5051
Epoch 12 Step 901 Train Loss: 0.5026
Epoch 12 Step 951 Train Loss: 0.5134
Epoch 12 Step 1001 Train Loss: 0.4721
Epoch 12 Step 1051 Train Loss: 0.4311
Epoch 12 Step 1101 Train Loss: 0.4585
Epoch 12 Step 1151 Train Loss: 0.4976
Epoch 12 Step 1201 Train Loss: 0.4711
Epoch 12 Step 1251 Train Loss: 0.5162
Epoch 12 Step 1301 Train Loss: 0.5400
Epoch 12 Step 1351 Train Loss: 0.4996
Epoch 12 Step 1401 Train Loss: 0.4644
Epoch 12 Step 1451 Train Loss: 0.4364
Epoch 12 Step 1501 Train Loss: 0.5120
Epoch 12 Step 1551 Train Loss: 0.4902
Epoch 12 Step 1601 Train Loss: 0.4964
Epoch 12 Step 1651 Train Loss: 0.5571
Epoch 12 Step 1701 Train Loss: 0.5327
Epoch 12 Step 1751 Train Loss: 0.5081
Epoch 12 Step 1801 Train Loss: 0.4769
Epoch 12 Step 1851 Train Loss: 0.5280
Epoch 12: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0835 Validation Top 20 DE MSE: 0.1680. 
Epoch 13 Step 1 Train Loss: 0.5219
Epoch 13 Step 51 Train Loss: 0.4836
Epoch 13 Step 101 Train Loss: 0.4958
Epoch 13 Step 151 Train Loss: 0.4397
Epoch 13 Step 201 Train Loss: 0.5216
Epoch 13 Step 251 Train Loss: 0.4499
Epoch 13 Step 301 Train Loss: 0.4589
Epoch 13 Step 351 Train Loss: 0.4745
Epoch 13 Step 401 Train Loss: 0.5295
Epoch 13 Step 451 Train Loss: 0.4780
Epoch 13 Step 501 Train Loss: 0.4771
Epoch 13 Step 551 Train Loss: 0.4511
Epoch 13 Step 601 Train Loss: 0.4890
Epoch 13 Step 651 Train Loss: 0.4573
Epoch 13 Step 701 Train Loss: 0.4838
Epoch 13 Step 751 Train Loss: 0.5188
Epoch 13 Step 801 Train Loss: 0.4755
Epoch 13 Step 851 Train Loss: 0.5262
Epoch 13 Step 901 Train Loss: 0.4994
Epoch 13 Step 951 Train Loss: 0.4736
Epoch 13 Step 1001 Train Loss: 0.5165
Epoch 13 Step 1051 Train Loss: 0.5158
Epoch 13 Step 1101 Train Loss: 0.4678
Epoch 13 Step 1151 Train Loss: 0.4512
Epoch 13 Step 1201 Train Loss: 0.4794
Epoch 13 Step 1251 Train Loss: 0.5350
Epoch 13 Step 1301 Train Loss: 0.4869
Epoch 13 Step 1351 Train Loss: 0.5242
Epoch 13 Step 1401 Train Loss: 0.4797
Epoch 13 Step 1451 Train Loss: 0.5138
Epoch 13 Step 1501 Train Loss: 0.4864
Epoch 13 Step 1551 Train Loss: 0.5501
Epoch 13 Step 1601 Train Loss: 0.5052
Epoch 13 Step 1651 Train Loss: 0.5516
Epoch 13 Step 1701 Train Loss: 0.5469
Epoch 13 Step 1751 Train Loss: 0.4869
Epoch 13 Step 1801 Train Loss: 0.4887
Epoch 13 Step 1851 Train Loss: 0.4776
Epoch 13: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0864 Validation Top 20 DE MSE: 0.1681. 
Epoch 14 Step 1 Train Loss: 0.5183
Epoch 14 Step 51 Train Loss: 0.4472
Epoch 14 Step 101 Train Loss: 0.5409
Epoch 14 Step 151 Train Loss: 0.4668
Epoch 14 Step 201 Train Loss: 0.5190
Epoch 14 Step 251 Train Loss: 0.4715
Epoch 14 Step 301 Train Loss: 0.4787
Epoch 14 Step 351 Train Loss: 0.4692
Epoch 14 Step 401 Train Loss: 0.4577
Epoch 14 Step 451 Train Loss: 0.4751
Epoch 14 Step 501 Train Loss: 0.4888
Epoch 14 Step 551 Train Loss: 0.4870
Epoch 14 Step 601 Train Loss: 0.4888
Epoch 14 Step 651 Train Loss: 0.4849
Epoch 14 Step 701 Train Loss: 0.4981
Epoch 14 Step 751 Train Loss: 0.4747
Epoch 14 Step 801 Train Loss: 0.4848
Epoch 14 Step 851 Train Loss: 0.4890
Epoch 14 Step 901 Train Loss: 0.5418
Epoch 14 Step 951 Train Loss: 0.4572
Epoch 14 Step 1001 Train Loss: 0.4982
Epoch 14 Step 1051 Train Loss: 0.4872
Epoch 14 Step 1101 Train Loss: 0.5016
Epoch 14 Step 1151 Train Loss: 0.5001
Epoch 14 Step 1201 Train Loss: 0.4483
Epoch 14 Step 1251 Train Loss: 0.4891
Epoch 14 Step 1301 Train Loss: 0.4606
Epoch 14 Step 1351 Train Loss: 0.4636
Epoch 14 Step 1401 Train Loss: 0.5293
Epoch 14 Step 1451 Train Loss: 0.5104
Epoch 14 Step 1501 Train Loss: 0.5048
Epoch 14 Step 1551 Train Loss: 0.5015
Epoch 14 Step 1601 Train Loss: 0.4846
Epoch 14 Step 1651 Train Loss: 0.5087
Epoch 14 Step 1701 Train Loss: 0.4632
Epoch 14 Step 1751 Train Loss: 0.4527
Epoch 14 Step 1801 Train Loss: 0.5222
Epoch 14 Step 1851 Train Loss: 0.4731
Epoch 14: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0904 Validation Top 20 DE MSE: 0.1681. 
Epoch 15 Step 1 Train Loss: 0.4876
Epoch 15 Step 51 Train Loss: 0.5164
Epoch 15 Step 101 Train Loss: 0.5062
Epoch 15 Step 151 Train Loss: 0.4586
Epoch 15 Step 201 Train Loss: 0.5071
Epoch 15 Step 251 Train Loss: 0.5603
Epoch 15 Step 301 Train Loss: 0.5418
Epoch 15 Step 351 Train Loss: 0.5404
Epoch 15 Step 401 Train Loss: 0.5091
Epoch 15 Step 451 Train Loss: 0.5456
Epoch 15 Step 501 Train Loss: 0.4907
Epoch 15 Step 551 Train Loss: 0.5531
Epoch 15 Step 601 Train Loss: 0.5491
Epoch 15 Step 651 Train Loss: 0.5656
Epoch 15 Step 701 Train Loss: 0.4849
Epoch 15 Step 751 Train Loss: 0.5072
Epoch 15 Step 801 Train Loss: 0.4723
Epoch 15 Step 851 Train Loss: 0.5342
Epoch 15 Step 901 Train Loss: 0.5286
Epoch 15 Step 951 Train Loss: 0.4577
Epoch 15 Step 1001 Train Loss: 0.4965
Epoch 15 Step 1051 Train Loss: 0.5031
Epoch 15 Step 1101 Train Loss: 0.5250
Epoch 15 Step 1151 Train Loss: 0.4581
Epoch 15 Step 1201 Train Loss: 0.4947
Epoch 15 Step 1251 Train Loss: 0.5074
Epoch 15 Step 1301 Train Loss: 0.4573
Epoch 15 Step 1351 Train Loss: 0.5270
Epoch 15 Step 1401 Train Loss: 0.4968
Epoch 15 Step 1451 Train Loss: 0.4927
Epoch 15 Step 1501 Train Loss: 0.5165
Epoch 15 Step 1551 Train Loss: 0.4819
Epoch 15 Step 1601 Train Loss: 0.5451
Epoch 15 Step 1651 Train Loss: 0.4745
Epoch 15 Step 1701 Train Loss: 0.5284
Epoch 15 Step 1751 Train Loss: 0.5160
Epoch 15 Step 1801 Train Loss: 0.4661
Epoch 15 Step 1851 Train Loss: 0.4875
Epoch 15: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0866 Validation Top 20 DE MSE: 0.1684. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1372
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0046182866
test_combo_seen0_pearson: 0.989293122768987
test_combo_seen0_mse_de: 0.3865374
test_combo_seen0_pearson_de: 0.9364566054895157
test_combo_seen1_mse: 0.0038510484
test_combo_seen1_pearson: 0.9913236114077457
test_combo_seen1_mse_de: 0.1404388
test_combo_seen1_pearson_de: 0.8255594765670237
test_combo_seen2_mse: 0.0035118782
test_combo_seen2_pearson: 0.9919689793127331
test_combo_seen2_mse_de: 0.12877157
test_combo_seen2_pearson_de: 0.8748153764030621
test_unseen_single_mse: 0.00215781
test_unseen_single_pearson: 0.9950779357780698
test_unseen_single_mse_de: 0.121127285
test_unseen_single_pearson_de: 0.9459462892730921
test_combo_seen0_pearson_delta: 0.6692106182615107
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.025
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.675
test_combo_seen0_mse_top20_de_non_dropout: 0.40031707
test_combo_seen1_pearson_delta: 0.5584245520401073
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.1461538461538462
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8910256410256411
test_combo_seen1_mse_top20_de_non_dropout: 0.17397796
test_combo_seen2_pearson_delta: 0.6180922057079695
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.11086956521739132
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.893478260869565
test_combo_seen2_mse_top20_de_non_dropout: 0.14654137
test_unseen_single_pearson_delta: 0.4404491161673478
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25925925925925924
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9388888888888888
test_unseen_single_mse_top20_de_non_dropout: 0.13410735
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.001 MB uploadedwandb: - 0.001 MB of 0.038 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.037 MB of 0.038 MB uploadedwandb: / 0.037 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÖ‚ñá‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà
wandb:                                                    train_mse ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñá‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÉ
wandb:                                                   val_de_mse ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÜ‚ñÉ‚ñÅ‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÑ‚ñÜ‚ñÅ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.025
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.675
wandb:                                         test_combo_seen0_mse 0.00462
wandb:                                      test_combo_seen0_mse_de 0.38654
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.40032
wandb:                                     test_combo_seen0_pearson 0.98929
wandb:                                  test_combo_seen0_pearson_de 0.93646
wandb:                               test_combo_seen0_pearson_delta 0.66921
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.14615
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.89103
wandb:                                         test_combo_seen1_mse 0.00385
wandb:                                      test_combo_seen1_mse_de 0.14044
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.17398
wandb:                                     test_combo_seen1_pearson 0.99132
wandb:                                  test_combo_seen1_pearson_de 0.82556
wandb:                               test_combo_seen1_pearson_delta 0.55842
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.11087
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.89348
wandb:                                         test_combo_seen2_mse 0.00351
wandb:                                      test_combo_seen2_mse_de 0.12877
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.14654
wandb:                                     test_combo_seen2_pearson 0.99197
wandb:                                  test_combo_seen2_pearson_de 0.87482
wandb:                               test_combo_seen2_pearson_delta 0.61809
wandb:                                                  test_de_mse 0.13717
wandb:                                              test_de_pearson 0.87617
wandb:               test_frac_opposite_direction_top20_non_dropout 0.16813
wandb:                          test_frac_sigma_below_1_non_dropout 0.9011
wandb:                                                     test_mse 0.00328
wandb:                                test_mse_top20_de_non_dropout 0.16019
wandb:                                                 test_pearson 0.99256
wandb:                                           test_pearson_delta 0.54094
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25926
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93889
wandb:                                       test_unseen_single_mse 0.00216
wandb:                                    test_unseen_single_mse_de 0.12113
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.13411
wandb:                                   test_unseen_single_pearson 0.99508
wandb:                                test_unseen_single_pearson_de 0.94595
wandb:                             test_unseen_single_pearson_delta 0.44045
wandb:                                                 train_de_mse 0.08657
wandb:                                             train_de_pearson 0.86659
wandb:                                                    train_mse 0.00213
wandb:                                                train_pearson 0.99517
wandb:                                                training_loss 0.45945
wandb:                                                   val_de_mse 0.16843
wandb:                                               val_de_pearson 0.82589
wandb:                                                      val_mse 0.00309
wandb:                                                  val_pearson 0.99303
wandb: 
wandb: üöÄ View run scbert_NormanWeissman2019_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/4ftb82a6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_102537-4ftb82a6/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:5
combo_seen1:44
combo_seen2:21
unseen_single:25
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241029_112927-75y1fnix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_NormanWeissman2019_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/75y1fnix
wandb: WARNING Serializing object of type ndarray that is 8059328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4933
Epoch 1 Step 51 Train Loss: 0.4455
Epoch 1 Step 101 Train Loss: 0.5066
Epoch 1 Step 151 Train Loss: 0.5071
Epoch 1 Step 201 Train Loss: 0.4930
Epoch 1 Step 251 Train Loss: 0.4149
Epoch 1 Step 301 Train Loss: 0.4785
Epoch 1 Step 351 Train Loss: 0.4364
Epoch 1 Step 401 Train Loss: 0.4768
Epoch 1 Step 451 Train Loss: 0.5397
Epoch 1 Step 501 Train Loss: 0.4261
Epoch 1 Step 551 Train Loss: 0.4296
Epoch 1 Step 601 Train Loss: 0.4966
Epoch 1 Step 651 Train Loss: 0.4979
Epoch 1 Step 701 Train Loss: 0.4740
Epoch 1 Step 751 Train Loss: 0.4893
Epoch 1 Step 801 Train Loss: 0.4552
Epoch 1 Step 851 Train Loss: 0.4528
Epoch 1 Step 901 Train Loss: 0.5530
Epoch 1 Step 951 Train Loss: 0.4758
Epoch 1 Step 1001 Train Loss: 0.4312
Epoch 1 Step 1051 Train Loss: 0.4536
Epoch 1 Step 1101 Train Loss: 0.4888
Epoch 1 Step 1151 Train Loss: 0.4538
Epoch 1 Step 1201 Train Loss: 0.4779
Epoch 1 Step 1251 Train Loss: 0.4755
Epoch 1 Step 1301 Train Loss: 0.4398
Epoch 1 Step 1351 Train Loss: 0.4553
Epoch 1 Step 1401 Train Loss: 0.5139
Epoch 1 Step 1451 Train Loss: 0.4713
Epoch 1 Step 1501 Train Loss: 0.4263
Epoch 1 Step 1551 Train Loss: 0.4716
Epoch 1 Step 1601 Train Loss: 0.4439
Epoch 1 Step 1651 Train Loss: 0.4786
Epoch 1 Step 1701 Train Loss: 0.4107
Epoch 1 Step 1751 Train Loss: 0.4629
Epoch 1 Step 1801 Train Loss: 0.4373
Epoch 1: Train Overall MSE: 0.0038 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.1169 Validation Top 20 DE MSE: 0.1855. 
Epoch 2 Step 1 Train Loss: 0.4975
Epoch 2 Step 51 Train Loss: 0.4227
Epoch 2 Step 101 Train Loss: 0.4459
Epoch 2 Step 151 Train Loss: 0.5132
Epoch 2 Step 201 Train Loss: 0.4091
Epoch 2 Step 251 Train Loss: 0.5438
Epoch 2 Step 301 Train Loss: 0.4916
Epoch 2 Step 351 Train Loss: 0.4468
Epoch 2 Step 401 Train Loss: 0.4492
Epoch 2 Step 451 Train Loss: 0.4831
Epoch 2 Step 501 Train Loss: 0.4900
Epoch 2 Step 551 Train Loss: 0.4899
Epoch 2 Step 601 Train Loss: 0.4719
Epoch 2 Step 651 Train Loss: 0.4086
Epoch 2 Step 701 Train Loss: 0.4315
Epoch 2 Step 751 Train Loss: 0.4668
Epoch 2 Step 801 Train Loss: 0.4513
Epoch 2 Step 851 Train Loss: 0.4570
Epoch 2 Step 901 Train Loss: 0.5540
Epoch 2 Step 951 Train Loss: 0.5003
Epoch 2 Step 1001 Train Loss: 0.4477
Epoch 2 Step 1051 Train Loss: 0.5026
Epoch 2 Step 1101 Train Loss: 0.4499
Epoch 2 Step 1151 Train Loss: 0.4406
Epoch 2 Step 1201 Train Loss: 0.4278
Epoch 2 Step 1251 Train Loss: 0.4580
Epoch 2 Step 1301 Train Loss: 0.4542
Epoch 2 Step 1351 Train Loss: 0.4994
Epoch 2 Step 1401 Train Loss: 0.4289
Epoch 2 Step 1451 Train Loss: 0.4629
Epoch 2 Step 1501 Train Loss: 0.4893
Epoch 2 Step 1551 Train Loss: 0.4738
Epoch 2 Step 1601 Train Loss: 0.5102
Epoch 2 Step 1651 Train Loss: 0.4384
Epoch 2 Step 1701 Train Loss: 0.4678
Epoch 2 Step 1751 Train Loss: 0.4707
Epoch 2 Step 1801 Train Loss: 0.4289
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.1001 Validation Top 20 DE MSE: 0.1614. 
Epoch 3 Step 1 Train Loss: 0.4595
Epoch 3 Step 51 Train Loss: 0.4466
Epoch 3 Step 101 Train Loss: 0.4164
Epoch 3 Step 151 Train Loss: 0.5509
Epoch 3 Step 201 Train Loss: 0.4557
Epoch 3 Step 251 Train Loss: 0.4715
Epoch 3 Step 301 Train Loss: 0.4484
Epoch 3 Step 351 Train Loss: 0.4875
Epoch 3 Step 401 Train Loss: 0.4513
Epoch 3 Step 451 Train Loss: 0.4746
Epoch 3 Step 501 Train Loss: 0.4578
Epoch 3 Step 551 Train Loss: 0.4421
Epoch 3 Step 601 Train Loss: 0.4515
Epoch 3 Step 651 Train Loss: 0.4905
Epoch 3 Step 701 Train Loss: 0.5469
Epoch 3 Step 751 Train Loss: 0.4634
Epoch 3 Step 801 Train Loss: 0.4723
Epoch 3 Step 851 Train Loss: 0.4671
Epoch 3 Step 901 Train Loss: 0.4764
Epoch 3 Step 951 Train Loss: 0.5237
Epoch 3 Step 1001 Train Loss: 0.4879
Epoch 3 Step 1051 Train Loss: 0.4176
Epoch 3 Step 1101 Train Loss: 0.4801
Epoch 3 Step 1151 Train Loss: 0.4463
Epoch 3 Step 1201 Train Loss: 0.4416
Epoch 3 Step 1251 Train Loss: 0.5111
Epoch 3 Step 1301 Train Loss: 0.4568
Epoch 3 Step 1351 Train Loss: 0.5697
Epoch 3 Step 1401 Train Loss: 0.4574
Epoch 3 Step 1451 Train Loss: 0.4816
Epoch 3 Step 1501 Train Loss: 0.4014
Epoch 3 Step 1551 Train Loss: 0.4860
Epoch 3 Step 1601 Train Loss: 0.4906
Epoch 3 Step 1651 Train Loss: 0.4651
Epoch 3 Step 1701 Train Loss: 0.4464
Epoch 3 Step 1751 Train Loss: 0.4515
Epoch 3 Step 1801 Train Loss: 0.4990
Epoch 3: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.1394 Validation Top 20 DE MSE: 0.1888. 
Epoch 4 Step 1 Train Loss: 0.4975
Epoch 4 Step 51 Train Loss: 0.4730
Epoch 4 Step 101 Train Loss: 0.5027
Epoch 4 Step 151 Train Loss: 0.4573
Epoch 4 Step 201 Train Loss: 0.5314
Epoch 4 Step 251 Train Loss: 0.4835
Epoch 4 Step 301 Train Loss: 0.4486
Epoch 4 Step 351 Train Loss: 0.4866
Epoch 4 Step 401 Train Loss: 0.5398
Epoch 4 Step 451 Train Loss: 0.4743
Epoch 4 Step 501 Train Loss: 0.5009
Epoch 4 Step 551 Train Loss: 0.4896
Epoch 4 Step 601 Train Loss: 0.4634
Epoch 4 Step 651 Train Loss: 0.5087
Epoch 4 Step 701 Train Loss: 0.5006
Epoch 4 Step 751 Train Loss: 0.4880
Epoch 4 Step 801 Train Loss: 0.4511
Epoch 4 Step 851 Train Loss: 0.4648
Epoch 4 Step 901 Train Loss: 0.4844
Epoch 4 Step 951 Train Loss: 0.4957
Epoch 4 Step 1001 Train Loss: 0.4763
Epoch 4 Step 1051 Train Loss: 0.4830
Epoch 4 Step 1101 Train Loss: 0.5056
Epoch 4 Step 1151 Train Loss: 0.4892
Epoch 4 Step 1201 Train Loss: 0.5149
Epoch 4 Step 1251 Train Loss: 0.5080
Epoch 4 Step 1301 Train Loss: 0.4607
Epoch 4 Step 1351 Train Loss: 0.5031
Epoch 4 Step 1401 Train Loss: 0.4875
Epoch 4 Step 1451 Train Loss: 0.5174
Epoch 4 Step 1501 Train Loss: 0.4879
Epoch 4 Step 1551 Train Loss: 0.5012
Epoch 4 Step 1601 Train Loss: 0.4642
Epoch 4 Step 1651 Train Loss: 0.4950
Epoch 4 Step 1701 Train Loss: 0.4808
Epoch 4 Step 1751 Train Loss: 0.4887
Epoch 4 Step 1801 Train Loss: 0.5104
Epoch 4: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.1027 Validation Top 20 DE MSE: 0.1682. 
Epoch 5 Step 1 Train Loss: 0.4873
Epoch 5 Step 51 Train Loss: 0.4431
Epoch 5 Step 101 Train Loss: 0.5038
Epoch 5 Step 151 Train Loss: 0.5217
Epoch 5 Step 201 Train Loss: 0.4615
Epoch 5 Step 251 Train Loss: 0.4642
Epoch 5 Step 301 Train Loss: 0.4689
Epoch 5 Step 351 Train Loss: 0.5100
Epoch 5 Step 401 Train Loss: 0.5060
Epoch 5 Step 451 Train Loss: 0.5023
Epoch 5 Step 501 Train Loss: 0.4580
Epoch 5 Step 551 Train Loss: 0.5068
Epoch 5 Step 601 Train Loss: 0.4602
Epoch 5 Step 651 Train Loss: 0.4883
Epoch 5 Step 701 Train Loss: 0.4619
Epoch 5 Step 751 Train Loss: 0.5197
Epoch 5 Step 801 Train Loss: 0.4672
Epoch 5 Step 851 Train Loss: 0.5124
Epoch 5 Step 901 Train Loss: 0.5034
Epoch 5 Step 951 Train Loss: 0.4849
Epoch 5 Step 1001 Train Loss: 0.4901
Epoch 5 Step 1051 Train Loss: 0.4695
Epoch 5 Step 1101 Train Loss: 0.5238
Epoch 5 Step 1151 Train Loss: 0.4627
Epoch 5 Step 1201 Train Loss: 0.5054
Epoch 5 Step 1251 Train Loss: 0.4846
Epoch 5 Step 1301 Train Loss: 0.5076
Epoch 5 Step 1351 Train Loss: 0.5086
Epoch 5 Step 1401 Train Loss: 0.4967
Epoch 5 Step 1451 Train Loss: 0.4267
Epoch 5 Step 1501 Train Loss: 0.5214
Epoch 5 Step 1551 Train Loss: 0.5273
Epoch 5 Step 1601 Train Loss: 0.4802
Epoch 5 Step 1651 Train Loss: 0.4711
Epoch 5 Step 1701 Train Loss: 0.5267
Epoch 5 Step 1751 Train Loss: 0.5356
Epoch 5 Step 1801 Train Loss: 0.4721
Epoch 5: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0964 Validation Top 20 DE MSE: 0.1650. 
Epoch 6 Step 1 Train Loss: 0.5546
Epoch 6 Step 51 Train Loss: 0.4353
Epoch 6 Step 101 Train Loss: 0.4281
Epoch 6 Step 151 Train Loss: 0.5121
Epoch 6 Step 201 Train Loss: 0.5184
Epoch 6 Step 251 Train Loss: 0.5429
Epoch 6 Step 301 Train Loss: 0.4946
Epoch 6 Step 351 Train Loss: 0.4815
Epoch 6 Step 401 Train Loss: 0.5752
Epoch 6 Step 451 Train Loss: 0.5382
Epoch 6 Step 501 Train Loss: 0.5248
Epoch 6 Step 551 Train Loss: 0.5059
Epoch 6 Step 601 Train Loss: 0.5341
Epoch 6 Step 651 Train Loss: 0.5238
Epoch 6 Step 701 Train Loss: 0.4848
Epoch 6 Step 751 Train Loss: 0.5728
Epoch 6 Step 801 Train Loss: 0.4917
Epoch 6 Step 851 Train Loss: 0.4448
Epoch 6 Step 901 Train Loss: 0.5161
Epoch 6 Step 951 Train Loss: 0.4919
Epoch 6 Step 1001 Train Loss: 0.5050
Epoch 6 Step 1051 Train Loss: 0.5280
Epoch 6 Step 1101 Train Loss: 0.5300
Epoch 6 Step 1151 Train Loss: 0.5178
Epoch 6 Step 1201 Train Loss: 0.4728
Epoch 6 Step 1251 Train Loss: 0.4663
Epoch 6 Step 1301 Train Loss: 0.5102
Epoch 6 Step 1351 Train Loss: 0.5932
Epoch 6 Step 1401 Train Loss: 0.5052
Epoch 6 Step 1451 Train Loss: 0.4744
Epoch 6 Step 1501 Train Loss: 0.4485
Epoch 6 Step 1551 Train Loss: 0.4806
Epoch 6 Step 1601 Train Loss: 0.5254
Epoch 6 Step 1651 Train Loss: 0.4432
Epoch 6 Step 1701 Train Loss: 0.5150
Epoch 6 Step 1751 Train Loss: 0.4931
Epoch 6 Step 1801 Train Loss: 0.4945
Epoch 6: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0938 Validation Top 20 DE MSE: 0.1647. 
Epoch 7 Step 1 Train Loss: 0.4845
Epoch 7 Step 51 Train Loss: 0.4770
Epoch 7 Step 101 Train Loss: 0.5195
Epoch 7 Step 151 Train Loss: 0.4942
Epoch 7 Step 201 Train Loss: 0.5332
Epoch 7 Step 251 Train Loss: 0.4840
Epoch 7 Step 301 Train Loss: 0.5049
Epoch 7 Step 351 Train Loss: 0.5191
Epoch 7 Step 401 Train Loss: 0.4740
Epoch 7 Step 451 Train Loss: 0.4965
Epoch 7 Step 501 Train Loss: 0.4948
Epoch 7 Step 551 Train Loss: 0.5310
Epoch 7 Step 601 Train Loss: 0.4703
Epoch 7 Step 651 Train Loss: 0.5295
Epoch 7 Step 701 Train Loss: 0.4639
Epoch 7 Step 751 Train Loss: 0.5042
Epoch 7 Step 801 Train Loss: 0.5005
Epoch 7 Step 851 Train Loss: 0.5328
Epoch 7 Step 901 Train Loss: 0.5059
Epoch 7 Step 951 Train Loss: 0.4880
Epoch 7 Step 1001 Train Loss: 0.4966
Epoch 7 Step 1051 Train Loss: 0.4930
Epoch 7 Step 1101 Train Loss: 0.4754
Epoch 7 Step 1151 Train Loss: 0.5708
Epoch 7 Step 1201 Train Loss: 0.5005
Epoch 7 Step 1251 Train Loss: 0.4838
Epoch 7 Step 1301 Train Loss: 0.4930
Epoch 7 Step 1351 Train Loss: 0.4948
Epoch 7 Step 1401 Train Loss: 0.4734
Epoch 7 Step 1451 Train Loss: 0.4851
Epoch 7 Step 1501 Train Loss: 0.6168
Epoch 7 Step 1551 Train Loss: 0.5196
Epoch 7 Step 1601 Train Loss: 0.4593
Epoch 7 Step 1651 Train Loss: 0.5096
Epoch 7 Step 1701 Train Loss: 0.5070
Epoch 7 Step 1751 Train Loss: 0.5090
Epoch 7 Step 1801 Train Loss: 0.4857
Epoch 7: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0921 Validation Top 20 DE MSE: 0.1666. 
Epoch 8 Step 1 Train Loss: 0.5276
Epoch 8 Step 51 Train Loss: 0.4893
Epoch 8 Step 101 Train Loss: 0.4589
Epoch 8 Step 151 Train Loss: 0.4725
Epoch 8 Step 201 Train Loss: 0.5247
Epoch 8 Step 251 Train Loss: 0.4582
Epoch 8 Step 301 Train Loss: 0.4803
Epoch 8 Step 351 Train Loss: 0.4808
Epoch 8 Step 401 Train Loss: 0.4796
Epoch 8 Step 451 Train Loss: 0.5507
Epoch 8 Step 501 Train Loss: 0.4658
Epoch 8 Step 551 Train Loss: 0.4646
Epoch 8 Step 601 Train Loss: 0.4601
Epoch 8 Step 651 Train Loss: 0.4599
Epoch 8 Step 701 Train Loss: 0.4756
Epoch 8 Step 751 Train Loss: 0.5135
Epoch 8 Step 801 Train Loss: 0.4604
Epoch 8 Step 851 Train Loss: 0.4646
Epoch 8 Step 901 Train Loss: 0.4833
Epoch 8 Step 951 Train Loss: 0.5236
Epoch 8 Step 1001 Train Loss: 0.5381
Epoch 8 Step 1051 Train Loss: 0.4730
Epoch 8 Step 1101 Train Loss: 0.4565
Epoch 8 Step 1151 Train Loss: 0.5947
Epoch 8 Step 1201 Train Loss: 0.5061
Epoch 8 Step 1251 Train Loss: 0.4783
Epoch 8 Step 1301 Train Loss: 0.5486
Epoch 8 Step 1351 Train Loss: 0.5130
Epoch 8 Step 1401 Train Loss: 0.5142
Epoch 8 Step 1451 Train Loss: 0.4889
Epoch 8 Step 1501 Train Loss: 0.4610
Epoch 8 Step 1551 Train Loss: 0.5875
Epoch 8 Step 1601 Train Loss: 0.4801
Epoch 8 Step 1651 Train Loss: 0.5340
Epoch 8 Step 1701 Train Loss: 0.4532
Epoch 8 Step 1751 Train Loss: 0.5325
Epoch 8 Step 1801 Train Loss: 0.4966
Epoch 8: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0907 Validation Top 20 DE MSE: 0.1678. 
Epoch 9 Step 1 Train Loss: 0.5277
Epoch 9 Step 51 Train Loss: 0.5025
Epoch 9 Step 101 Train Loss: 0.5197
Epoch 9 Step 151 Train Loss: 0.5114
Epoch 9 Step 201 Train Loss: 0.5157
Epoch 9 Step 251 Train Loss: 0.4836
Epoch 9 Step 301 Train Loss: 0.5336
Epoch 9 Step 351 Train Loss: 0.4915
Epoch 9 Step 401 Train Loss: 0.4516
Epoch 9 Step 451 Train Loss: 0.5549
Epoch 9 Step 501 Train Loss: 0.4791
Epoch 9 Step 551 Train Loss: 0.4671
Epoch 9 Step 601 Train Loss: 0.4891
Epoch 9 Step 651 Train Loss: 0.5058
Epoch 9 Step 701 Train Loss: 0.4933
Epoch 9 Step 751 Train Loss: 0.5077
Epoch 9 Step 801 Train Loss: 0.4784
Epoch 9 Step 851 Train Loss: 0.4898
Epoch 9 Step 901 Train Loss: 0.5117
Epoch 9 Step 951 Train Loss: 0.4745
Epoch 9 Step 1001 Train Loss: 0.4867
Epoch 9 Step 1051 Train Loss: 0.4867
Epoch 9 Step 1101 Train Loss: 0.4619
Epoch 9 Step 1151 Train Loss: 0.4828
Epoch 9 Step 1201 Train Loss: 0.5213
Epoch 9 Step 1251 Train Loss: 0.5181
Epoch 9 Step 1301 Train Loss: 0.5269
Epoch 9 Step 1351 Train Loss: 0.5190
Epoch 9 Step 1401 Train Loss: 0.4422
Epoch 9 Step 1451 Train Loss: 0.5245
Epoch 9 Step 1501 Train Loss: 0.5760
Epoch 9 Step 1551 Train Loss: 0.5083
Epoch 9 Step 1601 Train Loss: 0.5420
Epoch 9 Step 1651 Train Loss: 0.5205
Epoch 9 Step 1701 Train Loss: 0.4982
Epoch 9 Step 1751 Train Loss: 0.4785
Epoch 9 Step 1801 Train Loss: 0.5151
Epoch 9: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0905 Validation Top 20 DE MSE: 0.1668. 
Epoch 10 Step 1 Train Loss: 0.5035
Epoch 10 Step 51 Train Loss: 0.5139
Epoch 10 Step 101 Train Loss: 0.5217
Epoch 10 Step 151 Train Loss: 0.4904
Epoch 10 Step 201 Train Loss: 0.5639
Epoch 10 Step 251 Train Loss: 0.4676
Epoch 10 Step 301 Train Loss: 0.4677
Epoch 10 Step 351 Train Loss: 0.5227
Epoch 10 Step 401 Train Loss: 0.4811
Epoch 10 Step 451 Train Loss: 0.4274
Epoch 10 Step 501 Train Loss: 0.5189
Epoch 10 Step 551 Train Loss: 0.5326
Epoch 10 Step 601 Train Loss: 0.5356
Epoch 10 Step 651 Train Loss: 0.4545
Epoch 10 Step 701 Train Loss: 0.5616
Epoch 10 Step 751 Train Loss: 0.5029
Epoch 10 Step 801 Train Loss: 0.5114
Epoch 10 Step 851 Train Loss: 0.5243
Epoch 10 Step 901 Train Loss: 0.5030
Epoch 10 Step 951 Train Loss: 0.5071
Epoch 10 Step 1001 Train Loss: 0.5815
Epoch 10 Step 1051 Train Loss: 0.5126
Epoch 10 Step 1101 Train Loss: 0.4701
Epoch 10 Step 1151 Train Loss: 0.4873
Epoch 10 Step 1201 Train Loss: 0.4825
Epoch 10 Step 1251 Train Loss: 0.4497
Epoch 10 Step 1301 Train Loss: 0.4639
Epoch 10 Step 1351 Train Loss: 0.4863
Epoch 10 Step 1401 Train Loss: 0.4728
Epoch 10 Step 1451 Train Loss: 0.5788
Epoch 10 Step 1501 Train Loss: 0.4607
Epoch 10 Step 1551 Train Loss: 0.5430
Epoch 10 Step 1601 Train Loss: 0.5118
Epoch 10 Step 1651 Train Loss: 0.4646
Epoch 10 Step 1701 Train Loss: 0.4554
Epoch 10 Step 1751 Train Loss: 0.4769
Epoch 10 Step 1801 Train Loss: 0.4841
Epoch 10: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0901 Validation Top 20 DE MSE: 0.1680. 
Epoch 11 Step 1 Train Loss: 0.4915
Epoch 11 Step 51 Train Loss: 0.4745
Epoch 11 Step 101 Train Loss: 0.4651
Epoch 11 Step 151 Train Loss: 0.5462
Epoch 11 Step 201 Train Loss: 0.4855
Epoch 11 Step 251 Train Loss: 0.5174
Epoch 11 Step 301 Train Loss: 0.5140
Epoch 11 Step 351 Train Loss: 0.4825
Epoch 11 Step 401 Train Loss: 0.4958
Epoch 11 Step 451 Train Loss: 0.4920
Epoch 11 Step 501 Train Loss: 0.5489
Epoch 11 Step 551 Train Loss: 0.5505
Epoch 11 Step 601 Train Loss: 0.4816
Epoch 11 Step 651 Train Loss: 0.4725
Epoch 11 Step 701 Train Loss: 0.4640
Epoch 11 Step 751 Train Loss: 0.4781
Epoch 11 Step 801 Train Loss: 0.6272
Epoch 11 Step 851 Train Loss: 0.4946
Epoch 11 Step 901 Train Loss: 0.4840
Epoch 11 Step 951 Train Loss: 0.4954
Epoch 11 Step 1001 Train Loss: 0.4720
Epoch 11 Step 1051 Train Loss: 0.4588
Epoch 11 Step 1101 Train Loss: 0.4678
Epoch 11 Step 1151 Train Loss: 0.5345
Epoch 11 Step 1201 Train Loss: 0.5161
Epoch 11 Step 1251 Train Loss: 0.4580
Epoch 11 Step 1301 Train Loss: 0.5055
Epoch 11 Step 1351 Train Loss: 0.5005
Epoch 11 Step 1401 Train Loss: 0.5259
Epoch 11 Step 1451 Train Loss: 0.5020
Epoch 11 Step 1501 Train Loss: 0.5472
Epoch 11 Step 1551 Train Loss: 0.4772
Epoch 11 Step 1601 Train Loss: 0.5044
Epoch 11 Step 1651 Train Loss: 0.4938
Epoch 11 Step 1701 Train Loss: 0.5444
Epoch 11 Step 1751 Train Loss: 0.6008
Epoch 11 Step 1801 Train Loss: 0.4823
Epoch 11: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0883 Validation Top 20 DE MSE: 0.1650. 
Epoch 12 Step 1 Train Loss: 0.4990
Epoch 12 Step 51 Train Loss: 0.4940
Epoch 12 Step 101 Train Loss: 0.5061
Epoch 12 Step 151 Train Loss: 0.5703
Epoch 12 Step 201 Train Loss: 0.5074
Epoch 12 Step 251 Train Loss: 0.5293
Epoch 12 Step 301 Train Loss: 0.5438
Epoch 12 Step 351 Train Loss: 0.5252
Epoch 12 Step 401 Train Loss: 0.5357
Epoch 12 Step 451 Train Loss: 0.5223
Epoch 12 Step 501 Train Loss: 0.4917
Epoch 12 Step 551 Train Loss: 0.5091
Epoch 12 Step 601 Train Loss: 0.4878
Epoch 12 Step 651 Train Loss: 0.5704
Epoch 12 Step 701 Train Loss: 0.5361
Epoch 12 Step 751 Train Loss: 0.4878
Epoch 12 Step 801 Train Loss: 0.6057
Epoch 12 Step 851 Train Loss: 0.4746
Epoch 12 Step 901 Train Loss: 0.5329
Epoch 12 Step 951 Train Loss: 0.4899
Epoch 12 Step 1001 Train Loss: 0.4967
Epoch 12 Step 1051 Train Loss: 0.4922
Epoch 12 Step 1101 Train Loss: 0.5206
Epoch 12 Step 1151 Train Loss: 0.4533
Epoch 12 Step 1201 Train Loss: 0.5074
Epoch 12 Step 1251 Train Loss: 0.5338
Epoch 12 Step 1301 Train Loss: 0.4998
Epoch 12 Step 1351 Train Loss: 0.4791
Epoch 12 Step 1401 Train Loss: 0.5479
Epoch 12 Step 1451 Train Loss: 0.5679
Epoch 12 Step 1501 Train Loss: 0.5229
Epoch 12 Step 1551 Train Loss: 0.4988
Epoch 12 Step 1601 Train Loss: 0.4826
Epoch 12 Step 1651 Train Loss: 0.4387
Epoch 12 Step 1701 Train Loss: 0.4973
Epoch 12 Step 1751 Train Loss: 0.4976
Epoch 12 Step 1801 Train Loss: 0.5202
Epoch 12: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0886 Validation Top 20 DE MSE: 0.1670. 
Epoch 13 Step 1 Train Loss: 0.5248
Epoch 13 Step 51 Train Loss: 0.4977
Epoch 13 Step 101 Train Loss: 0.5089
Epoch 13 Step 151 Train Loss: 0.4584
Epoch 13 Step 201 Train Loss: 0.4606
Epoch 13 Step 251 Train Loss: 0.4798
Epoch 13 Step 301 Train Loss: 0.5485
Epoch 13 Step 351 Train Loss: 0.5068
Epoch 13 Step 401 Train Loss: 0.5415
Epoch 13 Step 451 Train Loss: 0.5302
Epoch 13 Step 501 Train Loss: 0.5165
Epoch 13 Step 551 Train Loss: 0.4834
Epoch 13 Step 601 Train Loss: 0.4912
Epoch 13 Step 651 Train Loss: 0.4908
Epoch 13 Step 701 Train Loss: 0.4807
Epoch 13 Step 751 Train Loss: 0.4544
Epoch 13 Step 801 Train Loss: 0.4754
Epoch 13 Step 851 Train Loss: 0.5143
Epoch 13 Step 901 Train Loss: 0.4702
Epoch 13 Step 951 Train Loss: 0.4897
Epoch 13 Step 1001 Train Loss: 0.4666
Epoch 13 Step 1051 Train Loss: 0.5444
Epoch 13 Step 1101 Train Loss: 0.4897
Epoch 13 Step 1151 Train Loss: 0.4563
Epoch 13 Step 1201 Train Loss: 0.5469
Epoch 13 Step 1251 Train Loss: 0.5081
Epoch 13 Step 1301 Train Loss: 0.6115
Epoch 13 Step 1351 Train Loss: 0.4754
Epoch 13 Step 1401 Train Loss: 0.5156
Epoch 13 Step 1451 Train Loss: 0.5281
Epoch 13 Step 1501 Train Loss: 0.5129
Epoch 13 Step 1551 Train Loss: 0.4849
Epoch 13 Step 1601 Train Loss: 0.5106
Epoch 13 Step 1651 Train Loss: 0.5099
Epoch 13 Step 1701 Train Loss: 0.5091
Epoch 13 Step 1751 Train Loss: 0.4725
Epoch 13 Step 1801 Train Loss: 0.5039
Epoch 13: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0882 Validation Top 20 DE MSE: 0.1646. 
Epoch 14 Step 1 Train Loss: 0.5129
Epoch 14 Step 51 Train Loss: 0.5295
Epoch 14 Step 101 Train Loss: 0.5460
Epoch 14 Step 151 Train Loss: 0.5121
Epoch 14 Step 201 Train Loss: 0.5007
Epoch 14 Step 251 Train Loss: 0.5244
Epoch 14 Step 301 Train Loss: 0.5205
Epoch 14 Step 351 Train Loss: 0.5029
Epoch 14 Step 401 Train Loss: 0.4954
Epoch 14 Step 451 Train Loss: 0.5111
Epoch 14 Step 501 Train Loss: 0.4530
Epoch 14 Step 551 Train Loss: 0.4942
Epoch 14 Step 601 Train Loss: 0.4972
Epoch 14 Step 651 Train Loss: 0.4992
Epoch 14 Step 701 Train Loss: 0.4910
Epoch 14 Step 751 Train Loss: 0.4856
Epoch 14 Step 801 Train Loss: 0.5402
Epoch 14 Step 851 Train Loss: 0.5132
Epoch 14 Step 901 Train Loss: 0.4957
Epoch 14 Step 951 Train Loss: 0.5090
Epoch 14 Step 1001 Train Loss: 0.4982
Epoch 14 Step 1051 Train Loss: 0.5796
Epoch 14 Step 1101 Train Loss: 0.4741
Epoch 14 Step 1151 Train Loss: 0.4970
Epoch 14 Step 1201 Train Loss: 0.4913
Epoch 14 Step 1251 Train Loss: 0.4951
Epoch 14 Step 1301 Train Loss: 0.4809
Epoch 14 Step 1351 Train Loss: 0.4939
Epoch 14 Step 1401 Train Loss: 0.4980
Epoch 14 Step 1451 Train Loss: 0.4801
Epoch 14 Step 1501 Train Loss: 0.5758
Epoch 14 Step 1551 Train Loss: 0.5038
Epoch 14 Step 1601 Train Loss: 0.4578
Epoch 14 Step 1651 Train Loss: 0.5467
Epoch 14 Step 1701 Train Loss: 0.5023
Epoch 14 Step 1751 Train Loss: 0.4969
Epoch 14 Step 1801 Train Loss: 0.4672
Epoch 14: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0866 Validation Top 20 DE MSE: 0.1646. 
Epoch 15 Step 1 Train Loss: 0.4934
Epoch 15 Step 51 Train Loss: 0.5046
Epoch 15 Step 101 Train Loss: 0.5144
Epoch 15 Step 151 Train Loss: 0.4901
Epoch 15 Step 201 Train Loss: 0.4958
Epoch 15 Step 251 Train Loss: 0.5137
Epoch 15 Step 301 Train Loss: 0.5021
Epoch 15 Step 351 Train Loss: 0.4948
Epoch 15 Step 401 Train Loss: 0.4290
Epoch 15 Step 451 Train Loss: 0.4835
Epoch 15 Step 501 Train Loss: 0.4707
Epoch 15 Step 551 Train Loss: 0.5141
Epoch 15 Step 601 Train Loss: 0.5012
Epoch 15 Step 651 Train Loss: 0.4797
Epoch 15 Step 701 Train Loss: 0.5748
Epoch 15 Step 751 Train Loss: 0.4760
Epoch 15 Step 801 Train Loss: 0.4833
Epoch 15 Step 851 Train Loss: 0.4716
Epoch 15 Step 901 Train Loss: 0.5185
Epoch 15 Step 951 Train Loss: 0.4928
Epoch 15 Step 1001 Train Loss: 0.4698
Epoch 15 Step 1051 Train Loss: 0.4864
Epoch 15 Step 1101 Train Loss: 0.4693
Epoch 15 Step 1151 Train Loss: 0.5168
Epoch 15 Step 1201 Train Loss: 0.5221
Epoch 15 Step 1251 Train Loss: 0.4779
Epoch 15 Step 1301 Train Loss: 0.4708
Epoch 15 Step 1351 Train Loss: 0.4814
Epoch 15 Step 1401 Train Loss: 0.4824
Epoch 15 Step 1451 Train Loss: 0.5302
Epoch 15 Step 1501 Train Loss: 0.4930
Epoch 15 Step 1551 Train Loss: 0.4738
Epoch 15 Step 1601 Train Loss: 0.4890
Epoch 15 Step 1651 Train Loss: 0.5112
Epoch 15 Step 1701 Train Loss: 0.4884
Epoch 15 Step 1751 Train Loss: 0.4786
Epoch 15 Step 1801 Train Loss: 0.5424
Epoch 15: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0905 Validation Top 20 DE MSE: 0.1660. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1615
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.004631061
test_combo_seen0_pearson: 0.9896373464754781
test_combo_seen0_mse_de: 0.22544952
test_combo_seen0_pearson_de: 0.7617780101908271
test_combo_seen1_mse: 0.003970841
test_combo_seen1_pearson: 0.9909948099599142
test_combo_seen1_mse_de: 0.17883983
test_combo_seen1_pearson_de: 0.8403978695114052
test_combo_seen2_mse: 0.0044493577
test_combo_seen2_pearson: 0.9897761025358951
test_combo_seen2_mse_de: 0.123151086
test_combo_seen2_pearson_de: 0.7796272697058859
test_unseen_single_mse: 0.0029048235
test_unseen_single_pearson: 0.9934753121935853
test_unseen_single_mse_de: 0.15021816
test_unseen_single_pearson_de: 0.9547547822661828
test_combo_seen0_pearson_delta: 0.4593453521180121
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.18
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8099999999999999
test_combo_seen0_mse_top20_de_non_dropout: 0.26640007
test_combo_seen1_pearson_delta: 0.5269059929183161
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.13977272727272727
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8738636363636364
test_combo_seen1_mse_top20_de_non_dropout: 0.20384087
test_combo_seen2_pearson_delta: 0.5070133108801553
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.19761904761904764
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.8690476190476192
test_combo_seen2_mse_top20_de_non_dropout: 0.16513292
test_unseen_single_pearson_delta: 0.42886094769751426
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.24400000000000002
test_unseen_single_frac_sigma_below_1_non_dropout: 0.898
test_unseen_single_mse_top20_de_non_dropout: 0.16169508
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.034 MB uploadedwandb: | 0.001 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb: | 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÉ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÑ
wandb:                                                   val_de_mse ‚ñá‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÑ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.18
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.81
wandb:                                         test_combo_seen0_mse 0.00463
wandb:                                      test_combo_seen0_mse_de 0.22545
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.2664
wandb:                                     test_combo_seen0_pearson 0.98964
wandb:                                  test_combo_seen0_pearson_de 0.76178
wandb:                               test_combo_seen0_pearson_delta 0.45935
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.13977
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.87386
wandb:                                         test_combo_seen1_mse 0.00397
wandb:                                      test_combo_seen1_mse_de 0.17884
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.20384
wandb:                                     test_combo_seen1_pearson 0.99099
wandb:                                  test_combo_seen1_pearson_de 0.8404
wandb:                               test_combo_seen1_pearson_delta 0.52691
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.19762
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.86905
wandb:                                         test_combo_seen2_mse 0.00445
wandb:                                      test_combo_seen2_mse_de 0.12315
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.16513
wandb:                                     test_combo_seen2_pearson 0.98978
wandb:                                  test_combo_seen2_pearson_de 0.77963
wandb:                               test_combo_seen2_pearson_delta 0.50701
wandb:                                                  test_de_mse 0.16145
wandb:                                              test_de_pearson 0.85292
wandb:               test_frac_opposite_direction_top20_non_dropout 0.18211
wandb:                          test_frac_sigma_below_1_non_dropout 0.87579
wandb:                                                     test_mse 0.00383
wandb:                                test_mse_top20_de_non_dropout 0.18749
wandb:                                                 test_pearson 0.99131
wandb:                                           test_pearson_delta 0.49315
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.244
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.898
wandb:                                       test_unseen_single_mse 0.0029
wandb:                                    test_unseen_single_mse_de 0.15022
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.1617
wandb:                                   test_unseen_single_pearson 0.99348
wandb:                                test_unseen_single_pearson_de 0.95475
wandb:                             test_unseen_single_pearson_delta 0.42886
wandb:                                                 train_de_mse 0.09048
wandb:                                             train_de_pearson 0.89325
wandb:                                                    train_mse 0.00235
wandb:                                                train_pearson 0.99473
wandb:                                                training_loss 0.46074
wandb:                                                   val_de_mse 0.16601
wandb:                                               val_de_pearson 0.76444
wandb:                                                      val_mse 0.0034
wandb:                                                  val_pearson 0.99226
wandb: 
wandb: üöÄ View run scbert_NormanWeissman2019_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/75y1fnix
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_112927-75y1fnix/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:10
combo_seen1:58
combo_seen2:16
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/scbert/scbert/wandb/run-20241029_123713-0kvfdrii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scbert_NormanWeissman2019_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/0kvfdrii
wandb: WARNING Serializing object of type ndarray that is 8059328 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5006
Epoch 1 Step 51 Train Loss: 0.4906
Epoch 1 Step 101 Train Loss: 0.4731
Epoch 1 Step 151 Train Loss: 0.4999
Epoch 1 Step 201 Train Loss: 0.4590
Epoch 1 Step 251 Train Loss: 0.4950
Epoch 1 Step 301 Train Loss: 0.4821
Epoch 1 Step 351 Train Loss: 0.4275
Epoch 1 Step 401 Train Loss: 0.4686
Epoch 1 Step 451 Train Loss: 0.3964
Epoch 1 Step 501 Train Loss: 0.4432
Epoch 1 Step 551 Train Loss: 0.4136
Epoch 1 Step 601 Train Loss: 0.4332
Epoch 1 Step 651 Train Loss: 0.4476
Epoch 1 Step 701 Train Loss: 0.4247
Epoch 1 Step 751 Train Loss: 0.4529
Epoch 1 Step 801 Train Loss: 0.4085
Epoch 1 Step 851 Train Loss: 0.4190
Epoch 1 Step 901 Train Loss: 0.4602
Epoch 1 Step 951 Train Loss: 0.4507
Epoch 1 Step 1001 Train Loss: 0.4360
Epoch 1 Step 1051 Train Loss: 0.4320
Epoch 1 Step 1101 Train Loss: 0.4996
Epoch 1 Step 1151 Train Loss: 0.4520
Epoch 1 Step 1201 Train Loss: 0.4569
Epoch 1 Step 1251 Train Loss: 0.4122
Epoch 1 Step 1301 Train Loss: 0.4321
Epoch 1 Step 1351 Train Loss: 0.4224
Epoch 1 Step 1401 Train Loss: 0.4991
Epoch 1 Step 1451 Train Loss: 0.4071
Epoch 1 Step 1501 Train Loss: 0.4409
Epoch 1 Step 1551 Train Loss: 0.4559
Epoch 1 Step 1601 Train Loss: 0.4183
Epoch 1 Step 1651 Train Loss: 0.4410
Epoch 1 Step 1701 Train Loss: 0.4205
Epoch 1 Step 1751 Train Loss: 0.4636
Epoch 1 Step 1801 Train Loss: 0.4500
Epoch 1: Train Overall MSE: 0.0035 Validation Overall MSE: 0.0042. 
Train Top 20 DE MSE: 0.1371 Validation Top 20 DE MSE: 0.1912. 
Epoch 2 Step 1 Train Loss: 0.4343
Epoch 2 Step 51 Train Loss: 0.4470
Epoch 2 Step 101 Train Loss: 0.5256
Epoch 2 Step 151 Train Loss: 0.4227
Epoch 2 Step 201 Train Loss: 0.4735
Epoch 2 Step 251 Train Loss: 0.4826
Epoch 2 Step 301 Train Loss: 0.4460
Epoch 2 Step 351 Train Loss: 0.4843
Epoch 2 Step 401 Train Loss: 0.4290
Epoch 2 Step 451 Train Loss: 0.4616
Epoch 2 Step 501 Train Loss: 0.4411
Epoch 2 Step 551 Train Loss: 0.4948
Epoch 2 Step 601 Train Loss: 0.4554
Epoch 2 Step 651 Train Loss: 0.4305
Epoch 2 Step 701 Train Loss: 0.4031
Epoch 2 Step 751 Train Loss: 0.4565
Epoch 2 Step 801 Train Loss: 0.4505
Epoch 2 Step 851 Train Loss: 0.4808
Epoch 2 Step 901 Train Loss: 0.4650
Epoch 2 Step 951 Train Loss: 0.4758
Epoch 2 Step 1001 Train Loss: 0.4341
Epoch 2 Step 1051 Train Loss: 0.4167
Epoch 2 Step 1101 Train Loss: 0.4415
Epoch 2 Step 1151 Train Loss: 0.4460
Epoch 2 Step 1201 Train Loss: 0.4533
Epoch 2 Step 1251 Train Loss: 0.4951
Epoch 2 Step 1301 Train Loss: 0.4659
Epoch 2 Step 1351 Train Loss: 0.4286
Epoch 2 Step 1401 Train Loss: 0.4470
Epoch 2 Step 1451 Train Loss: 0.4622
Epoch 2 Step 1501 Train Loss: 0.4879
Epoch 2 Step 1551 Train Loss: 0.4422
Epoch 2 Step 1601 Train Loss: 0.4475
Epoch 2 Step 1651 Train Loss: 0.4134
Epoch 2 Step 1701 Train Loss: 0.4376
Epoch 2 Step 1751 Train Loss: 0.5161
Epoch 2 Step 1801 Train Loss: 0.4869
Epoch 2: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0080. 
Train Top 20 DE MSE: 0.1694 Validation Top 20 DE MSE: 0.1721. 
Epoch 3 Step 1 Train Loss: 0.4524
Epoch 3 Step 51 Train Loss: 0.4467
Epoch 3 Step 101 Train Loss: 0.4623
Epoch 3 Step 151 Train Loss: 0.5017
Epoch 3 Step 201 Train Loss: 0.4771
Epoch 3 Step 251 Train Loss: 0.4619
Epoch 3 Step 301 Train Loss: 0.4345
Epoch 3 Step 351 Train Loss: 0.3997
Epoch 3 Step 401 Train Loss: 0.4374
Epoch 3 Step 451 Train Loss: 0.4287
Epoch 3 Step 501 Train Loss: 0.4443
Epoch 3 Step 551 Train Loss: 0.4567
Epoch 3 Step 601 Train Loss: 0.4999
Epoch 3 Step 651 Train Loss: 0.4344
Epoch 3 Step 701 Train Loss: 0.5228
Epoch 3 Step 751 Train Loss: 0.4294
Epoch 3 Step 801 Train Loss: 0.4640
Epoch 3 Step 851 Train Loss: 0.5193
Epoch 3 Step 901 Train Loss: 0.4239
Epoch 3 Step 951 Train Loss: 0.4458
Epoch 3 Step 1001 Train Loss: 0.4912
Epoch 3 Step 1051 Train Loss: 0.4263
Epoch 3 Step 1101 Train Loss: 0.4459
Epoch 3 Step 1151 Train Loss: 0.4441
Epoch 3 Step 1201 Train Loss: 0.5164
Epoch 3 Step 1251 Train Loss: 0.4367
Epoch 3 Step 1301 Train Loss: 0.5617
Epoch 3 Step 1351 Train Loss: 0.4140
Epoch 3 Step 1401 Train Loss: 0.4276
Epoch 3 Step 1451 Train Loss: 0.4614
Epoch 3 Step 1501 Train Loss: 0.4599
Epoch 3 Step 1551 Train Loss: 0.4258
Epoch 3 Step 1601 Train Loss: 0.4940
Epoch 3 Step 1651 Train Loss: 0.4699
Epoch 3 Step 1701 Train Loss: 0.4520
Epoch 3 Step 1751 Train Loss: 0.4153
Epoch 3 Step 1801 Train Loss: 0.4147
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.1127 Validation Top 20 DE MSE: 0.1996. 
Epoch 4 Step 1 Train Loss: 0.4445
Epoch 4 Step 51 Train Loss: 0.4656
Epoch 4 Step 101 Train Loss: 0.4280
Epoch 4 Step 151 Train Loss: 0.4728
Epoch 4 Step 201 Train Loss: 0.4469
Epoch 4 Step 251 Train Loss: 0.4749
Epoch 4 Step 301 Train Loss: 0.4852
Epoch 4 Step 351 Train Loss: 0.4120
Epoch 4 Step 401 Train Loss: 0.4409
Epoch 4 Step 451 Train Loss: 0.4720
Epoch 4 Step 501 Train Loss: 0.4315
Epoch 4 Step 551 Train Loss: 0.4558
Epoch 4 Step 601 Train Loss: 0.4910
Epoch 4 Step 651 Train Loss: 0.4443
Epoch 4 Step 701 Train Loss: 0.4904
Epoch 4 Step 751 Train Loss: 0.4262
Epoch 4 Step 801 Train Loss: 0.4184
Epoch 4 Step 851 Train Loss: 0.4579
Epoch 4 Step 901 Train Loss: 0.4818
Epoch 4 Step 951 Train Loss: 0.4329
Epoch 4 Step 1001 Train Loss: 0.4896
Epoch 4 Step 1051 Train Loss: 0.4570
Epoch 4 Step 1101 Train Loss: 0.5233
Epoch 4 Step 1151 Train Loss: 0.4924
Epoch 4 Step 1201 Train Loss: 0.4488
Epoch 4 Step 1251 Train Loss: 0.4434
Epoch 4 Step 1301 Train Loss: 0.4315
Epoch 4 Step 1351 Train Loss: 0.4187
Epoch 4 Step 1401 Train Loss: 0.4550
Epoch 4 Step 1451 Train Loss: 0.4519
Epoch 4 Step 1501 Train Loss: 0.4792
Epoch 4 Step 1551 Train Loss: 0.4243
Epoch 4 Step 1601 Train Loss: 0.4819
Epoch 4 Step 1651 Train Loss: 0.4555
Epoch 4 Step 1701 Train Loss: 0.4646
Epoch 4 Step 1751 Train Loss: 0.4568
Epoch 4 Step 1801 Train Loss: 0.4135
Epoch 4: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.1045 Validation Top 20 DE MSE: 0.1533. 
Epoch 5 Step 1 Train Loss: 0.4837
Epoch 5 Step 51 Train Loss: 0.4248
Epoch 5 Step 101 Train Loss: 0.4889
Epoch 5 Step 151 Train Loss: 0.4787
Epoch 5 Step 201 Train Loss: 0.4556
Epoch 5 Step 251 Train Loss: 0.4701
Epoch 5 Step 301 Train Loss: 0.4491
Epoch 5 Step 351 Train Loss: 0.4944
Epoch 5 Step 401 Train Loss: 0.4941
Epoch 5 Step 451 Train Loss: 0.4950
Epoch 5 Step 501 Train Loss: 0.4647
Epoch 5 Step 551 Train Loss: 0.4219
Epoch 5 Step 601 Train Loss: 0.4841
Epoch 5 Step 651 Train Loss: 0.4489
Epoch 5 Step 701 Train Loss: 0.4794
Epoch 5 Step 751 Train Loss: 0.4851
Epoch 5 Step 801 Train Loss: 0.4936
Epoch 5 Step 851 Train Loss: 0.4279
Epoch 5 Step 901 Train Loss: 0.4703
Epoch 5 Step 951 Train Loss: 0.4670
Epoch 5 Step 1001 Train Loss: 0.4450
Epoch 5 Step 1051 Train Loss: 0.4639
Epoch 5 Step 1101 Train Loss: 0.4183
Epoch 5 Step 1151 Train Loss: 0.4553
Epoch 5 Step 1201 Train Loss: 0.4724
Epoch 5 Step 1251 Train Loss: 0.4691
Epoch 5 Step 1301 Train Loss: 0.5908
Epoch 5 Step 1351 Train Loss: 0.4180
Epoch 5 Step 1401 Train Loss: 0.4690
Epoch 5 Step 1451 Train Loss: 0.4503
Epoch 5 Step 1501 Train Loss: 0.4758
Epoch 5 Step 1551 Train Loss: 0.4701
Epoch 5 Step 1601 Train Loss: 0.4679
Epoch 5 Step 1651 Train Loss: 0.4220
Epoch 5 Step 1701 Train Loss: 0.4559
Epoch 5 Step 1751 Train Loss: 0.5424
Epoch 5 Step 1801 Train Loss: 0.5243
Epoch 5: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0901 Validation Top 20 DE MSE: 0.1509. 
Epoch 6 Step 1 Train Loss: 0.4403
Epoch 6 Step 51 Train Loss: 0.4147
Epoch 6 Step 101 Train Loss: 0.4617
Epoch 6 Step 151 Train Loss: 0.4114
Epoch 6 Step 201 Train Loss: 0.4854
Epoch 6 Step 251 Train Loss: 0.4881
Epoch 6 Step 301 Train Loss: 0.4492
Epoch 6 Step 351 Train Loss: 0.4726
Epoch 6 Step 401 Train Loss: 0.5287
Epoch 6 Step 451 Train Loss: 0.5096
Epoch 6 Step 501 Train Loss: 0.4578
Epoch 6 Step 551 Train Loss: 0.3889
Epoch 6 Step 601 Train Loss: 0.4785
Epoch 6 Step 651 Train Loss: 0.5668
Epoch 6 Step 701 Train Loss: 0.4674
Epoch 6 Step 751 Train Loss: 0.5134
Epoch 6 Step 801 Train Loss: 0.5175
Epoch 6 Step 851 Train Loss: 0.4642
Epoch 6 Step 901 Train Loss: 0.4289
Epoch 6 Step 951 Train Loss: 0.4722
Epoch 6 Step 1001 Train Loss: 0.4574
Epoch 6 Step 1051 Train Loss: 0.4858
Epoch 6 Step 1101 Train Loss: 0.4827
Epoch 6 Step 1151 Train Loss: 0.4682
Epoch 6 Step 1201 Train Loss: 0.4980
Epoch 6 Step 1251 Train Loss: 0.4414
Epoch 6 Step 1301 Train Loss: 0.4734
Epoch 6 Step 1351 Train Loss: 0.4821
Epoch 6 Step 1401 Train Loss: 0.4885
Epoch 6 Step 1451 Train Loss: 0.4800
Epoch 6 Step 1501 Train Loss: 0.4720
Epoch 6 Step 1551 Train Loss: 0.5510
Epoch 6 Step 1601 Train Loss: 0.4663
Epoch 6 Step 1651 Train Loss: 0.4925
Epoch 6 Step 1701 Train Loss: 0.4186
Epoch 6 Step 1751 Train Loss: 0.4750
Epoch 6 Step 1801 Train Loss: 0.4295
Epoch 6: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0782 Validation Top 20 DE MSE: 0.1492. 
Epoch 7 Step 1 Train Loss: 0.5200
Epoch 7 Step 51 Train Loss: 0.5487
Epoch 7 Step 101 Train Loss: 0.4663
Epoch 7 Step 151 Train Loss: 0.4277
Epoch 7 Step 201 Train Loss: 0.4497
Epoch 7 Step 251 Train Loss: 0.4448
Epoch 7 Step 301 Train Loss: 0.4516
Epoch 7 Step 351 Train Loss: 0.4803
Epoch 7 Step 401 Train Loss: 0.4586
Epoch 7 Step 451 Train Loss: 0.4971
Epoch 7 Step 501 Train Loss: 0.5206
Epoch 7 Step 551 Train Loss: 0.4967
Epoch 7 Step 601 Train Loss: 0.4623
Epoch 7 Step 651 Train Loss: 0.4876
Epoch 7 Step 701 Train Loss: 0.4290
Epoch 7 Step 751 Train Loss: 0.4818
Epoch 7 Step 801 Train Loss: 0.4710
Epoch 7 Step 851 Train Loss: 0.4763
Epoch 7 Step 901 Train Loss: 0.4997
Epoch 7 Step 951 Train Loss: 0.4697
Epoch 7 Step 1001 Train Loss: 0.4597
Epoch 7 Step 1051 Train Loss: 0.5125
Epoch 7 Step 1101 Train Loss: 0.4875
Epoch 7 Step 1151 Train Loss: 0.4651
Epoch 7 Step 1201 Train Loss: 0.5271
Epoch 7 Step 1251 Train Loss: 0.4597
Epoch 7 Step 1301 Train Loss: 0.4779
Epoch 7 Step 1351 Train Loss: 0.4350
Epoch 7 Step 1401 Train Loss: 0.4496
Epoch 7 Step 1451 Train Loss: 0.4992
Epoch 7 Step 1501 Train Loss: 0.5250
Epoch 7 Step 1551 Train Loss: 0.4850
Epoch 7 Step 1601 Train Loss: 0.4252
Epoch 7 Step 1651 Train Loss: 0.5293
Epoch 7 Step 1701 Train Loss: 0.5727
Epoch 7 Step 1751 Train Loss: 0.5314
Epoch 7 Step 1801 Train Loss: 0.5199
Epoch 7: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0737 Validation Top 20 DE MSE: 0.1374. 
Epoch 8 Step 1 Train Loss: 0.4503
Epoch 8 Step 51 Train Loss: 0.4434
Epoch 8 Step 101 Train Loss: 0.4593
Epoch 8 Step 151 Train Loss: 0.4818
Epoch 8 Step 201 Train Loss: 0.5292
Epoch 8 Step 251 Train Loss: 0.5363
Epoch 8 Step 301 Train Loss: 0.5177
Epoch 8 Step 351 Train Loss: 0.4894
Epoch 8 Step 401 Train Loss: 0.4952
Epoch 8 Step 451 Train Loss: 0.5047
Epoch 8 Step 501 Train Loss: 0.5243
Epoch 8 Step 551 Train Loss: 0.5049
Epoch 8 Step 601 Train Loss: 0.4351
Epoch 8 Step 651 Train Loss: 0.4815
Epoch 8 Step 701 Train Loss: 0.4273
Epoch 8 Step 751 Train Loss: 0.4801
Epoch 8 Step 801 Train Loss: 0.5778
Epoch 8 Step 851 Train Loss: 0.4942
Epoch 8 Step 901 Train Loss: 0.4904
Epoch 8 Step 951 Train Loss: 0.4901
Epoch 8 Step 1001 Train Loss: 0.4511
Epoch 8 Step 1051 Train Loss: 0.4355
Epoch 8 Step 1101 Train Loss: 0.5001
Epoch 8 Step 1151 Train Loss: 0.4580
Epoch 8 Step 1201 Train Loss: 0.4456
Epoch 8 Step 1251 Train Loss: 0.4315
Epoch 8 Step 1301 Train Loss: 0.4516
Epoch 8 Step 1351 Train Loss: 0.4427
Epoch 8 Step 1401 Train Loss: 0.4788
Epoch 8 Step 1451 Train Loss: 0.5009
Epoch 8 Step 1501 Train Loss: 0.5130
Epoch 8 Step 1551 Train Loss: 0.4837
Epoch 8 Step 1601 Train Loss: 0.5004
Epoch 8 Step 1651 Train Loss: 0.4619
Epoch 8 Step 1701 Train Loss: 0.4788
Epoch 8 Step 1751 Train Loss: 0.4605
Epoch 8 Step 1801 Train Loss: 0.5340
Epoch 8: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0745 Validation Top 20 DE MSE: 0.1420. 
Epoch 9 Step 1 Train Loss: 0.4814
Epoch 9 Step 51 Train Loss: 0.4557
Epoch 9 Step 101 Train Loss: 0.4634
Epoch 9 Step 151 Train Loss: 0.4778
Epoch 9 Step 201 Train Loss: 0.4997
Epoch 9 Step 251 Train Loss: 0.4600
Epoch 9 Step 301 Train Loss: 0.4834
Epoch 9 Step 351 Train Loss: 0.4874
Epoch 9 Step 401 Train Loss: 0.4426
Epoch 9 Step 451 Train Loss: 0.4641
Epoch 9 Step 501 Train Loss: 0.4690
Epoch 9 Step 551 Train Loss: 0.4734
Epoch 9 Step 601 Train Loss: 0.4944
Epoch 9 Step 651 Train Loss: 0.4269
Epoch 9 Step 701 Train Loss: 0.4861
Epoch 9 Step 751 Train Loss: 0.4142
Epoch 9 Step 801 Train Loss: 0.4763
Epoch 9 Step 851 Train Loss: 0.4817
Epoch 9 Step 901 Train Loss: 0.4495
Epoch 9 Step 951 Train Loss: 0.4568
Epoch 9 Step 1001 Train Loss: 0.4764
Epoch 9 Step 1051 Train Loss: 0.4832
Epoch 9 Step 1101 Train Loss: 0.4960
Epoch 9 Step 1151 Train Loss: 0.4935
Epoch 9 Step 1201 Train Loss: 0.5281
Epoch 9 Step 1251 Train Loss: 0.4873
Epoch 9 Step 1301 Train Loss: 0.4766
Epoch 9 Step 1351 Train Loss: 0.4416
Epoch 9 Step 1401 Train Loss: 0.4744
Epoch 9 Step 1451 Train Loss: 0.4777
Epoch 9 Step 1501 Train Loss: 0.5525
Epoch 9 Step 1551 Train Loss: 0.4883
Epoch 9 Step 1601 Train Loss: 0.4661
Epoch 9 Step 1651 Train Loss: 0.4324
Epoch 9 Step 1701 Train Loss: 0.4467
Epoch 9 Step 1751 Train Loss: 0.5048
Epoch 9 Step 1801 Train Loss: 0.4497
Epoch 9: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0699 Validation Top 20 DE MSE: 0.1354. 
Epoch 10 Step 1 Train Loss: 0.5027
Epoch 10 Step 51 Train Loss: 0.4806
Epoch 10 Step 101 Train Loss: 0.5009
Epoch 10 Step 151 Train Loss: 0.5047
Epoch 10 Step 201 Train Loss: 0.4362
Epoch 10 Step 251 Train Loss: 0.5023
Epoch 10 Step 301 Train Loss: 0.4890
Epoch 10 Step 351 Train Loss: 0.5372
Epoch 10 Step 401 Train Loss: 0.4636
Epoch 10 Step 451 Train Loss: 0.4802
Epoch 10 Step 501 Train Loss: 0.4771
Epoch 10 Step 551 Train Loss: 0.4635
Epoch 10 Step 601 Train Loss: 0.4064
Epoch 10 Step 651 Train Loss: 0.5094
Epoch 10 Step 701 Train Loss: 0.4463
Epoch 10 Step 751 Train Loss: 0.4486
Epoch 10 Step 801 Train Loss: 0.4757
Epoch 10 Step 851 Train Loss: 0.5129
Epoch 10 Step 901 Train Loss: 0.5547
Epoch 10 Step 951 Train Loss: 0.4636
Epoch 10 Step 1001 Train Loss: 0.4480
Epoch 10 Step 1051 Train Loss: 0.4505
Epoch 10 Step 1101 Train Loss: 0.4947
Epoch 10 Step 1151 Train Loss: 0.4650
Epoch 10 Step 1201 Train Loss: 0.4717
Epoch 10 Step 1251 Train Loss: 0.4618
Epoch 10 Step 1301 Train Loss: 0.4659
Epoch 10 Step 1351 Train Loss: 0.5038
Epoch 10 Step 1401 Train Loss: 0.4991
Epoch 10 Step 1451 Train Loss: 0.4615
Epoch 10 Step 1501 Train Loss: 0.4609
Epoch 10 Step 1551 Train Loss: 0.4917
Epoch 10 Step 1601 Train Loss: 0.4568
Epoch 10 Step 1651 Train Loss: 0.5040
Epoch 10 Step 1701 Train Loss: 0.5443
Epoch 10 Step 1751 Train Loss: 0.4559
Epoch 10 Step 1801 Train Loss: 0.4755
Epoch 10: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0749 Validation Top 20 DE MSE: 0.1439. 
Epoch 11 Step 1 Train Loss: 0.4508
Epoch 11 Step 51 Train Loss: 0.4284
Epoch 11 Step 101 Train Loss: 0.5080
Epoch 11 Step 151 Train Loss: 0.5457
Epoch 11 Step 201 Train Loss: 0.5035
Epoch 11 Step 251 Train Loss: 0.4753
Epoch 11 Step 301 Train Loss: 0.4487
Epoch 11 Step 351 Train Loss: 0.4547
Epoch 11 Step 401 Train Loss: 0.6467
Epoch 11 Step 451 Train Loss: 0.4939
Epoch 11 Step 501 Train Loss: 0.5265
Epoch 11 Step 551 Train Loss: 0.4806
Epoch 11 Step 601 Train Loss: 0.4711
Epoch 11 Step 651 Train Loss: 0.4641
Epoch 11 Step 701 Train Loss: 0.4480
Epoch 11 Step 751 Train Loss: 0.4454
Epoch 11 Step 801 Train Loss: 0.4505
Epoch 11 Step 851 Train Loss: 0.4465
Epoch 11 Step 901 Train Loss: 0.4602
Epoch 11 Step 951 Train Loss: 0.4496
Epoch 11 Step 1001 Train Loss: 0.4436
Epoch 11 Step 1051 Train Loss: 0.4908
Epoch 11 Step 1101 Train Loss: 0.5603
Epoch 11 Step 1151 Train Loss: 0.4796
Epoch 11 Step 1201 Train Loss: 0.4642
Epoch 11 Step 1251 Train Loss: 0.4693
Epoch 11 Step 1301 Train Loss: 0.4441
Epoch 11 Step 1351 Train Loss: 0.4552
Epoch 11 Step 1401 Train Loss: 0.4707
Epoch 11 Step 1451 Train Loss: 0.4394
Epoch 11 Step 1501 Train Loss: 0.4748
Epoch 11 Step 1551 Train Loss: 0.4478
Epoch 11 Step 1601 Train Loss: 0.4483
Epoch 11 Step 1651 Train Loss: 0.4646
Epoch 11 Step 1701 Train Loss: 0.4584
Epoch 11 Step 1751 Train Loss: 0.5285
Epoch 11 Step 1801 Train Loss: 0.5766
Epoch 11: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0733 Validation Top 20 DE MSE: 0.1391. 
Epoch 12 Step 1 Train Loss: 0.4969
Epoch 12 Step 51 Train Loss: 0.5974
Epoch 12 Step 101 Train Loss: 0.4551
Epoch 12 Step 151 Train Loss: 0.4725
Epoch 12 Step 201 Train Loss: 0.4834
Epoch 12 Step 251 Train Loss: 0.4526
Epoch 12 Step 301 Train Loss: 0.5419
Epoch 12 Step 351 Train Loss: 0.4607
Epoch 12 Step 401 Train Loss: 0.4634
Epoch 12 Step 451 Train Loss: 0.4684
Epoch 12 Step 501 Train Loss: 0.4644
Epoch 12 Step 551 Train Loss: 0.4566
Epoch 12 Step 601 Train Loss: 0.4667
Epoch 12 Step 651 Train Loss: 0.4528
Epoch 12 Step 701 Train Loss: 0.4822
Epoch 12 Step 751 Train Loss: 0.5016
Epoch 12 Step 801 Train Loss: 0.5281
Epoch 12 Step 851 Train Loss: 0.5212
Epoch 12 Step 901 Train Loss: 0.4601
Epoch 12 Step 951 Train Loss: 0.5216
Epoch 12 Step 1001 Train Loss: 0.5079
Epoch 12 Step 1051 Train Loss: 0.5014
Epoch 12 Step 1101 Train Loss: 0.4574
Epoch 12 Step 1151 Train Loss: 0.5213
Epoch 12 Step 1201 Train Loss: 0.4954
Epoch 12 Step 1251 Train Loss: 0.5189
Epoch 12 Step 1301 Train Loss: 0.4837
Epoch 12 Step 1351 Train Loss: 0.4533
Epoch 12 Step 1401 Train Loss: 0.4699
Epoch 12 Step 1451 Train Loss: 0.4501
Epoch 12 Step 1501 Train Loss: 0.4755
Epoch 12 Step 1551 Train Loss: 0.4741
Epoch 12 Step 1601 Train Loss: 0.4723
Epoch 12 Step 1651 Train Loss: 0.4787
Epoch 12 Step 1701 Train Loss: 0.4720
Epoch 12 Step 1751 Train Loss: 0.4416
Epoch 12 Step 1801 Train Loss: 0.5057
Epoch 12: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0695 Validation Top 20 DE MSE: 0.1327. 
Epoch 13 Step 1 Train Loss: 0.4776
Epoch 13 Step 51 Train Loss: 0.4829
Epoch 13 Step 101 Train Loss: 0.4883
Epoch 13 Step 151 Train Loss: 0.4796
Epoch 13 Step 201 Train Loss: 0.4784
Epoch 13 Step 251 Train Loss: 0.5130
Epoch 13 Step 301 Train Loss: 0.4673
Epoch 13 Step 351 Train Loss: 0.4805
Epoch 13 Step 401 Train Loss: 0.4440
Epoch 13 Step 451 Train Loss: 0.4885
Epoch 13 Step 501 Train Loss: 0.5071
Epoch 13 Step 551 Train Loss: 0.5068
Epoch 13 Step 601 Train Loss: 0.4663
Epoch 13 Step 651 Train Loss: 0.5676
Epoch 13 Step 701 Train Loss: 0.4719
Epoch 13 Step 751 Train Loss: 0.5407
Epoch 13 Step 801 Train Loss: 0.4874
Epoch 13 Step 851 Train Loss: 0.4916
Epoch 13 Step 901 Train Loss: 0.4842
Epoch 13 Step 951 Train Loss: 0.4877
Epoch 13 Step 1001 Train Loss: 0.4800
Epoch 13 Step 1051 Train Loss: 0.4627
Epoch 13 Step 1101 Train Loss: 0.5099
Epoch 13 Step 1151 Train Loss: 0.4550
Epoch 13 Step 1201 Train Loss: 0.5212
Epoch 13 Step 1251 Train Loss: 0.5063
Epoch 13 Step 1301 Train Loss: 0.4490
Epoch 13 Step 1351 Train Loss: 0.4621
Epoch 13 Step 1401 Train Loss: 0.4480
Epoch 13 Step 1451 Train Loss: 0.4660
Epoch 13 Step 1501 Train Loss: 0.5220
Epoch 13 Step 1551 Train Loss: 0.4880
Epoch 13 Step 1601 Train Loss: 0.4395
Epoch 13 Step 1651 Train Loss: 0.4311
Epoch 13 Step 1701 Train Loss: 0.5325
Epoch 13 Step 1751 Train Loss: 0.5163
Epoch 13 Step 1801 Train Loss: 0.4928
Epoch 13: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0751 Validation Top 20 DE MSE: 0.1432. 
Epoch 14 Step 1 Train Loss: 0.4966
Epoch 14 Step 51 Train Loss: 0.4384
Epoch 14 Step 101 Train Loss: 0.4547
Epoch 14 Step 151 Train Loss: 0.4805
Epoch 14 Step 201 Train Loss: 0.5138
Epoch 14 Step 251 Train Loss: 0.4893
Epoch 14 Step 301 Train Loss: 0.4464
Epoch 14 Step 351 Train Loss: 0.4795
Epoch 14 Step 401 Train Loss: 0.4850
Epoch 14 Step 451 Train Loss: 0.6261
Epoch 14 Step 501 Train Loss: 0.5054
Epoch 14 Step 551 Train Loss: 0.5206
Epoch 14 Step 601 Train Loss: 0.5143
Epoch 14 Step 651 Train Loss: 0.5022
Epoch 14 Step 701 Train Loss: 0.5261
Epoch 14 Step 751 Train Loss: 0.4482
Epoch 14 Step 801 Train Loss: 0.4791
Epoch 14 Step 851 Train Loss: 0.4941
Epoch 14 Step 901 Train Loss: 0.4550
Epoch 14 Step 951 Train Loss: 0.4755
Epoch 14 Step 1001 Train Loss: 0.4780
Epoch 14 Step 1051 Train Loss: 0.4655
Epoch 14 Step 1101 Train Loss: 0.4759
Epoch 14 Step 1151 Train Loss: 0.4940
Epoch 14 Step 1201 Train Loss: 0.4764
Epoch 14 Step 1251 Train Loss: 0.4580
Epoch 14 Step 1301 Train Loss: 0.5066
Epoch 14 Step 1351 Train Loss: 0.4404
Epoch 14 Step 1401 Train Loss: 0.4732
Epoch 14 Step 1451 Train Loss: 0.4830
Epoch 14 Step 1501 Train Loss: 0.4588
Epoch 14 Step 1551 Train Loss: 0.4619
Epoch 14 Step 1601 Train Loss: 0.5092
Epoch 14 Step 1651 Train Loss: 0.4584
Epoch 14 Step 1701 Train Loss: 0.4259
Epoch 14 Step 1751 Train Loss: 0.5232
Epoch 14 Step 1801 Train Loss: 0.5007
Epoch 14: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0803 Validation Top 20 DE MSE: 0.1456. 
Epoch 15 Step 1 Train Loss: 0.4789
Epoch 15 Step 51 Train Loss: 0.5133
Epoch 15 Step 101 Train Loss: 0.4732
Epoch 15 Step 151 Train Loss: 0.4720
Epoch 15 Step 201 Train Loss: 0.4841
Epoch 15 Step 251 Train Loss: 0.4506
Epoch 15 Step 301 Train Loss: 0.4652
Epoch 15 Step 351 Train Loss: 0.4564
Epoch 15 Step 401 Train Loss: 0.4759
Epoch 15 Step 451 Train Loss: 0.4746
Epoch 15 Step 501 Train Loss: 0.5359
Epoch 15 Step 551 Train Loss: 0.4490
Epoch 15 Step 601 Train Loss: 0.4768
Epoch 15 Step 651 Train Loss: 0.5197
Epoch 15 Step 701 Train Loss: 0.4380
Epoch 15 Step 751 Train Loss: 0.4937
Epoch 15 Step 801 Train Loss: 0.4743
Epoch 15 Step 851 Train Loss: 0.4463
Epoch 15 Step 901 Train Loss: 0.4771
Epoch 15 Step 951 Train Loss: 0.4261
Epoch 15 Step 1001 Train Loss: 0.5376
Epoch 15 Step 1051 Train Loss: 0.4375
Epoch 15 Step 1101 Train Loss: 0.4513
Epoch 15 Step 1151 Train Loss: 0.4834
Epoch 15 Step 1201 Train Loss: 0.4246
Epoch 15 Step 1251 Train Loss: 0.4292
Epoch 15 Step 1301 Train Loss: 0.4397
Epoch 15 Step 1351 Train Loss: 0.4350
Epoch 15 Step 1401 Train Loss: 0.4529
Epoch 15 Step 1451 Train Loss: 0.4813
Epoch 15 Step 1501 Train Loss: 0.4407
Epoch 15 Step 1551 Train Loss: 0.4347
Epoch 15 Step 1601 Train Loss: 0.4488
Epoch 15 Step 1651 Train Loss: 0.4754
Epoch 15 Step 1701 Train Loss: 0.4622
Epoch 15 Step 1751 Train Loss: 0.4282
Epoch 15 Step 1801 Train Loss: 0.4841
Epoch 15: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0747 Validation Top 20 DE MSE: 0.1407. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1455
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0043127076
test_combo_seen0_pearson: 0.9901712467016581
test_combo_seen0_mse_de: 0.16024537
test_combo_seen0_pearson_de: 0.843209927603802
test_combo_seen1_mse: 0.00406372
test_combo_seen1_pearson: 0.9907779557063342
test_combo_seen1_mse_de: 0.13078749
test_combo_seen1_pearson_de: 0.7453587484025681
test_combo_seen2_mse: 0.0042730374
test_combo_seen2_pearson: 0.9900985031155927
test_combo_seen2_mse_de: 0.16715983
test_combo_seen2_pearson_de: 0.895286232607373
test_unseen_single_mse: 0.0022709018
test_unseen_single_pearson: 0.9947499482637705
test_unseen_single_mse_de: 0.15890954
test_unseen_single_pearson_de: 0.9483539688982633
test_combo_seen0_pearson_delta: 0.5310982626740792
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.2
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.86
test_combo_seen0_mse_top20_de_non_dropout: 0.19317207
test_combo_seen1_pearson_delta: 0.5765235740888114
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.14827586206896554
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8836206896551724
test_combo_seen1_mse_top20_de_non_dropout: 0.18449709
test_combo_seen2_pearson_delta: 0.6498067393474334
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.04375
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.8843749999999999
test_combo_seen2_mse_top20_de_non_dropout: 0.19179338
test_unseen_single_pearson_delta: 0.4651980195435542
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.19629629629629625
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9037037037037037
test_unseen_single_mse_top20_de_non_dropout: 0.16996796
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.001 MB of 0.038 MB uploadedwandb: / 0.034 MB of 0.038 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÖ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ
wandb:                                                   val_de_mse ‚ñá‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÜ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.2
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.86
wandb:                                         test_combo_seen0_mse 0.00431
wandb:                                      test_combo_seen0_mse_de 0.16025
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.19317
wandb:                                     test_combo_seen0_pearson 0.99017
wandb:                                  test_combo_seen0_pearson_de 0.84321
wandb:                               test_combo_seen0_pearson_delta 0.5311
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.14828
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.88362
wandb:                                         test_combo_seen1_mse 0.00406
wandb:                                      test_combo_seen1_mse_de 0.13079
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.1845
wandb:                                     test_combo_seen1_pearson 0.99078
wandb:                                  test_combo_seen1_pearson_de 0.74536
wandb:                               test_combo_seen1_pearson_delta 0.57652
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.04375
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.88437
wandb:                                         test_combo_seen2_mse 0.00427
wandb:                                      test_combo_seen2_mse_de 0.16716
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.19179
wandb:                                     test_combo_seen2_pearson 0.9901
wandb:                                  test_combo_seen2_pearson_de 0.89529
wandb:                               test_combo_seen2_pearson_delta 0.64981
wandb:                                                  test_de_mse 0.14552
wandb:                                              test_de_pearson 0.82516
wandb:               test_frac_opposite_direction_top20_non_dropout 0.14955
wandb:                          test_frac_sigma_below_1_non_dropout 0.88649
wandb:                                                     test_mse 0.00368
wandb:                                test_mse_top20_de_non_dropout 0.1828
wandb:                                                 test_pearson 0.99159
wandb:                                           test_pearson_delta 0.55592
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.1963
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9037
wandb:                                       test_unseen_single_mse 0.00227
wandb:                                    test_unseen_single_mse_de 0.15891
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.16997
wandb:                                   test_unseen_single_pearson 0.99475
wandb:                                test_unseen_single_pearson_de 0.94835
wandb:                             test_unseen_single_pearson_delta 0.4652
wandb:                                                 train_de_mse 0.0747
wandb:                                             train_de_pearson 0.90776
wandb:                                                    train_mse 0.00186
wandb:                                                train_pearson 0.99585
wandb:                                                training_loss 0.51961
wandb:                                                   val_de_mse 0.14066
wandb:                                               val_de_pearson 0.87516
wandb:                                                      val_mse 0.00269
wandb:                                                  val_pearson 0.99383
wandb: 
wandb: üöÄ View run scbert_NormanWeissman2019_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/0kvfdrii
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241029_123713-0kvfdrii/logs
