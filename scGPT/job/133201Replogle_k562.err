cuda-11.8.0 loaded successful
gcc-9.3.0 loaded successful
cmake-3.27.0 loaded successful
openmpi-4.1.2 loaded successful
Openblas-0.3.25 loaded successful
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x
  self.hub = sentry_sdk.Hub(client)
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240728_232602-ju6fdyfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Replogle_k562_essential_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/ju6fdyfj
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:268
Done!
Creating dataloaders....
Done!
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/Replogle_k562_essential.py", line 491, in <module>
    test_res = eval_perturb(test_loader, best_model, device, pert_data)
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/Replogle_k562_essential.py", line 277, in eval_perturb
    p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 322, in pred_perturb
    output_dict = self(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 206, in forward
    transformer_output = self._encode(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 141, in _encode
    output = self.transformer_encoder(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 315, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 591, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 599, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.26 GiB (GPU 0; 39.41 GiB total capacity; 18.94 GiB already allocated; 2.06 GiB free; 36.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/Replogle_k562_essential.py", line 491, in <module>
    test_res = eval_perturb(test_loader, best_model, device, pert_data)
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/Replogle_k562_essential.py", line 277, in eval_perturb
    p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 322, in pred_perturb
    output_dict = self(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 206, in forward
    transformer_output = self._encode(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 141, in _encode
    output = self.transformer_encoder(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 315, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 591, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 599, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.26 GiB (GPU 0; 39.41 GiB total capacity; 18.94 GiB already allocated; 2.06 GiB free; 36.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.057 MB uploadedwandb: | 0.001 MB of 0.057 MB uploadedwandb: / 0.057 MB of 0.057 MB uploadedwandb: - 0.057 MB of 0.057 MB uploadedwandb: 
wandb: Run history:
wandb: elapsed_time ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb:     val_loss ‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñà‚ñÖ
wandb:      val_mre ‚ñà‚ñà‚ñÜ‚ñÖ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 1270.10378
wandb:               epoch 6
wandb: loading_params_info Loading params trans...
wandb:          match_info match 5464/5656 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-28 23:26:18
wandb:            val_loss 0.19164
wandb:             val_mre 145391.914
wandb: 
wandb: üöÄ View run Replogle_k562_essential_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/ju6fdyfj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240728_232602-ju6fdyfj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
