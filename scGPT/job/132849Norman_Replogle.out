环境设置完成！
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/NormanWeissman2019/scgpt/split1
scGPT - INFO - Running on 2024-07-27 00:23:35
scGPT - INFO - match 4545/5037 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1888 batches | lr 0.0001 | ms/batch 449.61 | loss  0.23 | mse  0.23 |
scGPT - INFO - | epoch   1 | 200/1888 batches | lr 0.0001 | ms/batch 394.34 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 300/1888 batches | lr 0.0001 | ms/batch 393.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 400/1888 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 500/1888 batches | lr 0.0001 | ms/batch 392.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 600/1888 batches | lr 0.0001 | ms/batch 392.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 700/1888 batches | lr 0.0001 | ms/batch 393.19 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 800/1888 batches | lr 0.0001 | ms/batch 393.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 900/1888 batches | lr 0.0001 | ms/batch 394.54 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1000/1888 batches | lr 0.0001 | ms/batch 394.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1100/1888 batches | lr 0.0001 | ms/batch 392.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1200/1888 batches | lr 0.0001 | ms/batch 392.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1300/1888 batches | lr 0.0001 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1400/1888 batches | lr 0.0001 | ms/batch 394.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1500/1888 batches | lr 0.0001 | ms/batch 395.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1600/1888 batches | lr 0.0001 | ms/batch 395.19 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1700/1888 batches | lr 0.0001 | ms/batch 394.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1800/1888 batches | lr 0.0001 | ms/batch 393.13 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 795.99s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0589
scGPT - INFO - | epoch   2 | 100/1888 batches | lr 0.0001 | ms/batch 396.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 200/1888 batches | lr 0.0001 | ms/batch 393.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 300/1888 batches | lr 0.0001 | ms/batch 393.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 400/1888 batches | lr 0.0001 | ms/batch 393.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 500/1888 batches | lr 0.0001 | ms/batch 392.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 600/1888 batches | lr 0.0001 | ms/batch 392.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 700/1888 batches | lr 0.0001 | ms/batch 392.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 800/1888 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 900/1888 batches | lr 0.0001 | ms/batch 392.66 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1000/1888 batches | lr 0.0001 | ms/batch 392.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1100/1888 batches | lr 0.0001 | ms/batch 392.52 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1200/1888 batches | lr 0.0001 | ms/batch 392.82 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1300/1888 batches | lr 0.0001 | ms/batch 392.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1400/1888 batches | lr 0.0001 | ms/batch 392.67 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1500/1888 batches | lr 0.0001 | ms/batch 393.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1600/1888 batches | lr 0.0001 | ms/batch 392.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1700/1888 batches | lr 0.0001 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1800/1888 batches | lr 0.0001 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 788.97s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0589
scGPT - INFO - | epoch   3 | 100/1888 batches | lr 0.0001 | ms/batch 397.13 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 200/1888 batches | lr 0.0001 | ms/batch 392.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 300/1888 batches | lr 0.0001 | ms/batch 393.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 400/1888 batches | lr 0.0001 | ms/batch 392.95 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 500/1888 batches | lr 0.0001 | ms/batch 393.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 600/1888 batches | lr 0.0001 | ms/batch 393.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 700/1888 batches | lr 0.0001 | ms/batch 394.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 800/1888 batches | lr 0.0001 | ms/batch 392.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 900/1888 batches | lr 0.0001 | ms/batch 392.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1000/1888 batches | lr 0.0001 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1100/1888 batches | lr 0.0001 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1200/1888 batches | lr 0.0001 | ms/batch 393.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1300/1888 batches | lr 0.0001 | ms/batch 393.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1400/1888 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1500/1888 batches | lr 0.0001 | ms/batch 392.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1600/1888 batches | lr 0.0001 | ms/batch 392.65 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1700/1888 batches | lr 0.0001 | ms/batch 392.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1800/1888 batches | lr 0.0001 | ms/batch 392.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 789.61s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1888 batches | lr 0.0001 | ms/batch 398.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 200/1888 batches | lr 0.0001 | ms/batch 392.82 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 300/1888 batches | lr 0.0001 | ms/batch 393.58 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 400/1888 batches | lr 0.0001 | ms/batch 393.69 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 500/1888 batches | lr 0.0001 | ms/batch 393.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 600/1888 batches | lr 0.0001 | ms/batch 392.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 700/1888 batches | lr 0.0001 | ms/batch 393.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 800/1888 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 900/1888 batches | lr 0.0001 | ms/batch 392.60 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1000/1888 batches | lr 0.0001 | ms/batch 392.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1100/1888 batches | lr 0.0001 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1200/1888 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1300/1888 batches | lr 0.0001 | ms/batch 394.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1400/1888 batches | lr 0.0001 | ms/batch 392.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1500/1888 batches | lr 0.0001 | ms/batch 393.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1600/1888 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1700/1888 batches | lr 0.0001 | ms/batch 394.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1800/1888 batches | lr 0.0001 | ms/batch 395.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 789.80s | valid loss/mse 0.0583 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0583
scGPT - INFO - | epoch   5 | 100/1888 batches | lr 0.0001 | ms/batch 397.22 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 200/1888 batches | lr 0.0001 | ms/batch 393.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 300/1888 batches | lr 0.0001 | ms/batch 392.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 400/1888 batches | lr 0.0001 | ms/batch 393.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 500/1888 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 600/1888 batches | lr 0.0001 | ms/batch 393.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 700/1888 batches | lr 0.0001 | ms/batch 393.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 800/1888 batches | lr 0.0001 | ms/batch 392.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 900/1888 batches | lr 0.0001 | ms/batch 392.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1000/1888 batches | lr 0.0001 | ms/batch 393.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1100/1888 batches | lr 0.0001 | ms/batch 394.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1200/1888 batches | lr 0.0001 | ms/batch 392.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1300/1888 batches | lr 0.0001 | ms/batch 393.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1400/1888 batches | lr 0.0001 | ms/batch 392.69 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1500/1888 batches | lr 0.0001 | ms/batch 392.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1600/1888 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1700/1888 batches | lr 0.0001 | ms/batch 392.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1800/1888 batches | lr 0.0001 | ms/batch 392.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 789.55s | valid loss/mse 0.0591 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1888 batches | lr 0.0001 | ms/batch 396.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 200/1888 batches | lr 0.0001 | ms/batch 392.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 300/1888 batches | lr 0.0001 | ms/batch 392.63 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 400/1888 batches | lr 0.0001 | ms/batch 392.64 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 500/1888 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 600/1888 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 700/1888 batches | lr 0.0001 | ms/batch 392.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 800/1888 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 900/1888 batches | lr 0.0001 | ms/batch 392.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1000/1888 batches | lr 0.0001 | ms/batch 392.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1100/1888 batches | lr 0.0001 | ms/batch 392.66 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1200/1888 batches | lr 0.0001 | ms/batch 393.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1300/1888 batches | lr 0.0001 | ms/batch 392.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1400/1888 batches | lr 0.0001 | ms/batch 392.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1500/1888 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1600/1888 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1700/1888 batches | lr 0.0001 | ms/batch 394.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1800/1888 batches | lr 0.0001 | ms/batch 392.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 789.02s | valid loss/mse 0.0586 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1888 batches | lr 0.0001 | ms/batch 397.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 200/1888 batches | lr 0.0001 | ms/batch 395.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 300/1888 batches | lr 0.0001 | ms/batch 394.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 400/1888 batches | lr 0.0001 | ms/batch 397.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 500/1888 batches | lr 0.0001 | ms/batch 396.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 600/1888 batches | lr 0.0001 | ms/batch 396.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 700/1888 batches | lr 0.0001 | ms/batch 393.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 800/1888 batches | lr 0.0001 | ms/batch 393.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 900/1888 batches | lr 0.0001 | ms/batch 393.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1000/1888 batches | lr 0.0001 | ms/batch 393.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1100/1888 batches | lr 0.0001 | ms/batch 394.13 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1200/1888 batches | lr 0.0001 | ms/batch 393.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1300/1888 batches | lr 0.0001 | ms/batch 393.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1400/1888 batches | lr 0.0001 | ms/batch 392.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1500/1888 batches | lr 0.0001 | ms/batch 392.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1600/1888 batches | lr 0.0001 | ms/batch 392.52 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1700/1888 batches | lr 0.0001 | ms/batch 392.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1800/1888 batches | lr 0.0001 | ms/batch 392.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 791.01s | valid loss/mse 0.0585 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1888 batches | lr 0.0000 | ms/batch 397.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 200/1888 batches | lr 0.0000 | ms/batch 392.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 300/1888 batches | lr 0.0000 | ms/batch 393.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 400/1888 batches | lr 0.0000 | ms/batch 393.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 500/1888 batches | lr 0.0000 | ms/batch 393.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 600/1888 batches | lr 0.0000 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 700/1888 batches | lr 0.0000 | ms/batch 392.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 800/1888 batches | lr 0.0000 | ms/batch 392.64 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 900/1888 batches | lr 0.0000 | ms/batch 392.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1000/1888 batches | lr 0.0000 | ms/batch 393.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1100/1888 batches | lr 0.0000 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1200/1888 batches | lr 0.0000 | ms/batch 394.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1300/1888 batches | lr 0.0000 | ms/batch 394.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1400/1888 batches | lr 0.0000 | ms/batch 393.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1500/1888 batches | lr 0.0000 | ms/batch 393.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1600/1888 batches | lr 0.0000 | ms/batch 394.52 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1700/1888 batches | lr 0.0000 | ms/batch 393.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1800/1888 batches | lr 0.0000 | ms/batch 393.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 790.40s | valid loss/mse 0.0586 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1888 batches | lr 0.0000 | ms/batch 398.19 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 200/1888 batches | lr 0.0000 | ms/batch 393.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 300/1888 batches | lr 0.0000 | ms/batch 393.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 400/1888 batches | lr 0.0000 | ms/batch 393.60 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 500/1888 batches | lr 0.0000 | ms/batch 393.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 600/1888 batches | lr 0.0000 | ms/batch 393.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 700/1888 batches | lr 0.0000 | ms/batch 393.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 800/1888 batches | lr 0.0000 | ms/batch 393.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 900/1888 batches | lr 0.0000 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1000/1888 batches | lr 0.0000 | ms/batch 392.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1100/1888 batches | lr 0.0000 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1200/1888 batches | lr 0.0000 | ms/batch 392.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1300/1888 batches | lr 0.0000 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1400/1888 batches | lr 0.0000 | ms/batch 393.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1500/1888 batches | lr 0.0000 | ms/batch 393.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1600/1888 batches | lr 0.0000 | ms/batch 392.94 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1700/1888 batches | lr 0.0000 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1800/1888 batches | lr 0.0000 | ms/batch 392.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 789.78s | valid loss/mse 0.0585 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 9
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/NormanWeissman2019/scgpt/split2
scGPT - INFO - Running on 2024-07-27 03:49:05
scGPT - INFO - match 4545/5037 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1883 batches | lr 0.0001 | ms/batch 397.39 | loss  0.14 | mse  0.14 |
scGPT - INFO - | epoch   1 | 200/1883 batches | lr 0.0001 | ms/batch 392.22 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 300/1883 batches | lr 0.0001 | ms/batch 392.67 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 400/1883 batches | lr 0.0001 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 500/1883 batches | lr 0.0001 | ms/batch 392.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 600/1883 batches | lr 0.0001 | ms/batch 392.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 700/1883 batches | lr 0.0001 | ms/batch 392.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 800/1883 batches | lr 0.0001 | ms/batch 392.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 900/1883 batches | lr 0.0001 | ms/batch 392.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1000/1883 batches | lr 0.0001 | ms/batch 393.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1100/1883 batches | lr 0.0001 | ms/batch 392.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1200/1883 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1300/1883 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1400/1883 batches | lr 0.0001 | ms/batch 392.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1500/1883 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1600/1883 batches | lr 0.0001 | ms/batch 392.60 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1700/1883 batches | lr 0.0001 | ms/batch 392.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1800/1883 batches | lr 0.0001 | ms/batch 392.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 796.34s | valid loss/mse 0.0590 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0590
scGPT - INFO - | epoch   2 | 100/1883 batches | lr 0.0001 | ms/batch 396.65 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 200/1883 batches | lr 0.0001 | ms/batch 392.63 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 300/1883 batches | lr 0.0001 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 400/1883 batches | lr 0.0001 | ms/batch 392.68 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 500/1883 batches | lr 0.0001 | ms/batch 392.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 600/1883 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 700/1883 batches | lr 0.0001 | ms/batch 392.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 800/1883 batches | lr 0.0001 | ms/batch 392.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 900/1883 batches | lr 0.0001 | ms/batch 392.68 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1000/1883 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1100/1883 batches | lr 0.0001 | ms/batch 392.68 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1200/1883 batches | lr 0.0001 | ms/batch 392.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1300/1883 batches | lr 0.0001 | ms/batch 392.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1400/1883 batches | lr 0.0001 | ms/batch 392.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1500/1883 batches | lr 0.0001 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1600/1883 batches | lr 0.0001 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1700/1883 batches | lr 0.0001 | ms/batch 392.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1800/1883 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 795.82s | valid loss/mse 0.0590 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1883 batches | lr 0.0001 | ms/batch 396.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 200/1883 batches | lr 0.0001 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 300/1883 batches | lr 0.0001 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 400/1883 batches | lr 0.0001 | ms/batch 392.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 500/1883 batches | lr 0.0001 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 600/1883 batches | lr 0.0001 | ms/batch 392.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 700/1883 batches | lr 0.0001 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 800/1883 batches | lr 0.0001 | ms/batch 392.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 900/1883 batches | lr 0.0001 | ms/batch 392.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1000/1883 batches | lr 0.0001 | ms/batch 392.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1100/1883 batches | lr 0.0001 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1200/1883 batches | lr 0.0001 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1300/1883 batches | lr 0.0001 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1400/1883 batches | lr 0.0001 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1500/1883 batches | lr 0.0001 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1600/1883 batches | lr 0.0001 | ms/batch 392.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1700/1883 batches | lr 0.0001 | ms/batch 392.58 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1800/1883 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 795.32s | valid loss/mse 0.0593 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1883 batches | lr 0.0001 | ms/batch 396.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 200/1883 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 300/1883 batches | lr 0.0001 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 400/1883 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 500/1883 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 600/1883 batches | lr 0.0001 | ms/batch 392.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 700/1883 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 800/1883 batches | lr 0.0001 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 900/1883 batches | lr 0.0001 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1000/1883 batches | lr 0.0001 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1100/1883 batches | lr 0.0001 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1200/1883 batches | lr 0.0001 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1300/1883 batches | lr 0.0001 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1400/1883 batches | lr 0.0001 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1500/1883 batches | lr 0.0001 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1600/1883 batches | lr 0.0001 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1700/1883 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1800/1883 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 795.18s | valid loss/mse 0.0585 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0585
scGPT - INFO - | epoch   5 | 100/1883 batches | lr 0.0001 | ms/batch 396.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 200/1883 batches | lr 0.0001 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 300/1883 batches | lr 0.0001 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 400/1883 batches | lr 0.0001 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 500/1883 batches | lr 0.0001 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 600/1883 batches | lr 0.0001 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 700/1883 batches | lr 0.0001 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 800/1883 batches | lr 0.0001 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 900/1883 batches | lr 0.0001 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1000/1883 batches | lr 0.0001 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1100/1883 batches | lr 0.0001 | ms/batch 392.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1200/1883 batches | lr 0.0001 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1300/1883 batches | lr 0.0001 | ms/batch 392.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1400/1883 batches | lr 0.0001 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1500/1883 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1600/1883 batches | lr 0.0001 | ms/batch 392.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1700/1883 batches | lr 0.0001 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1800/1883 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 795.06s | valid loss/mse 0.0585 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0585
scGPT - INFO - | epoch   6 | 100/1883 batches | lr 0.0001 | ms/batch 396.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 200/1883 batches | lr 0.0001 | ms/batch 392.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 300/1883 batches | lr 0.0001 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 400/1883 batches | lr 0.0001 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 500/1883 batches | lr 0.0001 | ms/batch 392.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 600/1883 batches | lr 0.0001 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 700/1883 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 800/1883 batches | lr 0.0001 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 900/1883 batches | lr 0.0001 | ms/batch 392.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1000/1883 batches | lr 0.0001 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1100/1883 batches | lr 0.0001 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1200/1883 batches | lr 0.0001 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1300/1883 batches | lr 0.0001 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1400/1883 batches | lr 0.0001 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1500/1883 batches | lr 0.0001 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1600/1883 batches | lr 0.0001 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1700/1883 batches | lr 0.0001 | ms/batch 392.60 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1800/1883 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 795.22s | valid loss/mse 0.0584 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0584
scGPT - INFO - | epoch   7 | 100/1883 batches | lr 0.0001 | ms/batch 396.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 200/1883 batches | lr 0.0001 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 300/1883 batches | lr 0.0001 | ms/batch 392.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 400/1883 batches | lr 0.0001 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 500/1883 batches | lr 0.0001 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 600/1883 batches | lr 0.0001 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 700/1883 batches | lr 0.0001 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 800/1883 batches | lr 0.0001 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 900/1883 batches | lr 0.0001 | ms/batch 392.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1000/1883 batches | lr 0.0001 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1100/1883 batches | lr 0.0001 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1200/1883 batches | lr 0.0001 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1300/1883 batches | lr 0.0001 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1400/1883 batches | lr 0.0001 | ms/batch 392.52 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1500/1883 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1600/1883 batches | lr 0.0001 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1700/1883 batches | lr 0.0001 | ms/batch 392.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1800/1883 batches | lr 0.0001 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 795.27s | valid loss/mse 0.0583 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0583
scGPT - INFO - | epoch   8 | 100/1883 batches | lr 0.0000 | ms/batch 396.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 200/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 300/1883 batches | lr 0.0000 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 400/1883 batches | lr 0.0000 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 500/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 600/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 700/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 800/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 900/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1000/1883 batches | lr 0.0000 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1100/1883 batches | lr 0.0000 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1200/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1300/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1400/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1500/1883 batches | lr 0.0000 | ms/batch 392.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1600/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1700/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1800/1883 batches | lr 0.0000 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 795.19s | valid loss/mse 0.0586 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1883 batches | lr 0.0000 | ms/batch 396.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 200/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 300/1883 batches | lr 0.0000 | ms/batch 392.19 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 400/1883 batches | lr 0.0000 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 500/1883 batches | lr 0.0000 | ms/batch 392.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 600/1883 batches | lr 0.0000 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 700/1883 batches | lr 0.0000 | ms/batch 392.55 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 800/1883 batches | lr 0.0000 | ms/batch 392.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 900/1883 batches | lr 0.0000 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1000/1883 batches | lr 0.0000 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1100/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1200/1883 batches | lr 0.0000 | ms/batch 392.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1300/1883 batches | lr 0.0000 | ms/batch 392.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1400/1883 batches | lr 0.0000 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1500/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1600/1883 batches | lr 0.0000 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1700/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1800/1883 batches | lr 0.0000 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 795.23s | valid loss/mse 0.0587 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/1883 batches | lr 0.0000 | ms/batch 396.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 200/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 300/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 400/1883 batches | lr 0.0000 | ms/batch 392.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 500/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 600/1883 batches | lr 0.0000 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 700/1883 batches | lr 0.0000 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 800/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 900/1883 batches | lr 0.0000 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1000/1883 batches | lr 0.0000 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1100/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1200/1883 batches | lr 0.0000 | ms/batch 392.52 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1300/1883 batches | lr 0.0000 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1400/1883 batches | lr 0.0000 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1500/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1600/1883 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1700/1883 batches | lr 0.0000 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1800/1883 batches | lr 0.0000 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 795.17s | valid loss/mse 0.0582 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0582
scGPT - INFO - | epoch  11 | 100/1883 batches | lr 0.0000 | ms/batch 396.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 200/1883 batches | lr 0.0000 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 300/1883 batches | lr 0.0000 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 400/1883 batches | lr 0.0000 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 500/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 600/1883 batches | lr 0.0000 | ms/batch 392.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 700/1883 batches | lr 0.0000 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 800/1883 batches | lr 0.0000 | ms/batch 393.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 900/1883 batches | lr 0.0000 | ms/batch 392.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1000/1883 batches | lr 0.0000 | ms/batch 392.65 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1100/1883 batches | lr 0.0000 | ms/batch 392.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1200/1883 batches | lr 0.0000 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1300/1883 batches | lr 0.0000 | ms/batch 392.56 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1400/1883 batches | lr 0.0000 | ms/batch 392.58 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1500/1883 batches | lr 0.0000 | ms/batch 392.55 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1600/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1700/1883 batches | lr 0.0000 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1800/1883 batches | lr 0.0000 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 795.50s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/1883 batches | lr 0.0000 | ms/batch 396.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 200/1883 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 300/1883 batches | lr 0.0000 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 400/1883 batches | lr 0.0000 | ms/batch 392.43 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 500/1883 batches | lr 0.0000 | ms/batch 392.25 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 600/1883 batches | lr 0.0000 | ms/batch 392.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 700/1883 batches | lr 0.0000 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 800/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 900/1883 batches | lr 0.0000 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1000/1883 batches | lr 0.0000 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1100/1883 batches | lr 0.0000 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1200/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1300/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1400/1883 batches | lr 0.0000 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1500/1883 batches | lr 0.0000 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1600/1883 batches | lr 0.0000 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1700/1883 batches | lr 0.0000 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1800/1883 batches | lr 0.0000 | ms/batch 392.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 795.10s | valid loss/mse 0.0582 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/1883 batches | lr 0.0000 | ms/batch 396.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 200/1883 batches | lr 0.0000 | ms/batch 392.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 300/1883 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 400/1883 batches | lr 0.0000 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 500/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 600/1883 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 700/1883 batches | lr 0.0000 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 800/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 900/1883 batches | lr 0.0000 | ms/batch 392.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1000/1883 batches | lr 0.0000 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1100/1883 batches | lr 0.0000 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1200/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1300/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1400/1883 batches | lr 0.0000 | ms/batch 392.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1500/1883 batches | lr 0.0000 | ms/batch 392.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1600/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1700/1883 batches | lr 0.0000 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1800/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 795.01s | valid loss/mse 0.0583 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/1883 batches | lr 0.0000 | ms/batch 396.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 200/1883 batches | lr 0.0000 | ms/batch 392.25 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 300/1883 batches | lr 0.0000 | ms/batch 392.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 400/1883 batches | lr 0.0000 | ms/batch 392.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 500/1883 batches | lr 0.0000 | ms/batch 392.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 600/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 700/1883 batches | lr 0.0000 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 800/1883 batches | lr 0.0000 | ms/batch 392.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 900/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1000/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1100/1883 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1200/1883 batches | lr 0.0000 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1300/1883 batches | lr 0.0000 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1400/1883 batches | lr 0.0000 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1500/1883 batches | lr 0.0000 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1600/1883 batches | lr 0.0000 | ms/batch 392.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1700/1883 batches | lr 0.0000 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  14 | 1800/1883 batches | lr 0.0000 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 795.38s | valid loss/mse 0.0579 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0579
scGPT - INFO - | epoch  15 | 100/1883 batches | lr 0.0000 | ms/batch 396.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 200/1883 batches | lr 0.0000 | ms/batch 392.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 300/1883 batches | lr 0.0000 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 400/1883 batches | lr 0.0000 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 500/1883 batches | lr 0.0000 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 600/1883 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 700/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 800/1883 batches | lr 0.0000 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 900/1883 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1000/1883 batches | lr 0.0000 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1100/1883 batches | lr 0.0000 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1200/1883 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1300/1883 batches | lr 0.0000 | ms/batch 392.22 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1400/1883 batches | lr 0.0000 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1500/1883 batches | lr 0.0000 | ms/batch 392.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1600/1883 batches | lr 0.0000 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1700/1883 batches | lr 0.0000 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  15 | 1800/1883 batches | lr 0.0000 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 795.20s | valid loss/mse 0.0579 |
scGPT - INFO - -----------------------------------------------------------------------------------------
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/NormanWeissman2019/scgpt/split3
scGPT - INFO - Running on 2024-07-27 08:30:47
scGPT - INFO - match 4545/5037 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1912 batches | lr 0.0001 | ms/batch 400.06 | loss  0.15 | mse  0.15 |
scGPT - INFO - | epoch   1 | 200/1912 batches | lr 0.0001 | ms/batch 393.16 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 300/1912 batches | lr 0.0001 | ms/batch 393.00 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 400/1912 batches | lr 0.0001 | ms/batch 392.68 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 500/1912 batches | lr 0.0001 | ms/batch 392.65 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 600/1912 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 700/1912 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 800/1912 batches | lr 0.0001 | ms/batch 392.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 900/1912 batches | lr 0.0001 | ms/batch 392.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1000/1912 batches | lr 0.0001 | ms/batch 392.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1100/1912 batches | lr 0.0001 | ms/batch 392.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1200/1912 batches | lr 0.0001 | ms/batch 392.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1300/1912 batches | lr 0.0001 | ms/batch 392.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1400/1912 batches | lr 0.0001 | ms/batch 392.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1500/1912 batches | lr 0.0001 | ms/batch 392.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1600/1912 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1700/1912 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1800/1912 batches | lr 0.0001 | ms/batch 393.55 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1900/1912 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 813.41s | valid loss/mse 0.0565 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0565
scGPT - INFO - | epoch   2 | 100/1912 batches | lr 0.0001 | ms/batch 396.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 200/1912 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 300/1912 batches | lr 0.0001 | ms/batch 392.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 400/1912 batches | lr 0.0001 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 500/1912 batches | lr 0.0001 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 600/1912 batches | lr 0.0001 | ms/batch 392.54 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 700/1912 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 800/1912 batches | lr 0.0001 | ms/batch 392.65 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 900/1912 batches | lr 0.0001 | ms/batch 392.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1000/1912 batches | lr 0.0001 | ms/batch 392.64 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1100/1912 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1200/1912 batches | lr 0.0001 | ms/batch 392.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1300/1912 batches | lr 0.0001 | ms/batch 392.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1400/1912 batches | lr 0.0001 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1500/1912 batches | lr 0.0001 | ms/batch 392.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1600/1912 batches | lr 0.0001 | ms/batch 392.60 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1700/1912 batches | lr 0.0001 | ms/batch 392.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1800/1912 batches | lr 0.0001 | ms/batch 392.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1900/1912 batches | lr 0.0001 | ms/batch 392.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 811.82s | valid loss/mse 0.0573 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1912 batches | lr 0.0001 | ms/batch 396.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 200/1912 batches | lr 0.0001 | ms/batch 392.63 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 300/1912 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 400/1912 batches | lr 0.0001 | ms/batch 392.69 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 500/1912 batches | lr 0.0001 | ms/batch 392.57 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 600/1912 batches | lr 0.0001 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 700/1912 batches | lr 0.0001 | ms/batch 392.55 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 800/1912 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 900/1912 batches | lr 0.0001 | ms/batch 392.56 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1000/1912 batches | lr 0.0001 | ms/batch 392.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1100/1912 batches | lr 0.0001 | ms/batch 392.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1200/1912 batches | lr 0.0001 | ms/batch 392.63 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1300/1912 batches | lr 0.0001 | ms/batch 392.54 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1400/1912 batches | lr 0.0001 | ms/batch 392.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1500/1912 batches | lr 0.0001 | ms/batch 392.66 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1600/1912 batches | lr 0.0001 | ms/batch 392.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1700/1912 batches | lr 0.0001 | ms/batch 392.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1800/1912 batches | lr 0.0001 | ms/batch 392.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1900/1912 batches | lr 0.0001 | ms/batch 392.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 812.26s | valid loss/mse 0.0575 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1912 batches | lr 0.0001 | ms/batch 397.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 200/1912 batches | lr 0.0001 | ms/batch 393.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 300/1912 batches | lr 0.0001 | ms/batch 393.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 400/1912 batches | lr 0.0001 | ms/batch 393.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 500/1912 batches | lr 0.0001 | ms/batch 393.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 600/1912 batches | lr 0.0001 | ms/batch 393.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 700/1912 batches | lr 0.0001 | ms/batch 393.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 800/1912 batches | lr 0.0001 | ms/batch 393.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 900/1912 batches | lr 0.0001 | ms/batch 393.25 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1000/1912 batches | lr 0.0001 | ms/batch 393.46 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1100/1912 batches | lr 0.0001 | ms/batch 393.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1200/1912 batches | lr 0.0001 | ms/batch 393.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1300/1912 batches | lr 0.0001 | ms/batch 393.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1400/1912 batches | lr 0.0001 | ms/batch 393.25 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1500/1912 batches | lr 0.0001 | ms/batch 392.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1600/1912 batches | lr 0.0001 | ms/batch 393.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1700/1912 batches | lr 0.0001 | ms/batch 393.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1800/1912 batches | lr 0.0001 | ms/batch 393.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1900/1912 batches | lr 0.0001 | ms/batch 393.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 813.05s | valid loss/mse 0.0568 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1912 batches | lr 0.0001 | ms/batch 397.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 200/1912 batches | lr 0.0001 | ms/batch 392.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 300/1912 batches | lr 0.0001 | ms/batch 393.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 400/1912 batches | lr 0.0001 | ms/batch 393.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 500/1912 batches | lr 0.0001 | ms/batch 392.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 600/1912 batches | lr 0.0001 | ms/batch 393.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 700/1912 batches | lr 0.0001 | ms/batch 393.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 800/1912 batches | lr 0.0001 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 900/1912 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1000/1912 batches | lr 0.0001 | ms/batch 393.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1100/1912 batches | lr 0.0001 | ms/batch 393.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1200/1912 batches | lr 0.0001 | ms/batch 393.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1300/1912 batches | lr 0.0001 | ms/batch 393.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1400/1912 batches | lr 0.0001 | ms/batch 393.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1500/1912 batches | lr 0.0001 | ms/batch 393.19 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1600/1912 batches | lr 0.0001 | ms/batch 393.16 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1700/1912 batches | lr 0.0001 | ms/batch 393.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1800/1912 batches | lr 0.0001 | ms/batch 393.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1900/1912 batches | lr 0.0001 | ms/batch 393.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 813.08s | valid loss/mse 0.0573 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1912 batches | lr 0.0001 | ms/batch 397.36 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 200/1912 batches | lr 0.0001 | ms/batch 393.34 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 300/1912 batches | lr 0.0001 | ms/batch 393.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 400/1912 batches | lr 0.0001 | ms/batch 393.41 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 500/1912 batches | lr 0.0001 | ms/batch 393.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 600/1912 batches | lr 0.0001 | ms/batch 393.69 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 700/1912 batches | lr 0.0001 | ms/batch 393.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 800/1912 batches | lr 0.0001 | ms/batch 393.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 900/1912 batches | lr 0.0001 | ms/batch 393.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1000/1912 batches | lr 0.0001 | ms/batch 393.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1100/1912 batches | lr 0.0001 | ms/batch 393.47 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1200/1912 batches | lr 0.0001 | ms/batch 393.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1300/1912 batches | lr 0.0001 | ms/batch 393.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1400/1912 batches | lr 0.0001 | ms/batch 393.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1500/1912 batches | lr 0.0001 | ms/batch 393.37 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1600/1912 batches | lr 0.0001 | ms/batch 393.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1700/1912 batches | lr 0.0001 | ms/batch 393.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1800/1912 batches | lr 0.0001 | ms/batch 393.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1900/1912 batches | lr 0.0001 | ms/batch 393.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 813.41s | valid loss/mse 0.0567 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 6
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/NormanWeissman2019/scgpt/split4
scGPT - INFO - Running on 2024-07-27 11:10:23
scGPT - INFO - match 4545/5037 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1840 batches | lr 0.0001 | ms/batch 398.21 | loss  0.14 | mse  0.14 |
scGPT - INFO - | epoch   1 | 200/1840 batches | lr 0.0001 | ms/batch 393.19 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 300/1840 batches | lr 0.0001 | ms/batch 392.88 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 400/1840 batches | lr 0.0001 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 500/1840 batches | lr 0.0001 | ms/batch 392.94 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 600/1840 batches | lr 0.0001 | ms/batch 392.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 700/1840 batches | lr 0.0001 | ms/batch 392.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 800/1840 batches | lr 0.0001 | ms/batch 392.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 900/1840 batches | lr 0.0001 | ms/batch 392.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1000/1840 batches | lr 0.0001 | ms/batch 392.94 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1100/1840 batches | lr 0.0001 | ms/batch 392.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1200/1840 batches | lr 0.0001 | ms/batch 392.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1300/1840 batches | lr 0.0001 | ms/batch 393.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1400/1840 batches | lr 0.0001 | ms/batch 392.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1500/1840 batches | lr 0.0001 | ms/batch 392.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1600/1840 batches | lr 0.0001 | ms/batch 393.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1700/1840 batches | lr 0.0001 | ms/batch 392.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1800/1840 batches | lr 0.0001 | ms/batch 392.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 777.03s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0589
scGPT - INFO - | epoch   2 | 100/1840 batches | lr 0.0001 | ms/batch 396.58 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 200/1840 batches | lr 0.0001 | ms/batch 392.32 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 300/1840 batches | lr 0.0001 | ms/batch 391.82 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 400/1840 batches | lr 0.0001 | ms/batch 391.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 500/1840 batches | lr 0.0001 | ms/batch 392.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 600/1840 batches | lr 0.0001 | ms/batch 391.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 700/1840 batches | lr 0.0001 | ms/batch 391.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 800/1840 batches | lr 0.0001 | ms/batch 391.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 900/1840 batches | lr 0.0001 | ms/batch 391.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1000/1840 batches | lr 0.0001 | ms/batch 391.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1100/1840 batches | lr 0.0001 | ms/batch 391.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1200/1840 batches | lr 0.0001 | ms/batch 391.62 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1300/1840 batches | lr 0.0001 | ms/batch 391.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1400/1840 batches | lr 0.0001 | ms/batch 391.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1500/1840 batches | lr 0.0001 | ms/batch 391.68 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1600/1840 batches | lr 0.0001 | ms/batch 391.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1700/1840 batches | lr 0.0001 | ms/batch 391.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1800/1840 batches | lr 0.0001 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 774.61s | valid loss/mse 0.0582 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0582
scGPT - INFO - | epoch   3 | 100/1840 batches | lr 0.0001 | ms/batch 395.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 200/1840 batches | lr 0.0001 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 300/1840 batches | lr 0.0001 | ms/batch 391.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 400/1840 batches | lr 0.0001 | ms/batch 391.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 500/1840 batches | lr 0.0001 | ms/batch 391.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 600/1840 batches | lr 0.0001 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 700/1840 batches | lr 0.0001 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 800/1840 batches | lr 0.0001 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 900/1840 batches | lr 0.0001 | ms/batch 391.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1000/1840 batches | lr 0.0001 | ms/batch 391.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1100/1840 batches | lr 0.0001 | ms/batch 391.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1200/1840 batches | lr 0.0001 | ms/batch 391.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1300/1840 batches | lr 0.0001 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1400/1840 batches | lr 0.0001 | ms/batch 391.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1500/1840 batches | lr 0.0001 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1600/1840 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1700/1840 batches | lr 0.0001 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1800/1840 batches | lr 0.0001 | ms/batch 392.13 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 774.60s | valid loss/mse 0.0579 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0579
scGPT - INFO - | epoch   4 | 100/1840 batches | lr 0.0001 | ms/batch 395.95 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 200/1840 batches | lr 0.0001 | ms/batch 391.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 300/1840 batches | lr 0.0001 | ms/batch 391.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 400/1840 batches | lr 0.0001 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 500/1840 batches | lr 0.0001 | ms/batch 392.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 600/1840 batches | lr 0.0001 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 700/1840 batches | lr 0.0001 | ms/batch 391.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 800/1840 batches | lr 0.0001 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 900/1840 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1000/1840 batches | lr 0.0001 | ms/batch 391.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1100/1840 batches | lr 0.0001 | ms/batch 391.82 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1200/1840 batches | lr 0.0001 | ms/batch 391.94 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1300/1840 batches | lr 0.0001 | ms/batch 392.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1400/1840 batches | lr 0.0001 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1500/1840 batches | lr 0.0001 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1600/1840 batches | lr 0.0001 | ms/batch 392.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1700/1840 batches | lr 0.0001 | ms/batch 391.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1800/1840 batches | lr 0.0001 | ms/batch 391.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 774.98s | valid loss/mse 0.0583 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1840 batches | lr 0.0001 | ms/batch 396.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 200/1840 batches | lr 0.0001 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 300/1840 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 400/1840 batches | lr 0.0001 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 500/1840 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 600/1840 batches | lr 0.0001 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 700/1840 batches | lr 0.0001 | ms/batch 392.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 800/1840 batches | lr 0.0001 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 900/1840 batches | lr 0.0001 | ms/batch 392.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1000/1840 batches | lr 0.0001 | ms/batch 392.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1100/1840 batches | lr 0.0001 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1200/1840 batches | lr 0.0001 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1300/1840 batches | lr 0.0001 | ms/batch 392.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1400/1840 batches | lr 0.0001 | ms/batch 391.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1500/1840 batches | lr 0.0001 | ms/batch 392.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1600/1840 batches | lr 0.0001 | ms/batch 392.40 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1700/1840 batches | lr 0.0001 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1800/1840 batches | lr 0.0001 | ms/batch 392.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 775.18s | valid loss/mse 0.0580 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1840 batches | lr 0.0001 | ms/batch 400.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 200/1840 batches | lr 0.0001 | ms/batch 393.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 300/1840 batches | lr 0.0001 | ms/batch 392.64 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 400/1840 batches | lr 0.0001 | ms/batch 392.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 500/1840 batches | lr 0.0001 | ms/batch 392.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 600/1840 batches | lr 0.0001 | ms/batch 392.66 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 700/1840 batches | lr 0.0001 | ms/batch 392.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 800/1840 batches | lr 0.0001 | ms/batch 392.38 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 900/1840 batches | lr 0.0001 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1000/1840 batches | lr 0.0001 | ms/batch 392.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1100/1840 batches | lr 0.0001 | ms/batch 392.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1200/1840 batches | lr 0.0001 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1300/1840 batches | lr 0.0001 | ms/batch 392.16 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1400/1840 batches | lr 0.0001 | ms/batch 391.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1500/1840 batches | lr 0.0001 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1600/1840 batches | lr 0.0001 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1700/1840 batches | lr 0.0001 | ms/batch 391.95 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1800/1840 batches | lr 0.0001 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 776.11s | valid loss/mse 0.0582 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1840 batches | lr 0.0001 | ms/batch 396.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 200/1840 batches | lr 0.0001 | ms/batch 392.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 300/1840 batches | lr 0.0001 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 400/1840 batches | lr 0.0001 | ms/batch 392.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 500/1840 batches | lr 0.0001 | ms/batch 392.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 600/1840 batches | lr 0.0001 | ms/batch 392.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 700/1840 batches | lr 0.0001 | ms/batch 392.39 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 800/1840 batches | lr 0.0001 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 900/1840 batches | lr 0.0001 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1000/1840 batches | lr 0.0001 | ms/batch 392.19 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1100/1840 batches | lr 0.0001 | ms/batch 392.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1200/1840 batches | lr 0.0001 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1300/1840 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1400/1840 batches | lr 0.0001 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1500/1840 batches | lr 0.0001 | ms/batch 392.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1600/1840 batches | lr 0.0001 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1700/1840 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1800/1840 batches | lr 0.0001 | ms/batch 392.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 775.57s | valid loss/mse 0.0583 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1840 batches | lr 0.0000 | ms/batch 396.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 200/1840 batches | lr 0.0000 | ms/batch 392.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 300/1840 batches | lr 0.0000 | ms/batch 392.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 400/1840 batches | lr 0.0000 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 500/1840 batches | lr 0.0000 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 600/1840 batches | lr 0.0000 | ms/batch 392.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 700/1840 batches | lr 0.0000 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 800/1840 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 900/1840 batches | lr 0.0000 | ms/batch 392.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1000/1840 batches | lr 0.0000 | ms/batch 392.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1100/1840 batches | lr 0.0000 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1200/1840 batches | lr 0.0000 | ms/batch 392.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1300/1840 batches | lr 0.0000 | ms/batch 392.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1400/1840 batches | lr 0.0000 | ms/batch 392.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1500/1840 batches | lr 0.0000 | ms/batch 392.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1600/1840 batches | lr 0.0000 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1700/1840 batches | lr 0.0000 | ms/batch 392.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1800/1840 batches | lr 0.0000 | ms/batch 392.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 775.49s | valid loss/mse 0.0581 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 8
---Creating test_res
test_res saved successfully----
Split4 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/NormanWeissman2019/scgpt/split5
scGPT - INFO - Running on 2024-07-27 14:21:25
scGPT - INFO - match 4545/5037 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1856 batches | lr 0.0001 | ms/batch 397.92 | loss  0.15 | mse  0.15 |
scGPT - INFO - | epoch   1 | 200/1856 batches | lr 0.0001 | ms/batch 391.87 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 300/1856 batches | lr 0.0001 | ms/batch 391.64 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   1 | 400/1856 batches | lr 0.0001 | ms/batch 391.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 500/1856 batches | lr 0.0001 | ms/batch 391.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 600/1856 batches | lr 0.0001 | ms/batch 391.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 700/1856 batches | lr 0.0001 | ms/batch 391.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 800/1856 batches | lr 0.0001 | ms/batch 391.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 900/1856 batches | lr 0.0001 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1000/1856 batches | lr 0.0001 | ms/batch 392.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1100/1856 batches | lr 0.0001 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1200/1856 batches | lr 0.0001 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1300/1856 batches | lr 0.0001 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1400/1856 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1500/1856 batches | lr 0.0001 | ms/batch 391.61 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1600/1856 batches | lr 0.0001 | ms/batch 391.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1700/1856 batches | lr 0.0001 | ms/batch 391.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   1 | 1800/1856 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 776.15s | valid loss/mse 0.0591 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0591
scGPT - INFO - | epoch   2 | 100/1856 batches | lr 0.0001 | ms/batch 395.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 200/1856 batches | lr 0.0001 | ms/batch 391.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 300/1856 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 400/1856 batches | lr 0.0001 | ms/batch 392.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 500/1856 batches | lr 0.0001 | ms/batch 393.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 600/1856 batches | lr 0.0001 | ms/batch 394.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 700/1856 batches | lr 0.0001 | ms/batch 393.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 800/1856 batches | lr 0.0001 | ms/batch 393.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 900/1856 batches | lr 0.0001 | ms/batch 393.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1000/1856 batches | lr 0.0001 | ms/batch 393.51 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1100/1856 batches | lr 0.0001 | ms/batch 393.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1200/1856 batches | lr 0.0001 | ms/batch 393.64 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1300/1856 batches | lr 0.0001 | ms/batch 395.23 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1400/1856 batches | lr 0.0001 | ms/batch 395.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1500/1856 batches | lr 0.0001 | ms/batch 395.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1600/1856 batches | lr 0.0001 | ms/batch 394.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1700/1856 batches | lr 0.0001 | ms/batch 395.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   2 | 1800/1856 batches | lr 0.0001 | ms/batch 394.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 779.73s | valid loss/mse 0.0597 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1856 batches | lr 0.0001 | ms/batch 398.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 200/1856 batches | lr 0.0001 | ms/batch 393.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 300/1856 batches | lr 0.0001 | ms/batch 393.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 400/1856 batches | lr 0.0001 | ms/batch 394.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 500/1856 batches | lr 0.0001 | ms/batch 395.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 600/1856 batches | lr 0.0001 | ms/batch 394.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 700/1856 batches | lr 0.0001 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 800/1856 batches | lr 0.0001 | ms/batch 392.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 900/1856 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1000/1856 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1100/1856 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1200/1856 batches | lr 0.0001 | ms/batch 391.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1300/1856 batches | lr 0.0001 | ms/batch 392.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1400/1856 batches | lr 0.0001 | ms/batch 392.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1500/1856 batches | lr 0.0001 | ms/batch 392.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1600/1856 batches | lr 0.0001 | ms/batch 391.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1700/1856 batches | lr 0.0001 | ms/batch 391.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   3 | 1800/1856 batches | lr 0.0001 | ms/batch 391.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 777.52s | valid loss/mse 0.0592 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1856 batches | lr 0.0001 | ms/batch 396.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 200/1856 batches | lr 0.0001 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 300/1856 batches | lr 0.0001 | ms/batch 391.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 400/1856 batches | lr 0.0001 | ms/batch 391.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 500/1856 batches | lr 0.0001 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 600/1856 batches | lr 0.0001 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 700/1856 batches | lr 0.0001 | ms/batch 391.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 800/1856 batches | lr 0.0001 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 900/1856 batches | lr 0.0001 | ms/batch 391.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1000/1856 batches | lr 0.0001 | ms/batch 391.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1100/1856 batches | lr 0.0001 | ms/batch 391.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1200/1856 batches | lr 0.0001 | ms/batch 391.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1300/1856 batches | lr 0.0001 | ms/batch 391.94 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1400/1856 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1500/1856 batches | lr 0.0001 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1600/1856 batches | lr 0.0001 | ms/batch 391.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1700/1856 batches | lr 0.0001 | ms/batch 392.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   4 | 1800/1856 batches | lr 0.0001 | ms/batch 393.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 776.45s | valid loss/mse 0.0587 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0587
scGPT - INFO - | epoch   5 | 100/1856 batches | lr 0.0001 | ms/batch 398.44 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 200/1856 batches | lr 0.0001 | ms/batch 393.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 300/1856 batches | lr 0.0001 | ms/batch 393.29 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 400/1856 batches | lr 0.0001 | ms/batch 393.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 500/1856 batches | lr 0.0001 | ms/batch 392.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 600/1856 batches | lr 0.0001 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 700/1856 batches | lr 0.0001 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 800/1856 batches | lr 0.0001 | ms/batch 392.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 900/1856 batches | lr 0.0001 | ms/batch 392.49 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1000/1856 batches | lr 0.0001 | ms/batch 392.64 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1100/1856 batches | lr 0.0001 | ms/batch 392.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1200/1856 batches | lr 0.0001 | ms/batch 392.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1300/1856 batches | lr 0.0001 | ms/batch 393.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1400/1856 batches | lr 0.0001 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1500/1856 batches | lr 0.0001 | ms/batch 392.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1600/1856 batches | lr 0.0001 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1700/1856 batches | lr 0.0001 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   5 | 1800/1856 batches | lr 0.0001 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 777.42s | valid loss/mse 0.0590 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1856 batches | lr 0.0001 | ms/batch 396.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 200/1856 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 300/1856 batches | lr 0.0001 | ms/batch 392.42 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 400/1856 batches | lr 0.0001 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 500/1856 batches | lr 0.0001 | ms/batch 391.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 600/1856 batches | lr 0.0001 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 700/1856 batches | lr 0.0001 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 800/1856 batches | lr 0.0001 | ms/batch 392.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 900/1856 batches | lr 0.0001 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1000/1856 batches | lr 0.0001 | ms/batch 392.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1100/1856 batches | lr 0.0001 | ms/batch 392.20 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1200/1856 batches | lr 0.0001 | ms/batch 392.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1300/1856 batches | lr 0.0001 | ms/batch 392.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1400/1856 batches | lr 0.0001 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1500/1856 batches | lr 0.0001 | ms/batch 391.70 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1600/1856 batches | lr 0.0001 | ms/batch 391.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1700/1856 batches | lr 0.0001 | ms/batch 391.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   6 | 1800/1856 batches | lr 0.0001 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 776.20s | valid loss/mse 0.0586 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0586
scGPT - INFO - | epoch   7 | 100/1856 batches | lr 0.0001 | ms/batch 396.00 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 200/1856 batches | lr 0.0001 | ms/batch 392.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 300/1856 batches | lr 0.0001 | ms/batch 391.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 400/1856 batches | lr 0.0001 | ms/batch 392.16 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 500/1856 batches | lr 0.0001 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 600/1856 batches | lr 0.0001 | ms/batch 391.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 700/1856 batches | lr 0.0001 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 800/1856 batches | lr 0.0001 | ms/batch 391.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 900/1856 batches | lr 0.0001 | ms/batch 391.84 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1000/1856 batches | lr 0.0001 | ms/batch 391.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1100/1856 batches | lr 0.0001 | ms/batch 391.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1200/1856 batches | lr 0.0001 | ms/batch 391.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1300/1856 batches | lr 0.0001 | ms/batch 391.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1400/1856 batches | lr 0.0001 | ms/batch 391.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1500/1856 batches | lr 0.0001 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1600/1856 batches | lr 0.0001 | ms/batch 391.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1700/1856 batches | lr 0.0001 | ms/batch 391.82 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   7 | 1800/1856 batches | lr 0.0001 | ms/batch 391.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 776.16s | valid loss/mse 0.0585 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0585
scGPT - INFO - | epoch   8 | 100/1856 batches | lr 0.0000 | ms/batch 397.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 200/1856 batches | lr 0.0000 | ms/batch 392.28 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 300/1856 batches | lr 0.0000 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 400/1856 batches | lr 0.0000 | ms/batch 392.26 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 500/1856 batches | lr 0.0000 | ms/batch 392.04 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 600/1856 batches | lr 0.0000 | ms/batch 392.11 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 700/1856 batches | lr 0.0000 | ms/batch 392.16 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 800/1856 batches | lr 0.0000 | ms/batch 391.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 900/1856 batches | lr 0.0000 | ms/batch 392.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1000/1856 batches | lr 0.0000 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1100/1856 batches | lr 0.0000 | ms/batch 391.95 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1200/1856 batches | lr 0.0000 | ms/batch 391.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1300/1856 batches | lr 0.0000 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1400/1856 batches | lr 0.0000 | ms/batch 392.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1500/1856 batches | lr 0.0000 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1600/1856 batches | lr 0.0000 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1700/1856 batches | lr 0.0000 | ms/batch 391.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   8 | 1800/1856 batches | lr 0.0000 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 776.37s | valid loss/mse 0.0585 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0585
scGPT - INFO - | epoch   9 | 100/1856 batches | lr 0.0000 | ms/batch 396.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 200/1856 batches | lr 0.0000 | ms/batch 392.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 300/1856 batches | lr 0.0000 | ms/batch 391.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 400/1856 batches | lr 0.0000 | ms/batch 391.80 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 500/1856 batches | lr 0.0000 | ms/batch 391.77 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 600/1856 batches | lr 0.0000 | ms/batch 391.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 700/1856 batches | lr 0.0000 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 800/1856 batches | lr 0.0000 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 900/1856 batches | lr 0.0000 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1000/1856 batches | lr 0.0000 | ms/batch 391.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1100/1856 batches | lr 0.0000 | ms/batch 391.87 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1200/1856 batches | lr 0.0000 | ms/batch 391.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1300/1856 batches | lr 0.0000 | ms/batch 391.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1400/1856 batches | lr 0.0000 | ms/batch 391.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1500/1856 batches | lr 0.0000 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1600/1856 batches | lr 0.0000 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1700/1856 batches | lr 0.0000 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch   9 | 1800/1856 batches | lr 0.0000 | ms/batch 391.90 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 776.21s | valid loss/mse 0.0587 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/1856 batches | lr 0.0000 | ms/batch 396.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 200/1856 batches | lr 0.0000 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 300/1856 batches | lr 0.0000 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 400/1856 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 500/1856 batches | lr 0.0000 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 600/1856 batches | lr 0.0000 | ms/batch 391.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 700/1856 batches | lr 0.0000 | ms/batch 392.15 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 800/1856 batches | lr 0.0000 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 900/1856 batches | lr 0.0000 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1000/1856 batches | lr 0.0000 | ms/batch 392.30 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1100/1856 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1200/1856 batches | lr 0.0000 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1300/1856 batches | lr 0.0000 | ms/batch 391.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1400/1856 batches | lr 0.0000 | ms/batch 391.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1500/1856 batches | lr 0.0000 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1600/1856 batches | lr 0.0000 | ms/batch 392.06 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1700/1856 batches | lr 0.0000 | ms/batch 392.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  10 | 1800/1856 batches | lr 0.0000 | ms/batch 392.35 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 776.39s | valid loss/mse 0.0591 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/1856 batches | lr 0.0000 | ms/batch 395.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 200/1856 batches | lr 0.0000 | ms/batch 391.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 300/1856 batches | lr 0.0000 | ms/batch 391.95 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 400/1856 batches | lr 0.0000 | ms/batch 392.16 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 500/1856 batches | lr 0.0000 | ms/batch 391.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 600/1856 batches | lr 0.0000 | ms/batch 391.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 700/1856 batches | lr 0.0000 | ms/batch 391.78 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 800/1856 batches | lr 0.0000 | ms/batch 391.68 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 900/1856 batches | lr 0.0000 | ms/batch 391.71 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1000/1856 batches | lr 0.0000 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1100/1856 batches | lr 0.0000 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1200/1856 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1300/1856 batches | lr 0.0000 | ms/batch 392.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1400/1856 batches | lr 0.0000 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1500/1856 batches | lr 0.0000 | ms/batch 391.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1600/1856 batches | lr 0.0000 | ms/batch 391.76 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1700/1856 batches | lr 0.0000 | ms/batch 391.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  11 | 1800/1856 batches | lr 0.0000 | ms/batch 391.92 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 775.93s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/1856 batches | lr 0.0000 | ms/batch 396.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 200/1856 batches | lr 0.0000 | ms/batch 391.66 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 300/1856 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 400/1856 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 500/1856 batches | lr 0.0000 | ms/batch 391.75 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 600/1856 batches | lr 0.0000 | ms/batch 391.99 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 700/1856 batches | lr 0.0000 | ms/batch 392.18 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 800/1856 batches | lr 0.0000 | ms/batch 391.96 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 900/1856 batches | lr 0.0000 | ms/batch 391.85 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1000/1856 batches | lr 0.0000 | ms/batch 391.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1100/1856 batches | lr 0.0000 | ms/batch 392.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1200/1856 batches | lr 0.0000 | ms/batch 392.03 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1300/1856 batches | lr 0.0000 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1400/1856 batches | lr 0.0000 | ms/batch 392.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1500/1856 batches | lr 0.0000 | ms/batch 392.13 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1600/1856 batches | lr 0.0000 | ms/batch 392.09 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1700/1856 batches | lr 0.0000 | ms/batch 392.01 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  12 | 1800/1856 batches | lr 0.0000 | ms/batch 392.02 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 776.39s | valid loss/mse 0.0592 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/1856 batches | lr 0.0000 | ms/batch 399.97 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 200/1856 batches | lr 0.0000 | ms/batch 393.83 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 300/1856 batches | lr 0.0000 | ms/batch 392.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 400/1856 batches | lr 0.0000 | ms/batch 392.24 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 500/1856 batches | lr 0.0000 | ms/batch 391.91 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 600/1856 batches | lr 0.0000 | ms/batch 392.79 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 700/1856 batches | lr 0.0000 | ms/batch 391.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 800/1856 batches | lr 0.0000 | ms/batch 391.88 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 900/1856 batches | lr 0.0000 | ms/batch 392.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1000/1856 batches | lr 0.0000 | ms/batch 392.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1100/1856 batches | lr 0.0000 | ms/batch 392.10 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1200/1856 batches | lr 0.0000 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1300/1856 batches | lr 0.0000 | ms/batch 392.05 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1400/1856 batches | lr 0.0000 | ms/batch 392.21 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1500/1856 batches | lr 0.0000 | ms/batch 392.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1600/1856 batches | lr 0.0000 | ms/batch 392.08 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1700/1856 batches | lr 0.0000 | ms/batch 392.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - | epoch  13 | 1800/1856 batches | lr 0.0000 | ms/batch 391.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 779.87s | valid loss/mse 0.0589 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 13
---Creating test_res
test_res saved successfully----
Split5 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_k562_essential/scgpt/split1
scGPT - INFO - Running on 2024-07-27 19:00:36
scGPT - INFO - match 5464/5656 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/3333 batches | lr 0.0001 | ms/batch 405.90 | loss  0.33 | mse  0.33 |
scGPT - INFO - | epoch   1 | 200/3333 batches | lr 0.0001 | ms/batch 392.29 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 300/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 400/3333 batches | lr 0.0001 | ms/batch 392.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 500/3333 batches | lr 0.0001 | ms/batch 392.33 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 600/3333 batches | lr 0.0001 | ms/batch 392.31 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 700/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 800/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 900/3333 batches | lr 0.0001 | ms/batch 392.57 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1000/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1100/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1200/3333 batches | lr 0.0001 | ms/batch 392.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1300/3333 batches | lr 0.0001 | ms/batch 392.54 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1400/3333 batches | lr 0.0001 | ms/batch 392.36 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1500/3333 batches | lr 0.0001 | ms/batch 392.38 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1600/3333 batches | lr 0.0001 | ms/batch 392.54 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1700/3333 batches | lr 0.0001 | ms/batch 392.27 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1800/3333 batches | lr 0.0001 | ms/batch 392.35 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 1900/3333 batches | lr 0.0001 | ms/batch 392.62 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2000/3333 batches | lr 0.0001 | ms/batch 392.36 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2100/3333 batches | lr 0.0001 | ms/batch 392.37 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2200/3333 batches | lr 0.0001 | ms/batch 392.59 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2300/3333 batches | lr 0.0001 | ms/batch 392.57 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2400/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2500/3333 batches | lr 0.0001 | ms/batch 392.53 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2600/3333 batches | lr 0.0001 | ms/batch 392.57 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2700/3333 batches | lr 0.0001 | ms/batch 392.41 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2800/3333 batches | lr 0.0001 | ms/batch 392.58 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 2900/3333 batches | lr 0.0001 | ms/batch 392.53 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 3000/3333 batches | lr 0.0001 | ms/batch 392.61 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 3100/3333 batches | lr 0.0001 | ms/batch 392.29 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 3200/3333 batches | lr 0.0001 | ms/batch 392.40 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 3300/3333 batches | lr 0.0001 | ms/batch 392.48 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 1367.95s | valid loss/mse 0.1910 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1910
scGPT - INFO - | epoch   2 | 100/3333 batches | lr 0.0001 | ms/batch 396.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 200/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 300/3333 batches | lr 0.0001 | ms/batch 392.46 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 400/3333 batches | lr 0.0001 | ms/batch 392.71 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 500/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 600/3333 batches | lr 0.0001 | ms/batch 392.32 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 700/3333 batches | lr 0.0001 | ms/batch 392.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 800/3333 batches | lr 0.0001 | ms/batch 392.64 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 900/3333 batches | lr 0.0001 | ms/batch 392.56 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1000/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1100/3333 batches | lr 0.0001 | ms/batch 392.28 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1200/3333 batches | lr 0.0001 | ms/batch 392.30 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1300/3333 batches | lr 0.0001 | ms/batch 392.45 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1400/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1500/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1600/3333 batches | lr 0.0001 | ms/batch 392.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1700/3333 batches | lr 0.0001 | ms/batch 392.62 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1800/3333 batches | lr 0.0001 | ms/batch 392.68 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 1900/3333 batches | lr 0.0001 | ms/batch 392.71 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2000/3333 batches | lr 0.0001 | ms/batch 392.64 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2100/3333 batches | lr 0.0001 | ms/batch 392.55 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2200/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2300/3333 batches | lr 0.0001 | ms/batch 392.60 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2400/3333 batches | lr 0.0001 | ms/batch 392.69 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2500/3333 batches | lr 0.0001 | ms/batch 392.63 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2600/3333 batches | lr 0.0001 | ms/batch 392.62 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2700/3333 batches | lr 0.0001 | ms/batch 392.56 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2800/3333 batches | lr 0.0001 | ms/batch 392.57 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 2900/3333 batches | lr 0.0001 | ms/batch 392.59 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 3000/3333 batches | lr 0.0001 | ms/batch 392.62 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 3100/3333 batches | lr 0.0001 | ms/batch 392.56 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 3200/3333 batches | lr 0.0001 | ms/batch 392.58 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   2 | 3300/3333 batches | lr 0.0001 | ms/batch 392.55 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 1367.27s | valid loss/mse 0.1912 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/3333 batches | lr 0.0001 | ms/batch 396.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 200/3333 batches | lr 0.0001 | ms/batch 392.51 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 300/3333 batches | lr 0.0001 | ms/batch 392.71 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 400/3333 batches | lr 0.0001 | ms/batch 392.54 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 500/3333 batches | lr 0.0001 | ms/batch 392.61 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 600/3333 batches | lr 0.0001 | ms/batch 392.57 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 700/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 800/3333 batches | lr 0.0001 | ms/batch 392.24 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 900/3333 batches | lr 0.0001 | ms/batch 392.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1000/3333 batches | lr 0.0001 | ms/batch 392.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1100/3333 batches | lr 0.0001 | ms/batch 392.52 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1200/3333 batches | lr 0.0001 | ms/batch 392.53 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1300/3333 batches | lr 0.0001 | ms/batch 392.48 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1400/3333 batches | lr 0.0001 | ms/batch 392.45 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1500/3333 batches | lr 0.0001 | ms/batch 392.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1600/3333 batches | lr 0.0001 | ms/batch 392.19 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1700/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1800/3333 batches | lr 0.0001 | ms/batch 392.40 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 1900/3333 batches | lr 0.0001 | ms/batch 392.30 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2000/3333 batches | lr 0.0001 | ms/batch 392.36 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2100/3333 batches | lr 0.0001 | ms/batch 392.37 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2200/3333 batches | lr 0.0001 | ms/batch 392.20 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2300/3333 batches | lr 0.0001 | ms/batch 392.19 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2400/3333 batches | lr 0.0001 | ms/batch 392.16 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2500/3333 batches | lr 0.0001 | ms/batch 392.21 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2600/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2700/3333 batches | lr 0.0001 | ms/batch 392.40 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2800/3333 batches | lr 0.0001 | ms/batch 392.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 2900/3333 batches | lr 0.0001 | ms/batch 392.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 3000/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 3100/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 3200/3333 batches | lr 0.0001 | ms/batch 392.54 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   3 | 3300/3333 batches | lr 0.0001 | ms/batch 392.37 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 1366.87s | valid loss/mse 0.1918 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/3333 batches | lr 0.0001 | ms/batch 396.81 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 200/3333 batches | lr 0.0001 | ms/batch 392.57 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 300/3333 batches | lr 0.0001 | ms/batch 392.51 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 400/3333 batches | lr 0.0001 | ms/batch 392.46 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 500/3333 batches | lr 0.0001 | ms/batch 392.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 600/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 700/3333 batches | lr 0.0001 | ms/batch 392.51 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 800/3333 batches | lr 0.0001 | ms/batch 392.61 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 900/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1000/3333 batches | lr 0.0001 | ms/batch 392.51 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1100/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1200/3333 batches | lr 0.0001 | ms/batch 392.35 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1300/3333 batches | lr 0.0001 | ms/batch 392.36 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1400/3333 batches | lr 0.0001 | ms/batch 392.33 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1500/3333 batches | lr 0.0001 | ms/batch 392.35 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1600/3333 batches | lr 0.0001 | ms/batch 392.39 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1700/3333 batches | lr 0.0001 | ms/batch 392.25 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1800/3333 batches | lr 0.0001 | ms/batch 392.37 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 1900/3333 batches | lr 0.0001 | ms/batch 392.46 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2000/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2100/3333 batches | lr 0.0001 | ms/batch 392.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2200/3333 batches | lr 0.0001 | ms/batch 392.33 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2300/3333 batches | lr 0.0001 | ms/batch 392.31 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2400/3333 batches | lr 0.0001 | ms/batch 392.12 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2500/3333 batches | lr 0.0001 | ms/batch 392.26 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2600/3333 batches | lr 0.0001 | ms/batch 392.35 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2700/3333 batches | lr 0.0001 | ms/batch 392.26 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2800/3333 batches | lr 0.0001 | ms/batch 392.30 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 2900/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 3000/3333 batches | lr 0.0001 | ms/batch 392.33 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 3100/3333 batches | lr 0.0001 | ms/batch 392.39 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 3200/3333 batches | lr 0.0001 | ms/batch 392.45 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   4 | 3300/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 1366.85s | valid loss/mse 0.1918 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/3333 batches | lr 0.0001 | ms/batch 396.59 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 200/3333 batches | lr 0.0001 | ms/batch 392.24 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 300/3333 batches | lr 0.0001 | ms/batch 392.33 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 400/3333 batches | lr 0.0001 | ms/batch 392.47 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 500/3333 batches | lr 0.0001 | ms/batch 392.20 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 600/3333 batches | lr 0.0001 | ms/batch 392.32 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 700/3333 batches | lr 0.0001 | ms/batch 392.48 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 800/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 900/3333 batches | lr 0.0001 | ms/batch 392.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1000/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1100/3333 batches | lr 0.0001 | ms/batch 392.47 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1200/3333 batches | lr 0.0001 | ms/batch 392.37 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1300/3333 batches | lr 0.0001 | ms/batch 392.51 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1400/3333 batches | lr 0.0001 | ms/batch 392.34 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1500/3333 batches | lr 0.0001 | ms/batch 392.44 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1600/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1700/3333 batches | lr 0.0001 | ms/batch 392.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1800/3333 batches | lr 0.0001 | ms/batch 392.40 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 1900/3333 batches | lr 0.0001 | ms/batch 392.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2000/3333 batches | lr 0.0001 | ms/batch 392.69 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2100/3333 batches | lr 0.0001 | ms/batch 393.05 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2200/3333 batches | lr 0.0001 | ms/batch 393.04 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2300/3333 batches | lr 0.0001 | ms/batch 392.91 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2400/3333 batches | lr 0.0001 | ms/batch 393.14 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2500/3333 batches | lr 0.0001 | ms/batch 393.03 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2600/3333 batches | lr 0.0001 | ms/batch 392.85 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2700/3333 batches | lr 0.0001 | ms/batch 392.82 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2800/3333 batches | lr 0.0001 | ms/batch 392.85 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 2900/3333 batches | lr 0.0001 | ms/batch 392.95 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 3000/3333 batches | lr 0.0001 | ms/batch 392.98 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 3100/3333 batches | lr 0.0001 | ms/batch 392.72 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 3200/3333 batches | lr 0.0001 | ms/batch 392.72 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   5 | 3300/3333 batches | lr 0.0001 | ms/batch 392.90 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 1367.77s | valid loss/mse 0.1922 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/3333 batches | lr 0.0001 | ms/batch 396.86 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 200/3333 batches | lr 0.0001 | ms/batch 392.78 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 300/3333 batches | lr 0.0001 | ms/batch 393.04 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 400/3333 batches | lr 0.0001 | ms/batch 393.05 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 500/3333 batches | lr 0.0001 | ms/batch 392.73 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 600/3333 batches | lr 0.0001 | ms/batch 392.66 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 700/3333 batches | lr 0.0001 | ms/batch 392.82 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 800/3333 batches | lr 0.0001 | ms/batch 393.12 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 900/3333 batches | lr 0.0001 | ms/batch 393.07 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1000/3333 batches | lr 0.0001 | ms/batch 393.10 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1100/3333 batches | lr 0.0001 | ms/batch 392.96 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1200/3333 batches | lr 0.0001 | ms/batch 392.82 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1300/3333 batches | lr 0.0001 | ms/batch 393.05 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1400/3333 batches | lr 0.0001 | ms/batch 392.98 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1500/3333 batches | lr 0.0001 | ms/batch 393.10 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1600/3333 batches | lr 0.0001 | ms/batch 393.09 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1700/3333 batches | lr 0.0001 | ms/batch 393.01 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1800/3333 batches | lr 0.0001 | ms/batch 393.05 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 1900/3333 batches | lr 0.0001 | ms/batch 393.06 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2000/3333 batches | lr 0.0001 | ms/batch 393.28 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2100/3333 batches | lr 0.0001 | ms/batch 393.50 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2200/3333 batches | lr 0.0001 | ms/batch 393.56 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2300/3333 batches | lr 0.0001 | ms/batch 393.59 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2400/3333 batches | lr 0.0001 | ms/batch 393.47 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2500/3333 batches | lr 0.0001 | ms/batch 393.48 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2600/3333 batches | lr 0.0001 | ms/batch 393.49 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2700/3333 batches | lr 0.0001 | ms/batch 393.42 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2800/3333 batches | lr 0.0001 | ms/batch 393.36 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 2900/3333 batches | lr 0.0001 | ms/batch 393.18 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 3000/3333 batches | lr 0.0001 | ms/batch 393.27 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 3100/3333 batches | lr 0.0001 | ms/batch 393.39 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 3200/3333 batches | lr 0.0001 | ms/batch 393.11 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   6 | 3300/3333 batches | lr 0.0001 | ms/batch 393.28 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 1369.73s | valid loss/mse 0.1916 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 6
---Creating test_res
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_rpe1_essential/scgpt/split1
scGPT - INFO - Running on 2024-07-27 21:40:27
scGPT - INFO - match 5645/5753 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/3081 batches | lr 0.0001 | ms/batch 409.67 | loss  0.29 | mse  0.29 |
scGPT - INFO - | epoch   1 | 200/3081 batches | lr 0.0001 | ms/batch 393.40 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 300/3081 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 400/3081 batches | lr 0.0001 | ms/batch 393.06 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 500/3081 batches | lr 0.0001 | ms/batch 394.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 600/3081 batches | lr 0.0001 | ms/batch 394.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 700/3081 batches | lr 0.0001 | ms/batch 393.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 800/3081 batches | lr 0.0001 | ms/batch 395.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 900/3081 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1000/3081 batches | lr 0.0001 | ms/batch 394.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1100/3081 batches | lr 0.0001 | ms/batch 394.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1200/3081 batches | lr 0.0001 | ms/batch 395.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1300/3081 batches | lr 0.0001 | ms/batch 395.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1400/3081 batches | lr 0.0001 | ms/batch 393.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1500/3081 batches | lr 0.0001 | ms/batch 393.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1600/3081 batches | lr 0.0001 | ms/batch 393.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1700/3081 batches | lr 0.0001 | ms/batch 393.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1800/3081 batches | lr 0.0001 | ms/batch 393.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1900/3081 batches | lr 0.0001 | ms/batch 393.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2000/3081 batches | lr 0.0001 | ms/batch 392.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2100/3081 batches | lr 0.0001 | ms/batch 393.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2200/3081 batches | lr 0.0001 | ms/batch 393.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2300/3081 batches | lr 0.0001 | ms/batch 393.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2400/3081 batches | lr 0.0001 | ms/batch 393.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2500/3081 batches | lr 0.0001 | ms/batch 393.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2600/3081 batches | lr 0.0001 | ms/batch 392.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2700/3081 batches | lr 0.0001 | ms/batch 393.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2800/3081 batches | lr 0.0001 | ms/batch 393.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2900/3081 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3000/3081 batches | lr 0.0001 | ms/batch 394.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 1269.11s | valid loss/mse 0.1754 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1754
scGPT - INFO - | epoch   2 | 100/3081 batches | lr 0.0001 | ms/batch 399.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 200/3081 batches | lr 0.0001 | ms/batch 394.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 300/3081 batches | lr 0.0001 | ms/batch 394.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 400/3081 batches | lr 0.0001 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 500/3081 batches | lr 0.0001 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 600/3081 batches | lr 0.0001 | ms/batch 394.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 700/3081 batches | lr 0.0001 | ms/batch 394.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 800/3081 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 900/3081 batches | lr 0.0001 | ms/batch 393.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1000/3081 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1100/3081 batches | lr 0.0001 | ms/batch 394.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1200/3081 batches | lr 0.0001 | ms/batch 394.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1300/3081 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1400/3081 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1500/3081 batches | lr 0.0001 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1600/3081 batches | lr 0.0001 | ms/batch 393.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1700/3081 batches | lr 0.0001 | ms/batch 393.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1800/3081 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1900/3081 batches | lr 0.0001 | ms/batch 394.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2000/3081 batches | lr 0.0001 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2100/3081 batches | lr 0.0001 | ms/batch 395.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2200/3081 batches | lr 0.0001 | ms/batch 395.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2300/3081 batches | lr 0.0001 | ms/batch 394.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2400/3081 batches | lr 0.0001 | ms/batch 395.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2500/3081 batches | lr 0.0001 | ms/batch 394.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2600/3081 batches | lr 0.0001 | ms/batch 394.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2700/3081 batches | lr 0.0001 | ms/batch 394.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2800/3081 batches | lr 0.0001 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2900/3081 batches | lr 0.0001 | ms/batch 394.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3000/3081 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 1270.09s | valid loss/mse 0.1759 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/3081 batches | lr 0.0001 | ms/batch 400.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 200/3081 batches | lr 0.0001 | ms/batch 395.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 300/3081 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 400/3081 batches | lr 0.0001 | ms/batch 393.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 500/3081 batches | lr 0.0001 | ms/batch 393.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 600/3081 batches | lr 0.0001 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 700/3081 batches | lr 0.0001 | ms/batch 395.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 800/3081 batches | lr 0.0001 | ms/batch 395.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 900/3081 batches | lr 0.0001 | ms/batch 395.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1000/3081 batches | lr 0.0001 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1100/3081 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1200/3081 batches | lr 0.0001 | ms/batch 393.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1300/3081 batches | lr 0.0001 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1400/3081 batches | lr 0.0001 | ms/batch 396.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1500/3081 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1600/3081 batches | lr 0.0001 | ms/batch 394.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1700/3081 batches | lr 0.0001 | ms/batch 409.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1800/3081 batches | lr 0.0001 | ms/batch 393.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1900/3081 batches | lr 0.0001 | ms/batch 393.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2000/3081 batches | lr 0.0001 | ms/batch 394.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2100/3081 batches | lr 0.0001 | ms/batch 393.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2200/3081 batches | lr 0.0001 | ms/batch 393.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2300/3081 batches | lr 0.0001 | ms/batch 393.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2400/3081 batches | lr 0.0001 | ms/batch 393.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2500/3081 batches | lr 0.0001 | ms/batch 393.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2600/3081 batches | lr 0.0001 | ms/batch 395.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2700/3081 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2800/3081 batches | lr 0.0001 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2900/3081 batches | lr 0.0001 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3000/3081 batches | lr 0.0001 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 1271.08s | valid loss/mse 0.1755 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/3081 batches | lr 0.0001 | ms/batch 398.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 200/3081 batches | lr 0.0001 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 300/3081 batches | lr 0.0001 | ms/batch 395.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 400/3081 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 500/3081 batches | lr 0.0001 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 600/3081 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 700/3081 batches | lr 0.0001 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 800/3081 batches | lr 0.0001 | ms/batch 393.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 900/3081 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1000/3081 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1100/3081 batches | lr 0.0001 | ms/batch 394.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1200/3081 batches | lr 0.0001 | ms/batch 393.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1300/3081 batches | lr 0.0001 | ms/batch 393.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1400/3081 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1500/3081 batches | lr 0.0001 | ms/batch 393.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1600/3081 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1700/3081 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1800/3081 batches | lr 0.0001 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1900/3081 batches | lr 0.0001 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2000/3081 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2100/3081 batches | lr 0.0001 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2200/3081 batches | lr 0.0001 | ms/batch 394.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2300/3081 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2400/3081 batches | lr 0.0001 | ms/batch 393.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2500/3081 batches | lr 0.0001 | ms/batch 393.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2600/3081 batches | lr 0.0001 | ms/batch 393.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2700/3081 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2800/3081 batches | lr 0.0001 | ms/batch 393.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2900/3081 batches | lr 0.0001 | ms/batch 393.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3000/3081 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 1268.44s | valid loss/mse 0.1762 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/3081 batches | lr 0.0001 | ms/batch 398.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 200/3081 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 300/3081 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 400/3081 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 500/3081 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 600/3081 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 700/3081 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 800/3081 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 900/3081 batches | lr 0.0001 | ms/batch 394.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1000/3081 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1100/3081 batches | lr 0.0001 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1200/3081 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1300/3081 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1400/3081 batches | lr 0.0001 | ms/batch 394.34 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1500/3081 batches | lr 0.0001 | ms/batch 394.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1600/3081 batches | lr 0.0001 | ms/batch 394.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1700/3081 batches | lr 0.0001 | ms/batch 394.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1800/3081 batches | lr 0.0001 | ms/batch 394.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1900/3081 batches | lr 0.0001 | ms/batch 394.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2000/3081 batches | lr 0.0001 | ms/batch 394.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2100/3081 batches | lr 0.0001 | ms/batch 394.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2200/3081 batches | lr 0.0001 | ms/batch 394.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2300/3081 batches | lr 0.0001 | ms/batch 394.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2400/3081 batches | lr 0.0001 | ms/batch 394.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2500/3081 batches | lr 0.0001 | ms/batch 394.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2600/3081 batches | lr 0.0001 | ms/batch 394.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2700/3081 batches | lr 0.0001 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2800/3081 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2900/3081 batches | lr 0.0001 | ms/batch 394.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3000/3081 batches | lr 0.0001 | ms/batch 394.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 1270.05s | valid loss/mse 0.1754 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/3081 batches | lr 0.0001 | ms/batch 398.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 200/3081 batches | lr 0.0001 | ms/batch 394.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 300/3081 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 400/3081 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 500/3081 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 600/3081 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 700/3081 batches | lr 0.0001 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 800/3081 batches | lr 0.0001 | ms/batch 394.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 900/3081 batches | lr 0.0001 | ms/batch 394.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1000/3081 batches | lr 0.0001 | ms/batch 394.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1100/3081 batches | lr 0.0001 | ms/batch 394.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1200/3081 batches | lr 0.0001 | ms/batch 394.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1300/3081 batches | lr 0.0001 | ms/batch 394.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1400/3081 batches | lr 0.0001 | ms/batch 394.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1500/3081 batches | lr 0.0001 | ms/batch 395.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1600/3081 batches | lr 0.0001 | ms/batch 394.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1700/3081 batches | lr 0.0001 | ms/batch 395.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1800/3081 batches | lr 0.0001 | ms/batch 394.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1900/3081 batches | lr 0.0001 | ms/batch 394.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2000/3081 batches | lr 0.0001 | ms/batch 394.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2100/3081 batches | lr 0.0001 | ms/batch 394.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2200/3081 batches | lr 0.0001 | ms/batch 394.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2300/3081 batches | lr 0.0001 | ms/batch 394.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2400/3081 batches | lr 0.0001 | ms/batch 394.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2500/3081 batches | lr 0.0001 | ms/batch 394.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2600/3081 batches | lr 0.0001 | ms/batch 395.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2700/3081 batches | lr 0.0001 | ms/batch 395.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2800/3081 batches | lr 0.0001 | ms/batch 395.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2900/3081 batches | lr 0.0001 | ms/batch 394.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3000/3081 batches | lr 0.0001 | ms/batch 394.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 1271.18s | valid loss/mse 0.1752 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1752
scGPT - INFO - | epoch   7 | 100/3081 batches | lr 0.0001 | ms/batch 399.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 200/3081 batches | lr 0.0001 | ms/batch 395.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 300/3081 batches | lr 0.0001 | ms/batch 395.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 400/3081 batches | lr 0.0001 | ms/batch 395.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 500/3081 batches | lr 0.0001 | ms/batch 395.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 600/3081 batches | lr 0.0001 | ms/batch 395.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 700/3081 batches | lr 0.0001 | ms/batch 395.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 800/3081 batches | lr 0.0001 | ms/batch 395.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 900/3081 batches | lr 0.0001 | ms/batch 395.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1000/3081 batches | lr 0.0001 | ms/batch 395.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1100/3081 batches | lr 0.0001 | ms/batch 396.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1200/3081 batches | lr 0.0001 | ms/batch 395.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1300/3081 batches | lr 0.0001 | ms/batch 395.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1400/3081 batches | lr 0.0001 | ms/batch 395.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1500/3081 batches | lr 0.0001 | ms/batch 395.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1600/3081 batches | lr 0.0001 | ms/batch 395.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1700/3081 batches | lr 0.0001 | ms/batch 395.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1800/3081 batches | lr 0.0001 | ms/batch 395.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1900/3081 batches | lr 0.0001 | ms/batch 395.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2000/3081 batches | lr 0.0001 | ms/batch 395.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2100/3081 batches | lr 0.0001 | ms/batch 395.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2200/3081 batches | lr 0.0001 | ms/batch 395.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2300/3081 batches | lr 0.0001 | ms/batch 395.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2400/3081 batches | lr 0.0001 | ms/batch 395.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2500/3081 batches | lr 0.0001 | ms/batch 395.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2600/3081 batches | lr 0.0001 | ms/batch 395.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2700/3081 batches | lr 0.0001 | ms/batch 395.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2800/3081 batches | lr 0.0001 | ms/batch 395.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2900/3081 batches | lr 0.0001 | ms/batch 395.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3000/3081 batches | lr 0.0001 | ms/batch 395.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 1273.31s | valid loss/mse 0.1746 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1746
scGPT - INFO - | epoch   8 | 100/3081 batches | lr 0.0000 | ms/batch 399.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 200/3081 batches | lr 0.0000 | ms/batch 395.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 300/3081 batches | lr 0.0000 | ms/batch 395.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 400/3081 batches | lr 0.0000 | ms/batch 395.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 500/3081 batches | lr 0.0000 | ms/batch 395.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 600/3081 batches | lr 0.0000 | ms/batch 395.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 700/3081 batches | lr 0.0000 | ms/batch 396.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 800/3081 batches | lr 0.0000 | ms/batch 395.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 900/3081 batches | lr 0.0000 | ms/batch 397.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1000/3081 batches | lr 0.0000 | ms/batch 395.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1100/3081 batches | lr 0.0000 | ms/batch 395.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1200/3081 batches | lr 0.0000 | ms/batch 395.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1300/3081 batches | lr 0.0000 | ms/batch 395.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1400/3081 batches | lr 0.0000 | ms/batch 395.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1500/3081 batches | lr 0.0000 | ms/batch 395.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1600/3081 batches | lr 0.0000 | ms/batch 395.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1700/3081 batches | lr 0.0000 | ms/batch 395.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1800/3081 batches | lr 0.0000 | ms/batch 395.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1900/3081 batches | lr 0.0000 | ms/batch 395.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2000/3081 batches | lr 0.0000 | ms/batch 395.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2100/3081 batches | lr 0.0000 | ms/batch 395.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2200/3081 batches | lr 0.0000 | ms/batch 395.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2300/3081 batches | lr 0.0000 | ms/batch 395.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2400/3081 batches | lr 0.0000 | ms/batch 395.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2500/3081 batches | lr 0.0000 | ms/batch 395.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2600/3081 batches | lr 0.0000 | ms/batch 395.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2700/3081 batches | lr 0.0000 | ms/batch 395.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2800/3081 batches | lr 0.0000 | ms/batch 395.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2900/3081 batches | lr 0.0000 | ms/batch 395.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3000/3081 batches | lr 0.0000 | ms/batch 395.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 1274.09s | valid loss/mse 0.1752 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/3081 batches | lr 0.0000 | ms/batch 399.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 200/3081 batches | lr 0.0000 | ms/batch 395.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 300/3081 batches | lr 0.0000 | ms/batch 395.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 400/3081 batches | lr 0.0000 | ms/batch 395.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 500/3081 batches | lr 0.0000 | ms/batch 395.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 600/3081 batches | lr 0.0000 | ms/batch 395.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 700/3081 batches | lr 0.0000 | ms/batch 395.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 800/3081 batches | lr 0.0000 | ms/batch 395.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 900/3081 batches | lr 0.0000 | ms/batch 395.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1000/3081 batches | lr 0.0000 | ms/batch 395.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1100/3081 batches | lr 0.0000 | ms/batch 395.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1200/3081 batches | lr 0.0000 | ms/batch 395.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1300/3081 batches | lr 0.0000 | ms/batch 395.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1400/3081 batches | lr 0.0000 | ms/batch 395.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1500/3081 batches | lr 0.0000 | ms/batch 395.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1600/3081 batches | lr 0.0000 | ms/batch 395.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1700/3081 batches | lr 0.0000 | ms/batch 395.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1800/3081 batches | lr 0.0000 | ms/batch 395.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1900/3081 batches | lr 0.0000 | ms/batch 395.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2000/3081 batches | lr 0.0000 | ms/batch 395.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2100/3081 batches | lr 0.0000 | ms/batch 395.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2200/3081 batches | lr 0.0000 | ms/batch 395.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2300/3081 batches | lr 0.0000 | ms/batch 395.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2400/3081 batches | lr 0.0000 | ms/batch 395.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2500/3081 batches | lr 0.0000 | ms/batch 395.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2600/3081 batches | lr 0.0000 | ms/batch 395.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2700/3081 batches | lr 0.0000 | ms/batch 396.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2800/3081 batches | lr 0.0000 | ms/batch 395.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2900/3081 batches | lr 0.0000 | ms/batch 395.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3000/3081 batches | lr 0.0000 | ms/batch 395.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 1273.56s | valid loss/mse 0.1752 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/3081 batches | lr 0.0000 | ms/batch 399.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 200/3081 batches | lr 0.0000 | ms/batch 395.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 300/3081 batches | lr 0.0000 | ms/batch 395.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 400/3081 batches | lr 0.0000 | ms/batch 395.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 500/3081 batches | lr 0.0000 | ms/batch 395.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 600/3081 batches | lr 0.0000 | ms/batch 395.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 700/3081 batches | lr 0.0000 | ms/batch 395.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 800/3081 batches | lr 0.0000 | ms/batch 397.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 900/3081 batches | lr 0.0000 | ms/batch 396.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1000/3081 batches | lr 0.0000 | ms/batch 395.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1100/3081 batches | lr 0.0000 | ms/batch 395.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1200/3081 batches | lr 0.0000 | ms/batch 395.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1300/3081 batches | lr 0.0000 | ms/batch 395.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1400/3081 batches | lr 0.0000 | ms/batch 395.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1500/3081 batches | lr 0.0000 | ms/batch 395.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1600/3081 batches | lr 0.0000 | ms/batch 395.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1700/3081 batches | lr 0.0000 | ms/batch 395.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1800/3081 batches | lr 0.0000 | ms/batch 396.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1900/3081 batches | lr 0.0000 | ms/batch 395.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2000/3081 batches | lr 0.0000 | ms/batch 395.93 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2100/3081 batches | lr 0.0000 | ms/batch 396.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2200/3081 batches | lr 0.0000 | ms/batch 395.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2300/3081 batches | lr 0.0000 | ms/batch 395.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2400/3081 batches | lr 0.0000 | ms/batch 395.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2500/3081 batches | lr 0.0000 | ms/batch 395.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2600/3081 batches | lr 0.0000 | ms/batch 395.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2700/3081 batches | lr 0.0000 | ms/batch 395.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2800/3081 batches | lr 0.0000 | ms/batch 395.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2900/3081 batches | lr 0.0000 | ms/batch 399.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3000/3081 batches | lr 0.0000 | ms/batch 396.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 1274.80s | valid loss/mse 0.1752 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/3081 batches | lr 0.0000 | ms/batch 399.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 200/3081 batches | lr 0.0000 | ms/batch 396.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 300/3081 batches | lr 0.0000 | ms/batch 396.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 400/3081 batches | lr 0.0000 | ms/batch 396.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 500/3081 batches | lr 0.0000 | ms/batch 396.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 600/3081 batches | lr 0.0000 | ms/batch 396.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 700/3081 batches | lr 0.0000 | ms/batch 397.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 800/3081 batches | lr 0.0000 | ms/batch 397.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 900/3081 batches | lr 0.0000 | ms/batch 396.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1000/3081 batches | lr 0.0000 | ms/batch 396.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1100/3081 batches | lr 0.0000 | ms/batch 396.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1200/3081 batches | lr 0.0000 | ms/batch 396.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1300/3081 batches | lr 0.0000 | ms/batch 396.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1400/3081 batches | lr 0.0000 | ms/batch 396.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1500/3081 batches | lr 0.0000 | ms/batch 396.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1600/3081 batches | lr 0.0000 | ms/batch 396.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1700/3081 batches | lr 0.0000 | ms/batch 397.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1800/3081 batches | lr 0.0000 | ms/batch 396.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1900/3081 batches | lr 0.0000 | ms/batch 400.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2000/3081 batches | lr 0.0000 | ms/batch 397.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2100/3081 batches | lr 0.0000 | ms/batch 396.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2200/3081 batches | lr 0.0000 | ms/batch 396.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2300/3081 batches | lr 0.0000 | ms/batch 396.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2400/3081 batches | lr 0.0000 | ms/batch 397.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2500/3081 batches | lr 0.0000 | ms/batch 396.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2600/3081 batches | lr 0.0000 | ms/batch 396.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2700/3081 batches | lr 0.0000 | ms/batch 396.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2800/3081 batches | lr 0.0000 | ms/batch 396.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2900/3081 batches | lr 0.0000 | ms/batch 396.93 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 3000/3081 batches | lr 0.0000 | ms/batch 396.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 1277.98s | valid loss/mse 0.1754 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/3081 batches | lr 0.0000 | ms/batch 402.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 200/3081 batches | lr 0.0000 | ms/batch 397.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 300/3081 batches | lr 0.0000 | ms/batch 396.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 400/3081 batches | lr 0.0000 | ms/batch 396.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 500/3081 batches | lr 0.0000 | ms/batch 396.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 600/3081 batches | lr 0.0000 | ms/batch 396.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 700/3081 batches | lr 0.0000 | ms/batch 396.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 800/3081 batches | lr 0.0000 | ms/batch 396.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 900/3081 batches | lr 0.0000 | ms/batch 396.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1000/3081 batches | lr 0.0000 | ms/batch 396.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1100/3081 batches | lr 0.0000 | ms/batch 397.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1200/3081 batches | lr 0.0000 | ms/batch 396.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1300/3081 batches | lr 0.0000 | ms/batch 396.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1400/3081 batches | lr 0.0000 | ms/batch 396.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1500/3081 batches | lr 0.0000 | ms/batch 396.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1600/3081 batches | lr 0.0000 | ms/batch 396.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1700/3081 batches | lr 0.0000 | ms/batch 396.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1800/3081 batches | lr 0.0000 | ms/batch 396.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1900/3081 batches | lr 0.0000 | ms/batch 396.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2000/3081 batches | lr 0.0000 | ms/batch 396.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2100/3081 batches | lr 0.0000 | ms/batch 396.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2200/3081 batches | lr 0.0000 | ms/batch 397.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2300/3081 batches | lr 0.0000 | ms/batch 398.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2400/3081 batches | lr 0.0000 | ms/batch 396.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2500/3081 batches | lr 0.0000 | ms/batch 397.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2600/3081 batches | lr 0.0000 | ms/batch 397.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2700/3081 batches | lr 0.0000 | ms/batch 396.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2800/3081 batches | lr 0.0000 | ms/batch 396.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2900/3081 batches | lr 0.0000 | ms/batch 397.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 3000/3081 batches | lr 0.0000 | ms/batch 397.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 1278.72s | valid loss/mse 0.1750 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 12
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_rpe1_essential/scgpt/split2
scGPT - INFO - Running on 2024-07-28 03:46:15
scGPT - INFO - match 5645/5753 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/3188 batches | lr 0.0001 | ms/batch 400.38 | loss  0.32 | mse  0.32 |
scGPT - INFO - | epoch   1 | 200/3188 batches | lr 0.0001 | ms/batch 393.33 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 300/3188 batches | lr 0.0001 | ms/batch 393.43 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 400/3188 batches | lr 0.0001 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 500/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 600/3188 batches | lr 0.0001 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 700/3188 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 800/3188 batches | lr 0.0001 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 900/3188 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1000/3188 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1100/3188 batches | lr 0.0001 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1200/3188 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1300/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1400/3188 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1500/3188 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1600/3188 batches | lr 0.0001 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1700/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1800/3188 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1900/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2000/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2100/3188 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2200/3188 batches | lr 0.0001 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2300/3188 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2400/3188 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2500/3188 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2600/3188 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2700/3188 batches | lr 0.0001 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2800/3188 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2900/3188 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3000/3188 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3100/3188 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 1304.60s | valid loss/mse 0.1739 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1739
scGPT - INFO - | epoch   2 | 100/3188 batches | lr 0.0001 | ms/batch 399.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 200/3188 batches | lr 0.0001 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 300/3188 batches | lr 0.0001 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 400/3188 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 500/3188 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 600/3188 batches | lr 0.0001 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 700/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 800/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 900/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1000/3188 batches | lr 0.0001 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1100/3188 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1200/3188 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1300/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1400/3188 batches | lr 0.0001 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1500/3188 batches | lr 0.0001 | ms/batch 393.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1600/3188 batches | lr 0.0001 | ms/batch 393.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1700/3188 batches | lr 0.0001 | ms/batch 393.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1800/3188 batches | lr 0.0001 | ms/batch 393.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1900/3188 batches | lr 0.0001 | ms/batch 393.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2000/3188 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2100/3188 batches | lr 0.0001 | ms/batch 393.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2200/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2300/3188 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2400/3188 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2500/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2600/3188 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2700/3188 batches | lr 0.0001 | ms/batch 393.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2800/3188 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2900/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3000/3188 batches | lr 0.0001 | ms/batch 393.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3100/3188 batches | lr 0.0001 | ms/batch 393.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 1303.24s | valid loss/mse 0.1754 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/3188 batches | lr 0.0001 | ms/batch 397.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 200/3188 batches | lr 0.0001 | ms/batch 393.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 300/3188 batches | lr 0.0001 | ms/batch 393.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 400/3188 batches | lr 0.0001 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 500/3188 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 600/3188 batches | lr 0.0001 | ms/batch 393.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 700/3188 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 800/3188 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 900/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1000/3188 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1100/3188 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1200/3188 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1300/3188 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1400/3188 batches | lr 0.0001 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1500/3188 batches | lr 0.0001 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1600/3188 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1700/3188 batches | lr 0.0001 | ms/batch 393.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1800/3188 batches | lr 0.0001 | ms/batch 393.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1900/3188 batches | lr 0.0001 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2000/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2100/3188 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2200/3188 batches | lr 0.0001 | ms/batch 393.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2300/3188 batches | lr 0.0001 | ms/batch 393.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2400/3188 batches | lr 0.0001 | ms/batch 393.93 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2500/3188 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2600/3188 batches | lr 0.0001 | ms/batch 393.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2700/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2800/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2900/3188 batches | lr 0.0001 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3000/3188 batches | lr 0.0001 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3100/3188 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 1303.19s | valid loss/mse 0.1742 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/3188 batches | lr 0.0001 | ms/batch 397.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 200/3188 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 300/3188 batches | lr 0.0001 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 400/3188 batches | lr 0.0001 | ms/batch 393.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 500/3188 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 600/3188 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 700/3188 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 800/3188 batches | lr 0.0001 | ms/batch 393.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 900/3188 batches | lr 0.0001 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1000/3188 batches | lr 0.0001 | ms/batch 393.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1100/3188 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1200/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1300/3188 batches | lr 0.0001 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1400/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1500/3188 batches | lr 0.0001 | ms/batch 393.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1600/3188 batches | lr 0.0001 | ms/batch 393.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1700/3188 batches | lr 0.0001 | ms/batch 393.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1800/3188 batches | lr 0.0001 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1900/3188 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2000/3188 batches | lr 0.0001 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2100/3188 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2200/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2300/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2400/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2500/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2600/3188 batches | lr 0.0001 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2700/3188 batches | lr 0.0001 | ms/batch 393.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2800/3188 batches | lr 0.0001 | ms/batch 393.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2900/3188 batches | lr 0.0001 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3000/3188 batches | lr 0.0001 | ms/batch 393.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3100/3188 batches | lr 0.0001 | ms/batch 393.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 1303.07s | valid loss/mse 0.1743 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/3188 batches | lr 0.0001 | ms/batch 398.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 200/3188 batches | lr 0.0001 | ms/batch 393.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 300/3188 batches | lr 0.0001 | ms/batch 393.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 400/3188 batches | lr 0.0001 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 500/3188 batches | lr 0.0001 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 600/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 700/3188 batches | lr 0.0001 | ms/batch 393.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 800/3188 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 900/3188 batches | lr 0.0001 | ms/batch 393.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1000/3188 batches | lr 0.0001 | ms/batch 393.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1100/3188 batches | lr 0.0001 | ms/batch 393.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1200/3188 batches | lr 0.0001 | ms/batch 393.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1300/3188 batches | lr 0.0001 | ms/batch 393.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1400/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1500/3188 batches | lr 0.0001 | ms/batch 393.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1600/3188 batches | lr 0.0001 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1700/3188 batches | lr 0.0001 | ms/batch 393.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1800/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1900/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2000/3188 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2100/3188 batches | lr 0.0001 | ms/batch 393.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2200/3188 batches | lr 0.0001 | ms/batch 393.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2300/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2400/3188 batches | lr 0.0001 | ms/batch 393.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2500/3188 batches | lr 0.0001 | ms/batch 393.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2600/3188 batches | lr 0.0001 | ms/batch 393.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2700/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2800/3188 batches | lr 0.0001 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2900/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3000/3188 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3100/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 1302.86s | valid loss/mse 0.1738 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1738
scGPT - INFO - | epoch   6 | 100/3188 batches | lr 0.0001 | ms/batch 397.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 200/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 300/3188 batches | lr 0.0001 | ms/batch 393.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 400/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 500/3188 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 600/3188 batches | lr 0.0001 | ms/batch 393.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 700/3188 batches | lr 0.0001 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 800/3188 batches | lr 0.0001 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 900/3188 batches | lr 0.0001 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1000/3188 batches | lr 0.0001 | ms/batch 393.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1100/3188 batches | lr 0.0001 | ms/batch 393.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1200/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1300/3188 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1400/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1500/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1600/3188 batches | lr 0.0001 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1700/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1800/3188 batches | lr 0.0001 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1900/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2000/3188 batches | lr 0.0001 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2100/3188 batches | lr 0.0001 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2200/3188 batches | lr 0.0001 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2300/3188 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2400/3188 batches | lr 0.0001 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2500/3188 batches | lr 0.0001 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2600/3188 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2700/3188 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2800/3188 batches | lr 0.0001 | ms/batch 393.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2900/3188 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3000/3188 batches | lr 0.0001 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3100/3188 batches | lr 0.0001 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 1303.20s | valid loss/mse 0.1745 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/3188 batches | lr 0.0001 | ms/batch 397.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 200/3188 batches | lr 0.0001 | ms/batch 393.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 300/3188 batches | lr 0.0001 | ms/batch 393.71 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 400/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 500/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 600/3188 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 700/3188 batches | lr 0.0001 | ms/batch 394.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 800/3188 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 900/3188 batches | lr 0.0001 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1000/3188 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1100/3188 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1200/3188 batches | lr 0.0001 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1300/3188 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1400/3188 batches | lr 0.0001 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1500/3188 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1600/3188 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1700/3188 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1800/3188 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1900/3188 batches | lr 0.0001 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2000/3188 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2100/3188 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2200/3188 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2300/3188 batches | lr 0.0001 | ms/batch 393.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2400/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2500/3188 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2600/3188 batches | lr 0.0001 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2700/3188 batches | lr 0.0001 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2800/3188 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2900/3188 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3000/3188 batches | lr 0.0001 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3100/3188 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 1303.58s | valid loss/mse 0.1737 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1737
scGPT - INFO - | epoch   8 | 100/3188 batches | lr 0.0000 | ms/batch 398.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 200/3188 batches | lr 0.0000 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 300/3188 batches | lr 0.0000 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 400/3188 batches | lr 0.0000 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 500/3188 batches | lr 0.0000 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 600/3188 batches | lr 0.0000 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 700/3188 batches | lr 0.0000 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 800/3188 batches | lr 0.0000 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 900/3188 batches | lr 0.0000 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1000/3188 batches | lr 0.0000 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1100/3188 batches | lr 0.0000 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1200/3188 batches | lr 0.0000 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1300/3188 batches | lr 0.0000 | ms/batch 393.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1400/3188 batches | lr 0.0000 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1500/3188 batches | lr 0.0000 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1600/3188 batches | lr 0.0000 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1700/3188 batches | lr 0.0000 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1800/3188 batches | lr 0.0000 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1900/3188 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2000/3188 batches | lr 0.0000 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2100/3188 batches | lr 0.0000 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2200/3188 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2300/3188 batches | lr 0.0000 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2400/3188 batches | lr 0.0000 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2500/3188 batches | lr 0.0000 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2600/3188 batches | lr 0.0000 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2700/3188 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2800/3188 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2900/3188 batches | lr 0.0000 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3000/3188 batches | lr 0.0000 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3100/3188 batches | lr 0.0000 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 1303.94s | valid loss/mse 0.1747 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/3188 batches | lr 0.0000 | ms/batch 399.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 200/3188 batches | lr 0.0000 | ms/batch 394.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 300/3188 batches | lr 0.0000 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 400/3188 batches | lr 0.0000 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 500/3188 batches | lr 0.0000 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 600/3188 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 700/3188 batches | lr 0.0000 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 800/3188 batches | lr 0.0000 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 900/3188 batches | lr 0.0000 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1000/3188 batches | lr 0.0000 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1100/3188 batches | lr 0.0000 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1200/3188 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1300/3188 batches | lr 0.0000 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1400/3188 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1500/3188 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1600/3188 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1700/3188 batches | lr 0.0000 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1800/3188 batches | lr 0.0000 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1900/3188 batches | lr 0.0000 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2000/3188 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2100/3188 batches | lr 0.0000 | ms/batch 393.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2200/3188 batches | lr 0.0000 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2300/3188 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2400/3188 batches | lr 0.0000 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2500/3188 batches | lr 0.0000 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2600/3188 batches | lr 0.0000 | ms/batch 393.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2700/3188 batches | lr 0.0000 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2800/3188 batches | lr 0.0000 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2900/3188 batches | lr 0.0000 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3000/3188 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3100/3188 batches | lr 0.0000 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 1304.19s | valid loss/mse 0.1754 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/3188 batches | lr 0.0000 | ms/batch 398.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 200/3188 batches | lr 0.0000 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 300/3188 batches | lr 0.0000 | ms/batch 394.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 400/3188 batches | lr 0.0000 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 500/3188 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 600/3188 batches | lr 0.0000 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 700/3188 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 800/3188 batches | lr 0.0000 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 900/3188 batches | lr 0.0000 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1000/3188 batches | lr 0.0000 | ms/batch 394.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1100/3188 batches | lr 0.0000 | ms/batch 394.34 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1200/3188 batches | lr 0.0000 | ms/batch 394.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1300/3188 batches | lr 0.0000 | ms/batch 394.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1400/3188 batches | lr 0.0000 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1500/3188 batches | lr 0.0000 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1600/3188 batches | lr 0.0000 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1700/3188 batches | lr 0.0000 | ms/batch 394.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1800/3188 batches | lr 0.0000 | ms/batch 394.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1900/3188 batches | lr 0.0000 | ms/batch 394.34 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2000/3188 batches | lr 0.0000 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2100/3188 batches | lr 0.0000 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2200/3188 batches | lr 0.0000 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2300/3188 batches | lr 0.0000 | ms/batch 394.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2400/3188 batches | lr 0.0000 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2500/3188 batches | lr 0.0000 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2600/3188 batches | lr 0.0000 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2700/3188 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2800/3188 batches | lr 0.0000 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2900/3188 batches | lr 0.0000 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3000/3188 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3100/3188 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 1304.65s | valid loss/mse 0.1737 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/3188 batches | lr 0.0000 | ms/batch 398.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 200/3188 batches | lr 0.0000 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 300/3188 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 400/3188 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 500/3188 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 600/3188 batches | lr 0.0000 | ms/batch 394.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 700/3188 batches | lr 0.0000 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 800/3188 batches | lr 0.0000 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 900/3188 batches | lr 0.0000 | ms/batch 394.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1000/3188 batches | lr 0.0000 | ms/batch 394.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1100/3188 batches | lr 0.0000 | ms/batch 394.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1200/3188 batches | lr 0.0000 | ms/batch 394.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1300/3188 batches | lr 0.0000 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1400/3188 batches | lr 0.0000 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1500/3188 batches | lr 0.0000 | ms/batch 394.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1600/3188 batches | lr 0.0000 | ms/batch 394.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1700/3188 batches | lr 0.0000 | ms/batch 394.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1800/3188 batches | lr 0.0000 | ms/batch 394.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1900/3188 batches | lr 0.0000 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2000/3188 batches | lr 0.0000 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2100/3188 batches | lr 0.0000 | ms/batch 394.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2200/3188 batches | lr 0.0000 | ms/batch 395.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2300/3188 batches | lr 0.0000 | ms/batch 394.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2400/3188 batches | lr 0.0000 | ms/batch 394.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2500/3188 batches | lr 0.0000 | ms/batch 394.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2600/3188 batches | lr 0.0000 | ms/batch 395.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2700/3188 batches | lr 0.0000 | ms/batch 395.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2800/3188 batches | lr 0.0000 | ms/batch 395.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2900/3188 batches | lr 0.0000 | ms/batch 395.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 3000/3188 batches | lr 0.0000 | ms/batch 395.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 3100/3188 batches | lr 0.0000 | ms/batch 395.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 1306.34s | valid loss/mse 0.1746 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/3188 batches | lr 0.0000 | ms/batch 399.34 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 200/3188 batches | lr 0.0000 | ms/batch 395.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 300/3188 batches | lr 0.0000 | ms/batch 395.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 400/3188 batches | lr 0.0000 | ms/batch 394.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 500/3188 batches | lr 0.0000 | ms/batch 394.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 600/3188 batches | lr 0.0000 | ms/batch 394.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 700/3188 batches | lr 0.0000 | ms/batch 394.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 800/3188 batches | lr 0.0000 | ms/batch 395.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 900/3188 batches | lr 0.0000 | ms/batch 395.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1000/3188 batches | lr 0.0000 | ms/batch 395.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1100/3188 batches | lr 0.0000 | ms/batch 395.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1200/3188 batches | lr 0.0000 | ms/batch 394.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1300/3188 batches | lr 0.0000 | ms/batch 394.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1400/3188 batches | lr 0.0000 | ms/batch 394.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1500/3188 batches | lr 0.0000 | ms/batch 394.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1600/3188 batches | lr 0.0000 | ms/batch 395.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1700/3188 batches | lr 0.0000 | ms/batch 395.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1800/3188 batches | lr 0.0000 | ms/batch 394.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1900/3188 batches | lr 0.0000 | ms/batch 394.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2000/3188 batches | lr 0.0000 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2100/3188 batches | lr 0.0000 | ms/batch 395.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2200/3188 batches | lr 0.0000 | ms/batch 394.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2300/3188 batches | lr 0.0000 | ms/batch 394.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2400/3188 batches | lr 0.0000 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2500/3188 batches | lr 0.0000 | ms/batch 395.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2600/3188 batches | lr 0.0000 | ms/batch 395.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2700/3188 batches | lr 0.0000 | ms/batch 395.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2800/3188 batches | lr 0.0000 | ms/batch 395.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2900/3188 batches | lr 0.0000 | ms/batch 395.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 3000/3188 batches | lr 0.0000 | ms/batch 395.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 3100/3188 batches | lr 0.0000 | ms/batch 395.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 1307.02s | valid loss/mse 0.1741 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 12
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_rpe1_essential/scgpt/split3
scGPT - INFO - Running on 2024-07-28 09:51:18
scGPT - INFO - match 5645/5753 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/3107 batches | lr 0.0001 | ms/batch 404.10 | loss  0.30 | mse  0.30 |
scGPT - INFO - | epoch   1 | 200/3107 batches | lr 0.0001 | ms/batch 393.95 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 300/3107 batches | lr 0.0001 | ms/batch 393.87 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 400/3107 batches | lr 0.0001 | ms/batch 393.83 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 500/3107 batches | lr 0.0001 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 600/3107 batches | lr 0.0001 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 700/3107 batches | lr 0.0001 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 800/3107 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 900/3107 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1000/3107 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1100/3107 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1200/3107 batches | lr 0.0001 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1300/3107 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1400/3107 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1500/3107 batches | lr 0.0001 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1600/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1700/3107 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1800/3107 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1900/3107 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2000/3107 batches | lr 0.0001 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2100/3107 batches | lr 0.0001 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2200/3107 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2300/3107 batches | lr 0.0001 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2400/3107 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2500/3107 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2600/3107 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2700/3107 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2800/3107 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2900/3107 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3000/3107 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3100/3107 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 1287.48s | valid loss/mse 0.1726 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1726
scGPT - INFO - | epoch   2 | 100/3107 batches | lr 0.0001 | ms/batch 398.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 200/3107 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 300/3107 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 400/3107 batches | lr 0.0001 | ms/batch 394.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 500/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 600/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 700/3107 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 800/3107 batches | lr 0.0001 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 900/3107 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1000/3107 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1100/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1200/3107 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1300/3107 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1400/3107 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1500/3107 batches | lr 0.0001 | ms/batch 394.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1600/3107 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1700/3107 batches | lr 0.0001 | ms/batch 394.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1800/3107 batches | lr 0.0001 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1900/3107 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2000/3107 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2100/3107 batches | lr 0.0001 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2200/3107 batches | lr 0.0001 | ms/batch 394.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2300/3107 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2400/3107 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2500/3107 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2600/3107 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2700/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2800/3107 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2900/3107 batches | lr 0.0001 | ms/batch 394.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3000/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3100/3107 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 1287.17s | valid loss/mse 0.1754 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/3107 batches | lr 0.0001 | ms/batch 398.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 200/3107 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 300/3107 batches | lr 0.0001 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 400/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 500/3107 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 600/3107 batches | lr 0.0001 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 700/3107 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 800/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 900/3107 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1000/3107 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1100/3107 batches | lr 0.0001 | ms/batch 395.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1200/3107 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1300/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1400/3107 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1500/3107 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1600/3107 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1700/3107 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1800/3107 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1900/3107 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2000/3107 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2100/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2200/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2300/3107 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2400/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2500/3107 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2600/3107 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2700/3107 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2800/3107 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2900/3107 batches | lr 0.0001 | ms/batch 394.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3000/3107 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3100/3107 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 1287.15s | valid loss/mse 0.1740 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/3107 batches | lr 0.0001 | ms/batch 398.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 200/3107 batches | lr 0.0001 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 300/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 400/3107 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 500/3107 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 600/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 700/3107 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 800/3107 batches | lr 0.0001 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 900/3107 batches | lr 0.0001 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1000/3107 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1100/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1200/3107 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1300/3107 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1400/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1500/3107 batches | lr 0.0001 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1600/3107 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1700/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1800/3107 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1900/3107 batches | lr 0.0001 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2000/3107 batches | lr 0.0001 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2100/3107 batches | lr 0.0001 | ms/batch 394.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2200/3107 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2300/3107 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2400/3107 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2500/3107 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2600/3107 batches | lr 0.0001 | ms/batch 394.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2700/3107 batches | lr 0.0001 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2800/3107 batches | lr 0.0001 | ms/batch 394.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2900/3107 batches | lr 0.0001 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3000/3107 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3100/3107 batches | lr 0.0001 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 1287.42s | valid loss/mse 0.1727 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/3107 batches | lr 0.0001 | ms/batch 398.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 200/3107 batches | lr 0.0001 | ms/batch 394.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 300/3107 batches | lr 0.0001 | ms/batch 394.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 400/3107 batches | lr 0.0001 | ms/batch 394.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 500/3107 batches | lr 0.0001 | ms/batch 394.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 600/3107 batches | lr 0.0001 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 700/3107 batches | lr 0.0001 | ms/batch 394.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 800/3107 batches | lr 0.0001 | ms/batch 394.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 900/3107 batches | lr 0.0001 | ms/batch 394.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1000/3107 batches | lr 0.0001 | ms/batch 394.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1100/3107 batches | lr 0.0001 | ms/batch 394.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1200/3107 batches | lr 0.0001 | ms/batch 394.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1300/3107 batches | lr 0.0001 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1400/3107 batches | lr 0.0001 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1500/3107 batches | lr 0.0001 | ms/batch 394.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1600/3107 batches | lr 0.0001 | ms/batch 394.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1700/3107 batches | lr 0.0001 | ms/batch 395.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1800/3107 batches | lr 0.0001 | ms/batch 394.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1900/3107 batches | lr 0.0001 | ms/batch 394.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2000/3107 batches | lr 0.0001 | ms/batch 394.64 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2100/3107 batches | lr 0.0001 | ms/batch 394.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2200/3107 batches | lr 0.0001 | ms/batch 394.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2300/3107 batches | lr 0.0001 | ms/batch 394.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2400/3107 batches | lr 0.0001 | ms/batch 394.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2500/3107 batches | lr 0.0001 | ms/batch 394.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2600/3107 batches | lr 0.0001 | ms/batch 394.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2700/3107 batches | lr 0.0001 | ms/batch 394.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2800/3107 batches | lr 0.0001 | ms/batch 394.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2900/3107 batches | lr 0.0001 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3000/3107 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3100/3107 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 1288.42s | valid loss/mse 0.1722 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1722
scGPT - INFO - | epoch   6 | 100/3107 batches | lr 0.0001 | ms/batch 398.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 200/3107 batches | lr 0.0001 | ms/batch 394.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 300/3107 batches | lr 0.0001 | ms/batch 394.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 400/3107 batches | lr 0.0001 | ms/batch 394.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 500/3107 batches | lr 0.0001 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 600/3107 batches | lr 0.0001 | ms/batch 394.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 700/3107 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 800/3107 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 900/3107 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1000/3107 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1100/3107 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1200/3107 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1300/3107 batches | lr 0.0001 | ms/batch 395.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1400/3107 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1500/3107 batches | lr 0.0001 | ms/batch 394.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1600/3107 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1700/3107 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1800/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1900/3107 batches | lr 0.0001 | ms/batch 394.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2000/3107 batches | lr 0.0001 | ms/batch 394.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2100/3107 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2200/3107 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2300/3107 batches | lr 0.0001 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2400/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2500/3107 batches | lr 0.0001 | ms/batch 394.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2600/3107 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2700/3107 batches | lr 0.0001 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2800/3107 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2900/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3000/3107 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3100/3107 batches | lr 0.0001 | ms/batch 395.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 1287.70s | valid loss/mse 0.1725 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/3107 batches | lr 0.0001 | ms/batch 398.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 200/3107 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 300/3107 batches | lr 0.0001 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 400/3107 batches | lr 0.0001 | ms/batch 394.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 500/3107 batches | lr 0.0001 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 600/3107 batches | lr 0.0001 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 700/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 800/3107 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 900/3107 batches | lr 0.0001 | ms/batch 394.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1000/3107 batches | lr 0.0001 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1100/3107 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1200/3107 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1300/3107 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1400/3107 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1500/3107 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1600/3107 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1700/3107 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1800/3107 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1900/3107 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2000/3107 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2100/3107 batches | lr 0.0001 | ms/batch 394.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2200/3107 batches | lr 0.0001 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2300/3107 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2400/3107 batches | lr 0.0001 | ms/batch 394.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2500/3107 batches | lr 0.0001 | ms/batch 394.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2600/3107 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2700/3107 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2800/3107 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2900/3107 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3000/3107 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3100/3107 batches | lr 0.0001 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 1287.20s | valid loss/mse 0.1730 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/3107 batches | lr 0.0000 | ms/batch 398.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 200/3107 batches | lr 0.0000 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 300/3107 batches | lr 0.0000 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 400/3107 batches | lr 0.0000 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 500/3107 batches | lr 0.0000 | ms/batch 394.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 600/3107 batches | lr 0.0000 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 700/3107 batches | lr 0.0000 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 800/3107 batches | lr 0.0000 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 900/3107 batches | lr 0.0000 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1000/3107 batches | lr 0.0000 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1100/3107 batches | lr 0.0000 | ms/batch 394.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1200/3107 batches | lr 0.0000 | ms/batch 394.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1300/3107 batches | lr 0.0000 | ms/batch 394.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1400/3107 batches | lr 0.0000 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1500/3107 batches | lr 0.0000 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1600/3107 batches | lr 0.0000 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1700/3107 batches | lr 0.0000 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1800/3107 batches | lr 0.0000 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1900/3107 batches | lr 0.0000 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2000/3107 batches | lr 0.0000 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2100/3107 batches | lr 0.0000 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2200/3107 batches | lr 0.0000 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2300/3107 batches | lr 0.0000 | ms/batch 394.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2400/3107 batches | lr 0.0000 | ms/batch 394.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2500/3107 batches | lr 0.0000 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2600/3107 batches | lr 0.0000 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2700/3107 batches | lr 0.0000 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2800/3107 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2900/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3000/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3100/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 1287.43s | valid loss/mse 0.1732 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/3107 batches | lr 0.0000 | ms/batch 398.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 200/3107 batches | lr 0.0000 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 300/3107 batches | lr 0.0000 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 400/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 500/3107 batches | lr 0.0000 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 600/3107 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 700/3107 batches | lr 0.0000 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 800/3107 batches | lr 0.0000 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 900/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1000/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1100/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1200/3107 batches | lr 0.0000 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1300/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1400/3107 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1500/3107 batches | lr 0.0000 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1600/3107 batches | lr 0.0000 | ms/batch 394.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1700/3107 batches | lr 0.0000 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1800/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1900/3107 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2000/3107 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2100/3107 batches | lr 0.0000 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2200/3107 batches | lr 0.0000 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2300/3107 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2400/3107 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2500/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2600/3107 batches | lr 0.0000 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2700/3107 batches | lr 0.0000 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2800/3107 batches | lr 0.0000 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2900/3107 batches | lr 0.0000 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3000/3107 batches | lr 0.0000 | ms/batch 394.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3100/3107 batches | lr 0.0000 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 1286.87s | valid loss/mse 0.1720 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1720
scGPT - INFO - | epoch  10 | 100/3107 batches | lr 0.0000 | ms/batch 398.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 200/3107 batches | lr 0.0000 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 300/3107 batches | lr 0.0000 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 400/3107 batches | lr 0.0000 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 500/3107 batches | lr 0.0000 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 600/3107 batches | lr 0.0000 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 700/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 800/3107 batches | lr 0.0000 | ms/batch 393.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 900/3107 batches | lr 0.0000 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1000/3107 batches | lr 0.0000 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1100/3107 batches | lr 0.0000 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1200/3107 batches | lr 0.0000 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1300/3107 batches | lr 0.0000 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1400/3107 batches | lr 0.0000 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1500/3107 batches | lr 0.0000 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1600/3107 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1700/3107 batches | lr 0.0000 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1800/3107 batches | lr 0.0000 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1900/3107 batches | lr 0.0000 | ms/batch 393.93 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2000/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2100/3107 batches | lr 0.0000 | ms/batch 394.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2200/3107 batches | lr 0.0000 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2300/3107 batches | lr 0.0000 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2400/3107 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2500/3107 batches | lr 0.0000 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2600/3107 batches | lr 0.0000 | ms/batch 394.20 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2700/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2800/3107 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2900/3107 batches | lr 0.0000 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3000/3107 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3100/3107 batches | lr 0.0000 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 1286.49s | valid loss/mse 0.1726 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/3107 batches | lr 0.0000 | ms/batch 398.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 200/3107 batches | lr 0.0000 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 300/3107 batches | lr 0.0000 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 400/3107 batches | lr 0.0000 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 500/3107 batches | lr 0.0000 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 600/3107 batches | lr 0.0000 | ms/batch 393.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 700/3107 batches | lr 0.0000 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 800/3107 batches | lr 0.0000 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 900/3107 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1000/3107 batches | lr 0.0000 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1100/3107 batches | lr 0.0000 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1200/3107 batches | lr 0.0000 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1300/3107 batches | lr 0.0000 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1400/3107 batches | lr 0.0000 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1500/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1600/3107 batches | lr 0.0000 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1700/3107 batches | lr 0.0000 | ms/batch 394.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1800/3107 batches | lr 0.0000 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 1900/3107 batches | lr 0.0000 | ms/batch 394.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2000/3107 batches | lr 0.0000 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2100/3107 batches | lr 0.0000 | ms/batch 394.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2200/3107 batches | lr 0.0000 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2300/3107 batches | lr 0.0000 | ms/batch 393.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2400/3107 batches | lr 0.0000 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2500/3107 batches | lr 0.0000 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2600/3107 batches | lr 0.0000 | ms/batch 393.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2700/3107 batches | lr 0.0000 | ms/batch 393.88 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2800/3107 batches | lr 0.0000 | ms/batch 393.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 2900/3107 batches | lr 0.0000 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 3000/3107 batches | lr 0.0000 | ms/batch 393.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  11 | 3100/3107 batches | lr 0.0000 | ms/batch 393.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 1286.38s | valid loss/mse 0.1721 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/3107 batches | lr 0.0000 | ms/batch 398.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 200/3107 batches | lr 0.0000 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 300/3107 batches | lr 0.0000 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 400/3107 batches | lr 0.0000 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 500/3107 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 600/3107 batches | lr 0.0000 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 700/3107 batches | lr 0.0000 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 800/3107 batches | lr 0.0000 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 900/3107 batches | lr 0.0000 | ms/batch 394.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1000/3107 batches | lr 0.0000 | ms/batch 394.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1100/3107 batches | lr 0.0000 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1200/3107 batches | lr 0.0000 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1300/3107 batches | lr 0.0000 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1400/3107 batches | lr 0.0000 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1500/3107 batches | lr 0.0000 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1600/3107 batches | lr 0.0000 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1700/3107 batches | lr 0.0000 | ms/batch 394.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1800/3107 batches | lr 0.0000 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 1900/3107 batches | lr 0.0000 | ms/batch 394.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2000/3107 batches | lr 0.0000 | ms/batch 394.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2100/3107 batches | lr 0.0000 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2200/3107 batches | lr 0.0000 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2300/3107 batches | lr 0.0000 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2400/3107 batches | lr 0.0000 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2500/3107 batches | lr 0.0000 | ms/batch 394.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2600/3107 batches | lr 0.0000 | ms/batch 394.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2700/3107 batches | lr 0.0000 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2800/3107 batches | lr 0.0000 | ms/batch 394.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 2900/3107 batches | lr 0.0000 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 3000/3107 batches | lr 0.0000 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  12 | 3100/3107 batches | lr 0.0000 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 1287.47s | valid loss/mse 0.1720 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/3107 batches | lr 0.0000 | ms/batch 399.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 200/3107 batches | lr 0.0000 | ms/batch 395.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 300/3107 batches | lr 0.0000 | ms/batch 394.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 400/3107 batches | lr 0.0000 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 500/3107 batches | lr 0.0000 | ms/batch 394.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 600/3107 batches | lr 0.0000 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 700/3107 batches | lr 0.0000 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 800/3107 batches | lr 0.0000 | ms/batch 394.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 900/3107 batches | lr 0.0000 | ms/batch 394.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1000/3107 batches | lr 0.0000 | ms/batch 394.63 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1100/3107 batches | lr 0.0000 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1200/3107 batches | lr 0.0000 | ms/batch 394.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1300/3107 batches | lr 0.0000 | ms/batch 394.58 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1400/3107 batches | lr 0.0000 | ms/batch 395.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1500/3107 batches | lr 0.0000 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1600/3107 batches | lr 0.0000 | ms/batch 394.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1700/3107 batches | lr 0.0000 | ms/batch 394.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1800/3107 batches | lr 0.0000 | ms/batch 394.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 1900/3107 batches | lr 0.0000 | ms/batch 394.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2000/3107 batches | lr 0.0000 | ms/batch 394.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2100/3107 batches | lr 0.0000 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2200/3107 batches | lr 0.0000 | ms/batch 394.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2300/3107 batches | lr 0.0000 | ms/batch 394.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2400/3107 batches | lr 0.0000 | ms/batch 394.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2500/3107 batches | lr 0.0000 | ms/batch 394.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2600/3107 batches | lr 0.0000 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2700/3107 batches | lr 0.0000 | ms/batch 394.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2800/3107 batches | lr 0.0000 | ms/batch 394.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 2900/3107 batches | lr 0.0000 | ms/batch 394.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 3000/3107 batches | lr 0.0000 | ms/batch 394.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  13 | 3100/3107 batches | lr 0.0000 | ms/batch 395.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 1288.92s | valid loss/mse 0.1725 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/3107 batches | lr 0.0000 | ms/batch 398.93 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 200/3107 batches | lr 0.0000 | ms/batch 395.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 300/3107 batches | lr 0.0000 | ms/batch 395.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 400/3107 batches | lr 0.0000 | ms/batch 395.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 500/3107 batches | lr 0.0000 | ms/batch 394.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 600/3107 batches | lr 0.0000 | ms/batch 395.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 700/3107 batches | lr 0.0000 | ms/batch 395.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 800/3107 batches | lr 0.0000 | ms/batch 394.93 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 900/3107 batches | lr 0.0000 | ms/batch 394.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1000/3107 batches | lr 0.0000 | ms/batch 394.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1100/3107 batches | lr 0.0000 | ms/batch 394.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1200/3107 batches | lr 0.0000 | ms/batch 395.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1300/3107 batches | lr 0.0000 | ms/batch 395.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1400/3107 batches | lr 0.0000 | ms/batch 395.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1500/3107 batches | lr 0.0000 | ms/batch 394.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1600/3107 batches | lr 0.0000 | ms/batch 394.91 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1700/3107 batches | lr 0.0000 | ms/batch 394.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1800/3107 batches | lr 0.0000 | ms/batch 395.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 1900/3107 batches | lr 0.0000 | ms/batch 394.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2000/3107 batches | lr 0.0000 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2100/3107 batches | lr 0.0000 | ms/batch 394.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2200/3107 batches | lr 0.0000 | ms/batch 394.97 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2300/3107 batches | lr 0.0000 | ms/batch 394.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2400/3107 batches | lr 0.0000 | ms/batch 395.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2500/3107 batches | lr 0.0000 | ms/batch 394.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2600/3107 batches | lr 0.0000 | ms/batch 394.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2700/3107 batches | lr 0.0000 | ms/batch 395.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2800/3107 batches | lr 0.0000 | ms/batch 395.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 2900/3107 batches | lr 0.0000 | ms/batch 395.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 3000/3107 batches | lr 0.0000 | ms/batch 395.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  14 | 3100/3107 batches | lr 0.0000 | ms/batch 394.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 1289.77s | valid loss/mse 0.1726 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 14
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_rpe1_essential/scgpt/split4
scGPT - INFO - Running on 2024-07-28 16:35:11
scGPT - INFO - match 5645/5753 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/3101 batches | lr 0.0001 | ms/batch 402.94 | loss  0.33 | mse  0.33 |
scGPT - INFO - | epoch   1 | 200/3101 batches | lr 0.0001 | ms/batch 394.06 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 300/3101 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 400/3101 batches | lr 0.0001 | ms/batch 394.11 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 500/3101 batches | lr 0.0001 | ms/batch 393.80 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 600/3101 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 700/3101 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 800/3101 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 900/3101 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1000/3101 batches | lr 0.0001 | ms/batch 394.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1100/3101 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1200/3101 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1300/3101 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1400/3101 batches | lr 0.0001 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1500/3101 batches | lr 0.0001 | ms/batch 394.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1600/3101 batches | lr 0.0001 | ms/batch 394.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1700/3101 batches | lr 0.0001 | ms/batch 394.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1800/3101 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 1900/3101 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2000/3101 batches | lr 0.0001 | ms/batch 394.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2100/3101 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2200/3101 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2300/3101 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2400/3101 batches | lr 0.0001 | ms/batch 394.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2500/3101 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2600/3101 batches | lr 0.0001 | ms/batch 394.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2700/3101 batches | lr 0.0001 | ms/batch 394.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2800/3101 batches | lr 0.0001 | ms/batch 394.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 2900/3101 batches | lr 0.0001 | ms/batch 394.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3000/3101 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 3100/3101 batches | lr 0.0001 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 1277.26s | valid loss/mse 0.1744 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1744
scGPT - INFO - | epoch   2 | 100/3101 batches | lr 0.0001 | ms/batch 397.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 200/3101 batches | lr 0.0001 | ms/batch 393.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 300/3101 batches | lr 0.0001 | ms/batch 393.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 400/3101 batches | lr 0.0001 | ms/batch 393.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 500/3101 batches | lr 0.0001 | ms/batch 393.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 600/3101 batches | lr 0.0001 | ms/batch 393.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 700/3101 batches | lr 0.0001 | ms/batch 393.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 800/3101 batches | lr 0.0001 | ms/batch 393.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 900/3101 batches | lr 0.0001 | ms/batch 393.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1000/3101 batches | lr 0.0001 | ms/batch 393.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1100/3101 batches | lr 0.0001 | ms/batch 393.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1200/3101 batches | lr 0.0001 | ms/batch 393.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1300/3101 batches | lr 0.0001 | ms/batch 393.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1400/3101 batches | lr 0.0001 | ms/batch 393.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1500/3101 batches | lr 0.0001 | ms/batch 393.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1600/3101 batches | lr 0.0001 | ms/batch 393.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1700/3101 batches | lr 0.0001 | ms/batch 393.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1800/3101 batches | lr 0.0001 | ms/batch 393.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 1900/3101 batches | lr 0.0001 | ms/batch 393.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2000/3101 batches | lr 0.0001 | ms/batch 393.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2100/3101 batches | lr 0.0001 | ms/batch 394.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2200/3101 batches | lr 0.0001 | ms/batch 394.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2300/3101 batches | lr 0.0001 | ms/batch 393.83 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2400/3101 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2500/3101 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2600/3101 batches | lr 0.0001 | ms/batch 394.18 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2700/3101 batches | lr 0.0001 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2800/3101 batches | lr 0.0001 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 2900/3101 batches | lr 0.0001 | ms/batch 394.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3000/3101 batches | lr 0.0001 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   2 | 3100/3101 batches | lr 0.0001 | ms/batch 394.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 1275.47s | valid loss/mse 0.1734 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1734
scGPT - INFO - | epoch   3 | 100/3101 batches | lr 0.0001 | ms/batch 398.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 200/3101 batches | lr 0.0001 | ms/batch 394.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 300/3101 batches | lr 0.0001 | ms/batch 393.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 400/3101 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 500/3101 batches | lr 0.0001 | ms/batch 394.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 600/3101 batches | lr 0.0001 | ms/batch 393.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 700/3101 batches | lr 0.0001 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 800/3101 batches | lr 0.0001 | ms/batch 394.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 900/3101 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1000/3101 batches | lr 0.0001 | ms/batch 394.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1100/3101 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1200/3101 batches | lr 0.0001 | ms/batch 394.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1300/3101 batches | lr 0.0001 | ms/batch 394.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1400/3101 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1500/3101 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1600/3101 batches | lr 0.0001 | ms/batch 393.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1700/3101 batches | lr 0.0001 | ms/batch 394.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1800/3101 batches | lr 0.0001 | ms/batch 394.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 1900/3101 batches | lr 0.0001 | ms/batch 394.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2000/3101 batches | lr 0.0001 | ms/batch 394.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2100/3101 batches | lr 0.0001 | ms/batch 394.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2200/3101 batches | lr 0.0001 | ms/batch 394.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2300/3101 batches | lr 0.0001 | ms/batch 394.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2400/3101 batches | lr 0.0001 | ms/batch 394.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2500/3101 batches | lr 0.0001 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2600/3101 batches | lr 0.0001 | ms/batch 394.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2700/3101 batches | lr 0.0001 | ms/batch 394.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2800/3101 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 2900/3101 batches | lr 0.0001 | ms/batch 393.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3000/3101 batches | lr 0.0001 | ms/batch 394.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   3 | 3100/3101 batches | lr 0.0001 | ms/batch 394.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 1276.88s | valid loss/mse 0.1732 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1732
scGPT - INFO - | epoch   4 | 100/3101 batches | lr 0.0001 | ms/batch 398.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 200/3101 batches | lr 0.0001 | ms/batch 394.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 300/3101 batches | lr 0.0001 | ms/batch 394.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 400/3101 batches | lr 0.0001 | ms/batch 394.44 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 500/3101 batches | lr 0.0001 | ms/batch 394.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 600/3101 batches | lr 0.0001 | ms/batch 394.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 700/3101 batches | lr 0.0001 | ms/batch 394.56 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 800/3101 batches | lr 0.0001 | ms/batch 394.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 900/3101 batches | lr 0.0001 | ms/batch 394.66 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1000/3101 batches | lr 0.0001 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1100/3101 batches | lr 0.0001 | ms/batch 394.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1200/3101 batches | lr 0.0001 | ms/batch 394.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1300/3101 batches | lr 0.0001 | ms/batch 394.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1400/3101 batches | lr 0.0001 | ms/batch 394.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1500/3101 batches | lr 0.0001 | ms/batch 394.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1600/3101 batches | lr 0.0001 | ms/batch 394.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1700/3101 batches | lr 0.0001 | ms/batch 394.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1800/3101 batches | lr 0.0001 | ms/batch 394.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 1900/3101 batches | lr 0.0001 | ms/batch 394.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2000/3101 batches | lr 0.0001 | ms/batch 394.61 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2100/3101 batches | lr 0.0001 | ms/batch 394.62 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2200/3101 batches | lr 0.0001 | ms/batch 394.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2300/3101 batches | lr 0.0001 | ms/batch 394.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2400/3101 batches | lr 0.0001 | ms/batch 394.78 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2500/3101 batches | lr 0.0001 | ms/batch 394.76 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2600/3101 batches | lr 0.0001 | ms/batch 394.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2700/3101 batches | lr 0.0001 | ms/batch 394.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2800/3101 batches | lr 0.0001 | ms/batch 395.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 2900/3101 batches | lr 0.0001 | ms/batch 395.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3000/3101 batches | lr 0.0001 | ms/batch 395.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   4 | 3100/3101 batches | lr 0.0001 | ms/batch 395.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 1278.42s | valid loss/mse 0.1724 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1724
scGPT - INFO - | epoch   5 | 100/3101 batches | lr 0.0001 | ms/batch 399.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 200/3101 batches | lr 0.0001 | ms/batch 395.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 300/3101 batches | lr 0.0001 | ms/batch 395.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 400/3101 batches | lr 0.0001 | ms/batch 395.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 500/3101 batches | lr 0.0001 | ms/batch 395.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 600/3101 batches | lr 0.0001 | ms/batch 395.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 700/3101 batches | lr 0.0001 | ms/batch 395.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 800/3101 batches | lr 0.0001 | ms/batch 395.03 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 900/3101 batches | lr 0.0001 | ms/batch 395.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1000/3101 batches | lr 0.0001 | ms/batch 395.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1100/3101 batches | lr 0.0001 | ms/batch 396.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1200/3101 batches | lr 0.0001 | ms/batch 395.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1300/3101 batches | lr 0.0001 | ms/batch 395.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1400/3101 batches | lr 0.0001 | ms/batch 395.31 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1500/3101 batches | lr 0.0001 | ms/batch 395.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1600/3101 batches | lr 0.0001 | ms/batch 395.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1700/3101 batches | lr 0.0001 | ms/batch 395.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1800/3101 batches | lr 0.0001 | ms/batch 395.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 1900/3101 batches | lr 0.0001 | ms/batch 395.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2000/3101 batches | lr 0.0001 | ms/batch 395.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2100/3101 batches | lr 0.0001 | ms/batch 395.34 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2200/3101 batches | lr 0.0001 | ms/batch 395.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2300/3101 batches | lr 0.0001 | ms/batch 395.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2400/3101 batches | lr 0.0001 | ms/batch 395.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2500/3101 batches | lr 0.0001 | ms/batch 395.68 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2600/3101 batches | lr 0.0001 | ms/batch 398.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2700/3101 batches | lr 0.0001 | ms/batch 396.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2800/3101 batches | lr 0.0001 | ms/batch 396.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 2900/3101 batches | lr 0.0001 | ms/batch 395.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3000/3101 batches | lr 0.0001 | ms/batch 395.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   5 | 3100/3101 batches | lr 0.0001 | ms/batch 395.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 1281.22s | valid loss/mse 0.1724 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1724
scGPT - INFO - | epoch   6 | 100/3101 batches | lr 0.0001 | ms/batch 399.79 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 200/3101 batches | lr 0.0001 | ms/batch 395.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 300/3101 batches | lr 0.0001 | ms/batch 395.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 400/3101 batches | lr 0.0001 | ms/batch 396.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 500/3101 batches | lr 0.0001 | ms/batch 396.07 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 600/3101 batches | lr 0.0001 | ms/batch 395.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 700/3101 batches | lr 0.0001 | ms/batch 395.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 800/3101 batches | lr 0.0001 | ms/batch 395.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 900/3101 batches | lr 0.0001 | ms/batch 395.90 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1000/3101 batches | lr 0.0001 | ms/batch 395.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1100/3101 batches | lr 0.0001 | ms/batch 395.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1200/3101 batches | lr 0.0001 | ms/batch 395.51 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1300/3101 batches | lr 0.0001 | ms/batch 395.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1400/3101 batches | lr 0.0001 | ms/batch 395.59 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1500/3101 batches | lr 0.0001 | ms/batch 395.95 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1600/3101 batches | lr 0.0001 | ms/batch 396.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1700/3101 batches | lr 0.0001 | ms/batch 396.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1800/3101 batches | lr 0.0001 | ms/batch 396.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 1900/3101 batches | lr 0.0001 | ms/batch 396.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2000/3101 batches | lr 0.0001 | ms/batch 396.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2100/3101 batches | lr 0.0001 | ms/batch 396.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2200/3101 batches | lr 0.0001 | ms/batch 396.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2300/3101 batches | lr 0.0001 | ms/batch 396.42 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2400/3101 batches | lr 0.0001 | ms/batch 396.46 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2500/3101 batches | lr 0.0001 | ms/batch 396.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2600/3101 batches | lr 0.0001 | ms/batch 396.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2700/3101 batches | lr 0.0001 | ms/batch 396.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2800/3101 batches | lr 0.0001 | ms/batch 396.36 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 2900/3101 batches | lr 0.0001 | ms/batch 396.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3000/3101 batches | lr 0.0001 | ms/batch 396.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   6 | 3100/3101 batches | lr 0.0001 | ms/batch 396.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 1283.44s | valid loss/mse 0.1724 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/3101 batches | lr 0.0001 | ms/batch 400.70 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 200/3101 batches | lr 0.0001 | ms/batch 396.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 300/3101 batches | lr 0.0001 | ms/batch 396.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 400/3101 batches | lr 0.0001 | ms/batch 396.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 500/3101 batches | lr 0.0001 | ms/batch 396.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 600/3101 batches | lr 0.0001 | ms/batch 396.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 700/3101 batches | lr 0.0001 | ms/batch 396.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 800/3101 batches | lr 0.0001 | ms/batch 396.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 900/3101 batches | lr 0.0001 | ms/batch 396.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1000/3101 batches | lr 0.0001 | ms/batch 396.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1100/3101 batches | lr 0.0001 | ms/batch 396.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1200/3101 batches | lr 0.0001 | ms/batch 396.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1300/3101 batches | lr 0.0001 | ms/batch 396.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1400/3101 batches | lr 0.0001 | ms/batch 396.14 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1500/3101 batches | lr 0.0001 | ms/batch 396.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1600/3101 batches | lr 0.0001 | ms/batch 396.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1700/3101 batches | lr 0.0001 | ms/batch 396.47 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1800/3101 batches | lr 0.0001 | ms/batch 396.85 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 1900/3101 batches | lr 0.0001 | ms/batch 396.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2000/3101 batches | lr 0.0001 | ms/batch 397.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2100/3101 batches | lr 0.0001 | ms/batch 396.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2200/3101 batches | lr 0.0001 | ms/batch 396.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2300/3101 batches | lr 0.0001 | ms/batch 396.67 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2400/3101 batches | lr 0.0001 | ms/batch 396.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2500/3101 batches | lr 0.0001 | ms/batch 396.92 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2600/3101 batches | lr 0.0001 | ms/batch 397.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2700/3101 batches | lr 0.0001 | ms/batch 397.21 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2800/3101 batches | lr 0.0001 | ms/batch 396.98 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 2900/3101 batches | lr 0.0001 | ms/batch 396.94 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3000/3101 batches | lr 0.0001 | ms/batch 397.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   7 | 3100/3101 batches | lr 0.0001 | ms/batch 396.86 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 1285.06s | valid loss/mse 0.1733 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/3101 batches | lr 0.0000 | ms/batch 401.23 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 200/3101 batches | lr 0.0000 | ms/batch 397.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 300/3101 batches | lr 0.0000 | ms/batch 397.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 400/3101 batches | lr 0.0000 | ms/batch 397.33 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 500/3101 batches | lr 0.0000 | ms/batch 397.28 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 600/3101 batches | lr 0.0000 | ms/batch 397.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 700/3101 batches | lr 0.0000 | ms/batch 397.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 800/3101 batches | lr 0.0000 | ms/batch 397.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 900/3101 batches | lr 0.0000 | ms/batch 397.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1000/3101 batches | lr 0.0000 | ms/batch 397.32 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1100/3101 batches | lr 0.0000 | ms/batch 397.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1200/3101 batches | lr 0.0000 | ms/batch 397.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1300/3101 batches | lr 0.0000 | ms/batch 397.27 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1400/3101 batches | lr 0.0000 | ms/batch 396.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1500/3101 batches | lr 0.0000 | ms/batch 396.89 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1600/3101 batches | lr 0.0000 | ms/batch 396.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1700/3101 batches | lr 0.0000 | ms/batch 396.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1800/3101 batches | lr 0.0000 | ms/batch 396.96 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 1900/3101 batches | lr 0.0000 | ms/batch 397.24 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2000/3101 batches | lr 0.0000 | ms/batch 396.69 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2100/3101 batches | lr 0.0000 | ms/batch 397.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2200/3101 batches | lr 0.0000 | ms/batch 397.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2300/3101 batches | lr 0.0000 | ms/batch 397.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2400/3101 batches | lr 0.0000 | ms/batch 397.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2500/3101 batches | lr 0.0000 | ms/batch 397.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2600/3101 batches | lr 0.0000 | ms/batch 397.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2700/3101 batches | lr 0.0000 | ms/batch 397.45 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2800/3101 batches | lr 0.0000 | ms/batch 397.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 2900/3101 batches | lr 0.0000 | ms/batch 397.49 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3000/3101 batches | lr 0.0000 | ms/batch 397.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   8 | 3100/3101 batches | lr 0.0000 | ms/batch 397.09 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 1286.90s | valid loss/mse 0.1731 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/3101 batches | lr 0.0000 | ms/batch 401.08 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 200/3101 batches | lr 0.0000 | ms/batch 396.87 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 300/3101 batches | lr 0.0000 | ms/batch 397.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 400/3101 batches | lr 0.0000 | ms/batch 397.05 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 500/3101 batches | lr 0.0000 | ms/batch 397.37 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 600/3101 batches | lr 0.0000 | ms/batch 397.25 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 700/3101 batches | lr 0.0000 | ms/batch 397.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 800/3101 batches | lr 0.0000 | ms/batch 396.99 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 900/3101 batches | lr 0.0000 | ms/batch 396.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1000/3101 batches | lr 0.0000 | ms/batch 396.77 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1100/3101 batches | lr 0.0000 | ms/batch 396.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1200/3101 batches | lr 0.0000 | ms/batch 396.72 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1300/3101 batches | lr 0.0000 | ms/batch 396.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1400/3101 batches | lr 0.0000 | ms/batch 397.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1500/3101 batches | lr 0.0000 | ms/batch 397.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1600/3101 batches | lr 0.0000 | ms/batch 397.06 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1700/3101 batches | lr 0.0000 | ms/batch 397.41 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1800/3101 batches | lr 0.0000 | ms/batch 397.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 1900/3101 batches | lr 0.0000 | ms/batch 397.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2000/3101 batches | lr 0.0000 | ms/batch 397.65 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2100/3101 batches | lr 0.0000 | ms/batch 397.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2200/3101 batches | lr 0.0000 | ms/batch 397.73 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2300/3101 batches | lr 0.0000 | ms/batch 398.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2400/3101 batches | lr 0.0000 | ms/batch 398.52 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2500/3101 batches | lr 0.0000 | ms/batch 398.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2600/3101 batches | lr 0.0000 | ms/batch 398.75 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2700/3101 batches | lr 0.0000 | ms/batch 398.82 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2800/3101 batches | lr 0.0000 | ms/batch 398.55 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 2900/3101 batches | lr 0.0000 | ms/batch 398.81 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3000/3101 batches | lr 0.0000 | ms/batch 398.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   9 | 3100/3101 batches | lr 0.0000 | ms/batch 398.38 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 1288.25s | valid loss/mse 0.1738 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/3101 batches | lr 0.0000 | ms/batch 402.15 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 200/3101 batches | lr 0.0000 | ms/batch 398.29 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 300/3101 batches | lr 0.0000 | ms/batch 398.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 400/3101 batches | lr 0.0000 | ms/batch 398.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 500/3101 batches | lr 0.0000 | ms/batch 398.22 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 600/3101 batches | lr 0.0000 | ms/batch 398.54 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 700/3101 batches | lr 0.0000 | ms/batch 398.04 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 800/3101 batches | lr 0.0000 | ms/batch 398.00 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 900/3101 batches | lr 0.0000 | ms/batch 398.10 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1000/3101 batches | lr 0.0000 | ms/batch 397.84 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1100/3101 batches | lr 0.0000 | ms/batch 398.01 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1200/3101 batches | lr 0.0000 | ms/batch 398.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1300/3101 batches | lr 0.0000 | ms/batch 398.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1400/3101 batches | lr 0.0000 | ms/batch 397.74 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1500/3101 batches | lr 0.0000 | ms/batch 398.02 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1600/3101 batches | lr 0.0000 | ms/batch 398.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1700/3101 batches | lr 0.0000 | ms/batch 398.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1800/3101 batches | lr 0.0000 | ms/batch 398.12 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 1900/3101 batches | lr 0.0000 | ms/batch 398.39 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2000/3101 batches | lr 0.0000 | ms/batch 398.16 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2100/3101 batches | lr 0.0000 | ms/batch 398.26 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2200/3101 batches | lr 0.0000 | ms/batch 398.13 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2300/3101 batches | lr 0.0000 | ms/batch 398.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2400/3101 batches | lr 0.0000 | ms/batch 398.60 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2500/3101 batches | lr 0.0000 | ms/batch 398.30 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2600/3101 batches | lr 0.0000 | ms/batch 398.35 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2700/3101 batches | lr 0.0000 | ms/batch 398.53 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2800/3101 batches | lr 0.0000 | ms/batch 398.48 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 2900/3101 batches | lr 0.0000 | ms/batch 398.40 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3000/3101 batches | lr 0.0000 | ms/batch 398.50 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch  10 | 3100/3101 batches | lr 0.0000 | ms/batch 398.57 | loss  0.17 | mse  0.17 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 1290.47s | valid loss/mse 0.1726 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 10
---Creating test_res
