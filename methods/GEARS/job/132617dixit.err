cuda-11.8.0 loaded successful
gcc-12.2.0 loaded successful
cmake-3.27.0 loaded successful
openmpi-4.1.2 loaded successful
Openblas-0.3.25 loaded successful
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/23 [00:00<?, ?it/s]  4%|â–         | 1/23 [00:22<08:14, 22.47s/it]  9%|â–Š         | 2/23 [00:38<06:34, 18.76s/it] 13%|â–ˆâ–Ž        | 3/23 [00:44<04:17, 12.87s/it] 17%|â–ˆâ–‹        | 4/23 [00:51<03:20, 10.57s/it] 22%|â–ˆâ–ˆâ–       | 5/23 [01:01<03:03, 10.20s/it] 26%|â–ˆâ–ˆâ–Œ       | 6/23 [01:02<02:03,  7.27s/it] 30%|â–ˆâ–ˆâ–ˆ       | 7/23 [01:09<01:52,  7.03s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [01:17<01:50,  7.39s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [01:24<01:43,  7.40s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [01:32<01:36,  7.39s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [01:42<01:40,  8.40s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [01:55<01:47,  9.82s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [02:04<01:35,  9.52s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [02:06<01:04,  7.14s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [02:09<00:48,  6.01s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [02:16<00:43,  6.16s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [02:22<00:37,  6.33s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [02:32<00:36,  7.27s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [02:37<00:26,  6.68s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [02:51<00:26,  8.82s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [02:59<00:17,  8.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [03:01<00:06,  6.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [03:04<00:00,  5.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [03:04<00:00,  8.04s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_combined/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_combined/splits/dixit_combined_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_053545-4xb1fem8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_combined_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/4xb1fem8
  0%|          | 0/3646 [00:00<?, ?it/s]  0%|          | 7/3646 [00:00<00:56, 63.91it/s]  0%|          | 15/3646 [00:00<00:52, 69.62it/s]  1%|          | 22/3646 [00:00<00:52, 68.81it/s]  1%|          | 32/3646 [00:00<00:46, 77.98it/s]  1%|          | 41/3646 [00:00<00:44, 81.18it/s]  1%|â–         | 50/3646 [00:00<00:43, 81.76it/s]  2%|â–         | 59/3646 [00:00<00:43, 83.18it/s]  2%|â–         | 68/3646 [00:00<00:42, 84.15it/s]  2%|â–         | 77/3646 [00:00<00:43, 81.45it/s]  2%|â–         | 87/3646 [00:01<00:41, 85.20it/s]  3%|â–Ž         | 96/3646 [00:01<00:43, 81.85it/s]  3%|â–Ž         | 105/3646 [00:01<00:42, 82.95it/s]  3%|â–Ž         | 114/3646 [00:01<00:45, 78.21it/s]  3%|â–Ž         | 122/3646 [00:01<00:48, 72.13it/s]  4%|â–Ž         | 130/3646 [00:01<00:52, 67.39it/s]  4%|â–         | 137/3646 [00:01<00:54, 64.45it/s]  4%|â–         | 144/3646 [00:01<00:55, 62.96it/s]  4%|â–         | 151/3646 [00:02<00:57, 61.18it/s]  4%|â–         | 158/3646 [00:02<00:57, 60.81it/s]  5%|â–         | 165/3646 [00:02<00:57, 60.80it/s]  5%|â–         | 172/3646 [00:02<00:57, 60.91it/s]  5%|â–         | 179/3646 [00:02<00:56, 60.90it/s]  5%|â–Œ         | 186/3646 [00:02<00:56, 60.87it/s]  5%|â–Œ         | 193/3646 [00:02<00:56, 61.15it/s]  5%|â–Œ         | 200/3646 [00:02<00:56, 61.12it/s]  6%|â–Œ         | 207/3646 [00:03<00:59, 58.01it/s]  6%|â–Œ         | 214/3646 [00:03<00:56, 60.87it/s]  6%|â–Œ         | 221/3646 [00:03<00:56, 60.78it/s]  6%|â–‹         | 228/3646 [00:03<00:56, 60.70it/s]  6%|â–‹         | 235/3646 [00:03<00:55, 61.07it/s]  7%|â–‹         | 242/3646 [00:03<00:55, 60.95it/s]  7%|â–‹         | 249/3646 [00:03<00:55, 60.99it/s]  7%|â–‹         | 256/3646 [00:03<00:56, 60.36it/s]  7%|â–‹         | 263/3646 [00:03<00:56, 59.59it/s]  7%|â–‹         | 269/3646 [00:04<00:59, 56.62it/s]  8%|â–Š         | 276/3646 [00:04<00:59, 56.62it/s]  8%|â–Š         | 284/3646 [00:04<00:55, 60.56it/s]  8%|â–Š         | 291/3646 [00:04<00:55, 60.63it/s]  8%|â–Š         | 298/3646 [00:04<00:54, 61.13it/s]  8%|â–Š         | 305/3646 [00:04<00:55, 60.69it/s]  9%|â–Š         | 312/3646 [00:04<00:55, 60.19it/s]  9%|â–Š         | 319/3646 [00:04<00:55, 60.24it/s]  9%|â–‰         | 326/3646 [00:04<00:54, 60.46it/s]  9%|â–‰         | 333/3646 [00:05<00:54, 60.47it/s]  9%|â–‰         | 340/3646 [00:05<00:54, 60.66it/s] 10%|â–‰         | 347/3646 [00:05<00:54, 60.68it/s] 10%|â–‰         | 354/3646 [00:05<00:54, 60.67it/s] 10%|â–‰         | 361/3646 [00:05<00:54, 60.12it/s] 10%|â–ˆ         | 368/3646 [00:05<00:54, 60.26it/s] 10%|â–ˆ         | 375/3646 [00:05<00:54, 60.10it/s] 10%|â–ˆ         | 382/3646 [00:05<00:54, 59.92it/s] 11%|â–ˆ         | 389/3646 [00:06<00:54, 59.72it/s] 11%|â–ˆ         | 395/3646 [00:06<00:55, 58.07it/s] 11%|â–ˆ         | 401/3646 [00:06<00:55, 58.51it/s] 11%|â–ˆ         | 408/3646 [00:06<00:54, 59.49it/s] 11%|â–ˆâ–        | 415/3646 [00:06<00:53, 59.89it/s] 12%|â–ˆâ–        | 422/3646 [00:06<00:53, 60.60it/s] 12%|â–ˆâ–        | 429/3646 [00:06<00:53, 60.44it/s] 12%|â–ˆâ–        | 436/3646 [00:06<00:53, 60.48it/s] 12%|â–ˆâ–        | 443/3646 [00:06<00:53, 59.72it/s] 12%|â–ˆâ–        | 449/3646 [00:07<00:54, 58.97it/s] 13%|â–ˆâ–Ž        | 456/3646 [00:07<00:53, 59.80it/s] 13%|â–ˆâ–Ž        | 463/3646 [00:07<00:53, 60.02it/s] 13%|â–ˆâ–Ž        | 470/3646 [00:07<00:52, 60.19it/s] 13%|â–ˆâ–Ž        | 477/3646 [00:07<00:51, 61.14it/s] 13%|â–ˆâ–Ž        | 484/3646 [00:07<00:51, 61.17it/s] 13%|â–ˆâ–Ž        | 491/3646 [00:07<00:51, 61.14it/s] 14%|â–ˆâ–Ž        | 498/3646 [00:07<00:51, 61.12it/s] 14%|â–ˆâ–        | 505/3646 [00:07<00:51, 61.19it/s] 14%|â–ˆâ–        | 512/3646 [00:08<00:51, 60.88it/s] 14%|â–ˆâ–        | 519/3646 [00:08<00:51, 60.33it/s] 14%|â–ˆâ–        | 526/3646 [00:08<00:50, 62.27it/s] 15%|â–ˆâ–        | 534/3646 [00:08<00:49, 62.78it/s] 15%|â–ˆâ–        | 543/3646 [00:08<00:45, 67.75it/s] 15%|â–ˆâ–Œ        | 550/3646 [00:08<00:47, 65.26it/s] 15%|â–ˆâ–Œ        | 557/3646 [00:08<00:46, 66.20it/s] 16%|â–ˆâ–Œ        | 566/3646 [00:08<00:43, 70.91it/s] 16%|â–ˆâ–Œ        | 574/3646 [00:08<00:42, 72.11it/s] 16%|â–ˆâ–Œ        | 582/3646 [00:09<00:41, 74.30it/s] 16%|â–ˆâ–Œ        | 590/3646 [00:09<00:41, 74.49it/s] 16%|â–ˆâ–‹        | 598/3646 [00:09<00:43, 70.41it/s] 17%|â–ˆâ–‹        | 606/3646 [00:09<00:42, 72.10it/s] 17%|â–ˆâ–‹        | 616/3646 [00:09<00:38, 79.14it/s] 17%|â–ˆâ–‹        | 625/3646 [00:09<00:37, 80.49it/s] 17%|â–ˆâ–‹        | 634/3646 [00:09<00:37, 81.06it/s] 18%|â–ˆâ–Š        | 643/3646 [00:09<00:36, 82.54it/s] 18%|â–ˆâ–Š        | 652/3646 [00:09<00:35, 83.58it/s] 18%|â–ˆâ–Š        | 661/3646 [00:10<00:36, 82.21it/s] 18%|â–ˆâ–Š        | 671/3646 [00:10<00:34, 85.75it/s] 19%|â–ˆâ–Š        | 680/3646 [00:10<00:34, 86.72it/s] 19%|â–ˆâ–‰        | 690/3646 [00:10<00:33, 88.23it/s] 19%|â–ˆâ–‰        | 701/3646 [00:10<00:33, 89.11it/s] 20%|â–ˆâ–‰        | 711/3646 [00:10<00:32, 89.73it/s] 20%|â–ˆâ–‰        | 722/3646 [00:10<00:31, 91.79it/s] 20%|â–ˆâ–ˆ        | 732/3646 [00:10<00:32, 89.33it/s] 20%|â–ˆâ–ˆ        | 742/3646 [00:10<00:32, 90.75it/s] 21%|â–ˆâ–ˆ        | 752/3646 [00:11<00:31, 91.13it/s] 21%|â–ˆâ–ˆ        | 762/3646 [00:11<00:31, 91.95it/s] 21%|â–ˆâ–ˆ        | 773/3646 [00:11<00:30, 94.61it/s] 21%|â–ˆâ–ˆâ–       | 783/3646 [00:11<00:30, 93.21it/s] 22%|â–ˆâ–ˆâ–       | 794/3646 [00:11<00:29, 95.77it/s] 22%|â–ˆâ–ˆâ–       | 804/3646 [00:11<00:29, 95.32it/s] 22%|â–ˆâ–ˆâ–       | 814/3646 [00:11<00:30, 92.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 825/3646 [00:11<00:28, 97.39it/s] 23%|â–ˆâ–ˆâ–Ž       | 835/3646 [00:11<00:29, 95.90it/s] 23%|â–ˆâ–ˆâ–Ž       | 846/3646 [00:12<00:28, 97.55it/s] 23%|â–ˆâ–ˆâ–Ž       | 856/3646 [00:12<00:29, 95.92it/s] 24%|â–ˆâ–ˆâ–       | 866/3646 [00:12<00:28, 96.65it/s] 24%|â–ˆâ–ˆâ–       | 876/3646 [00:12<00:28, 97.14it/s] 24%|â–ˆâ–ˆâ–       | 886/3646 [00:12<00:28, 96.64it/s] 25%|â–ˆâ–ˆâ–       | 897/3646 [00:12<00:28, 98.15it/s] 25%|â–ˆâ–ˆâ–       | 907/3646 [00:12<00:28, 95.45it/s] 25%|â–ˆâ–ˆâ–Œ       | 917/3646 [00:12<00:28, 95.59it/s] 25%|â–ˆâ–ˆâ–Œ       | 927/3646 [00:12<00:28, 95.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 937/3646 [00:12<00:28, 94.16it/s] 26%|â–ˆâ–ˆâ–Œ       | 947/3646 [00:13<00:28, 94.62it/s] 26%|â–ˆâ–ˆâ–Œ       | 957/3646 [00:13<00:28, 94.22it/s] 27%|â–ˆâ–ˆâ–‹       | 967/3646 [00:13<00:28, 94.14it/s] 27%|â–ˆâ–ˆâ–‹       | 977/3646 [00:13<00:28, 92.99it/s] 27%|â–ˆâ–ˆâ–‹       | 987/3646 [00:13<00:29, 91.35it/s] 27%|â–ˆâ–ˆâ–‹       | 997/3646 [00:13<00:29, 89.36it/s] 28%|â–ˆâ–ˆâ–Š       | 1007/3646 [00:13<00:29, 89.79it/s] 28%|â–ˆâ–ˆâ–Š       | 1017/3646 [00:13<00:29, 90.64it/s] 28%|â–ˆâ–ˆâ–Š       | 1027/3646 [00:13<00:28, 91.03it/s] 28%|â–ˆâ–ˆâ–Š       | 1037/3646 [00:14<00:28, 90.58it/s] 29%|â–ˆâ–ˆâ–Š       | 1047/3646 [00:14<00:29, 87.05it/s] 29%|â–ˆâ–ˆâ–‰       | 1059/3646 [00:14<00:28, 91.79it/s] 29%|â–ˆâ–ˆâ–‰       | 1069/3646 [00:14<00:27, 92.16it/s] 30%|â–ˆâ–ˆâ–‰       | 1079/3646 [00:14<00:28, 90.75it/s] 30%|â–ˆâ–ˆâ–‰       | 1089/3646 [00:14<00:29, 86.26it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1100/3646 [00:14<00:27, 91.51it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1110/3646 [00:14<00:28, 89.24it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1119/3646 [00:14<00:29, 85.27it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1129/3646 [00:15<00:29, 86.29it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1138/3646 [00:15<00:29, 86.13it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1147/3646 [00:15<00:30, 83.13it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1156/3646 [00:15<00:30, 81.86it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1165/3646 [00:15<00:30, 81.58it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1174/3646 [00:15<00:32, 77.06it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1183/3646 [00:15<00:32, 76.51it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1192/3646 [00:15<00:31, 78.53it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1201/3646 [00:16<00:30, 80.82it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1210/3646 [00:16<00:31, 76.19it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1219/3646 [00:16<00:30, 79.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1229/3646 [00:16<00:29, 82.25it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1238/3646 [00:16<00:29, 81.40it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1249/3646 [00:16<00:28, 83.15it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1258/3646 [00:16<00:30, 79.38it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1266/3646 [00:16<00:30, 78.08it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1275/3646 [00:16<00:29, 80.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1284/3646 [00:17<00:32, 72.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1292/3646 [00:17<00:31, 74.28it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1302/3646 [00:17<00:30, 78.10it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1310/3646 [00:17<00:31, 73.52it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1320/3646 [00:17<00:29, 79.47it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1329/3646 [00:17<00:29, 79.70it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1340/3646 [00:17<00:26, 85.61it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1349/3646 [00:17<00:27, 82.07it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1359/3646 [00:17<00:26, 85.53it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1368/3646 [00:18<00:26, 86.38it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1377/3646 [00:18<00:26, 87.10it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1388/3646 [00:18<00:24, 91.12it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1398/3646 [00:18<00:24, 89.96it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1408/3646 [00:18<00:26, 85.81it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1418/3646 [00:18<00:25, 88.68it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1427/3646 [00:18<00:25, 88.08it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1438/3646 [00:18<00:24, 88.86it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1449/3646 [00:19<00:25, 87.19it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1459/3646 [00:19<00:24, 89.26it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1470/3646 [00:19<00:23, 91.77it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1480/3646 [00:19<00:23, 90.81it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1490/3646 [00:19<00:23, 91.40it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1500/3646 [00:19<00:22, 93.40it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1510/3646 [00:19<00:23, 89.20it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1521/3646 [00:19<00:22, 92.84it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1531/3646 [00:19<00:24, 87.60it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1541/3646 [00:20<00:23, 89.04it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1551/3646 [00:20<00:22, 91.30it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1561/3646 [00:20<00:22, 91.57it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1572/3646 [00:20<00:21, 94.53it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1582/3646 [00:20<00:22, 91.10it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1592/3646 [00:20<00:22, 91.32it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1602/3646 [00:20<00:22, 90.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1612/3646 [00:20<00:24, 82.32it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1621/3646 [00:20<00:27, 73.91it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1629/3646 [00:21<00:29, 68.81it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1637/3646 [00:21<00:30, 64.88it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1644/3646 [00:21<00:32, 60.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1651/3646 [00:21<00:32, 61.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1661/3646 [00:21<00:28, 70.79it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1670/3646 [00:21<00:26, 74.55it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1678/3646 [00:21<00:26, 75.10it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1688/3646 [00:21<00:24, 81.09it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1697/3646 [00:22<00:23, 81.76it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1706/3646 [00:22<00:23, 83.15it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1715/3646 [00:22<00:23, 83.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1724/3646 [00:22<00:22, 83.76it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1733/3646 [00:22<00:23, 79.74it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1744/3646 [00:22<00:21, 86.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1753/3646 [00:22<00:22, 83.40it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1763/3646 [00:22<00:21, 86.75it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1772/3646 [00:22<00:22, 82.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1781/3646 [00:23<00:22, 83.40it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1791/3646 [00:23<00:21, 86.81it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1800/3646 [00:23<00:21, 85.97it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1809/3646 [00:23<00:21, 84.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1819/3646 [00:23<00:20, 87.20it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1828/3646 [00:23<00:20, 86.84it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1837/3646 [00:23<00:20, 87.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1846/3646 [00:23<00:20, 86.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1855/3646 [00:23<00:22, 81.01it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1864/3646 [00:24<00:24, 72.82it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1872/3646 [00:24<00:25, 69.14it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1880/3646 [00:24<00:26, 67.59it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1887/3646 [00:24<00:26, 65.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1894/3646 [00:24<00:27, 64.30it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1901/3646 [00:24<00:27, 64.11it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1908/3646 [00:24<00:27, 63.89it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1915/3646 [00:24<00:26, 64.39it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1922/3646 [00:24<00:27, 62.78it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1929/3646 [00:25<00:26, 63.68it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1936/3646 [00:25<00:29, 58.52it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1943/3646 [00:25<00:28, 59.28it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1950/3646 [00:25<00:28, 60.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1957/3646 [00:25<00:27, 60.80it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1964/3646 [00:25<00:28, 59.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1971/3646 [00:25<00:27, 61.78it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1978/3646 [00:25<00:26, 62.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1985/3646 [00:26<00:27, 61.32it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1992/3646 [00:26<00:26, 63.31it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2000/3646 [00:26<00:24, 66.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2007/3646 [00:26<00:25, 65.32it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2015/3646 [00:26<00:24, 66.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2022/3646 [00:26<00:24, 65.10it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2029/3646 [00:26<00:25, 62.52it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2037/3646 [00:26<00:24, 66.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2047/3646 [00:26<00:21, 75.01it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2056/3646 [00:27<00:20, 77.01it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2064/3646 [00:27<00:21, 74.81it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2073/3646 [00:27<00:20, 76.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2082/3646 [00:27<00:19, 78.72it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2092/3646 [00:27<00:18, 83.46it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2101/3646 [00:27<00:18, 82.37it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2110/3646 [00:27<00:18, 84.04it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2119/3646 [00:27<00:18, 82.95it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2129/3646 [00:27<00:17, 87.35it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2140/3646 [00:28<00:16, 90.98it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2150/3646 [00:28<00:16, 90.51it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2160/3646 [00:28<00:16, 90.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2170/3646 [00:28<00:16, 88.03it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2179/3646 [00:28<00:16, 88.31it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2189/3646 [00:28<00:16, 88.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2198/3646 [00:28<00:16, 87.36it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2208/3646 [00:28<00:16, 89.10it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2217/3646 [00:28<00:16, 89.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2226/3646 [00:28<00:15, 89.16it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2236/3646 [00:29<00:15, 89.66it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2245/3646 [00:29<00:16, 86.01it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2255/3646 [00:29<00:15, 89.93it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2265/3646 [00:29<00:14, 92.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2275/3646 [00:29<00:14, 91.77it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2285/3646 [00:29<00:15, 89.51it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2295/3646 [00:29<00:15, 89.78it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2305/3646 [00:29<00:14, 89.63it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2314/3646 [00:29<00:15, 86.73it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2323/3646 [00:30<00:16, 79.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2332/3646 [00:30<00:16, 81.32it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2341/3646 [00:30<00:17, 72.88it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2349/3646 [00:30<00:17, 72.88it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2358/3646 [00:30<00:17, 75.23it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2368/3646 [00:30<00:16, 78.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2379/3646 [00:30<00:15, 84.34it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2389/3646 [00:30<00:14, 86.85it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2400/3646 [00:31<00:13, 91.43it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2410/3646 [00:31<00:13, 91.86it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2420/3646 [00:31<00:13, 90.86it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2430/3646 [00:31<00:13, 90.93it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2441/3646 [00:31<00:12, 95.11it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2451/3646 [00:31<00:13, 91.69it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2461/3646 [00:31<00:13, 90.07it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2472/3646 [00:31<00:12, 94.25it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2482/3646 [00:31<00:12, 93.07it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2492/3646 [00:32<00:12, 94.05it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2503/3646 [00:32<00:12, 92.80it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2513/3646 [00:32<00:12, 93.47it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2523/3646 [00:32<00:11, 93.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2533/3646 [00:32<00:12, 92.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2543/3646 [00:32<00:11, 92.72it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2553/3646 [00:32<00:11, 91.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2563/3646 [00:32<00:12, 90.01it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2574/3646 [00:32<00:11, 93.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2585/3646 [00:33<00:11, 93.48it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2596/3646 [00:33<00:11, 93.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2607/3646 [00:33<00:10, 95.76it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2617/3646 [00:33<00:11, 92.88it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2628/3646 [00:33<00:10, 94.81it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2638/3646 [00:33<00:10, 92.26it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2649/3646 [00:33<00:11, 89.42it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2660/3646 [00:33<00:10, 92.38it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2670/3646 [00:33<00:10, 92.49it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2680/3646 [00:34<00:10, 91.54it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2690/3646 [00:34<00:10, 91.99it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2700/3646 [00:34<00:10, 92.15it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2710/3646 [00:34<00:10, 92.12it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2721/3646 [00:34<00:09, 92.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2732/3646 [00:34<00:09, 94.88it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2742/3646 [00:34<00:09, 94.55it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2752/3646 [00:34<00:09, 93.74it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2762/3646 [00:34<00:09, 93.80it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2772/3646 [00:35<00:09, 90.83it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2782/3646 [00:35<00:09, 93.01it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2792/3646 [00:35<00:09, 92.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2802/3646 [00:35<00:09, 92.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2812/3646 [00:35<00:09, 92.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2822/3646 [00:35<00:09, 89.67it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2831/3646 [00:35<00:09, 89.33it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2842/3646 [00:35<00:08, 92.30it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2852/3646 [00:35<00:08, 91.59it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2862/3646 [00:36<00:08, 89.47it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2871/3646 [00:36<00:08, 86.99it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2883/3646 [00:36<00:08, 93.59it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2893/3646 [00:36<00:08, 90.60it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2904/3646 [00:36<00:08, 90.98it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2914/3646 [00:36<00:08, 89.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2923/3646 [00:36<00:08, 82.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2932/3646 [00:36<00:09, 75.23it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2940/3646 [00:36<00:09, 73.51it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2948/3646 [00:37<00:10, 69.66it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2956/3646 [00:37<00:10, 65.15it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2964/3646 [00:37<00:10, 67.95it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2973/3646 [00:37<00:09, 72.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2982/3646 [00:37<00:08, 75.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2992/3646 [00:37<00:08, 77.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3001/3646 [00:37<00:08, 79.87it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3010/3646 [00:37<00:07, 82.29it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3019/3646 [00:38<00:07, 82.99it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3028/3646 [00:38<00:07, 84.06it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3037/3646 [00:38<00:07, 85.11it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3047/3646 [00:38<00:06, 87.94it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3056/3646 [00:38<00:06, 87.90it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3065/3646 [00:38<00:06, 84.83it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3074/3646 [00:38<00:06, 84.16it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3084/3646 [00:38<00:06, 87.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3093/3646 [00:38<00:06, 87.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3102/3646 [00:38<00:06, 83.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3113/3646 [00:39<00:06, 85.68it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3123/3646 [00:39<00:05, 87.86it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3132/3646 [00:39<00:05, 86.96it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3141/3646 [00:39<00:06, 83.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3150/3646 [00:39<00:05, 83.47it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3159/3646 [00:39<00:05, 85.15it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3169/3646 [00:39<00:05, 84.85it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3179/3646 [00:39<00:05, 85.72it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3189/3646 [00:39<00:05, 88.95it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3198/3646 [00:40<00:05, 88.21it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3207/3646 [00:40<00:04, 88.22it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3216/3646 [00:40<00:04, 86.87it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3225/3646 [00:40<00:05, 84.02it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3236/3646 [00:40<00:04, 88.71it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3245/3646 [00:40<00:04, 84.01it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3254/3646 [00:40<00:04, 80.42it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3263/3646 [00:40<00:05, 75.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3271/3646 [00:41<00:05, 70.13it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3279/3646 [00:41<00:05, 66.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3288/3646 [00:41<00:05, 70.78it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3296/3646 [00:41<00:04, 70.98it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3304/3646 [00:41<00:04, 71.91it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3312/3646 [00:41<00:04, 72.37it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3320/3646 [00:41<00:04, 71.05it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3329/3646 [00:41<00:04, 75.01it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3339/3646 [00:41<00:03, 80.29it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3348/3646 [00:42<00:03, 82.11it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3357/3646 [00:42<00:03, 82.51it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3366/3646 [00:42<00:03, 83.92it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3375/3646 [00:42<00:03, 81.65it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3385/3646 [00:42<00:03, 85.85it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3394/3646 [00:42<00:02, 85.37it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3403/3646 [00:42<00:02, 82.24it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3412/3646 [00:42<00:02, 83.80it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3421/3646 [00:42<00:02, 84.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3430/3646 [00:43<00:02, 84.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3440/3646 [00:43<00:02, 84.19it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3450/3646 [00:43<00:02, 84.27it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3460/3646 [00:43<00:02, 88.09it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3469/3646 [00:43<00:02, 86.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3478/3646 [00:43<00:01, 84.88it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3487/3646 [00:43<00:02, 79.19it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3496/3646 [00:43<00:01, 80.32it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3505/3646 [00:43<00:01, 79.36it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3514/3646 [00:44<00:01, 76.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3523/3646 [00:44<00:01, 77.99it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3531/3646 [00:44<00:01, 76.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3539/3646 [00:44<00:01, 72.88it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3548/3646 [00:44<00:01, 75.65it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3556/3646 [00:44<00:01, 74.69it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3564/3646 [00:44<00:01, 74.01it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3572/3646 [00:44<00:00, 74.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3580/3646 [00:44<00:00, 75.36it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3588/3646 [00:45<00:00, 75.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3596/3646 [00:45<00:00, 75.13it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3604/3646 [00:45<00:00, 75.08it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3612/3646 [00:45<00:00, 75.11it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3620/3646 [00:45<00:00, 72.23it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3629/3646 [00:45<00:00, 73.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3637/3646 [00:45<00:00, 73.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3646/3646 [00:45<00:00, 77.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3646/3646 [00:45<00:00, 79.56it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 1.5050
Epoch 1 Step 51 Train Loss: 0.7423
Epoch 1 Step 101 Train Loss: 0.7041
Epoch 1 Step 151 Train Loss: 0.6865
Epoch 1 Step 201 Train Loss: 0.5576
Epoch 1 Step 251 Train Loss: 0.5740
Epoch 1 Step 301 Train Loss: 0.6512
Epoch 1 Step 351 Train Loss: 0.5029
Epoch 1 Step 401 Train Loss: 0.6308
Epoch 1 Step 451 Train Loss: 0.5473
Epoch 1 Step 501 Train Loss: 0.5740
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0325 Validation Top 20 DE MSE: 0.1009. 
Epoch 2 Step 1 Train Loss: 0.5674
Epoch 2 Step 51 Train Loss: 0.5904
Epoch 2 Step 101 Train Loss: 0.5787
Epoch 2 Step 151 Train Loss: 0.7736
Epoch 2 Step 201 Train Loss: 0.6626
Epoch 2 Step 251 Train Loss: 0.6782
Epoch 2 Step 301 Train Loss: 0.5456
Epoch 2 Step 351 Train Loss: 0.5250
Epoch 2 Step 401 Train Loss: 0.5405
Epoch 2 Step 451 Train Loss: 0.6065
Epoch 2 Step 501 Train Loss: 0.5484
Epoch 2: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.0433. 
Epoch 3 Step 1 Train Loss: 0.5293
Epoch 3 Step 51 Train Loss: 0.5209
Epoch 3 Step 101 Train Loss: 0.6515
Epoch 3 Step 151 Train Loss: 0.5222
Epoch 3 Step 201 Train Loss: 0.5959
Epoch 3 Step 251 Train Loss: 0.6031
Epoch 3 Step 301 Train Loss: 0.5366
Epoch 3 Step 351 Train Loss: 0.5974
Epoch 3 Step 401 Train Loss: 0.6268
Epoch 3 Step 451 Train Loss: 0.6161
Epoch 3 Step 501 Train Loss: 0.6283
Epoch 3: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.1547. 
Epoch 4 Step 1 Train Loss: 0.6207
Epoch 4 Step 51 Train Loss: 0.5564
Epoch 4 Step 101 Train Loss: 0.6940
Epoch 4 Step 151 Train Loss: 0.5593
Epoch 4 Step 201 Train Loss: 0.6308
Epoch 4 Step 251 Train Loss: 0.5669
Epoch 4 Step 301 Train Loss: 0.5771
Epoch 4 Step 351 Train Loss: 0.5561
Epoch 4 Step 401 Train Loss: 0.6728
Epoch 4 Step 451 Train Loss: 0.6069
Epoch 4 Step 501 Train Loss: 0.5227
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0939. 
Epoch 5 Step 1 Train Loss: 0.5502
Epoch 5 Step 51 Train Loss: 0.5394
Epoch 5 Step 101 Train Loss: 0.5584
Epoch 5 Step 151 Train Loss: 0.7924
Epoch 5 Step 201 Train Loss: 0.4905
Epoch 5 Step 251 Train Loss: 0.6054
Epoch 5 Step 301 Train Loss: 0.6042
Epoch 5 Step 351 Train Loss: 0.6324
Epoch 5 Step 401 Train Loss: 0.6062
Epoch 5 Step 451 Train Loss: 0.5683
Epoch 5 Step 501 Train Loss: 0.7117
Epoch 5: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.1257. 
Epoch 6 Step 1 Train Loss: 0.4921
Epoch 6 Step 51 Train Loss: 0.5802
Epoch 6 Step 101 Train Loss: 0.5471
Epoch 6 Step 151 Train Loss: 0.5818
Epoch 6 Step 201 Train Loss: 0.5470
Epoch 6 Step 251 Train Loss: 0.6123
Epoch 6 Step 301 Train Loss: 0.5307
Epoch 6 Step 351 Train Loss: 0.4870
Epoch 6 Step 401 Train Loss: 0.6951
Epoch 6 Step 451 Train Loss: 0.5045
Epoch 6 Step 501 Train Loss: 0.6154
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.1172. 
Epoch 7 Step 1 Train Loss: 0.6375
Epoch 7 Step 51 Train Loss: 0.5571
Epoch 7 Step 101 Train Loss: 0.5198
Epoch 7 Step 151 Train Loss: 0.5419
Epoch 7 Step 201 Train Loss: 0.4852
Epoch 7 Step 251 Train Loss: 0.5353
Epoch 7 Step 301 Train Loss: 0.4831
Epoch 7 Step 351 Train Loss: 0.5744
Epoch 7 Step 401 Train Loss: 0.4623
Epoch 7 Step 451 Train Loss: 0.6472
Epoch 7 Step 501 Train Loss: 0.6706
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0818. 
Epoch 8 Step 1 Train Loss: 0.5418
Epoch 8 Step 51 Train Loss: 0.5840
Epoch 8 Step 101 Train Loss: 0.4825
Epoch 8 Step 151 Train Loss: 0.5941
Epoch 8 Step 201 Train Loss: 0.5743
Epoch 8 Step 251 Train Loss: 0.6217
Epoch 8 Step 301 Train Loss: 0.6054
Epoch 8 Step 351 Train Loss: 0.6012
Epoch 8 Step 401 Train Loss: 0.5577
Epoch 8 Step 451 Train Loss: 0.4825
Epoch 8 Step 501 Train Loss: 0.5803
Epoch 8: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0076 Validation Top 20 DE MSE: 0.0657. 
Epoch 9 Step 1 Train Loss: 0.5730
Epoch 9 Step 51 Train Loss: 0.5786
Epoch 9 Step 101 Train Loss: 0.5589
Epoch 9 Step 151 Train Loss: 0.5985
Epoch 9 Step 201 Train Loss: 0.5691
Epoch 9 Step 251 Train Loss: 0.5059
Epoch 9 Step 301 Train Loss: 0.6350
Epoch 9 Step 351 Train Loss: 0.6148
Epoch 9 Step 401 Train Loss: 0.6266
Epoch 9 Step 451 Train Loss: 0.6649
Epoch 9 Step 501 Train Loss: 0.6201
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0762. 
Epoch 10 Step 1 Train Loss: 0.7876
Epoch 10 Step 51 Train Loss: 0.6276
Epoch 10 Step 101 Train Loss: 0.6148
Epoch 10 Step 151 Train Loss: 0.5933
Epoch 10 Step 201 Train Loss: 0.5264
Epoch 10 Step 251 Train Loss: 0.5930
Epoch 10 Step 301 Train Loss: 0.6160
Epoch 10 Step 351 Train Loss: 0.6045
Epoch 10 Step 401 Train Loss: 0.5183
Epoch 10 Step 451 Train Loss: 0.5935
Epoch 10 Step 501 Train Loss: 0.5042
Epoch 10: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0715. 
Epoch 11 Step 1 Train Loss: 0.5080
Epoch 11 Step 51 Train Loss: 0.7780
Epoch 11 Step 101 Train Loss: 0.5475
Epoch 11 Step 151 Train Loss: 0.5625
Epoch 11 Step 201 Train Loss: 0.5047
Epoch 11 Step 251 Train Loss: 0.5160
Epoch 11 Step 301 Train Loss: 0.5531
Epoch 11 Step 351 Train Loss: 0.6236
Epoch 11 Step 401 Train Loss: 0.5702
Epoch 11 Step 451 Train Loss: 0.5705
Epoch 11 Step 501 Train Loss: 0.5163
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0087 Validation Top 20 DE MSE: 0.0873. 
Epoch 12 Step 1 Train Loss: 0.5435
Epoch 12 Step 51 Train Loss: 0.5975
Epoch 12 Step 101 Train Loss: 0.5893
Epoch 12 Step 151 Train Loss: 0.4837
Epoch 12 Step 201 Train Loss: 0.5713
Epoch 12 Step 251 Train Loss: 0.5843
Epoch 12 Step 301 Train Loss: 0.5127
Epoch 12 Step 351 Train Loss: 0.7572
Epoch 12 Step 401 Train Loss: 0.6024
Epoch 12 Step 451 Train Loss: 0.5766
Epoch 12 Step 501 Train Loss: 0.5902
Epoch 12: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.0643. 
Epoch 13 Step 1 Train Loss: 0.6353
Epoch 13 Step 51 Train Loss: 0.5759
Epoch 13 Step 101 Train Loss: 0.5787
Epoch 13 Step 151 Train Loss: 0.6455
Epoch 13 Step 201 Train Loss: 0.4986
Epoch 13 Step 251 Train Loss: 0.5425
Epoch 13 Step 301 Train Loss: 0.5910
Epoch 13 Step 351 Train Loss: 0.5250
Epoch 13 Step 401 Train Loss: 0.4844
Epoch 13 Step 451 Train Loss: 0.6082
Epoch 13 Step 501 Train Loss: 0.5388
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0936. 
Epoch 14 Step 1 Train Loss: 0.5293
Epoch 14 Step 51 Train Loss: 0.5262
Epoch 14 Step 101 Train Loss: 0.4998
Epoch 14 Step 151 Train Loss: 0.5053
Epoch 14 Step 201 Train Loss: 0.5772
Epoch 14 Step 251 Train Loss: 0.5518
Epoch 14 Step 301 Train Loss: 0.5676
Epoch 14 Step 351 Train Loss: 0.5378
Epoch 14 Step 401 Train Loss: 0.5423
Epoch 14 Step 451 Train Loss: 0.5745
Epoch 14 Step 501 Train Loss: 0.5495
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0109 Validation Top 20 DE MSE: 0.1199. 
Epoch 15 Step 1 Train Loss: 0.5253
Epoch 15 Step 51 Train Loss: 0.6028
Epoch 15 Step 101 Train Loss: 0.6105
Epoch 15 Step 151 Train Loss: 0.5392
Epoch 15 Step 201 Train Loss: 0.6717
Epoch 15 Step 251 Train Loss: 0.5183
Epoch 15 Step 301 Train Loss: 0.5179
Epoch 15 Step 351 Train Loss: 0.5455
Epoch 15 Step 401 Train Loss: 0.4932
Epoch 15 Step 451 Train Loss: 0.4754
Epoch 15 Step 501 Train Loss: 0.6081
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0846. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0349
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0015776801
test_unseen_single_pearson: 0.9972119073489226
test_unseen_single_mse_de: 0.034878794
test_unseen_single_pearson_de: 0.9905322693976347
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.6074740994587488
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.016666666666666666
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9750000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.0348731743615204
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.022 MB of 0.023 MB uploadedwandb: / 0.022 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–
wandb:                                             train_de_pearson â–â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡
wandb:                                                    train_mse â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–…â–…â–†â–†â–…â–ƒâ–ƒâ–‡â–‡â–…â–†â–„â–…â–„â–ƒâ–…â–„â–†â–…â–ƒâ–‚â–ƒâ–ˆâ–ˆâ–…â–†â–„â–â–‚â–‚â–ƒâ–†â–†â–â–ƒâ–ƒâ–…â–ƒâ–‚â–…
wandb:                                                   val_de_mse â–…â–â–ˆâ–„â–†â–†â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–„â–†â–„
wandb:                                               val_de_pearson â–…â–ˆâ–â–…â–ƒâ–„â–†â–‡â–†â–‡â–†â–‡â–…â–ƒâ–†
wandb:                                                      val_mse â–ˆâ–ƒâ–‡â–ƒâ–…â–…â–‚â–â–‚â–â–ƒâ–â–ƒâ–…â–‚
wandb:                                                  val_pearson â–â–†â–‚â–†â–„â–…â–‡â–ˆâ–‡â–ˆâ–†â–ˆâ–†â–„â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.03488
wandb:                                              test_de_pearson 0.99053
wandb:               test_frac_opposite_direction_top20_non_dropout 0.01667
wandb:                          test_frac_sigma_below_1_non_dropout 0.975
wandb:                                                     test_mse 0.00158
wandb:                                test_mse_top20_de_non_dropout 0.03487
wandb:                                                 test_pearson 0.99721
wandb:                                           test_pearson_delta 0.60747
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.01667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.975
wandb:                                       test_unseen_single_mse 0.00158
wandb:                                    test_unseen_single_mse_de 0.03488
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.03487
wandb:                                   test_unseen_single_pearson 0.99721
wandb:                                test_unseen_single_pearson_de 0.99053
wandb:                             test_unseen_single_pearson_delta 0.60747
wandb:                                                 train_de_mse 0.00934
wandb:                                             train_de_pearson 0.99701
wandb:                                                    train_mse 0.00092
wandb:                                                train_pearson 0.99852
wandb:                                                training_loss 0.52994
wandb:                                                   val_de_mse 0.0846
wandb:                                               val_de_pearson 0.98433
wandb:                                                      val_mse 0.00159
wandb:                                                  val_pearson 0.99723
wandb: 
wandb: ðŸš€ View run Dixit_combined_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/4xb1fem8
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_053545-4xb1fem8/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_combined/splits/dixit_combined_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_055519-h1dy0uc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_combined_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/h1dy0uc3
Start Training...
Epoch 1 Step 1 Train Loss: 1.3370
Epoch 1 Step 51 Train Loss: 0.7492
Epoch 1 Step 101 Train Loss: 0.6243
Epoch 1 Step 151 Train Loss: 0.5117
Epoch 1 Step 201 Train Loss: 0.6063
Epoch 1 Step 251 Train Loss: 0.5636
Epoch 1 Step 301 Train Loss: 0.6967
Epoch 1 Step 351 Train Loss: 0.6887
Epoch 1 Step 401 Train Loss: 0.5195
Epoch 1 Step 451 Train Loss: 0.5029
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0195 Validation Top 20 DE MSE: 0.1133. 
Epoch 2 Step 1 Train Loss: 0.5701
Epoch 2 Step 51 Train Loss: 0.5865
Epoch 2 Step 101 Train Loss: 0.6677
Epoch 2 Step 151 Train Loss: 0.5152
Epoch 2 Step 201 Train Loss: 0.6403
Epoch 2 Step 251 Train Loss: 0.5790
Epoch 2 Step 301 Train Loss: 0.7020
Epoch 2 Step 351 Train Loss: 0.6598
Epoch 2 Step 401 Train Loss: 0.4736
Epoch 2 Step 451 Train Loss: 0.6857
Epoch 2: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0114 Validation Top 20 DE MSE: 0.1300. 
Epoch 3 Step 1 Train Loss: 0.6483
Epoch 3 Step 51 Train Loss: 0.6209
Epoch 3 Step 101 Train Loss: 0.5328
Epoch 3 Step 151 Train Loss: 0.4818
Epoch 3 Step 201 Train Loss: 0.6400
Epoch 3 Step 251 Train Loss: 0.5675
Epoch 3 Step 301 Train Loss: 0.5375
Epoch 3 Step 351 Train Loss: 0.5660
Epoch 3 Step 401 Train Loss: 0.6910
Epoch 3 Step 451 Train Loss: 0.5726
Epoch 3: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0282 Validation Top 20 DE MSE: 0.1150. 
Epoch 4 Step 1 Train Loss: 0.5103
Epoch 4 Step 51 Train Loss: 0.7516
Epoch 4 Step 101 Train Loss: 0.5336
Epoch 4 Step 151 Train Loss: 0.6183
Epoch 4 Step 201 Train Loss: 0.6104
Epoch 4 Step 251 Train Loss: 0.6165
Epoch 4 Step 301 Train Loss: 0.7225
Epoch 4 Step 351 Train Loss: 0.5487
Epoch 4 Step 401 Train Loss: 0.6095
Epoch 4 Step 451 Train Loss: 0.5567
Epoch 4: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.1102. 
Epoch 5 Step 1 Train Loss: 0.5863
Epoch 5 Step 51 Train Loss: 0.5519
Epoch 5 Step 101 Train Loss: 0.5761
Epoch 5 Step 151 Train Loss: 0.5305
Epoch 5 Step 201 Train Loss: 0.6124
Epoch 5 Step 251 Train Loss: 0.6424
Epoch 5 Step 301 Train Loss: 0.5559
Epoch 5 Step 351 Train Loss: 0.5884
Epoch 5 Step 401 Train Loss: 0.4305
Epoch 5 Step 451 Train Loss: 0.4979
Epoch 5: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0140 Validation Top 20 DE MSE: 0.2053. 
Epoch 6 Step 1 Train Loss: 0.4828
Epoch 6 Step 51 Train Loss: 0.5940
Epoch 6 Step 101 Train Loss: 0.5408
Epoch 6 Step 151 Train Loss: 0.6429
Epoch 6 Step 201 Train Loss: 0.5875
Epoch 6 Step 251 Train Loss: 0.5984
Epoch 6 Step 301 Train Loss: 0.6625
Epoch 6 Step 351 Train Loss: 0.4764
Epoch 6 Step 401 Train Loss: 0.5364
Epoch 6 Step 451 Train Loss: 0.6835
Epoch 6: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0680. 
Epoch 7 Step 1 Train Loss: 0.6286
Epoch 7 Step 51 Train Loss: 0.5723
Epoch 7 Step 101 Train Loss: 0.5677
Epoch 7 Step 151 Train Loss: 0.4686
Epoch 7 Step 201 Train Loss: 0.5783
Epoch 7 Step 251 Train Loss: 0.6313
Epoch 7 Step 301 Train Loss: 0.5419
Epoch 7 Step 351 Train Loss: 0.5088
Epoch 7 Step 401 Train Loss: 0.5382
Epoch 7 Step 451 Train Loss: 0.5407
Epoch 7: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0103 Validation Top 20 DE MSE: 0.1305. 
Epoch 8 Step 1 Train Loss: 0.6336
Epoch 8 Step 51 Train Loss: 0.5482
Epoch 8 Step 101 Train Loss: 0.5344
Epoch 8 Step 151 Train Loss: 0.5708
Epoch 8 Step 201 Train Loss: 0.5136
Epoch 8 Step 251 Train Loss: 0.5756
Epoch 8 Step 301 Train Loss: 0.4851
Epoch 8 Step 351 Train Loss: 0.5130
Epoch 8 Step 401 Train Loss: 0.6106
Epoch 8 Step 451 Train Loss: 0.5330
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0802. 
Epoch 9 Step 1 Train Loss: 0.5923
Epoch 9 Step 51 Train Loss: 0.5325
Epoch 9 Step 101 Train Loss: 0.5875
Epoch 9 Step 151 Train Loss: 0.5440
Epoch 9 Step 201 Train Loss: 0.5747
Epoch 9 Step 251 Train Loss: 0.5359
Epoch 9 Step 301 Train Loss: 0.4982
Epoch 9 Step 351 Train Loss: 0.5712
Epoch 9 Step 401 Train Loss: 0.6867
Epoch 9 Step 451 Train Loss: 0.5154
Epoch 9: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.1210. 
Epoch 10 Step 1 Train Loss: 0.7234
Epoch 10 Step 51 Train Loss: 0.6502
Epoch 10 Step 101 Train Loss: 0.5713
Epoch 10 Step 151 Train Loss: 0.4797
Epoch 10 Step 201 Train Loss: 0.5596
Epoch 10 Step 251 Train Loss: 0.5595
Epoch 10 Step 301 Train Loss: 0.5761
Epoch 10 Step 351 Train Loss: 0.5372
Epoch 10 Step 401 Train Loss: 0.4581
Epoch 10 Step 451 Train Loss: 0.4777
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.1039. 
Epoch 11 Step 1 Train Loss: 0.6731
Epoch 11 Step 51 Train Loss: 0.6367
Epoch 11 Step 101 Train Loss: 0.6061
Epoch 11 Step 151 Train Loss: 0.5995
Epoch 11 Step 201 Train Loss: 0.6622
Epoch 11 Step 251 Train Loss: 0.5684
Epoch 11 Step 301 Train Loss: 0.5252
Epoch 11 Step 351 Train Loss: 0.5967
Epoch 11 Step 401 Train Loss: 0.6021
Epoch 11 Step 451 Train Loss: 0.6702
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.1554. 
Epoch 12 Step 1 Train Loss: 0.6524
Epoch 12 Step 51 Train Loss: 0.5312
Epoch 12 Step 101 Train Loss: 0.5336
Epoch 12 Step 151 Train Loss: 0.5164
Epoch 12 Step 201 Train Loss: 0.5675
Epoch 12 Step 251 Train Loss: 0.6459
Epoch 12 Step 301 Train Loss: 0.5398
Epoch 12 Step 351 Train Loss: 0.4907
Epoch 12 Step 401 Train Loss: 0.6203
Epoch 12 Step 451 Train Loss: 0.5954
Epoch 12: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.1459. 
Epoch 13 Step 1 Train Loss: 0.5983
Epoch 13 Step 51 Train Loss: 0.5189
Epoch 13 Step 101 Train Loss: 0.5516
Epoch 13 Step 151 Train Loss: 0.5512
Epoch 13 Step 201 Train Loss: 0.6179
Epoch 13 Step 251 Train Loss: 0.6383
Epoch 13 Step 301 Train Loss: 0.5319
Epoch 13 Step 351 Train Loss: 0.6119
Epoch 13 Step 401 Train Loss: 0.5054
Epoch 13 Step 451 Train Loss: 0.4990
Epoch 13: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.1801. 
Epoch 14 Step 1 Train Loss: 0.5291
Epoch 14 Step 51 Train Loss: 0.5776
Epoch 14 Step 101 Train Loss: 0.5363
Epoch 14 Step 151 Train Loss: 0.6036
Epoch 14 Step 201 Train Loss: 0.4469
Epoch 14 Step 251 Train Loss: 0.5468
Epoch 14 Step 301 Train Loss: 0.5584
Epoch 14 Step 351 Train Loss: 0.5108
Epoch 14 Step 401 Train Loss: 0.5094
Epoch 14 Step 451 Train Loss: 0.5385
Epoch 14: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0086 Validation Top 20 DE MSE: 0.0991. 
Epoch 15 Step 1 Train Loss: 0.5983
Epoch 15 Step 51 Train Loss: 0.5607
Epoch 15 Step 101 Train Loss: 0.5553
Epoch 15 Step 151 Train Loss: 0.6401
Epoch 15 Step 201 Train Loss: 0.4886
Epoch 15 Step 251 Train Loss: 0.5657
Epoch 15 Step 301 Train Loss: 0.6022
Epoch 15 Step 351 Train Loss: 0.5100
Epoch 15 Step 401 Train Loss: 0.4730
Epoch 15 Step 451 Train Loss: 0.6349
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0988. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0459
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00095749827
test_unseen_single_pearson: 0.9982351139456535
test_unseen_single_mse_de: 0.045901272
test_unseen_single_pearson_de: 0.9868355839491455
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.6605283165462582
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.041666666666666664
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9833333333333334
test_unseen_single_mse_top20_de_non_dropout: 0.045901273273144284
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.021 MB of 0.022 MB uploadedwandb: / 0.021 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–…â–‚â–ˆâ–‚â–ƒâ–â–‚â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–…â–‡â–â–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–„â–…â–‡â–…â–„â–…â–‚â–ƒâ–â–„â–…â–â–†â–„â–„â–‚â–‚â–ƒâ–…â–„â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–†â–…â–â–„â–†â–‚â–‚â–ƒâ–‚â–„â–ˆâ–…
wandb:                                                   val_de_mse â–ƒâ–„â–ƒâ–ƒâ–ˆâ–â–„â–‚â–„â–ƒâ–…â–…â–‡â–ƒâ–ƒ
wandb:                                               val_de_pearson â–„â–…â–…â–†â–â–ˆâ–…â–ˆâ–†â–‡â–„â–…â–ƒâ–‡â–‡
wandb:                                                      val_mse â–…â–„â–„â–ƒâ–ˆâ–â–„â–‚â–„â–ƒâ–†â–…â–‡â–ƒâ–ƒ
wandb:                                                  val_pearson â–ƒâ–…â–…â–†â–â–ˆâ–…â–‡â–…â–†â–ƒâ–„â–‚â–†â–†
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.0459
wandb:                                              test_de_pearson 0.98684
wandb:               test_frac_opposite_direction_top20_non_dropout 0.04167
wandb:                          test_frac_sigma_below_1_non_dropout 0.98333
wandb:                                                     test_mse 0.00096
wandb:                                test_mse_top20_de_non_dropout 0.0459
wandb:                                                 test_pearson 0.99824
wandb:                                           test_pearson_delta 0.66053
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.04167
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98333
wandb:                                       test_unseen_single_mse 0.00096
wandb:                                    test_unseen_single_mse_de 0.0459
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0459
wandb:                                   test_unseen_single_pearson 0.99824
wandb:                                test_unseen_single_pearson_de 0.98684
wandb:                             test_unseen_single_pearson_delta 0.66053
wandb:                                                 train_de_mse 0.00844
wandb:                                             train_de_pearson 0.99737
wandb:                                                    train_mse 0.00115
wandb:                                                train_pearson 0.99816
wandb:                                                training_loss 0.67081
wandb:                                                   val_de_mse 0.09877
wandb:                                               val_de_pearson 0.98132
wandb:                                                      val_mse 0.00188
wandb:                                                  val_pearson 0.99678
wandb: 
wandb: ðŸš€ View run Dixit_combined_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/h1dy0uc3
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_055519-h1dy0uc3/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_combined/splits/dixit_combined_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_061152-s7ktd5f2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_combined_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/s7ktd5f2
Start Training...
Epoch 1 Step 1 Train Loss: 0.8664
Epoch 1 Step 51 Train Loss: 0.6403
Epoch 1 Step 101 Train Loss: 0.5921
Epoch 1 Step 151 Train Loss: 0.5326
Epoch 1 Step 201 Train Loss: 0.6719
Epoch 1 Step 251 Train Loss: 0.5412
Epoch 1 Step 301 Train Loss: 0.6176
Epoch 1 Step 351 Train Loss: 0.5860
Epoch 1 Step 401 Train Loss: 0.5893
Epoch 1 Step 451 Train Loss: 0.5238
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0253 Validation Top 20 DE MSE: 0.0128. 
Epoch 2 Step 1 Train Loss: 0.5258
Epoch 2 Step 51 Train Loss: 0.4549
Epoch 2 Step 101 Train Loss: 0.5739
Epoch 2 Step 151 Train Loss: 0.6821
Epoch 2 Step 201 Train Loss: 0.5195
Epoch 2 Step 251 Train Loss: 0.6131
Epoch 2 Step 301 Train Loss: 0.5592
Epoch 2 Step 351 Train Loss: 0.6569
Epoch 2 Step 401 Train Loss: 0.5666
Epoch 2 Step 451 Train Loss: 0.6314
Epoch 2: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0134 Validation Top 20 DE MSE: 0.0067. 
Epoch 3 Step 1 Train Loss: 0.4835
Epoch 3 Step 51 Train Loss: 0.5896
Epoch 3 Step 101 Train Loss: 0.6722
Epoch 3 Step 151 Train Loss: 0.4874
Epoch 3 Step 201 Train Loss: 0.5750
Epoch 3 Step 251 Train Loss: 0.5212
Epoch 3 Step 301 Train Loss: 0.5603
Epoch 3 Step 351 Train Loss: 0.5573
Epoch 3 Step 401 Train Loss: 0.5583
Epoch 3 Step 451 Train Loss: 0.7529
Epoch 3: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.0081. 
Epoch 4 Step 1 Train Loss: 0.4823
Epoch 4 Step 51 Train Loss: 0.6065
Epoch 4 Step 101 Train Loss: 0.5296
Epoch 4 Step 151 Train Loss: 0.5243
Epoch 4 Step 201 Train Loss: 0.4958
Epoch 4 Step 251 Train Loss: 0.5981
Epoch 4 Step 301 Train Loss: 0.6165
Epoch 4 Step 351 Train Loss: 0.5199
Epoch 4 Step 401 Train Loss: 0.5062
Epoch 4 Step 451 Train Loss: 0.5955
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0059 Validation Top 20 DE MSE: 0.0119. 
Epoch 5 Step 1 Train Loss: 0.5437
Epoch 5 Step 51 Train Loss: 0.5693
Epoch 5 Step 101 Train Loss: 0.5199
Epoch 5 Step 151 Train Loss: 0.5862
Epoch 5 Step 201 Train Loss: 0.5326
Epoch 5 Step 251 Train Loss: 0.6203
Epoch 5 Step 301 Train Loss: 0.5149
Epoch 5 Step 351 Train Loss: 0.5391
Epoch 5 Step 401 Train Loss: 0.5183
Epoch 5 Step 451 Train Loss: 0.5432
Epoch 5: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.0090. 
Epoch 6 Step 1 Train Loss: 0.5135
Epoch 6 Step 51 Train Loss: 0.5784
Epoch 6 Step 101 Train Loss: 0.5770
Epoch 6 Step 151 Train Loss: 0.5814
Epoch 6 Step 201 Train Loss: 0.4890
Epoch 6 Step 251 Train Loss: 0.6191
Epoch 6 Step 301 Train Loss: 0.5067
Epoch 6 Step 351 Train Loss: 0.6509
Epoch 6 Step 401 Train Loss: 0.5123
Epoch 6 Step 451 Train Loss: 0.7507
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0098. 
Epoch 7 Step 1 Train Loss: 0.5795
Epoch 7 Step 51 Train Loss: 0.5436
Epoch 7 Step 101 Train Loss: 0.5345
Epoch 7 Step 151 Train Loss: 0.5563
Epoch 7 Step 201 Train Loss: 0.4707
Epoch 7 Step 251 Train Loss: 0.4578
Epoch 7 Step 301 Train Loss: 0.5871
Epoch 7 Step 351 Train Loss: 0.5996
Epoch 7 Step 401 Train Loss: 0.5727
Epoch 7 Step 451 Train Loss: 0.5506
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0074 Validation Top 20 DE MSE: 0.0124. 
Epoch 8 Step 1 Train Loss: 0.5403
Epoch 8 Step 51 Train Loss: 0.4699
Epoch 8 Step 101 Train Loss: 0.6403
Epoch 8 Step 151 Train Loss: 0.6211
Epoch 8 Step 201 Train Loss: 0.6588
Epoch 8 Step 251 Train Loss: 0.5392
Epoch 8 Step 301 Train Loss: 0.5749
Epoch 8 Step 351 Train Loss: 0.4756
Epoch 8 Step 401 Train Loss: 0.5156
Epoch 8 Step 451 Train Loss: 0.5612
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0129. 
Epoch 9 Step 1 Train Loss: 0.5550
Epoch 9 Step 51 Train Loss: 0.5673
Epoch 9 Step 101 Train Loss: 0.7373
Epoch 9 Step 151 Train Loss: 0.5763
Epoch 9 Step 201 Train Loss: 0.6041
Epoch 9 Step 251 Train Loss: 0.5635
Epoch 9 Step 301 Train Loss: 0.5783
Epoch 9 Step 351 Train Loss: 0.5561
Epoch 9 Step 401 Train Loss: 0.6877
Epoch 9 Step 451 Train Loss: 0.5016
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0086 Validation Top 20 DE MSE: 0.0138. 
Epoch 10 Step 1 Train Loss: 0.4933
Epoch 10 Step 51 Train Loss: 0.4804
Epoch 10 Step 101 Train Loss: 0.5320
Epoch 10 Step 151 Train Loss: 0.6641
Epoch 10 Step 201 Train Loss: 0.5597
Epoch 10 Step 251 Train Loss: 0.5102
Epoch 10 Step 301 Train Loss: 0.5202
Epoch 10 Step 351 Train Loss: 0.5741
Epoch 10 Step 401 Train Loss: 0.6006
Epoch 10 Step 451 Train Loss: 0.5186
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0092. 
Epoch 11 Step 1 Train Loss: 0.6749
Epoch 11 Step 51 Train Loss: 0.5879
Epoch 11 Step 101 Train Loss: 0.6851
Epoch 11 Step 151 Train Loss: 0.5184
Epoch 11 Step 201 Train Loss: 0.7181
Epoch 11 Step 251 Train Loss: 0.5956
Epoch 11 Step 301 Train Loss: 0.6232
Epoch 11 Step 351 Train Loss: 0.5314
Epoch 11 Step 401 Train Loss: 0.5215
Epoch 11 Step 451 Train Loss: 0.5369
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.0119. 
Epoch 12 Step 1 Train Loss: 0.5321
Epoch 12 Step 51 Train Loss: 0.5331
Epoch 12 Step 101 Train Loss: 0.5216
Epoch 12 Step 151 Train Loss: 0.7744
Epoch 12 Step 201 Train Loss: 0.5281
Epoch 12 Step 251 Train Loss: 0.5393
Epoch 12 Step 301 Train Loss: 0.5899
Epoch 12 Step 351 Train Loss: 0.5187
Epoch 12 Step 401 Train Loss: 0.5875
Epoch 12 Step 451 Train Loss: 0.4987
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0073 Validation Top 20 DE MSE: 0.0116. 
Epoch 13 Step 1 Train Loss: 0.5782
Epoch 13 Step 51 Train Loss: 0.6816
Epoch 13 Step 101 Train Loss: 0.5868
Epoch 13 Step 151 Train Loss: 0.6156
Epoch 13 Step 201 Train Loss: 0.5585
Epoch 13 Step 251 Train Loss: 0.4984
Epoch 13 Step 301 Train Loss: 0.5873
Epoch 13 Step 351 Train Loss: 0.6332
Epoch 13 Step 401 Train Loss: 0.5511
Epoch 13 Step 451 Train Loss: 0.5249
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.0112. 
Epoch 14 Step 1 Train Loss: 0.5924
Epoch 14 Step 51 Train Loss: 0.4848
Epoch 14 Step 101 Train Loss: 0.4579
Epoch 14 Step 151 Train Loss: 0.5431
Epoch 14 Step 201 Train Loss: 0.5415
Epoch 14 Step 251 Train Loss: 0.6099
Epoch 14 Step 301 Train Loss: 0.6326
Epoch 14 Step 351 Train Loss: 0.5731
Epoch 14 Step 401 Train Loss: 0.5159
Epoch 14 Step 451 Train Loss: 0.5745
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.0127. 
Epoch 15 Step 1 Train Loss: 0.5016
Epoch 15 Step 51 Train Loss: 0.4718
Epoch 15 Step 101 Train Loss: 0.5074
Epoch 15 Step 151 Train Loss: 0.6960
Epoch 15 Step 201 Train Loss: 0.6502
Epoch 15 Step 251 Train Loss: 0.4646
Epoch 15 Step 301 Train Loss: 0.5129
Epoch 15 Step 351 Train Loss: 0.5930
Epoch 15 Step 401 Train Loss: 0.4927
Epoch 15 Step 451 Train Loss: 0.5961
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0077 Validation Top 20 DE MSE: 0.0115. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0718
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0019453531
test_unseen_single_pearson: 0.9964999639371662
test_unseen_single_mse_de: 0.07181606
test_unseen_single_pearson_de: 0.9789892309210219
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4837598705793811
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.18333333333333335
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9416666666666668
test_unseen_single_mse_top20_de_non_dropout: 0.07181605517089913
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.021 MB of 0.022 MB uploadedwandb: - 0.021 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb:                                             train_de_pearson â–â–…â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡
wandb:                                                    train_mse â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–‡â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–ƒâ–„â–ƒâ–†â–‚â–ƒâ–‚â–ƒâ–†â–„â–‚â–„â–„â–†â–â–†â–‚â–†â–„â–„â–ƒâ–‚â–ƒâ–‚â–…
wandb:                                                   val_de_mse â–‡â–â–‚â–†â–ƒâ–„â–‡â–‡â–ˆâ–ƒâ–†â–†â–…â–‡â–†
wandb:                                               val_de_pearson â–â–ˆâ–‡â–ƒâ–†â–…â–ƒâ–‚â–â–†â–ƒâ–ƒâ–„â–‚â–ƒ
wandb:                                                      val_mse â–ˆâ–ƒâ–â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–‚â–
wandb:                                                  val_pearson â–â–†â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.07182
wandb:                                              test_de_pearson 0.97899
wandb:               test_frac_opposite_direction_top20_non_dropout 0.18333
wandb:                          test_frac_sigma_below_1_non_dropout 0.94167
wandb:                                                     test_mse 0.00195
wandb:                                test_mse_top20_de_non_dropout 0.07182
wandb:                                                 test_pearson 0.9965
wandb:                                           test_pearson_delta 0.48376
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.18333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.94167
wandb:                                       test_unseen_single_mse 0.00195
wandb:                                    test_unseen_single_mse_de 0.07182
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.07182
wandb:                                   test_unseen_single_pearson 0.9965
wandb:                                test_unseen_single_pearson_de 0.97899
wandb:                             test_unseen_single_pearson_delta 0.48376
wandb:                                                 train_de_mse 0.00766
wandb:                                             train_de_pearson 0.9976
wandb:                                                    train_mse 0.00093
wandb:                                                train_pearson 0.99851
wandb:                                                training_loss 0.68031
wandb:                                                   val_de_mse 0.01154
wandb:                                               val_de_pearson 0.99723
wandb:                                                      val_mse 0.00057
wandb:                                                  val_pearson 0.99895
wandb: 
wandb: ðŸš€ View run Dixit_combined_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/s7ktd5f2
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_061152-s7ktd5f2/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_combined/splits/dixit_combined_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_062922-v1j2r8bi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_combined_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/v1j2r8bi
Start Training...
Epoch 1 Step 1 Train Loss: 1.3600
Epoch 1 Step 51 Train Loss: 0.7173
Epoch 1 Step 101 Train Loss: 0.6183
Epoch 1 Step 151 Train Loss: 0.7046
Epoch 1 Step 201 Train Loss: 0.5030
Epoch 1 Step 251 Train Loss: 0.5215
Epoch 1 Step 301 Train Loss: 0.6183
Epoch 1 Step 351 Train Loss: 0.4736
Epoch 1 Step 401 Train Loss: 0.5767
Epoch 1 Step 451 Train Loss: 0.5596
Epoch 1 Step 501 Train Loss: 0.5723
Epoch 1 Step 551 Train Loss: 0.5662
Epoch 1: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0773. 
Epoch 2 Step 1 Train Loss: 0.5000
Epoch 2 Step 51 Train Loss: 0.5354
Epoch 2 Step 101 Train Loss: 0.6955
Epoch 2 Step 151 Train Loss: 0.6329
Epoch 2 Step 201 Train Loss: 0.6646
Epoch 2 Step 251 Train Loss: 0.5689
Epoch 2 Step 301 Train Loss: 0.5788
Epoch 2 Step 351 Train Loss: 0.5355
Epoch 2 Step 401 Train Loss: 0.5750
Epoch 2 Step 451 Train Loss: 0.4938
Epoch 2 Step 501 Train Loss: 0.5283
Epoch 2 Step 551 Train Loss: 0.6255
Epoch 2: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0120 Validation Top 20 DE MSE: 0.2163. 
Epoch 3 Step 1 Train Loss: 0.5633
Epoch 3 Step 51 Train Loss: 0.5642
Epoch 3 Step 101 Train Loss: 0.5526
Epoch 3 Step 151 Train Loss: 0.5366
Epoch 3 Step 201 Train Loss: 0.7035
Epoch 3 Step 251 Train Loss: 0.5009
Epoch 3 Step 301 Train Loss: 0.5493
Epoch 3 Step 351 Train Loss: 0.5496
Epoch 3 Step 401 Train Loss: 0.5419
Epoch 3 Step 451 Train Loss: 0.6808
Epoch 3 Step 501 Train Loss: 0.4900
Epoch 3 Step 551 Train Loss: 0.5112
Epoch 3: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0078 Validation Top 20 DE MSE: 0.2314. 
Epoch 4 Step 1 Train Loss: 0.5029
Epoch 4 Step 51 Train Loss: 0.5757
Epoch 4 Step 101 Train Loss: 0.5702
Epoch 4 Step 151 Train Loss: 0.5594
Epoch 4 Step 201 Train Loss: 0.5317
Epoch 4 Step 251 Train Loss: 0.5425
Epoch 4 Step 301 Train Loss: 0.5255
Epoch 4 Step 351 Train Loss: 0.6648
Epoch 4 Step 401 Train Loss: 0.4979
Epoch 4 Step 451 Train Loss: 0.5634
Epoch 4 Step 501 Train Loss: 0.6334
Epoch 4 Step 551 Train Loss: 0.5166
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0136 Validation Top 20 DE MSE: 0.1133. 
Epoch 5 Step 1 Train Loss: 0.4947
Epoch 5 Step 51 Train Loss: 0.6033
Epoch 5 Step 101 Train Loss: 0.5745
Epoch 5 Step 151 Train Loss: 0.5547
Epoch 5 Step 201 Train Loss: 0.5280
Epoch 5 Step 251 Train Loss: 0.6343
Epoch 5 Step 301 Train Loss: 0.6303
Epoch 5 Step 351 Train Loss: 0.6352
Epoch 5 Step 401 Train Loss: 0.5262
Epoch 5 Step 451 Train Loss: 0.5704
Epoch 5 Step 501 Train Loss: 0.5161
Epoch 5 Step 551 Train Loss: 0.6109
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0086 Validation Top 20 DE MSE: 0.1702. 
Epoch 6 Step 1 Train Loss: 0.5437
Epoch 6 Step 51 Train Loss: 0.5618
Epoch 6 Step 101 Train Loss: 0.5061
Epoch 6 Step 151 Train Loss: 0.6758
Epoch 6 Step 201 Train Loss: 0.5822
Epoch 6 Step 251 Train Loss: 0.6562
Epoch 6 Step 301 Train Loss: 0.4928
Epoch 6 Step 351 Train Loss: 0.6357
Epoch 6 Step 401 Train Loss: 0.5193
Epoch 6 Step 451 Train Loss: 0.4490
Epoch 6 Step 501 Train Loss: 0.5667
Epoch 6 Step 551 Train Loss: 0.5257
Epoch 6: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0049. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.2439. 
Epoch 7 Step 1 Train Loss: 0.5290
Epoch 7 Step 51 Train Loss: 0.5132
Epoch 7 Step 101 Train Loss: 0.5017
Epoch 7 Step 151 Train Loss: 0.6217
Epoch 7 Step 201 Train Loss: 0.5272
Epoch 7 Step 251 Train Loss: 0.5570
Epoch 7 Step 301 Train Loss: 0.6084
Epoch 7 Step 351 Train Loss: 0.5398
Epoch 7 Step 401 Train Loss: 0.4853
Epoch 7 Step 451 Train Loss: 0.5607
Epoch 7 Step 501 Train Loss: 0.4913
Epoch 7 Step 551 Train Loss: 0.5330
Epoch 7: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0068 Validation Top 20 DE MSE: 0.1419. 
Epoch 8 Step 1 Train Loss: 0.5025
Epoch 8 Step 51 Train Loss: 0.5927
Epoch 8 Step 101 Train Loss: 0.5661
Epoch 8 Step 151 Train Loss: 0.5700
Epoch 8 Step 201 Train Loss: 0.6018
Epoch 8 Step 251 Train Loss: 0.6720
Epoch 8 Step 301 Train Loss: 0.6205
Epoch 8 Step 351 Train Loss: 0.6452
Epoch 8 Step 401 Train Loss: 0.5350
Epoch 8 Step 451 Train Loss: 0.6824
Epoch 8 Step 501 Train Loss: 0.5763
Epoch 8 Step 551 Train Loss: 0.5042
Epoch 8: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0069 Validation Top 20 DE MSE: 0.1341. 
Epoch 9 Step 1 Train Loss: 0.5397
Epoch 9 Step 51 Train Loss: 0.4878
Epoch 9 Step 101 Train Loss: 0.5349
Epoch 9 Step 151 Train Loss: 0.5057
Epoch 9 Step 201 Train Loss: 0.5890
Epoch 9 Step 251 Train Loss: 0.4536
Epoch 9 Step 301 Train Loss: 0.4886
Epoch 9 Step 351 Train Loss: 0.4920
Epoch 9 Step 401 Train Loss: 0.4832
Epoch 9 Step 451 Train Loss: 0.5060
Epoch 9 Step 501 Train Loss: 0.5787
Epoch 9 Step 551 Train Loss: 0.4717
Epoch 9: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.2390. 
Epoch 10 Step 1 Train Loss: 0.5710
Epoch 10 Step 51 Train Loss: 0.5570
Epoch 10 Step 101 Train Loss: 0.6049
Epoch 10 Step 151 Train Loss: 0.5008
Epoch 10 Step 201 Train Loss: 0.5153
Epoch 10 Step 251 Train Loss: 0.5520
Epoch 10 Step 301 Train Loss: 0.5421
Epoch 10 Step 351 Train Loss: 0.5611
Epoch 10 Step 401 Train Loss: 0.6101
Epoch 10 Step 451 Train Loss: 0.5997
Epoch 10 Step 501 Train Loss: 0.5732
Epoch 10 Step 551 Train Loss: 0.6036
Epoch 10: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0052. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.2640. 
Epoch 11 Step 1 Train Loss: 0.5268
Epoch 11 Step 51 Train Loss: 0.5752
Epoch 11 Step 101 Train Loss: 0.6382
Epoch 11 Step 151 Train Loss: 0.5002
Epoch 11 Step 201 Train Loss: 0.5115
Epoch 11 Step 251 Train Loss: 0.5315
Epoch 11 Step 301 Train Loss: 0.5865
Epoch 11 Step 351 Train Loss: 0.5250
Epoch 11 Step 401 Train Loss: 0.5865
Epoch 11 Step 451 Train Loss: 0.5352
Epoch 11 Step 501 Train Loss: 0.5384
Epoch 11 Step 551 Train Loss: 0.6032
Epoch 11: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0038. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.1867. 
Epoch 12 Step 1 Train Loss: 0.5206
Epoch 12 Step 51 Train Loss: 0.5276
Epoch 12 Step 101 Train Loss: 0.5869
Epoch 12 Step 151 Train Loss: 0.5638
Epoch 12 Step 201 Train Loss: 0.5188
Epoch 12 Step 251 Train Loss: 0.4466
Epoch 12 Step 301 Train Loss: 0.6076
Epoch 12 Step 351 Train Loss: 0.5764
Epoch 12 Step 401 Train Loss: 0.5626
Epoch 12 Step 451 Train Loss: 0.5598
Epoch 12 Step 501 Train Loss: 0.5778
Epoch 12 Step 551 Train Loss: 0.5750
Epoch 12: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.2337. 
Epoch 13 Step 1 Train Loss: 0.5009
Epoch 13 Step 51 Train Loss: 0.4961
Epoch 13 Step 101 Train Loss: 0.5179
Epoch 13 Step 151 Train Loss: 0.5215
Epoch 13 Step 201 Train Loss: 0.5750
Epoch 13 Step 251 Train Loss: 0.5547
Epoch 13 Step 301 Train Loss: 0.4832
Epoch 13 Step 351 Train Loss: 0.5646
Epoch 13 Step 401 Train Loss: 0.5286
Epoch 13 Step 451 Train Loss: 0.5934
Epoch 13 Step 501 Train Loss: 0.5273
Epoch 13 Step 551 Train Loss: 0.5496
Epoch 13: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0069 Validation Top 20 DE MSE: 0.1691. 
Epoch 14 Step 1 Train Loss: 0.6434
Epoch 14 Step 51 Train Loss: 0.5124
Epoch 14 Step 101 Train Loss: 0.5011
Epoch 14 Step 151 Train Loss: 0.5677
Epoch 14 Step 201 Train Loss: 0.5331
Epoch 14 Step 251 Train Loss: 0.6024
Epoch 14 Step 301 Train Loss: 0.6399
Epoch 14 Step 351 Train Loss: 0.5771
Epoch 14 Step 401 Train Loss: 0.5453
Epoch 14 Step 451 Train Loss: 0.5467
Epoch 14 Step 501 Train Loss: 0.4836
Epoch 14 Step 551 Train Loss: 0.5590
Epoch 14: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.1883. 
Epoch 15 Step 1 Train Loss: 0.5962
Epoch 15 Step 51 Train Loss: 0.7342
Epoch 15 Step 101 Train Loss: 0.5787
Epoch 15 Step 151 Train Loss: 0.5075
Epoch 15 Step 201 Train Loss: 0.5596
Epoch 15 Step 251 Train Loss: 0.4656
Epoch 15 Step 301 Train Loss: 0.5287
Epoch 15 Step 351 Train Loss: 0.5055
Epoch 15 Step 401 Train Loss: 0.4931
Epoch 15 Step 451 Train Loss: 0.4906
Epoch 15 Step 501 Train Loss: 0.6381
Epoch 15 Step 551 Train Loss: 0.5679
Epoch 15: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0068 Validation Top 20 DE MSE: 0.1686. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0939
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0026972154
test_unseen_single_pearson: 0.9953365161300908
test_unseen_single_mse_de: 0.09388932
test_unseen_single_pearson_de: 0.9790919096318546
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.7260848571904518
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.7999999999999999
test_unseen_single_mse_top20_de_non_dropout: 0.09391709942738358
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.022 MB of 0.023 MB uploadedwandb: / 0.022 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–„â–†â–‚â–ˆâ–ƒâ–ƒâ–â–â–ƒâ–ƒâ–â–‚â–â–â–
wandb:                                             train_de_pearson â–„â–‚â–ˆâ–â–†â–…â–‡â–‡â–†â–†â–ˆâ–‡â–‡â–‡â–‡
wandb:                                                    train_mse â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–…â–†â–‡â–…â–„â–†â–‚â–…â–„â–‚â–ƒâ–â–â–â–„â–ˆâ–…â–„â–ƒâ–ˆâ–…â–„â–ƒâ–…â–ƒâ–…â–ƒâ–â–â–‚â–ƒâ–ƒâ–„â–‚â–†â–ƒâ–ƒâ–‚â–„â–‚
wandb:                                                   val_de_mse â–â–†â–‡â–‚â–„â–‡â–ƒâ–ƒâ–‡â–ˆâ–…â–‡â–„â–…â–„
wandb:                                               val_de_pearson â–ˆâ–ƒâ–‚â–‡â–…â–‚â–†â–†â–‚â–â–„â–‚â–…â–„â–…
wandb:                                                      val_mse â–â–‡â–‡â–â–„â–‡â–‚â–‚â–‡â–ˆâ–„â–‡â–„â–…â–„
wandb:                                                  val_pearson â–ˆâ–‚â–‚â–ˆâ–…â–‚â–‡â–‡â–‚â–â–…â–‚â–…â–…â–…
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.09389
wandb:                                              test_de_pearson 0.97909
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.8
wandb:                                                     test_mse 0.0027
wandb:                                test_mse_top20_de_non_dropout 0.09392
wandb:                                                 test_pearson 0.99534
wandb:                                           test_pearson_delta 0.72608
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.8
wandb:                                       test_unseen_single_mse 0.0027
wandb:                                    test_unseen_single_mse_de 0.09389
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.09392
wandb:                                   test_unseen_single_pearson 0.99534
wandb:                                test_unseen_single_pearson_de 0.97909
wandb:                             test_unseen_single_pearson_delta 0.72608
wandb:                                                 train_de_mse 0.00684
wandb:                                             train_de_pearson 0.9979
wandb:                                                    train_mse 0.00071
wandb:                                                train_pearson 0.99886
wandb:                                                training_loss 0.53687
wandb:                                                   val_de_mse 0.16858
wandb:                                               val_de_pearson 0.96154
wandb:                                                      val_mse 0.0035
wandb:                                                  val_pearson 0.99381
wandb: 
wandb: ðŸš€ View run Dixit_combined_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/v1j2r8bi
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_062922-v1j2r8bi/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_combined/splits/dixit_combined_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_064940-5xv0e8j0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_combined_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/5xv0e8j0
Start Training...
Epoch 1 Step 1 Train Loss: 1.7411
Epoch 1 Step 51 Train Loss: 0.7477
Epoch 1 Step 101 Train Loss: 0.5937
Epoch 1 Step 151 Train Loss: 0.6579
Epoch 1 Step 201 Train Loss: 0.6514
Epoch 1 Step 251 Train Loss: 0.6439
Epoch 1 Step 301 Train Loss: 0.6369
Epoch 1 Step 351 Train Loss: 0.5949
Epoch 1 Step 401 Train Loss: 0.5030
Epoch 1 Step 451 Train Loss: 0.5995
Epoch 1 Step 501 Train Loss: 0.5902
Epoch 1 Step 551 Train Loss: 0.5904
Epoch 1: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0049. 
Train Top 20 DE MSE: 0.0116 Validation Top 20 DE MSE: 0.1813. 
Epoch 2 Step 1 Train Loss: 0.6136
Epoch 2 Step 51 Train Loss: 0.5300
Epoch 2 Step 101 Train Loss: 0.6348
Epoch 2 Step 151 Train Loss: 0.5747
Epoch 2 Step 201 Train Loss: 0.5628
Epoch 2 Step 251 Train Loss: 0.5429
Epoch 2 Step 301 Train Loss: 0.5401
Epoch 2 Step 351 Train Loss: 0.6015
Epoch 2 Step 401 Train Loss: 0.5429
Epoch 2 Step 451 Train Loss: 0.5973
Epoch 2 Step 501 Train Loss: 0.4613
Epoch 2 Step 551 Train Loss: 0.4833
Epoch 2: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0043. 
Train Top 20 DE MSE: 0.0267 Validation Top 20 DE MSE: 0.2098. 
Epoch 3 Step 1 Train Loss: 0.6148
Epoch 3 Step 51 Train Loss: 0.5456
Epoch 3 Step 101 Train Loss: 0.4692
Epoch 3 Step 151 Train Loss: 0.5588
Epoch 3 Step 201 Train Loss: 0.5925
Epoch 3 Step 251 Train Loss: 0.6199
Epoch 3 Step 301 Train Loss: 0.5682
Epoch 3 Step 351 Train Loss: 0.5617
Epoch 3 Step 401 Train Loss: 0.5848
Epoch 3 Step 451 Train Loss: 0.5522
Epoch 3 Step 501 Train Loss: 0.6846
Epoch 3 Step 551 Train Loss: 0.5058
Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0052. 
Train Top 20 DE MSE: 0.0421 Validation Top 20 DE MSE: 0.2649. 
Epoch 4 Step 1 Train Loss: 0.6050
Epoch 4 Step 51 Train Loss: 0.6182
Epoch 4 Step 101 Train Loss: 0.6181
Epoch 4 Step 151 Train Loss: 0.5820
Epoch 4 Step 201 Train Loss: 0.6207
Epoch 4 Step 251 Train Loss: 0.5183
Epoch 4 Step 301 Train Loss: 0.5506
Epoch 4 Step 351 Train Loss: 0.5047
Epoch 4 Step 401 Train Loss: 0.5200
Epoch 4 Step 451 Train Loss: 0.6542
Epoch 4 Step 501 Train Loss: 0.4805
Epoch 4 Step 551 Train Loss: 0.5722
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0064. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.3475. 
Epoch 5 Step 1 Train Loss: 0.6331
Epoch 5 Step 51 Train Loss: 0.5286
Epoch 5 Step 101 Train Loss: 0.5297
Epoch 5 Step 151 Train Loss: 0.6306
Epoch 5 Step 201 Train Loss: 0.5563
Epoch 5 Step 251 Train Loss: 0.5895
Epoch 5 Step 301 Train Loss: 0.4689
Epoch 5 Step 351 Train Loss: 0.5795
Epoch 5 Step 401 Train Loss: 0.4683
Epoch 5 Step 451 Train Loss: 0.5617
Epoch 5 Step 501 Train Loss: 0.5250
Epoch 5 Step 551 Train Loss: 0.5163
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0065. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.3607. 
Epoch 6 Step 1 Train Loss: 0.7136
Epoch 6 Step 51 Train Loss: 0.4793
Epoch 6 Step 101 Train Loss: 0.6447
Epoch 6 Step 151 Train Loss: 0.5692
Epoch 6 Step 201 Train Loss: 0.5981
Epoch 6 Step 251 Train Loss: 0.5932
Epoch 6 Step 301 Train Loss: 0.5360
Epoch 6 Step 351 Train Loss: 0.6451
Epoch 6 Step 401 Train Loss: 0.5100
Epoch 6 Step 451 Train Loss: 0.5441
Epoch 6 Step 501 Train Loss: 0.5538
Epoch 6 Step 551 Train Loss: 0.5874
Epoch 6: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0061. 
Train Top 20 DE MSE: 0.0086 Validation Top 20 DE MSE: 0.3287. 
Epoch 7 Step 1 Train Loss: 0.5713
Epoch 7 Step 51 Train Loss: 0.5933
Epoch 7 Step 101 Train Loss: 0.6263
Epoch 7 Step 151 Train Loss: 0.5554
Epoch 7 Step 201 Train Loss: 0.6351
Epoch 7 Step 251 Train Loss: 0.6294
Epoch 7 Step 301 Train Loss: 0.6228
Epoch 7 Step 351 Train Loss: 0.5841
Epoch 7 Step 401 Train Loss: 0.5454
Epoch 7 Step 451 Train Loss: 0.5404
Epoch 7 Step 501 Train Loss: 0.5988
Epoch 7 Step 551 Train Loss: 0.5533
Epoch 7: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0065. 
Train Top 20 DE MSE: 0.0079 Validation Top 20 DE MSE: 0.3564. 
Epoch 8 Step 1 Train Loss: 0.6441
Epoch 8 Step 51 Train Loss: 0.5190
Epoch 8 Step 101 Train Loss: 0.5920
Epoch 8 Step 151 Train Loss: 0.5837
Epoch 8 Step 201 Train Loss: 0.5639
Epoch 8 Step 251 Train Loss: 0.5157
Epoch 8 Step 301 Train Loss: 0.5501
Epoch 8 Step 351 Train Loss: 0.5259
Epoch 8 Step 401 Train Loss: 0.5698
Epoch 8 Step 451 Train Loss: 0.5897
Epoch 8 Step 501 Train Loss: 0.6069
Epoch 8 Step 551 Train Loss: 0.5437
Epoch 8: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0081. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.4549. 
Epoch 9 Step 1 Train Loss: 0.5960
Epoch 9 Step 51 Train Loss: 0.4999
Epoch 9 Step 101 Train Loss: 0.5339
Epoch 9 Step 151 Train Loss: 0.5414
Epoch 9 Step 201 Train Loss: 0.5301
Epoch 9 Step 251 Train Loss: 0.4935
Epoch 9 Step 301 Train Loss: 0.5397
Epoch 9 Step 351 Train Loss: 0.6560
Epoch 9 Step 401 Train Loss: 0.5148
Epoch 9 Step 451 Train Loss: 0.5678
Epoch 9 Step 501 Train Loss: 0.5448
Epoch 9 Step 551 Train Loss: 0.6130
Epoch 9: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0065. 
Train Top 20 DE MSE: 0.0074 Validation Top 20 DE MSE: 0.3560. 
Epoch 10 Step 1 Train Loss: 0.6300
Epoch 10 Step 51 Train Loss: 0.5559
Epoch 10 Step 101 Train Loss: 0.6404
Epoch 10 Step 151 Train Loss: 0.4803
Epoch 10 Step 201 Train Loss: 0.5033
Epoch 10 Step 251 Train Loss: 0.5570
Epoch 10 Step 301 Train Loss: 0.5481
Epoch 10 Step 351 Train Loss: 0.5032
Epoch 10 Step 401 Train Loss: 0.5230
Epoch 10 Step 451 Train Loss: 0.5558
Epoch 10 Step 501 Train Loss: 0.6964
Epoch 10 Step 551 Train Loss: 0.6031
Epoch 10: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0053. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.2835. 
Epoch 11 Step 1 Train Loss: 0.5749
Epoch 11 Step 51 Train Loss: 0.4619
Epoch 11 Step 101 Train Loss: 0.5751
Epoch 11 Step 151 Train Loss: 0.6431
Epoch 11 Step 201 Train Loss: 0.5818
Epoch 11 Step 251 Train Loss: 0.5689
Epoch 11 Step 301 Train Loss: 0.5124
Epoch 11 Step 351 Train Loss: 0.5726
Epoch 11 Step 401 Train Loss: 0.4894
Epoch 11 Step 451 Train Loss: 0.5447
Epoch 11 Step 501 Train Loss: 0.5294
Epoch 11 Step 551 Train Loss: 0.6520
Epoch 11: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0055. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.2943. 
Epoch 12 Step 1 Train Loss: 0.5394
Epoch 12 Step 51 Train Loss: 0.5370
Epoch 12 Step 101 Train Loss: 0.5800
Epoch 12 Step 151 Train Loss: 0.5979
Epoch 12 Step 201 Train Loss: 0.5119
Epoch 12 Step 251 Train Loss: 0.6168
Epoch 12 Step 301 Train Loss: 0.5049
Epoch 12 Step 351 Train Loss: 0.5709
Epoch 12 Step 401 Train Loss: 0.4743
Epoch 12 Step 451 Train Loss: 0.4892
Epoch 12 Step 501 Train Loss: 0.5891
Epoch 12 Step 551 Train Loss: 0.5088
Epoch 12: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0049. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.2600. 
Epoch 13 Step 1 Train Loss: 0.5550
Epoch 13 Step 51 Train Loss: 0.7603
Epoch 13 Step 101 Train Loss: 0.5583
Epoch 13 Step 151 Train Loss: 0.5856
Epoch 13 Step 201 Train Loss: 0.5846
Epoch 13 Step 251 Train Loss: 0.5946
Epoch 13 Step 301 Train Loss: 0.6272
Epoch 13 Step 351 Train Loss: 0.4858
Epoch 13 Step 401 Train Loss: 0.5671
Epoch 13 Step 451 Train Loss: 0.5067
Epoch 13 Step 501 Train Loss: 0.5557
Epoch 13 Step 551 Train Loss: 0.6739
Epoch 13: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0053. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.2816. 
Epoch 14 Step 1 Train Loss: 0.6654
Epoch 14 Step 51 Train Loss: 0.6440
Epoch 14 Step 101 Train Loss: 0.5128
Epoch 14 Step 151 Train Loss: 0.5467
Epoch 14 Step 201 Train Loss: 0.5448
Epoch 14 Step 251 Train Loss: 0.5289
Epoch 14 Step 301 Train Loss: 0.5908
Epoch 14 Step 351 Train Loss: 0.5628
Epoch 14 Step 401 Train Loss: 0.5709
Epoch 14 Step 451 Train Loss: 0.5641
Epoch 14 Step 501 Train Loss: 0.5006
Epoch 14 Step 551 Train Loss: 0.5271
Epoch 14: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0068. 
Train Top 20 DE MSE: 0.0077 Validation Top 20 DE MSE: 0.3730. 
Epoch 15 Step 1 Train Loss: 0.5668
Epoch 15 Step 51 Train Loss: 0.4638
Epoch 15 Step 101 Train Loss: 0.5636
Epoch 15 Step 151 Train Loss: 0.5743
Epoch 15 Step 201 Train Loss: 0.5435
Epoch 15 Step 251 Train Loss: 0.5226
Epoch 15 Step 301 Train Loss: 0.6390
Epoch 15 Step 351 Train Loss: 0.5099
Epoch 15 Step 401 Train Loss: 0.6083
Epoch 15 Step 451 Train Loss: 0.6398
Epoch 15 Step 501 Train Loss: 0.6707
Epoch 15 Step 551 Train Loss: 0.5923
Epoch 15: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0059. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.3216. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0551
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002216852
test_unseen_single_pearson: 0.996087002347073
test_unseen_single_mse_de: 0.055075858
test_unseen_single_pearson_de: 0.9870701113423697
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5963152759606584
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.09999999999999999
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9583333333333335
test_unseen_single_mse_top20_de_non_dropout: 0.055075854792869666
Done!
wandb: - 0.001 MB of 0.020 MB uploadedwandb: \ 0.003 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–‚â–…â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–‡â–„â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–‡â–„â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–‚â–„â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–‚â–„â–‡â–†â–ˆâ–†â–‚â–ƒâ–…â–‚â–†â–„â–…â–‚â–„â–â–„â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–…â–ƒâ–â–„â–ƒâ–„â–‚â–„â–†â–…â–ƒâ–‚â–â–„â–„
wandb:                                                   val_de_mse â–â–‚â–ƒâ–…â–†â–…â–…â–ˆâ–…â–„â–„â–ƒâ–„â–†â–…
wandb:                                               val_de_pearson â–ˆâ–‡â–†â–„â–„â–„â–„â–â–„â–†â–…â–†â–†â–ƒâ–…
wandb:                                                      val_mse â–‚â–â–ƒâ–…â–…â–„â–…â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–†â–„
wandb:                                                  val_pearson â–‡â–ˆâ–†â–„â–„â–…â–„â–â–„â–†â–†â–‡â–†â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.05508
wandb:                                              test_de_pearson 0.98707
wandb:               test_frac_opposite_direction_top20_non_dropout 0.1
wandb:                          test_frac_sigma_below_1_non_dropout 0.95833
wandb:                                                     test_mse 0.00222
wandb:                                test_mse_top20_de_non_dropout 0.05508
wandb:                                                 test_pearson 0.99609
wandb:                                           test_pearson_delta 0.59632
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.1
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.95833
wandb:                                       test_unseen_single_mse 0.00222
wandb:                                    test_unseen_single_mse_de 0.05508
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.05508
wandb:                                   test_unseen_single_pearson 0.99609
wandb:                                test_unseen_single_pearson_de 0.98707
wandb:                             test_unseen_single_pearson_delta 0.59632
wandb:                                                 train_de_mse 0.00745
wandb:                                             train_de_pearson 0.99759
wandb:                                                    train_mse 0.00079
wandb:                                                train_pearson 0.99872
wandb:                                                training_loss 0.6065
wandb:                                                   val_de_mse 0.32156
wandb:                                               val_de_pearson 0.92243
wandb:                                                      val_mse 0.00593
wandb:                                                  val_pearson 0.98962
wandb: 
wandb: ðŸš€ View run Dixit_combined_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/5xv0e8j0
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_064940-5xv0e8j0/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/11 [00:00<?, ?it/s]  9%|â–‰         | 1/11 [04:47<47:56, 287.68s/it] 18%|â–ˆâ–Š        | 2/11 [08:37<38:00, 253.37s/it] 27%|â–ˆâ–ˆâ–‹       | 3/11 [09:57<23:14, 174.32s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 4/11 [11:37<16:54, 144.95s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [13:52<14:08, 141.44s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [13:53<07:49, 93.84s/it]  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 7/11 [15:30<06:18, 94.65s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [17:24<05:02, 100.96s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9/11 [19:05<03:21, 100.89s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [20:52<01:42, 102.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [23:25<00:00, 118.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [23:25<00:00, 127.73s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396858/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396858/splits/dixit_gsm2396858_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_073336-meq4ru3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396858_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/meq4ru3c
  0%|          | 0/3625 [00:00<?, ?it/s]  0%|          | 7/3625 [00:00<00:54, 66.97it/s]  0%|          | 15/3625 [00:00<00:50, 71.69it/s]  1%|          | 24/3625 [00:00<00:46, 77.59it/s]  1%|          | 34/3625 [00:00<00:41, 85.67it/s]  1%|          | 44/3625 [00:00<00:40, 89.25it/s]  1%|â–         | 53/3625 [00:00<00:41, 85.61it/s]  2%|â–         | 64/3625 [00:00<00:39, 91.28it/s]  2%|â–         | 74/3625 [00:00<00:40, 88.25it/s]  2%|â–         | 84/3625 [00:00<00:39, 89.23it/s]  3%|â–Ž         | 94/3625 [00:01<00:38, 90.91it/s]  3%|â–Ž         | 104/3625 [00:01<00:39, 89.83it/s]  3%|â–Ž         | 114/3625 [00:01<00:39, 89.22it/s]  3%|â–Ž         | 124/3625 [00:01<00:38, 90.38it/s]  4%|â–Ž         | 134/3625 [00:01<00:38, 90.34it/s]  4%|â–         | 144/3625 [00:01<00:43, 79.73it/s]  4%|â–         | 153/3625 [00:01<00:47, 72.38it/s]  4%|â–         | 161/3625 [00:01<00:48, 70.89it/s]  5%|â–         | 169/3625 [00:02<00:50, 67.83it/s]  5%|â–         | 176/3625 [00:02<00:53, 64.83it/s]  5%|â–Œ         | 183/3625 [00:02<00:53, 64.68it/s]  5%|â–Œ         | 191/3625 [00:02<00:51, 66.31it/s]  5%|â–Œ         | 198/3625 [00:02<00:55, 61.91it/s]  6%|â–Œ         | 206/3625 [00:02<00:53, 63.83it/s]  6%|â–Œ         | 213/3625 [00:02<00:53, 63.44it/s]  6%|â–Œ         | 220/3625 [00:02<00:53, 63.16it/s]  6%|â–‹         | 227/3625 [00:03<00:54, 62.15it/s]  6%|â–‹         | 234/3625 [00:03<00:54, 62.79it/s]  7%|â–‹         | 241/3625 [00:03<00:52, 64.11it/s]  7%|â–‹         | 248/3625 [00:03<00:53, 63.55it/s]  7%|â–‹         | 255/3625 [00:03<00:53, 63.22it/s]  7%|â–‹         | 262/3625 [00:03<00:52, 64.62it/s]  7%|â–‹         | 269/3625 [00:03<00:52, 64.25it/s]  8%|â–Š         | 276/3625 [00:03<00:53, 62.82it/s]  8%|â–Š         | 283/3625 [00:03<00:53, 62.63it/s]  8%|â–Š         | 291/3625 [00:04<00:51, 64.80it/s]  8%|â–Š         | 298/3625 [00:04<00:52, 62.88it/s]  8%|â–Š         | 305/3625 [00:04<00:53, 62.13it/s]  9%|â–Š         | 312/3625 [00:04<00:54, 61.30it/s]  9%|â–‰         | 320/3625 [00:04<00:51, 64.61it/s]  9%|â–‰         | 327/3625 [00:04<00:51, 63.50it/s]  9%|â–‰         | 334/3625 [00:04<00:51, 63.49it/s]  9%|â–‰         | 342/3625 [00:04<00:49, 65.68it/s] 10%|â–‰         | 349/3625 [00:04<00:50, 64.95it/s] 10%|â–‰         | 356/3625 [00:05<00:51, 63.43it/s] 10%|â–ˆ         | 363/3625 [00:05<00:50, 65.13it/s] 10%|â–ˆ         | 370/3625 [00:05<00:50, 64.22it/s] 10%|â–ˆ         | 377/3625 [00:05<00:51, 63.58it/s] 11%|â–ˆ         | 384/3625 [00:05<00:50, 64.21it/s] 11%|â–ˆ         | 392/3625 [00:05<00:48, 66.00it/s] 11%|â–ˆ         | 399/3625 [00:05<00:50, 64.02it/s] 11%|â–ˆ         | 406/3625 [00:05<00:50, 63.25it/s] 11%|â–ˆâ–        | 413/3625 [00:05<00:49, 64.50it/s] 12%|â–ˆâ–        | 420/3625 [00:06<00:50, 63.84it/s] 12%|â–ˆâ–        | 427/3625 [00:06<00:50, 63.60it/s] 12%|â–ˆâ–        | 434/3625 [00:06<00:49, 63.89it/s] 12%|â–ˆâ–        | 442/3625 [00:06<00:48, 66.04it/s] 12%|â–ˆâ–        | 449/3625 [00:06<00:48, 65.07it/s] 13%|â–ˆâ–Ž        | 456/3625 [00:06<00:49, 63.71it/s] 13%|â–ˆâ–Ž        | 463/3625 [00:06<00:48, 64.55it/s] 13%|â–ˆâ–Ž        | 470/3625 [00:06<00:49, 64.10it/s] 13%|â–ˆâ–Ž        | 477/3625 [00:06<00:50, 62.80it/s] 13%|â–ˆâ–Ž        | 484/3625 [00:07<00:49, 63.81it/s] 14%|â–ˆâ–Ž        | 492/3625 [00:07<00:47, 65.69it/s] 14%|â–ˆâ–        | 499/3625 [00:07<00:48, 64.00it/s] 14%|â–ˆâ–        | 506/3625 [00:07<00:48, 64.06it/s] 14%|â–ˆâ–        | 514/3625 [00:07<00:46, 66.88it/s] 14%|â–ˆâ–        | 521/3625 [00:07<00:47, 65.02it/s] 15%|â–ˆâ–        | 528/3625 [00:07<00:48, 63.48it/s] 15%|â–ˆâ–        | 535/3625 [00:07<00:47, 64.80it/s] 15%|â–ˆâ–        | 542/3625 [00:07<00:47, 65.55it/s] 15%|â–ˆâ–Œ        | 549/3625 [00:08<00:47, 64.78it/s] 15%|â–ˆâ–Œ        | 556/3625 [00:08<00:47, 64.05it/s] 16%|â–ˆâ–Œ        | 563/3625 [00:08<00:47, 64.66it/s] 16%|â–ˆâ–Œ        | 570/3625 [00:08<00:47, 64.99it/s] 16%|â–ˆâ–Œ        | 577/3625 [00:08<00:48, 63.31it/s] 16%|â–ˆâ–Œ        | 584/3625 [00:08<00:48, 63.03it/s] 16%|â–ˆâ–‹        | 591/3625 [00:08<00:47, 64.52it/s] 16%|â–ˆâ–‹        | 598/3625 [00:08<00:48, 62.69it/s] 17%|â–ˆâ–‹        | 605/3625 [00:08<00:50, 59.70it/s] 17%|â–ˆâ–‹        | 613/3625 [00:09<00:46, 65.07it/s] 17%|â–ˆâ–‹        | 620/3625 [00:09<00:46, 64.69it/s] 17%|â–ˆâ–‹        | 627/3625 [00:09<00:46, 63.85it/s] 17%|â–ˆâ–‹        | 634/3625 [00:09<00:46, 64.64it/s] 18%|â–ˆâ–Š        | 641/3625 [00:09<00:45, 66.09it/s] 18%|â–ˆâ–Š        | 648/3625 [00:09<00:45, 65.10it/s] 18%|â–ˆâ–Š        | 655/3625 [00:09<00:48, 60.72it/s] 18%|â–ˆâ–Š        | 664/3625 [00:09<00:44, 66.46it/s] 19%|â–ˆâ–Š        | 671/3625 [00:09<00:45, 64.96it/s] 19%|â–ˆâ–Š        | 678/3625 [00:10<00:47, 61.79it/s] 19%|â–ˆâ–‰        | 686/3625 [00:10<00:44, 66.40it/s] 19%|â–ˆâ–‰        | 693/3625 [00:10<00:44, 65.80it/s] 19%|â–ˆâ–‰        | 700/3625 [00:10<00:45, 64.64it/s] 20%|â–ˆâ–‰        | 707/3625 [00:10<00:46, 63.07it/s] 20%|â–ˆâ–‰        | 715/3625 [00:10<00:44, 65.70it/s] 20%|â–ˆâ–‰        | 722/3625 [00:10<00:45, 63.45it/s] 20%|â–ˆâ–ˆ        | 729/3625 [00:10<00:45, 63.14it/s] 20%|â–ˆâ–ˆ        | 736/3625 [00:10<00:44, 64.53it/s] 20%|â–ˆâ–ˆ        | 743/3625 [00:11<00:44, 65.06it/s] 21%|â–ˆâ–ˆ        | 750/3625 [00:11<00:44, 64.37it/s] 21%|â–ˆâ–ˆ        | 757/3625 [00:11<00:45, 62.66it/s] 21%|â–ˆâ–ˆ        | 765/3625 [00:11<00:43, 65.14it/s] 21%|â–ˆâ–ˆâ–       | 772/3625 [00:11<00:44, 63.43it/s] 21%|â–ˆâ–ˆâ–       | 779/3625 [00:11<00:44, 63.42it/s] 22%|â–ˆâ–ˆâ–       | 786/3625 [00:11<00:44, 63.97it/s] 22%|â–ˆâ–ˆâ–       | 793/3625 [00:11<00:43, 64.82it/s] 22%|â–ˆâ–ˆâ–       | 800/3625 [00:11<00:44, 63.90it/s] 22%|â–ˆâ–ˆâ–       | 807/3625 [00:12<00:44, 63.85it/s] 22%|â–ˆâ–ˆâ–       | 815/3625 [00:12<00:42, 66.19it/s] 23%|â–ˆâ–ˆâ–Ž       | 825/3625 [00:12<00:37, 74.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 833/3625 [00:12<00:37, 73.82it/s] 23%|â–ˆâ–ˆâ–Ž       | 843/3625 [00:12<00:35, 78.90it/s] 23%|â–ˆâ–ˆâ–Ž       | 851/3625 [00:12<00:35, 78.26it/s] 24%|â–ˆâ–ˆâ–Ž       | 860/3625 [00:12<00:35, 77.91it/s] 24%|â–ˆâ–ˆâ–       | 870/3625 [00:12<00:33, 82.51it/s] 24%|â–ˆâ–ˆâ–       | 879/3625 [00:12<00:32, 83.37it/s] 25%|â–ˆâ–ˆâ–       | 889/3625 [00:13<00:34, 80.41it/s] 25%|â–ˆâ–ˆâ–       | 900/3625 [00:13<00:32, 83.65it/s] 25%|â–ˆâ–ˆâ–Œ       | 909/3625 [00:13<00:32, 84.15it/s] 25%|â–ˆâ–ˆâ–Œ       | 918/3625 [00:13<00:32, 84.21it/s] 26%|â–ˆâ–ˆâ–Œ       | 928/3625 [00:13<00:30, 87.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 937/3625 [00:13<00:32, 83.94it/s] 26%|â–ˆâ–ˆâ–Œ       | 946/3625 [00:13<00:31, 84.15it/s] 26%|â–ˆâ–ˆâ–‹       | 956/3625 [00:13<00:30, 87.04it/s] 27%|â–ˆâ–ˆâ–‹       | 965/3625 [00:13<00:30, 86.24it/s] 27%|â–ˆâ–ˆâ–‹       | 974/3625 [00:14<00:31, 83.55it/s] 27%|â–ˆâ–ˆâ–‹       | 983/3625 [00:14<00:32, 82.06it/s] 27%|â–ˆâ–ˆâ–‹       | 992/3625 [00:14<00:33, 78.67it/s] 28%|â–ˆâ–ˆâ–Š       | 1001/3625 [00:14<00:33, 78.65it/s] 28%|â–ˆâ–ˆâ–Š       | 1009/3625 [00:14<00:33, 78.77it/s] 28%|â–ˆâ–ˆâ–Š       | 1017/3625 [00:14<00:33, 78.59it/s] 28%|â–ˆâ–ˆâ–Š       | 1026/3625 [00:14<00:33, 78.42it/s] 29%|â–ˆâ–ˆâ–Š       | 1035/3625 [00:14<00:31, 81.42it/s] 29%|â–ˆâ–ˆâ–‰       | 1044/3625 [00:14<00:31, 81.17it/s] 29%|â–ˆâ–ˆâ–‰       | 1053/3625 [00:15<00:32, 78.14it/s] 29%|â–ˆâ–ˆâ–‰       | 1061/3625 [00:15<00:32, 78.57it/s] 30%|â–ˆâ–ˆâ–‰       | 1070/3625 [00:15<00:31, 80.99it/s] 30%|â–ˆâ–ˆâ–‰       | 1079/3625 [00:15<00:32, 78.02it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1088/3625 [00:15<00:32, 78.71it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1097/3625 [00:15<00:30, 81.72it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1106/3625 [00:15<00:31, 80.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1115/3625 [00:15<00:32, 77.79it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1124/3625 [00:15<00:30, 80.72it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1133/3625 [00:16<00:31, 80.08it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1142/3625 [00:16<00:32, 77.30it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1151/3625 [00:16<00:30, 80.45it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1160/3625 [00:16<00:31, 77.68it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1169/3625 [00:16<00:30, 80.61it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1178/3625 [00:16<00:30, 80.13it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1187/3625 [00:16<00:31, 77.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1196/3625 [00:16<00:30, 80.57it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1205/3625 [00:16<00:30, 79.94it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1214/3625 [00:17<00:31, 77.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1223/3625 [00:17<00:29, 80.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1232/3625 [00:17<00:30, 79.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1241/3625 [00:17<00:30, 77.37it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1250/3625 [00:17<00:29, 80.21it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1259/3625 [00:17<00:30, 77.39it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1267/3625 [00:17<00:30, 77.80it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1276/3625 [00:17<00:30, 78.14it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1285/3625 [00:17<00:28, 81.04it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1294/3625 [00:18<00:29, 80.36it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1303/3625 [00:18<00:29, 79.38it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1311/3625 [00:18<00:30, 76.33it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1320/3625 [00:18<00:29, 79.04it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1328/3625 [00:18<00:29, 78.47it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1336/3625 [00:18<00:30, 75.83it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1345/3625 [00:18<00:28, 79.32it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1353/3625 [00:18<00:28, 78.85it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1361/3625 [00:18<00:30, 75.36it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1370/3625 [00:19<00:29, 76.04it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1379/3625 [00:19<00:28, 79.26it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1387/3625 [00:19<00:28, 78.93it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1395/3625 [00:19<00:29, 76.00it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1404/3625 [00:19<00:29, 76.45it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1412/3625 [00:19<00:29, 74.34it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1422/3625 [00:19<00:27, 80.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1431/3625 [00:19<00:27, 79.11it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1439/3625 [00:19<00:27, 78.80it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1447/3625 [00:20<00:27, 78.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1455/3625 [00:20<00:28, 75.10it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1463/3625 [00:20<00:29, 73.25it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1472/3625 [00:20<00:27, 77.09it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1481/3625 [00:20<00:26, 79.64it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1490/3625 [00:20<00:27, 76.31it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1499/3625 [00:20<00:26, 78.99it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1507/3625 [00:20<00:26, 78.68it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1515/3625 [00:20<00:27, 76.24it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1524/3625 [00:21<00:26, 78.97it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1532/3625 [00:21<00:26, 78.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1540/3625 [00:21<00:27, 75.99it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1549/3625 [00:21<00:26, 79.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1557/3625 [00:21<00:26, 78.84it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1565/3625 [00:21<00:27, 76.04it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1574/3625 [00:21<00:25, 79.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1582/3625 [00:21<00:25, 79.00it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1590/3625 [00:21<00:25, 78.77it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1598/3625 [00:22<00:25, 78.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1606/3625 [00:22<00:26, 75.14it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1615/3625 [00:22<00:26, 75.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1624/3625 [00:22<00:25, 78.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1632/3625 [00:22<00:25, 78.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1640/3625 [00:22<00:26, 75.89it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1649/3625 [00:22<00:25, 79.03it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1657/3625 [00:22<00:25, 78.52it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1665/3625 [00:22<00:25, 75.54it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1673/3625 [00:23<00:25, 75.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1682/3625 [00:23<00:24, 78.24it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1690/3625 [00:23<00:25, 75.56it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1699/3625 [00:23<00:24, 78.94it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1707/3625 [00:23<00:24, 77.88it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1715/3625 [00:23<00:26, 72.67it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1725/3625 [00:23<00:24, 78.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1733/3625 [00:23<00:24, 77.76it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1741/3625 [00:23<00:24, 77.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1749/3625 [00:23<00:24, 77.87it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1757/3625 [00:24<00:24, 77.35it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1765/3625 [00:24<00:25, 72.32it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1775/3625 [00:24<00:23, 78.92it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1783/3625 [00:24<00:23, 78.10it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1791/3625 [00:24<00:23, 77.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1799/3625 [00:24<00:23, 77.68it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1807/3625 [00:24<00:23, 77.20it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1815/3625 [00:24<00:24, 74.54it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1824/3625 [00:24<00:23, 78.04it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1832/3625 [00:25<00:23, 77.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1840/3625 [00:25<00:22, 77.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1848/3625 [00:25<00:22, 77.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1856/3625 [00:25<00:22, 77.27it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1864/3625 [00:25<00:22, 77.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1872/3625 [00:25<00:22, 77.73it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1880/3625 [00:25<00:22, 77.12it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1888/3625 [00:25<00:23, 74.89it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1897/3625 [00:25<00:21, 78.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1905/3625 [00:26<00:21, 78.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1913/3625 [00:26<00:23, 72.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1923/3625 [00:26<00:21, 79.38it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1932/3625 [00:26<00:21, 79.02it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1940/3625 [00:26<00:21, 78.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1948/3625 [00:26<00:21, 78.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1956/3625 [00:26<00:21, 77.92it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1964/3625 [00:26<00:21, 77.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1972/3625 [00:26<00:21, 77.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1980/3625 [00:26<00:21, 77.15it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1988/3625 [00:27<00:21, 77.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1996/3625 [00:27<00:21, 77.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2004/3625 [00:27<00:21, 77.13it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2012/3625 [00:27<00:22, 72.35it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2021/3625 [00:27<00:20, 76.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2030/3625 [00:27<00:20, 79.45it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2038/3625 [00:27<00:21, 73.86it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2048/3625 [00:27<00:19, 80.25it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2057/3625 [00:27<00:19, 79.22it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2066/3625 [00:28<00:19, 78.69it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2074/3625 [00:28<00:19, 78.05it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2082/3625 [00:28<00:19, 77.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2090/3625 [00:28<00:20, 74.69it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2099/3625 [00:28<00:19, 77.75it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2107/3625 [00:28<00:19, 77.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2115/3625 [00:28<00:20, 74.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2124/3625 [00:28<00:19, 78.15it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2132/3625 [00:28<00:19, 77.93it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2140/3625 [00:29<00:19, 77.84it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2148/3625 [00:29<00:19, 77.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2156/3625 [00:29<00:18, 77.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2164/3625 [00:29<00:18, 77.99it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2172/3625 [00:29<00:18, 77.93it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2180/3625 [00:29<00:18, 77.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2188/3625 [00:29<00:18, 78.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2196/3625 [00:29<00:18, 77.93it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2204/3625 [00:29<00:18, 74.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2212/3625 [00:29<00:18, 75.61it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2220/3625 [00:30<00:19, 73.49it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2230/3625 [00:30<00:17, 79.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2239/3625 [00:30<00:17, 78.86it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2247/3625 [00:30<00:18, 75.61it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2255/3625 [00:30<00:18, 76.00it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2263/3625 [00:30<00:17, 76.36it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2272/3625 [00:30<00:17, 77.30it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2280/3625 [00:30<00:18, 71.95it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2288/3625 [00:31<00:19, 70.35it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2296/3625 [00:31<00:20, 65.68it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2304/3625 [00:31<00:19, 67.45it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2311/3625 [00:31<00:19, 67.32it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2318/3625 [00:31<00:19, 68.02it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2325/3625 [00:31<00:19, 66.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2332/3625 [00:31<00:19, 65.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2339/3625 [00:31<00:19, 66.01it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2346/3625 [00:31<00:19, 66.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2353/3625 [00:32<00:19, 64.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2360/3625 [00:32<00:19, 64.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2367/3625 [00:32<00:19, 65.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2374/3625 [00:32<00:19, 64.85it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2381/3625 [00:32<00:19, 63.46it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2388/3625 [00:32<00:19, 62.88it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2399/3625 [00:32<00:16, 74.56it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2407/3625 [00:32<00:16, 75.84it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2415/3625 [00:32<00:15, 76.37it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2425/3625 [00:32<00:14, 80.02it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2435/3625 [00:33<00:14, 82.91it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2444/3625 [00:33<00:13, 84.45it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2454/3625 [00:33<00:13, 88.87it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2463/3625 [00:33<00:13, 85.93it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2472/3625 [00:33<00:13, 86.66it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2481/3625 [00:33<00:13, 87.28it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2490/3625 [00:33<00:12, 87.74it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2500/3625 [00:33<00:12, 88.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2509/3625 [00:33<00:12, 88.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2518/3625 [00:34<00:13, 82.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2530/3625 [00:34<00:11, 91.99it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2540/3625 [00:34<00:11, 90.99it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2550/3625 [00:34<00:12, 87.09it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2559/3625 [00:34<00:12, 86.86it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2569/3625 [00:34<00:12, 83.69it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2579/3625 [00:34<00:12, 86.46it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2588/3625 [00:34<00:12, 86.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2598/3625 [00:34<00:11, 86.48it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2607/3625 [00:35<00:11, 86.91it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2616/3625 [00:35<00:11, 86.95it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2626/3625 [00:35<00:11, 86.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2636/3625 [00:35<00:11, 89.49it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2645/3625 [00:35<00:11, 85.25it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2654/3625 [00:35<00:11, 84.98it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2664/3625 [00:35<00:11, 85.14it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2673/3625 [00:35<00:11, 82.67it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2683/3625 [00:35<00:10, 85.78it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2692/3625 [00:36<00:10, 85.95it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2702/3625 [00:36<00:10, 85.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2712/3625 [00:36<00:10, 87.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2721/3625 [00:36<00:11, 81.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2732/3625 [00:36<00:10, 88.09it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2741/3625 [00:36<00:10, 82.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2752/3625 [00:36<00:10, 86.09it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2762/3625 [00:36<00:09, 88.74it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2771/3625 [00:36<00:09, 86.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2780/3625 [00:37<00:09, 86.55it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2790/3625 [00:37<00:09, 89.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2799/3625 [00:37<00:09, 89.19it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2808/3625 [00:37<00:09, 88.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2817/3625 [00:37<00:09, 88.57it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2826/3625 [00:37<00:09, 88.36it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2835/3625 [00:37<00:09, 87.72it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2845/3625 [00:37<00:08, 88.73it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2854/3625 [00:37<00:08, 88.95it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2863/3625 [00:38<00:08, 85.60it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2872/3625 [00:38<00:08, 85.73it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2882/3625 [00:38<00:08, 89.40it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2892/3625 [00:38<00:08, 89.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2901/3625 [00:38<00:08, 88.94it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2910/3625 [00:38<00:08, 89.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2919/3625 [00:38<00:07, 88.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2928/3625 [00:38<00:08, 85.26it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2937/3625 [00:38<00:08, 85.69it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2947/3625 [00:38<00:07, 88.71it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2956/3625 [00:39<00:07, 87.95it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2965/3625 [00:39<00:07, 88.06it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2974/3625 [00:39<00:07, 86.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2983/3625 [00:39<00:07, 84.04it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2993/3625 [00:39<00:07, 87.65it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3002/3625 [00:39<00:07, 84.46it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3011/3625 [00:39<00:07, 85.07it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3021/3625 [00:39<00:06, 88.76it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3030/3625 [00:39<00:06, 88.35it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3039/3625 [00:40<00:06, 88.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3048/3625 [00:40<00:06, 82.48it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3060/3625 [00:40<00:06, 90.01it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3070/3625 [00:40<00:06, 89.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3079/3625 [00:40<00:06, 89.49it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3088/3625 [00:40<00:06, 87.03it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3098/3625 [00:40<00:05, 90.02it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3108/3625 [00:40<00:06, 86.10it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3118/3625 [00:40<00:05, 86.22it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3127/3625 [00:41<00:05, 86.99it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3137/3625 [00:41<00:05, 90.26it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3147/3625 [00:41<00:05, 89.37it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3156/3625 [00:41<00:05, 89.31it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3165/3625 [00:41<00:05, 88.78it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3174/3625 [00:41<00:05, 85.47it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3184/3625 [00:41<00:04, 89.05it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3194/3625 [00:41<00:04, 89.26it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3203/3625 [00:41<00:04, 89.47it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3212/3625 [00:41<00:04, 86.98it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3222/3625 [00:42<00:04, 86.98it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3232/3625 [00:42<00:04, 86.88it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3243/3625 [00:42<00:04, 90.60it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3253/3625 [00:42<00:04, 89.14it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3262/3625 [00:42<00:04, 88.97it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3272/3625 [00:42<00:03, 89.36it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3281/3625 [00:42<00:04, 85.88it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3291/3625 [00:42<00:03, 87.34it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3301/3625 [00:42<00:03, 90.06it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3311/3625 [00:43<00:03, 86.90it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3322/3625 [00:43<00:03, 90.59it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3332/3625 [00:43<00:03, 89.85it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3342/3625 [00:43<00:03, 87.54it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3351/3625 [00:43<00:03, 87.85it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3361/3625 [00:43<00:02, 90.79it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3371/3625 [00:43<00:02, 87.99it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3380/3625 [00:43<00:02, 85.48it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3392/3625 [00:44<00:02, 89.62it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3403/3625 [00:44<00:02, 89.89it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3413/3625 [00:44<00:02, 91.91it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3423/3625 [00:44<00:02, 91.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3433/3625 [00:44<00:02, 90.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3443/3625 [00:44<00:02, 90.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3453/3625 [00:44<00:02, 85.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3465/3625 [00:44<00:01, 89.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3474/3625 [00:44<00:01, 89.29it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3484/3625 [00:45<00:01, 91.77it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3494/3625 [00:45<00:01, 91.94it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3504/3625 [00:45<00:01, 91.56it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3514/3625 [00:45<00:01, 88.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3524/3625 [00:45<00:01, 89.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3534/3625 [00:45<00:01, 89.00it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3544/3625 [00:45<00:00, 89.71it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3553/3625 [00:45<00:00, 84.67it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3562/3625 [00:45<00:00, 81.15it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3571/3625 [00:46<00:00, 78.79it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3579/3625 [00:46<00:00, 77.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3587/3625 [00:46<00:00, 77.07it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3595/3625 [00:46<00:00, 76.47it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3603/3625 [00:46<00:00, 76.14it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3614/3625 [00:46<00:00, 83.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3623/3625 [00:46<00:00, 84.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3625/3625 [00:46<00:00, 77.56it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 1.2547
Epoch 1 Step 51 Train Loss: 0.6066
Epoch 1 Step 101 Train Loss: 0.5270
Epoch 1 Step 151 Train Loss: 0.5740
Epoch 1 Step 201 Train Loss: 0.4705
Epoch 1 Step 251 Train Loss: 0.4383
Epoch 1 Step 301 Train Loss: 0.4384
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0065 Validation Top 20 DE MSE: 0.0036. 
Epoch 2 Step 1 Train Loss: 0.4902
Epoch 2 Step 51 Train Loss: 0.3711
Epoch 2 Step 101 Train Loss: 0.4120
Epoch 2 Step 151 Train Loss: 0.4315
Epoch 2 Step 201 Train Loss: 0.4788
Epoch 2 Step 251 Train Loss: 0.5158
Epoch 2 Step 301 Train Loss: 0.6605
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0021. 
Epoch 3 Step 1 Train Loss: 0.4481
Epoch 3 Step 51 Train Loss: 0.4392
Epoch 3 Step 101 Train Loss: 0.4906
Epoch 3 Step 151 Train Loss: 0.5319
Epoch 3 Step 201 Train Loss: 0.4365
Epoch 3 Step 251 Train Loss: 0.4545
Epoch 3 Step 301 Train Loss: 0.4952
Epoch 3: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0021. 
Epoch 4 Step 1 Train Loss: 0.4547
Epoch 4 Step 51 Train Loss: 0.4685
Epoch 4 Step 101 Train Loss: 0.4790
Epoch 4 Step 151 Train Loss: 0.4430
Epoch 4 Step 201 Train Loss: 0.4489
Epoch 4 Step 251 Train Loss: 0.4447
Epoch 4 Step 301 Train Loss: 0.5188
Epoch 4: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0019. 
Epoch 5 Step 1 Train Loss: 0.4658
Epoch 5 Step 51 Train Loss: 0.4463
Epoch 5 Step 101 Train Loss: 0.4395
Epoch 5 Step 151 Train Loss: 0.4695
Epoch 5 Step 201 Train Loss: 0.4604
Epoch 5 Step 251 Train Loss: 0.4885
Epoch 5 Step 301 Train Loss: 0.5111
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0020. 
Epoch 6 Step 1 Train Loss: 0.4640
Epoch 6 Step 51 Train Loss: 0.5306
Epoch 6 Step 101 Train Loss: 0.4309
Epoch 6 Step 151 Train Loss: 0.4371
Epoch 6 Step 201 Train Loss: 0.5644
Epoch 6 Step 251 Train Loss: 0.4504
Epoch 6 Step 301 Train Loss: 0.4394
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0016. 
Epoch 7 Step 1 Train Loss: 0.4131
Epoch 7 Step 51 Train Loss: 0.4615
Epoch 7 Step 101 Train Loss: 0.4206
Epoch 7 Step 151 Train Loss: 0.4274
Epoch 7 Step 201 Train Loss: 0.5748
Epoch 7 Step 251 Train Loss: 0.4682
Epoch 7 Step 301 Train Loss: 0.4620
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0017. 
Epoch 8 Step 1 Train Loss: 0.4368
Epoch 8 Step 51 Train Loss: 0.4274
Epoch 8 Step 101 Train Loss: 0.4428
Epoch 8 Step 151 Train Loss: 0.4550
Epoch 8 Step 201 Train Loss: 0.4387
Epoch 8 Step 251 Train Loss: 0.5666
Epoch 8 Step 301 Train Loss: 0.4549
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0018. 
Epoch 9 Step 1 Train Loss: 0.5264
Epoch 9 Step 51 Train Loss: 0.4329
Epoch 9 Step 101 Train Loss: 0.4727
Epoch 9 Step 151 Train Loss: 0.4807
Epoch 9 Step 201 Train Loss: 0.3997
Epoch 9 Step 251 Train Loss: 0.4758
Epoch 9 Step 301 Train Loss: 0.5060
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0017. 
Epoch 10 Step 1 Train Loss: 0.4075
Epoch 10 Step 51 Train Loss: 0.4695
Epoch 10 Step 101 Train Loss: 0.4513
Epoch 10 Step 151 Train Loss: 0.4745
Epoch 10 Step 201 Train Loss: 0.4896
Epoch 10 Step 251 Train Loss: 0.4223
Epoch 10 Step 301 Train Loss: 0.4436
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0017. 
Epoch 11 Step 1 Train Loss: 0.4974
Epoch 11 Step 51 Train Loss: 0.4870
Epoch 11 Step 101 Train Loss: 0.3973
Epoch 11 Step 151 Train Loss: 0.4595
Epoch 11 Step 201 Train Loss: 0.4220
Epoch 11 Step 251 Train Loss: 0.4685
Epoch 11 Step 301 Train Loss: 0.4663
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0017. 
Epoch 12 Step 1 Train Loss: 0.4958
Epoch 12 Step 51 Train Loss: 0.4471
Epoch 12 Step 101 Train Loss: 0.5175
Epoch 12 Step 151 Train Loss: 0.4815
Epoch 12 Step 201 Train Loss: 0.4610
Epoch 12 Step 251 Train Loss: 0.4537
Epoch 12 Step 301 Train Loss: 0.5536
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0016. 
Epoch 13 Step 1 Train Loss: 0.4762
Epoch 13 Step 51 Train Loss: 0.3997
Epoch 13 Step 101 Train Loss: 0.4430
Epoch 13 Step 151 Train Loss: 0.4858
Epoch 13 Step 201 Train Loss: 0.5108
Epoch 13 Step 251 Train Loss: 0.4822
Epoch 13 Step 301 Train Loss: 0.3911
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0017. 
Epoch 14 Step 1 Train Loss: 0.5602
Epoch 14 Step 51 Train Loss: 0.4785
Epoch 14 Step 101 Train Loss: 0.4629
Epoch 14 Step 151 Train Loss: 0.4353
Epoch 14 Step 201 Train Loss: 0.4679
Epoch 14 Step 251 Train Loss: 0.4830
Epoch 14 Step 301 Train Loss: 0.4856
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0017. 
Epoch 15 Step 1 Train Loss: 0.4907
Epoch 15 Step 51 Train Loss: 0.5360
Epoch 15 Step 101 Train Loss: 0.5576
Epoch 15 Step 151 Train Loss: 0.4288
Epoch 15 Step 201 Train Loss: 0.4314
Epoch 15 Step 251 Train Loss: 0.4918
Epoch 15 Step 301 Train Loss: 0.4733
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0017. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0019
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0004232946
test_unseen_single_pearson: 0.9992457279853345
test_unseen_single_mse_de: 0.0019258164
test_unseen_single_pearson_de: 0.9997112873919011
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.44085886833369364
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.06666666666666667
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9833333333333334
test_unseen_single_mse_top20_de_non_dropout: 0.002294180720734449
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.017 MB uploadedwandb: | 0.003 MB of 0.021 MB uploadedwandb: / 0.021 MB of 0.021 MB uploadedwandb: - 0.021 MB of 0.021 MB uploadedwandb: \ 0.021 MB of 0.021 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb: / 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–…â–ƒâ–„â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–ƒâ–†â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–‡â–†â–ˆâ–„â–†â–„â–â–‚â–„â–„â–…â–ƒâ–…â–ƒâ–â–…â–„â–…â–‚â–â–ƒâ–‚â–ƒâ–‚â–…â–ƒâ–‚â–‚â–ƒâ–‚â–â–â–‚â–ƒâ–‚â–‚â–„â–‚â–ƒ
wandb:                                                   val_de_mse â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–â–‚â–â–â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚
wandb:                                                  val_pearson â–â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–†â–‡â–†â–†â–‡â–†â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00193
wandb:                                              test_de_pearson 0.99971
wandb:               test_frac_opposite_direction_top20_non_dropout 0.06667
wandb:                          test_frac_sigma_below_1_non_dropout 0.98333
wandb:                                                     test_mse 0.00042
wandb:                                test_mse_top20_de_non_dropout 0.00229
wandb:                                                 test_pearson 0.99925
wandb:                                           test_pearson_delta 0.44086
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.06667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98333
wandb:                                       test_unseen_single_mse 0.00042
wandb:                                    test_unseen_single_mse_de 0.00193
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00229
wandb:                                   test_unseen_single_pearson 0.99925
wandb:                                test_unseen_single_pearson_de 0.99971
wandb:                             test_unseen_single_pearson_delta 0.44086
wandb:                                                 train_de_mse 0.00128
wandb:                                             train_de_pearson 0.9998
wandb:                                                    train_mse 0.00033
wandb:                                                train_pearson 0.99941
wandb:                                                training_loss 0.42815
wandb:                                                   val_de_mse 0.00175
wandb:                                               val_de_pearson 0.99974
wandb:                                                      val_mse 0.00036
wandb:                                                  val_pearson 0.99934
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396858_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/meq4ru3c
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_073336-meq4ru3c/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396858/splits/dixit_gsm2396858_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_074536-4axtige8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396858_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/4axtige8
Start Training...
Epoch 1 Step 1 Train Loss: 1.0020
Epoch 1 Step 51 Train Loss: 0.4807
Epoch 1 Step 101 Train Loss: 0.4781
Epoch 1 Step 151 Train Loss: 0.5017
Epoch 1 Step 201 Train Loss: 0.4364
Epoch 1 Step 251 Train Loss: 0.5018
Epoch 1 Step 301 Train Loss: 0.4483
Epoch 1: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0028. 
Epoch 2 Step 1 Train Loss: 0.4228
Epoch 2 Step 51 Train Loss: 0.5614
Epoch 2 Step 101 Train Loss: 0.4929
Epoch 2 Step 151 Train Loss: 0.4639
Epoch 2 Step 201 Train Loss: 0.5308
Epoch 2 Step 251 Train Loss: 0.4315
Epoch 2 Step 301 Train Loss: 0.4654
Epoch 2: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0033. 
Epoch 3 Step 1 Train Loss: 0.4671
Epoch 3 Step 51 Train Loss: 0.4352
Epoch 3 Step 101 Train Loss: 0.5223
Epoch 3 Step 151 Train Loss: 0.4266
Epoch 3 Step 201 Train Loss: 0.4441
Epoch 3 Step 251 Train Loss: 0.4566
Epoch 3 Step 301 Train Loss: 0.5132
Epoch 3: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0038. 
Epoch 4 Step 1 Train Loss: 0.4288
Epoch 4 Step 51 Train Loss: 0.4746
Epoch 4 Step 101 Train Loss: 0.5218
Epoch 4 Step 151 Train Loss: 0.4492
Epoch 4 Step 201 Train Loss: 0.4484
Epoch 4 Step 251 Train Loss: 0.5117
Epoch 4 Step 301 Train Loss: 0.4900
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0022. 
Epoch 5 Step 1 Train Loss: 0.4453
Epoch 5 Step 51 Train Loss: 0.4315
Epoch 5 Step 101 Train Loss: 0.4723
Epoch 5 Step 151 Train Loss: 0.4417
Epoch 5 Step 201 Train Loss: 0.5409
Epoch 5 Step 251 Train Loss: 0.4946
Epoch 5 Step 301 Train Loss: 0.5305
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0026. 
Epoch 6 Step 1 Train Loss: 0.4202
Epoch 6 Step 51 Train Loss: 0.4635
Epoch 6 Step 101 Train Loss: 0.5082
Epoch 6 Step 151 Train Loss: 0.4476
Epoch 6 Step 201 Train Loss: 0.4648
Epoch 6 Step 251 Train Loss: 0.4950
Epoch 6 Step 301 Train Loss: 0.4936
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0025. 
Epoch 7 Step 1 Train Loss: 0.5202
Epoch 7 Step 51 Train Loss: 0.4321
Epoch 7 Step 101 Train Loss: 0.5198
Epoch 7 Step 151 Train Loss: 0.4224
Epoch 7 Step 201 Train Loss: 0.4892
Epoch 7 Step 251 Train Loss: 0.4625
Epoch 7 Step 301 Train Loss: 0.4249
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0025. 
Epoch 8 Step 1 Train Loss: 0.4346
Epoch 8 Step 51 Train Loss: 0.5336
Epoch 8 Step 101 Train Loss: 0.4861
Epoch 8 Step 151 Train Loss: 0.4172
Epoch 8 Step 201 Train Loss: 0.4712
Epoch 8 Step 251 Train Loss: 0.4744
Epoch 8 Step 301 Train Loss: 0.4721
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0026. 
Epoch 9 Step 1 Train Loss: 0.4586
Epoch 9 Step 51 Train Loss: 0.4769
Epoch 9 Step 101 Train Loss: 0.4811
Epoch 9 Step 151 Train Loss: 0.5283
Epoch 9 Step 201 Train Loss: 0.6096
Epoch 9 Step 251 Train Loss: 0.4684
Epoch 9 Step 301 Train Loss: 0.4731
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0027. 
Epoch 10 Step 1 Train Loss: 0.4844
Epoch 10 Step 51 Train Loss: 0.4852
Epoch 10 Step 101 Train Loss: 0.4791
Epoch 10 Step 151 Train Loss: 0.4037
Epoch 10 Step 201 Train Loss: 0.4336
Epoch 10 Step 251 Train Loss: 0.4633
Epoch 10 Step 301 Train Loss: 0.4295
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0027. 
Epoch 11 Step 1 Train Loss: 0.5211
Epoch 11 Step 51 Train Loss: 0.3998
Epoch 11 Step 101 Train Loss: 0.5198
Epoch 11 Step 151 Train Loss: 0.4277
Epoch 11 Step 201 Train Loss: 0.5160
Epoch 11 Step 251 Train Loss: 0.5226
Epoch 11 Step 301 Train Loss: 0.4507
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0026. 
Epoch 12 Step 1 Train Loss: 0.5072
Epoch 12 Step 51 Train Loss: 0.4391
Epoch 12 Step 101 Train Loss: 0.4521
Epoch 12 Step 151 Train Loss: 0.4568
Epoch 12 Step 201 Train Loss: 0.6447
Epoch 12 Step 251 Train Loss: 0.4282
Epoch 12 Step 301 Train Loss: 0.4606
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0028. 
Epoch 13 Step 1 Train Loss: 0.5156
Epoch 13 Step 51 Train Loss: 0.4429
Epoch 13 Step 101 Train Loss: 0.4572
Epoch 13 Step 151 Train Loss: 0.4196
Epoch 13 Step 201 Train Loss: 0.4253
Epoch 13 Step 251 Train Loss: 0.4180
Epoch 13 Step 301 Train Loss: 0.4252
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0027. 
Epoch 14 Step 1 Train Loss: 0.4407
Epoch 14 Step 51 Train Loss: 0.4688
Epoch 14 Step 101 Train Loss: 0.4252
Epoch 14 Step 151 Train Loss: 0.4495
Epoch 14 Step 201 Train Loss: 0.4672
Epoch 14 Step 251 Train Loss: 0.4646
Epoch 14 Step 301 Train Loss: 0.4931
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0027. 
Epoch 15 Step 1 Train Loss: 0.4998
Epoch 15 Step 51 Train Loss: 0.4115
Epoch 15 Step 101 Train Loss: 0.4482
Epoch 15 Step 151 Train Loss: 0.4175
Epoch 15 Step 201 Train Loss: 0.5426
Epoch 15 Step 251 Train Loss: 0.4173
Epoch 15 Step 301 Train Loss: 0.4690
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0010 Validation Top 20 DE MSE: 0.0027. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0085
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00045938385
test_unseen_single_pearson: 0.9991510509404767
test_unseen_single_mse_de: 0.008532556
test_unseen_single_pearson_de: 0.9987799607397289
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.35224354504945027
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.13333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.008549121544193695
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–†â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–‚â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–â–ƒâ–â–ƒâ–ƒâ–„â–‚â–…â–…â–ˆâ–…â–ƒâ–‚â–ƒâ–„â–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–ˆâ–ƒâ–ƒâ–â–…â–ƒâ–ƒâ–„
wandb:                                                   val_de_mse â–„â–†â–ˆâ–â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒ
wandb:                                               val_de_pearson â–â–â–…â–ˆâ–„â–…â–†â–„â–„â–„â–…â–„â–„â–„â–„
wandb:                                                      val_mse â–ˆâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                  val_pearson â–â–†â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00853
wandb:                                              test_de_pearson 0.99878
wandb:               test_frac_opposite_direction_top20_non_dropout 0.13333
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00046
wandb:                                test_mse_top20_de_non_dropout 0.00855
wandb:                                                 test_pearson 0.99915
wandb:                                           test_pearson_delta 0.35224
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.13333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00046
wandb:                                    test_unseen_single_mse_de 0.00853
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00855
wandb:                                   test_unseen_single_pearson 0.99915
wandb:                                test_unseen_single_pearson_de 0.99878
wandb:                             test_unseen_single_pearson_delta 0.35224
wandb:                                                 train_de_mse 0.001
wandb:                                             train_de_pearson 0.99985
wandb:                                                    train_mse 0.0004
wandb:                                                train_pearson 0.99931
wandb:                                                training_loss 0.43386
wandb:                                                   val_de_mse 0.00267
wandb:                                               val_de_pearson 0.99938
wandb:                                                      val_mse 0.00051
wandb:                                                  val_pearson 0.99912
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396858_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/4axtige8
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_074536-4axtige8/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396858/splits/dixit_gsm2396858_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_075543-lzxaia73
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396858_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/lzxaia73
Start Training...
Epoch 1 Step 1 Train Loss: 1.0854
Epoch 1 Step 51 Train Loss: 0.5261
Epoch 1 Step 101 Train Loss: 0.5806
Epoch 1 Step 151 Train Loss: 0.5948
Epoch 1 Step 201 Train Loss: 0.5019
Epoch 1 Step 251 Train Loss: 0.4591
Epoch 1: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.0072. 
Epoch 2 Step 1 Train Loss: 0.4637
Epoch 2 Step 51 Train Loss: 0.4503
Epoch 2 Step 101 Train Loss: 0.4685
Epoch 2 Step 151 Train Loss: 0.4861
Epoch 2 Step 201 Train Loss: 0.4129
Epoch 2 Step 251 Train Loss: 0.4040
Epoch 2: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0052. 
Epoch 3 Step 1 Train Loss: 0.4964
Epoch 3 Step 51 Train Loss: 0.4750
Epoch 3 Step 101 Train Loss: 0.5780
Epoch 3 Step 151 Train Loss: 0.3883
Epoch 3 Step 201 Train Loss: 0.4735
Epoch 3 Step 251 Train Loss: 0.5302
Epoch 3: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0024. 
Epoch 4 Step 1 Train Loss: 0.5124
Epoch 4 Step 51 Train Loss: 0.5576
Epoch 4 Step 101 Train Loss: 0.4552
Epoch 4 Step 151 Train Loss: 0.4337
Epoch 4 Step 201 Train Loss: 0.4649
Epoch 4 Step 251 Train Loss: 0.4526
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0021. 
Epoch 5 Step 1 Train Loss: 0.5061
Epoch 5 Step 51 Train Loss: 0.4726
Epoch 5 Step 101 Train Loss: 0.4663
Epoch 5 Step 151 Train Loss: 0.4686
Epoch 5 Step 201 Train Loss: 0.5190
Epoch 5 Step 251 Train Loss: 0.4551
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0030. 
Epoch 6 Step 1 Train Loss: 0.4363
Epoch 6 Step 51 Train Loss: 0.4871
Epoch 6 Step 101 Train Loss: 0.5842
Epoch 6 Step 151 Train Loss: 0.5993
Epoch 6 Step 201 Train Loss: 0.4476
Epoch 6 Step 251 Train Loss: 0.5363
Epoch 6: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0023. 
Epoch 7 Step 1 Train Loss: 0.4512
Epoch 7 Step 51 Train Loss: 0.4040
Epoch 7 Step 101 Train Loss: 0.5776
Epoch 7 Step 151 Train Loss: 0.4852
Epoch 7 Step 201 Train Loss: 0.4482
Epoch 7 Step 251 Train Loss: 0.5388
Epoch 7: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0023. 
Epoch 8 Step 1 Train Loss: 0.5895
Epoch 8 Step 51 Train Loss: 0.4933
Epoch 8 Step 101 Train Loss: 0.5572
Epoch 8 Step 151 Train Loss: 0.4799
Epoch 8 Step 201 Train Loss: 0.4828
Epoch 8 Step 251 Train Loss: 0.4783
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0024. 
Epoch 9 Step 1 Train Loss: 0.4925
Epoch 9 Step 51 Train Loss: 0.4838
Epoch 9 Step 101 Train Loss: 0.5476
Epoch 9 Step 151 Train Loss: 0.4921
Epoch 9 Step 201 Train Loss: 0.5190
Epoch 9 Step 251 Train Loss: 0.4715
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0024. 
Epoch 10 Step 1 Train Loss: 0.4432
Epoch 10 Step 51 Train Loss: 0.5686
Epoch 10 Step 101 Train Loss: 0.4519
Epoch 10 Step 151 Train Loss: 0.4830
Epoch 10 Step 201 Train Loss: 0.4471
Epoch 10 Step 251 Train Loss: 0.4194
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0025. 
Epoch 11 Step 1 Train Loss: 0.4708
Epoch 11 Step 51 Train Loss: 0.5179
Epoch 11 Step 101 Train Loss: 0.4826
Epoch 11 Step 151 Train Loss: 0.4991
Epoch 11 Step 201 Train Loss: 0.4762
Epoch 11 Step 251 Train Loss: 0.4671
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0024. 
Epoch 12 Step 1 Train Loss: 0.4386
Epoch 12 Step 51 Train Loss: 0.5037
Epoch 12 Step 101 Train Loss: 0.5095
Epoch 12 Step 151 Train Loss: 0.4237
Epoch 12 Step 201 Train Loss: 0.5191
Epoch 12 Step 251 Train Loss: 0.4576
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0023. 
Epoch 13 Step 1 Train Loss: 0.4438
Epoch 13 Step 51 Train Loss: 0.4685
Epoch 13 Step 101 Train Loss: 0.4430
Epoch 13 Step 151 Train Loss: 0.4290
Epoch 13 Step 201 Train Loss: 0.5088
Epoch 13 Step 251 Train Loss: 0.4560
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0024. 
Epoch 14 Step 1 Train Loss: 0.4865
Epoch 14 Step 51 Train Loss: 0.4015
Epoch 14 Step 101 Train Loss: 0.5134
Epoch 14 Step 151 Train Loss: 0.4801
Epoch 14 Step 201 Train Loss: 0.4764
Epoch 14 Step 251 Train Loss: 0.3937
Epoch 14: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0023. 
Epoch 15 Step 1 Train Loss: 0.4857
Epoch 15 Step 51 Train Loss: 0.4446
Epoch 15 Step 101 Train Loss: 0.4569
Epoch 15 Step 151 Train Loss: 0.4507
Epoch 15 Step 201 Train Loss: 0.5030
Epoch 15 Step 251 Train Loss: 0.5698
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0012 Validation Top 20 DE MSE: 0.0025. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0018
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00068874075
test_unseen_single_pearson: 0.9987403425315974
test_unseen_single_mse_de: 0.0018076628
test_unseen_single_pearson_de: 0.9995930751695411
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.402866623614129
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.03333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.0018076628561718878
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.004 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–…â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–…â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–ƒâ–„â–ƒâ–…â–„â–„â–„â–„â–‚â–„â–ˆâ–â–ƒâ–‚â–‚â–„â–†â–…â–„â–‚â–…â–ƒâ–„â–ƒâ–…â–â–…â–„â–„â–†â–…â–„â–â–‡â–…â–‚â–‚â–‡â–ƒ
wandb:                                                   val_de_mse â–ˆâ–…â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚
wandb:                                               val_de_pearson â–â–ƒâ–ˆâ–ˆâ–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–ˆâ–…â–„â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                  val_pearson â–â–„â–…â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00181
wandb:                                              test_de_pearson 0.99959
wandb:               test_frac_opposite_direction_top20_non_dropout 0.03333
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00069
wandb:                                test_mse_top20_de_non_dropout 0.00181
wandb:                                                 test_pearson 0.99874
wandb:                                           test_pearson_delta 0.40287
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.03333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00069
wandb:                                    test_unseen_single_mse_de 0.00181
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00181
wandb:                                   test_unseen_single_pearson 0.99874
wandb:                                test_unseen_single_pearson_de 0.99959
wandb:                             test_unseen_single_pearson_delta 0.40287
wandb:                                                 train_de_mse 0.00123
wandb:                                             train_de_pearson 0.99986
wandb:                                                    train_mse 0.00063
wandb:                                                train_pearson 0.99885
wandb:                                                training_loss 0.4519
wandb:                                                   val_de_mse 0.00246
wandb:                                               val_de_pearson 0.99962
wandb:                                                      val_mse 0.00054
wandb:                                                  val_pearson 0.99901
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396858_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/lzxaia73
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_075543-lzxaia73/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396858/splits/dixit_gsm2396858_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_080416-w2pin4c3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396858_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/w2pin4c3
Start Training...
Epoch 1 Step 1 Train Loss: 0.9055
Epoch 1 Step 51 Train Loss: 0.4753
Epoch 1 Step 101 Train Loss: 0.4982
Epoch 1 Step 151 Train Loss: 0.5258
Epoch 1 Step 201 Train Loss: 0.5756
Epoch 1 Step 251 Train Loss: 0.4998
Epoch 1 Step 301 Train Loss: 0.5091
Epoch 1: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0040. 
Epoch 2 Step 1 Train Loss: 0.4628
Epoch 2 Step 51 Train Loss: 0.4697
Epoch 2 Step 101 Train Loss: 0.4665
Epoch 2 Step 151 Train Loss: 0.5242
Epoch 2 Step 201 Train Loss: 0.5244
Epoch 2 Step 251 Train Loss: 0.4300
Epoch 2 Step 301 Train Loss: 0.4487
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0014. 
Epoch 3 Step 1 Train Loss: 0.4834
Epoch 3 Step 51 Train Loss: 0.4590
Epoch 3 Step 101 Train Loss: 0.4725
Epoch 3 Step 151 Train Loss: 0.4399
Epoch 3 Step 201 Train Loss: 0.3615
Epoch 3 Step 251 Train Loss: 0.4563
Epoch 3 Step 301 Train Loss: 0.4295
Epoch 3: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0009. 
Epoch 4 Step 1 Train Loss: 0.4735
Epoch 4 Step 51 Train Loss: 0.4499
Epoch 4 Step 101 Train Loss: 0.4538
Epoch 4 Step 151 Train Loss: 0.4421
Epoch 4 Step 201 Train Loss: 0.4372
Epoch 4 Step 251 Train Loss: 0.4611
Epoch 4 Step 301 Train Loss: 0.4587
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0009. 
Epoch 5 Step 1 Train Loss: 0.4643
Epoch 5 Step 51 Train Loss: 0.4506
Epoch 5 Step 101 Train Loss: 0.4440
Epoch 5 Step 151 Train Loss: 0.4497
Epoch 5 Step 201 Train Loss: 0.4700
Epoch 5 Step 251 Train Loss: 0.5302
Epoch 5 Step 301 Train Loss: 0.3950
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0009. 
Epoch 6 Step 1 Train Loss: 0.4504
Epoch 6 Step 51 Train Loss: 0.3927
Epoch 6 Step 101 Train Loss: 0.5142
Epoch 6 Step 151 Train Loss: 0.3894
Epoch 6 Step 201 Train Loss: 0.4007
Epoch 6 Step 251 Train Loss: 0.4569
Epoch 6 Step 301 Train Loss: 0.4353
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0011. 
Epoch 7 Step 1 Train Loss: 0.4179
Epoch 7 Step 51 Train Loss: 0.4601
Epoch 7 Step 101 Train Loss: 0.4956
Epoch 7 Step 151 Train Loss: 0.4051
Epoch 7 Step 201 Train Loss: 0.5058
Epoch 7 Step 251 Train Loss: 0.4577
Epoch 7 Step 301 Train Loss: 0.4420
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0011. 
Epoch 8 Step 1 Train Loss: 0.4655
Epoch 8 Step 51 Train Loss: 0.5088
Epoch 8 Step 101 Train Loss: 0.4535
Epoch 8 Step 151 Train Loss: 0.4358
Epoch 8 Step 201 Train Loss: 0.4440
Epoch 8 Step 251 Train Loss: 0.4503
Epoch 8 Step 301 Train Loss: 0.4903
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0010. 
Epoch 9 Step 1 Train Loss: 0.3911
Epoch 9 Step 51 Train Loss: 0.4970
Epoch 9 Step 101 Train Loss: 0.4300
Epoch 9 Step 151 Train Loss: 0.4212
Epoch 9 Step 201 Train Loss: 0.5129
Epoch 9 Step 251 Train Loss: 0.4651
Epoch 9 Step 301 Train Loss: 0.5049
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0011. 
Epoch 10 Step 1 Train Loss: 0.4529
Epoch 10 Step 51 Train Loss: 0.4759
Epoch 10 Step 101 Train Loss: 0.4647
Epoch 10 Step 151 Train Loss: 0.4392
Epoch 10 Step 201 Train Loss: 0.4780
Epoch 10 Step 251 Train Loss: 0.4477
Epoch 10 Step 301 Train Loss: 0.4043
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0011. 
Epoch 11 Step 1 Train Loss: 0.4383
Epoch 11 Step 51 Train Loss: 0.4754
Epoch 11 Step 101 Train Loss: 0.4736
Epoch 11 Step 151 Train Loss: 0.4753
Epoch 11 Step 201 Train Loss: 0.5305
Epoch 11 Step 251 Train Loss: 0.4644
Epoch 11 Step 301 Train Loss: 0.4570
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0010. 
Epoch 12 Step 1 Train Loss: 0.4138
Epoch 12 Step 51 Train Loss: 0.4806
Epoch 12 Step 101 Train Loss: 0.4747
Epoch 12 Step 151 Train Loss: 0.3942
Epoch 12 Step 201 Train Loss: 0.4695
Epoch 12 Step 251 Train Loss: 0.4260
Epoch 12 Step 301 Train Loss: 0.4841
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0010. 
Epoch 13 Step 1 Train Loss: 0.4608
Epoch 13 Step 51 Train Loss: 0.4617
Epoch 13 Step 101 Train Loss: 0.4876
Epoch 13 Step 151 Train Loss: 0.4679
Epoch 13 Step 201 Train Loss: 0.4381
Epoch 13 Step 251 Train Loss: 0.4158
Epoch 13 Step 301 Train Loss: 0.4978
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0013 Validation Top 20 DE MSE: 0.0011. 
Epoch 14 Step 1 Train Loss: 0.4310
Epoch 14 Step 51 Train Loss: 0.4560
Epoch 14 Step 101 Train Loss: 0.4448
Epoch 14 Step 151 Train Loss: 0.4051
Epoch 14 Step 201 Train Loss: 0.5443
Epoch 14 Step 251 Train Loss: 0.4252
Epoch 14 Step 301 Train Loss: 0.4045
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0011. 
Epoch 15 Step 1 Train Loss: 0.4523
Epoch 15 Step 51 Train Loss: 0.4476
Epoch 15 Step 101 Train Loss: 0.4745
Epoch 15 Step 151 Train Loss: 0.4597
Epoch 15 Step 201 Train Loss: 0.4113
Epoch 15 Step 251 Train Loss: 0.4653
Epoch 15 Step 301 Train Loss: 0.4846
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0011. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0011
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00040471586
test_unseen_single_pearson: 0.9992630859721695
test_unseen_single_mse_de: 0.001129488
test_unseen_single_pearson_de: 0.9998881701643453
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.42618371656961984
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.05000000000000001
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9833333333333334
test_unseen_single_mse_top20_de_non_dropout: 0.0014868799652796437
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.021 MB uploadedwandb: | 0.016 MB of 0.021 MB uploadedwandb: / 0.017 MB of 0.021 MB uploadedwandb: - 0.021 MB of 0.021 MB uploadedwandb: \ 0.021 MB of 0.021 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–„â–„â–ƒâ–‡â–†â–…â–…â–ƒâ–‚â–„â–„â–â–†â–ƒâ–ƒâ–â–‚â–‡â–‚â–„â–†â–‡â–‚â–‚â–†â–‡â–…â–„â–ƒâ–„â–…â–…â–…â–ˆâ–‚â–„â–‚â–ƒâ–‚
wandb:                                                   val_de_mse â–ˆâ–‚â–â–â–â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–
wandb:                                               val_de_pearson â–â–…â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–…â–ˆâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                  val_pearson â–ƒâ–â–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00113
wandb:                                              test_de_pearson 0.99989
wandb:               test_frac_opposite_direction_top20_non_dropout 0.05
wandb:                          test_frac_sigma_below_1_non_dropout 0.98333
wandb:                                                     test_mse 0.0004
wandb:                                test_mse_top20_de_non_dropout 0.00149
wandb:                                                 test_pearson 0.99926
wandb:                                           test_pearson_delta 0.42618
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.05
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98333
wandb:                                       test_unseen_single_mse 0.0004
wandb:                                    test_unseen_single_mse_de 0.00113
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00149
wandb:                                   test_unseen_single_pearson 0.99926
wandb:                                test_unseen_single_pearson_de 0.99989
wandb:                             test_unseen_single_pearson_delta 0.42618
wandb:                                                 train_de_mse 0.00139
wandb:                                             train_de_pearson 0.99977
wandb:                                                    train_mse 0.00038
wandb:                                                train_pearson 0.99933
wandb:                                                training_loss 0.5856
wandb:                                                   val_de_mse 0.00106
wandb:                                               val_de_pearson 0.99976
wandb:                                                      val_mse 0.00038
wandb:                                                  val_pearson 0.99934
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396858_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/w2pin4c3
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_080416-w2pin4c3/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396858/splits/dixit_gsm2396858_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_081425-khms6pbd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396858_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/khms6pbd
Start Training...
Epoch 1 Step 1 Train Loss: 0.9814
Epoch 1 Step 51 Train Loss: 0.5481
Epoch 1 Step 101 Train Loss: 0.6787
Epoch 1 Step 151 Train Loss: 0.4894
Epoch 1 Step 201 Train Loss: 0.5239
Epoch 1 Step 251 Train Loss: 0.4629
Epoch 1: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0049. 
Epoch 2 Step 1 Train Loss: 0.4994
Epoch 2 Step 51 Train Loss: 0.5268
Epoch 2 Step 101 Train Loss: 0.4402
Epoch 2 Step 151 Train Loss: 0.4888
Epoch 2 Step 201 Train Loss: 0.4492
Epoch 2 Step 251 Train Loss: 0.4735
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0099. 
Epoch 3 Step 1 Train Loss: 0.4415
Epoch 3 Step 51 Train Loss: 0.4507
Epoch 3 Step 101 Train Loss: 0.3859
Epoch 3 Step 151 Train Loss: 0.4693
Epoch 3 Step 201 Train Loss: 0.4535
Epoch 3 Step 251 Train Loss: 0.4697
Epoch 3: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0038. 
Epoch 4 Step 1 Train Loss: 0.6186
Epoch 4 Step 51 Train Loss: 0.4263
Epoch 4 Step 101 Train Loss: 0.4928
Epoch 4 Step 151 Train Loss: 0.4606
Epoch 4 Step 201 Train Loss: 0.5431
Epoch 4 Step 251 Train Loss: 0.4705
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0065. 
Epoch 5 Step 1 Train Loss: 0.4504
Epoch 5 Step 51 Train Loss: 0.4369
Epoch 5 Step 101 Train Loss: 0.6101
Epoch 5 Step 151 Train Loss: 0.5204
Epoch 5 Step 201 Train Loss: 0.4597
Epoch 5 Step 251 Train Loss: 0.4622
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0058. 
Epoch 6 Step 1 Train Loss: 0.4537
Epoch 6 Step 51 Train Loss: 0.4514
Epoch 6 Step 101 Train Loss: 0.4945
Epoch 6 Step 151 Train Loss: 0.4776
Epoch 6 Step 201 Train Loss: 0.4415
Epoch 6 Step 251 Train Loss: 0.4209
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0056. 
Epoch 7 Step 1 Train Loss: 0.5070
Epoch 7 Step 51 Train Loss: 0.4476
Epoch 7 Step 101 Train Loss: 0.5174
Epoch 7 Step 151 Train Loss: 0.4556
Epoch 7 Step 201 Train Loss: 0.4685
Epoch 7 Step 251 Train Loss: 0.4802
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0057. 
Epoch 8 Step 1 Train Loss: 0.4119
Epoch 8 Step 51 Train Loss: 0.4434
Epoch 8 Step 101 Train Loss: 0.4370
Epoch 8 Step 151 Train Loss: 0.4414
Epoch 8 Step 201 Train Loss: 0.5162
Epoch 8 Step 251 Train Loss: 0.4749
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0061. 
Epoch 9 Step 1 Train Loss: 0.4616
Epoch 9 Step 51 Train Loss: 0.4688
Epoch 9 Step 101 Train Loss: 0.5003
Epoch 9 Step 151 Train Loss: 0.3939
Epoch 9 Step 201 Train Loss: 0.3982
Epoch 9 Step 251 Train Loss: 0.4927
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0057. 
Epoch 10 Step 1 Train Loss: 0.4242
Epoch 10 Step 51 Train Loss: 0.4703
Epoch 10 Step 101 Train Loss: 0.4627
Epoch 10 Step 151 Train Loss: 0.5167
Epoch 10 Step 201 Train Loss: 0.5350
Epoch 10 Step 251 Train Loss: 0.5197
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0061. 
Epoch 11 Step 1 Train Loss: 0.4542
Epoch 11 Step 51 Train Loss: 0.4798
Epoch 11 Step 101 Train Loss: 0.4569
Epoch 11 Step 151 Train Loss: 0.4621
Epoch 11 Step 201 Train Loss: 0.4249
Epoch 11 Step 251 Train Loss: 0.5624
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0060. 
Epoch 12 Step 1 Train Loss: 0.4904
Epoch 12 Step 51 Train Loss: 0.4957
Epoch 12 Step 101 Train Loss: 0.4515
Epoch 12 Step 151 Train Loss: 0.4598
Epoch 12 Step 201 Train Loss: 0.4244
Epoch 12 Step 251 Train Loss: 0.5218
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0056. 
Epoch 13 Step 1 Train Loss: 0.4664
Epoch 13 Step 51 Train Loss: 0.5180
Epoch 13 Step 101 Train Loss: 0.5411
Epoch 13 Step 151 Train Loss: 0.4312
Epoch 13 Step 201 Train Loss: 0.4342
Epoch 13 Step 251 Train Loss: 0.4946
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0060. 
Epoch 14 Step 1 Train Loss: 0.4760
Epoch 14 Step 51 Train Loss: 0.5051
Epoch 14 Step 101 Train Loss: 0.5175
Epoch 14 Step 151 Train Loss: 0.4075
Epoch 14 Step 201 Train Loss: 0.5905
Epoch 14 Step 251 Train Loss: 0.5127
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0059. 
Epoch 15 Step 1 Train Loss: 0.4388
Epoch 15 Step 51 Train Loss: 0.4984
Epoch 15 Step 101 Train Loss: 0.4665
Epoch 15 Step 151 Train Loss: 0.5017
Epoch 15 Step 201 Train Loss: 0.4177
Epoch 15 Step 251 Train Loss: 0.5270
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0064. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0024
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0012049748
test_unseen_single_pearson: 0.9977292248465529
test_unseen_single_mse_de: 0.0023544766
test_unseen_single_pearson_de: 0.9995142015489775
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.28006024336128715
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.11666666666666668
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002366721502816712
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.001 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–ƒâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–„â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–‡â–ƒâ–„â–„â–†â–‡â–„â–†â–†â–…â–â–„â–ƒâ–ƒâ–†â–„â–ƒâ–„â–ƒâ–‚â–„â–ƒâ–…â–â–†â–â–ƒâ–…â–‚â–ƒâ–ˆâ–„â–…â–ƒâ–„â–„â–„â–…â–†â–„
wandb:                                                   val_de_mse â–‚â–ˆâ–â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–„
wandb:                                               val_de_pearson â–†â–â–ˆâ–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–†
wandb:                                                      val_mse â–ƒâ–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚
wandb:                                                  val_pearson â–†â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00235
wandb:                                              test_de_pearson 0.99951
wandb:               test_frac_opposite_direction_top20_non_dropout 0.11667
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.0012
wandb:                                test_mse_top20_de_non_dropout 0.00237
wandb:                                                 test_pearson 0.99773
wandb:                                           test_pearson_delta 0.28006
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.11667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.0012
wandb:                                    test_unseen_single_mse_de 0.00235
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00237
wandb:                                   test_unseen_single_pearson 0.99773
wandb:                                test_unseen_single_pearson_de 0.99951
wandb:                             test_unseen_single_pearson_delta 0.28006
wandb:                                                 train_de_mse 0.00148
wandb:                                             train_de_pearson 0.99979
wandb:                                                    train_mse 0.00058
wandb:                                                train_pearson 0.99893
wandb:                                                training_loss 0.46724
wandb:                                                   val_de_mse 0.00645
wandb:                                               val_de_pearson 0.99916
wandb:                                                      val_mse 0.00124
wandb:                                                  val_pearson 0.99759
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396858_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/khms6pbd
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_081425-khms6pbd/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/14 [00:00<?, ?it/s]  7%|â–‹         | 1/14 [03:01<39:16, 181.23s/it] 14%|â–ˆâ–        | 2/14 [04:50<27:46, 138.84s/it] 21%|â–ˆâ–ˆâ–       | 3/14 [05:09<15:26, 84.25s/it]  29%|â–ˆâ–ˆâ–Š       | 4/14 [05:54<11:28, 68.87s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [06:33<08:41, 57.94s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [07:53<08:42, 65.36s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [09:16<08:17, 71.03s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [09:16<04:52, 48.69s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [11:10<05:44, 68.88s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [12:15<04:30, 67.69s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [15:03<04:55, 98.56s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [16:38<03:14, 97.46s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [17:07<01:16, 76.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [17:44<00:00, 64.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [17:44<00:00, 76.03s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396861/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396861/splits/dixit_gsm2396861_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_084132-xd990f7v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396861_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/xd990f7v
  0%|          | 0/3718 [00:00<?, ?it/s]  0%|          | 7/3718 [00:00<00:59, 62.40it/s]  0%|          | 15/3718 [00:00<00:53, 69.03it/s]  1%|          | 23/3718 [00:00<00:51, 72.24it/s]  1%|          | 33/3718 [00:00<00:46, 79.01it/s]  1%|          | 43/3718 [00:00<00:43, 85.27it/s]  1%|â–         | 53/3718 [00:00<00:43, 83.94it/s]  2%|â–         | 63/3718 [00:00<00:43, 84.85it/s]  2%|â–         | 73/3718 [00:00<00:42, 86.13it/s]  2%|â–         | 84/3718 [00:01<00:40, 90.25it/s]  3%|â–Ž         | 94/3718 [00:01<00:40, 89.78it/s]  3%|â–Ž         | 103/3718 [00:01<00:41, 87.06it/s]  3%|â–Ž         | 113/3718 [00:01<00:40, 89.22it/s]  3%|â–Ž         | 122/3718 [00:01<00:40, 88.38it/s]  4%|â–Ž         | 131/3718 [00:01<00:41, 87.36it/s]  4%|â–         | 140/3718 [00:01<00:41, 85.88it/s]  4%|â–         | 149/3718 [00:01<00:41, 86.17it/s]  4%|â–         | 158/3718 [00:01<00:42, 83.82it/s]  4%|â–         | 167/3718 [00:01<00:42, 84.36it/s]  5%|â–         | 177/3718 [00:02<00:41, 84.61it/s]  5%|â–Œ         | 187/3718 [00:02<00:40, 86.34it/s]  5%|â–Œ         | 197/3718 [00:02<00:39, 89.95it/s]  6%|â–Œ         | 207/3718 [00:02<00:39, 89.77it/s]  6%|â–Œ         | 216/3718 [00:02<00:40, 86.81it/s]  6%|â–Œ         | 226/3718 [00:02<00:38, 89.77it/s]  6%|â–‹         | 236/3718 [00:02<00:39, 89.01it/s]  7%|â–‹         | 246/3718 [00:02<00:38, 89.38it/s]  7%|â–‹         | 255/3718 [00:02<00:38, 89.32it/s]  7%|â–‹         | 264/3718 [00:03<00:39, 86.41it/s]  7%|â–‹         | 274/3718 [00:03<00:38, 89.67it/s]  8%|â–Š         | 283/3718 [00:03<00:39, 86.18it/s]  8%|â–Š         | 293/3718 [00:03<00:39, 86.00it/s]  8%|â–Š         | 303/3718 [00:03<00:39, 86.93it/s]  8%|â–Š         | 313/3718 [00:03<00:37, 90.26it/s]  9%|â–Š         | 323/3718 [00:03<00:37, 89.45it/s]  9%|â–‰         | 333/3718 [00:03<00:37, 89.77it/s]  9%|â–‰         | 342/3718 [00:03<00:37, 89.63it/s]  9%|â–‰         | 351/3718 [00:04<00:37, 89.04it/s] 10%|â–‰         | 360/3718 [00:04<00:38, 88.28it/s] 10%|â–‰         | 369/3718 [00:04<00:39, 85.24it/s] 10%|â–ˆ         | 379/3718 [00:04<00:37, 88.89it/s] 10%|â–ˆ         | 389/3718 [00:04<00:38, 86.76it/s] 11%|â–ˆ         | 398/3718 [00:04<00:38, 87.32it/s] 11%|â–ˆ         | 407/3718 [00:04<00:37, 87.45it/s] 11%|â–ˆ         | 417/3718 [00:04<00:36, 89.93it/s] 11%|â–ˆâ–        | 427/3718 [00:04<00:36, 89.50it/s] 12%|â–ˆâ–        | 436/3718 [00:05<00:36, 89.42it/s] 12%|â–ˆâ–        | 445/3718 [00:05<00:36, 88.83it/s] 12%|â–ˆâ–        | 454/3718 [00:05<00:36, 88.86it/s] 12%|â–ˆâ–        | 463/3718 [00:05<00:36, 88.40it/s] 13%|â–ˆâ–Ž        | 472/3718 [00:05<00:37, 87.55it/s] 13%|â–ˆâ–Ž        | 481/3718 [00:05<00:37, 85.35it/s] 13%|â–ˆâ–Ž        | 491/3718 [00:05<00:36, 89.53it/s] 13%|â–ˆâ–Ž        | 500/3718 [00:05<00:35, 89.42it/s] 14%|â–ˆâ–Ž        | 509/3718 [00:05<00:35, 89.33it/s] 14%|â–ˆâ–        | 518/3718 [00:05<00:37, 86.01it/s] 14%|â–ˆâ–        | 528/3718 [00:06<00:35, 89.83it/s] 14%|â–ˆâ–        | 538/3718 [00:06<00:35, 89.66it/s] 15%|â–ˆâ–        | 547/3718 [00:06<00:36, 86.33it/s] 15%|â–ˆâ–        | 557/3718 [00:06<00:35, 90.12it/s] 15%|â–ˆâ–Œ        | 567/3718 [00:06<00:36, 87.31it/s] 16%|â–ˆâ–Œ        | 577/3718 [00:06<00:35, 87.45it/s] 16%|â–ˆâ–Œ        | 587/3718 [00:06<00:35, 87.41it/s] 16%|â–ˆâ–Œ        | 597/3718 [00:06<00:35, 86.78it/s] 16%|â–ˆâ–‹        | 607/3718 [00:06<00:35, 87.34it/s] 17%|â–ˆâ–‹        | 617/3718 [00:07<00:34, 89.85it/s] 17%|â–ˆâ–‹        | 627/3718 [00:07<00:34, 88.76it/s] 17%|â–ˆâ–‹        | 636/3718 [00:07<00:34, 88.87it/s] 17%|â–ˆâ–‹        | 645/3718 [00:07<00:34, 88.26it/s] 18%|â–ˆâ–Š        | 654/3718 [00:07<00:34, 88.29it/s] 18%|â–ˆâ–Š        | 663/3718 [00:07<00:34, 88.44it/s] 18%|â–ˆâ–Š        | 672/3718 [00:07<00:34, 88.13it/s] 18%|â–ˆâ–Š        | 681/3718 [00:07<00:34, 88.30it/s] 19%|â–ˆâ–Š        | 690/3718 [00:07<00:34, 88.34it/s] 19%|â–ˆâ–‰        | 699/3718 [00:07<00:34, 88.20it/s] 19%|â–ˆâ–‰        | 708/3718 [00:08<00:35, 85.22it/s] 19%|â–ˆâ–‰        | 718/3718 [00:08<00:34, 88.11it/s] 20%|â–ˆâ–‰        | 727/3718 [00:08<00:33, 88.18it/s] 20%|â–ˆâ–‰        | 736/3718 [00:08<00:33, 88.24it/s] 20%|â–ˆâ–ˆ        | 745/3718 [00:08<00:33, 88.66it/s] 20%|â–ˆâ–ˆ        | 754/3718 [00:08<00:34, 85.52it/s] 21%|â–ˆâ–ˆ        | 764/3718 [00:08<00:34, 86.00it/s] 21%|â–ˆâ–ˆ        | 774/3718 [00:08<00:34, 85.88it/s] 21%|â–ˆâ–ˆ        | 784/3718 [00:08<00:32, 89.32it/s] 21%|â–ˆâ–ˆâ–       | 793/3718 [00:09<00:33, 86.42it/s] 22%|â–ˆâ–ˆâ–       | 802/3718 [00:09<00:33, 87.31it/s] 22%|â–ˆâ–ˆâ–       | 812/3718 [00:09<00:32, 90.50it/s] 22%|â–ˆâ–ˆâ–       | 822/3718 [00:09<00:33, 87.71it/s] 22%|â–ˆâ–ˆâ–       | 831/3718 [00:09<00:32, 88.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 841/3718 [00:09<00:32, 88.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 851/3718 [00:09<00:32, 89.11it/s] 23%|â–ˆâ–ˆâ–Ž       | 860/3718 [00:09<00:32, 88.73it/s] 23%|â–ˆâ–ˆâ–Ž       | 870/3718 [00:09<00:31, 91.44it/s] 24%|â–ˆâ–ˆâ–Ž       | 880/3718 [00:10<00:31, 90.31it/s] 24%|â–ˆâ–ˆâ–       | 890/3718 [00:10<00:32, 86.47it/s] 24%|â–ˆâ–ˆâ–       | 900/3718 [00:10<00:31, 89.62it/s] 24%|â–ˆâ–ˆâ–       | 910/3718 [00:10<00:31, 89.71it/s] 25%|â–ˆâ–ˆâ–       | 920/3718 [00:10<00:31, 89.22it/s] 25%|â–ˆâ–ˆâ–       | 929/3718 [00:10<00:31, 89.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 938/3718 [00:10<00:31, 89.16it/s] 25%|â–ˆâ–ˆâ–Œ       | 947/3718 [00:10<00:31, 89.10it/s] 26%|â–ˆâ–ˆâ–Œ       | 956/3718 [00:10<00:31, 89.08it/s] 26%|â–ˆâ–ˆâ–Œ       | 965/3718 [00:11<00:31, 86.33it/s] 26%|â–ˆâ–ˆâ–Œ       | 975/3718 [00:11<00:31, 87.52it/s] 26%|â–ˆâ–ˆâ–‹       | 985/3718 [00:11<00:30, 90.78it/s] 27%|â–ˆâ–ˆâ–‹       | 995/3718 [00:11<00:30, 90.13it/s] 27%|â–ˆâ–ˆâ–‹       | 1005/3718 [00:11<00:31, 86.98it/s] 27%|â–ˆâ–ˆâ–‹       | 1015/3718 [00:11<00:30, 87.58it/s] 28%|â–ˆâ–ˆâ–Š       | 1024/3718 [00:11<00:30, 87.97it/s] 28%|â–ˆâ–ˆâ–Š       | 1034/3718 [00:11<00:29, 91.25it/s] 28%|â–ˆâ–ˆâ–Š       | 1044/3718 [00:11<00:29, 90.38it/s] 28%|â–ˆâ–ˆâ–Š       | 1054/3718 [00:12<00:29, 89.08it/s] 29%|â–ˆâ–ˆâ–Š       | 1063/3718 [00:12<00:30, 87.62it/s] 29%|â–ˆâ–ˆâ–‰       | 1072/3718 [00:12<00:30, 87.14it/s] 29%|â–ˆâ–ˆâ–‰       | 1081/3718 [00:12<00:30, 87.00it/s] 29%|â–ˆâ–ˆâ–‰       | 1090/3718 [00:12<00:30, 86.76it/s] 30%|â–ˆâ–ˆâ–‰       | 1099/3718 [00:12<00:30, 86.88it/s] 30%|â–ˆâ–ˆâ–‰       | 1108/3718 [00:12<00:30, 86.95it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1117/3718 [00:12<00:29, 87.27it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1126/3718 [00:12<00:29, 87.14it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1135/3718 [00:12<00:29, 86.52it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1144/3718 [00:13<00:29, 86.88it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1153/3718 [00:13<00:31, 81.93it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1164/3718 [00:13<00:28, 89.14it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1173/3718 [00:13<00:28, 88.66it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1182/3718 [00:13<00:29, 84.96it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1192/3718 [00:13<00:28, 87.54it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1201/3718 [00:13<00:28, 87.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1210/3718 [00:13<00:28, 87.51it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1219/3718 [00:13<00:29, 85.21it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1228/3718 [00:14<00:28, 85.92it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1237/3718 [00:14<00:28, 86.72it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1247/3718 [00:14<00:27, 89.95it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1257/3718 [00:14<00:27, 89.54it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1266/3718 [00:14<00:27, 89.26it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1275/3718 [00:14<00:27, 89.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1284/3718 [00:14<00:28, 86.41it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1294/3718 [00:14<00:26, 90.15it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1304/3718 [00:14<00:27, 86.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1314/3718 [00:14<00:27, 86.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1323/3718 [00:15<00:27, 86.99it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1333/3718 [00:15<00:27, 87.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1343/3718 [00:15<00:26, 90.97it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1353/3718 [00:15<00:27, 86.92it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1362/3718 [00:15<00:26, 87.34it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1372/3718 [00:15<00:26, 89.62it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1382/3718 [00:15<00:27, 86.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1392/3718 [00:15<00:25, 90.05it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1402/3718 [00:15<00:25, 89.61it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1412/3718 [00:16<00:25, 89.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1421/3718 [00:16<00:25, 89.20it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1431/3718 [00:16<00:25, 89.55it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1440/3718 [00:16<00:25, 88.83it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1449/3718 [00:16<00:25, 88.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1459/3718 [00:16<00:25, 89.19it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1468/3718 [00:16<00:25, 89.14it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1477/3718 [00:16<00:26, 85.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1487/3718 [00:16<00:25, 88.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1496/3718 [00:17<00:25, 88.07it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1505/3718 [00:17<00:25, 87.93it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1514/3718 [00:17<00:25, 86.05it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1524/3718 [00:17<00:24, 89.26it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1534/3718 [00:17<00:24, 89.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1543/3718 [00:17<00:24, 89.13it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1552/3718 [00:17<00:25, 86.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1562/3718 [00:17<00:23, 90.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1572/3718 [00:17<00:23, 89.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1581/3718 [00:17<00:23, 89.46it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1590/3718 [00:18<00:23, 89.55it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1599/3718 [00:18<00:23, 89.10it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1608/3718 [00:18<00:24, 85.53it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1618/3718 [00:18<00:23, 89.01it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1627/3718 [00:18<00:23, 89.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1636/3718 [00:18<00:23, 89.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1645/3718 [00:18<00:23, 88.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1654/3718 [00:18<00:23, 88.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1663/3718 [00:18<00:23, 87.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1672/3718 [00:19<00:23, 87.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1681/3718 [00:19<00:23, 87.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1690/3718 [00:19<00:23, 87.82it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1699/3718 [00:19<00:22, 88.23it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1708/3718 [00:19<00:22, 88.36it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1717/3718 [00:19<00:22, 88.37it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1726/3718 [00:19<00:22, 88.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1735/3718 [00:19<00:22, 88.80it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1744/3718 [00:19<00:22, 88.33it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1753/3718 [00:19<00:22, 85.47it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1763/3718 [00:20<00:21, 89.30it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1772/3718 [00:20<00:22, 86.00it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1782/3718 [00:20<00:21, 88.93it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1791/3718 [00:20<00:21, 88.24it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1800/3718 [00:20<00:21, 88.07it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1809/3718 [00:20<00:22, 86.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1819/3718 [00:20<00:21, 86.99it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1829/3718 [00:20<00:20, 90.40it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1839/3718 [00:20<00:20, 89.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1848/3718 [00:21<00:20, 89.07it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1857/3718 [00:21<00:20, 89.14it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1866/3718 [00:21<00:21, 86.37it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1876/3718 [00:21<00:20, 90.04it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1886/3718 [00:21<00:20, 90.22it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1896/3718 [00:21<00:20, 90.00it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1906/3718 [00:21<00:20, 87.20it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1916/3718 [00:21<00:20, 89.37it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1925/3718 [00:21<00:20, 88.84it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1935/3718 [00:22<00:19, 89.24it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1944/3718 [00:22<00:20, 86.61it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1954/3718 [00:22<00:19, 89.81it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1964/3718 [00:22<00:20, 86.16it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1974/3718 [00:22<00:20, 86.34it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1983/3718 [00:22<00:19, 87.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1994/3718 [00:22<00:18, 90.84it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2004/3718 [00:22<00:19, 88.27it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2014/3718 [00:22<00:18, 91.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2024/3718 [00:23<00:18, 90.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2034/3718 [00:23<00:18, 89.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2044/3718 [00:23<00:18, 89.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2053/3718 [00:23<00:19, 86.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2062/3718 [00:23<00:18, 87.21it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2071/3718 [00:23<00:18, 87.14it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2080/3718 [00:23<00:18, 87.67it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2089/3718 [00:23<00:18, 87.06it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2099/3718 [00:23<00:17, 90.29it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2109/3718 [00:23<00:17, 90.22it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2119/3718 [00:24<00:17, 90.20it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2129/3718 [00:24<00:17, 89.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2138/3718 [00:24<00:17, 89.02it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2147/3718 [00:24<00:17, 88.41it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2157/3718 [00:24<00:17, 88.96it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2167/3718 [00:24<00:17, 89.34it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2177/3718 [00:24<00:17, 89.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2186/3718 [00:24<00:17, 89.25it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2195/3718 [00:24<00:17, 89.46it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2205/3718 [00:25<00:17, 87.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2214/3718 [00:25<00:17, 87.05it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2225/3718 [00:25<00:16, 90.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2235/3718 [00:25<00:16, 90.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2245/3718 [00:25<00:16, 90.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2255/3718 [00:25<00:16, 89.63it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2264/3718 [00:25<00:16, 88.61it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2273/3718 [00:25<00:16, 85.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2283/3718 [00:25<00:15, 89.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2293/3718 [00:26<00:15, 89.54it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2302/3718 [00:26<00:16, 87.14it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2312/3718 [00:26<00:16, 87.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2322/3718 [00:26<00:15, 88.26it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2333/3718 [00:26<00:15, 91.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2343/3718 [00:26<00:15, 90.58it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2353/3718 [00:26<00:15, 90.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2363/3718 [00:26<00:15, 90.21it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2373/3718 [00:26<00:15, 89.38it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2382/3718 [00:27<00:15, 88.81it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2391/3718 [00:27<00:15, 86.14it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2400/3718 [00:27<00:15, 86.91it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2410/3718 [00:27<00:14, 87.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2420/3718 [00:27<00:14, 90.98it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2430/3718 [00:27<00:14, 90.15it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2440/3718 [00:27<00:14, 89.55it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2449/3718 [00:27<00:14, 88.98it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2458/3718 [00:27<00:14, 86.47it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2469/3718 [00:28<00:13, 90.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2479/3718 [00:28<00:13, 90.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2489/3718 [00:28<00:13, 90.39it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2499/3718 [00:28<00:13, 89.97it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2508/3718 [00:28<00:13, 86.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2519/3718 [00:28<00:13, 90.26it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2529/3718 [00:28<00:13, 90.08it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2539/3718 [00:28<00:13, 89.91it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2548/3718 [00:28<00:13, 89.39it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2557/3718 [00:28<00:13, 88.91it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2566/3718 [00:29<00:13, 85.61it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2575/3718 [00:29<00:13, 86.40it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2585/3718 [00:29<00:13, 86.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2595/3718 [00:29<00:12, 89.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2605/3718 [00:29<00:12, 88.60it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2614/3718 [00:29<00:13, 83.70it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2624/3718 [00:29<00:12, 85.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2633/3718 [00:29<00:13, 81.37it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2643/3718 [00:30<00:13, 81.55it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2652/3718 [00:30<00:13, 81.76it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2661/3718 [00:30<00:12, 81.68it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2671/3718 [00:30<00:12, 84.35it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2680/3718 [00:30<00:12, 83.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2689/3718 [00:30<00:13, 78.92it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2698/3718 [00:30<00:12, 79.72it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2707/3718 [00:30<00:12, 79.90it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2716/3718 [00:30<00:12, 81.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2725/3718 [00:31<00:12, 79.53it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2735/3718 [00:31<00:11, 82.83it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2744/3718 [00:31<00:11, 82.24it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2753/3718 [00:31<00:11, 82.08it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2762/3718 [00:31<00:12, 78.91it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2771/3718 [00:31<00:11, 81.01it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2780/3718 [00:31<00:12, 77.51it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2788/3718 [00:31<00:11, 77.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2796/3718 [00:31<00:12, 73.03it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2804/3718 [00:32<00:13, 69.23it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2812/3718 [00:32<00:13, 67.98it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2822/3718 [00:32<00:12, 74.11it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2831/3718 [00:32<00:11, 76.91it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2840/3718 [00:32<00:11, 75.68it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2849/3718 [00:32<00:11, 77.51it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2859/3718 [00:32<00:10, 79.76it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2868/3718 [00:32<00:10, 81.33it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2878/3718 [00:32<00:09, 85.87it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2887/3718 [00:33<00:09, 83.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2898/3718 [00:33<00:09, 88.27it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2907/3718 [00:33<00:09, 87.20it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2916/3718 [00:33<00:09, 87.88it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2926/3718 [00:33<00:08, 88.72it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2935/3718 [00:33<00:08, 87.66it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2945/3718 [00:33<00:08, 89.34it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2954/3718 [00:33<00:09, 84.59it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2963/3718 [00:33<00:09, 82.02it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2973/3718 [00:34<00:08, 83.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2983/3718 [00:34<00:08, 86.21it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2992/3718 [00:34<00:08, 81.21it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3001/3718 [00:34<00:08, 83.55it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3010/3718 [00:34<00:08, 82.25it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3019/3718 [00:34<00:08, 83.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3028/3718 [00:34<00:08, 81.94it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3038/3718 [00:34<00:07, 86.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3047/3718 [00:34<00:07, 87.45it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3056/3718 [00:35<00:07, 83.96it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3066/3718 [00:35<00:07, 88.31it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3076/3718 [00:35<00:07, 86.59it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3085/3718 [00:35<00:07, 86.82it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3095/3718 [00:35<00:07, 88.54it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3105/3718 [00:35<00:06, 88.52it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3114/3718 [00:35<00:06, 88.25it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3123/3718 [00:35<00:06, 88.35it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3133/3718 [00:35<00:06, 90.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3143/3718 [00:36<00:06, 90.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3153/3718 [00:36<00:06, 90.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3163/3718 [00:36<00:06, 89.00it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3173/3718 [00:36<00:06, 86.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3183/3718 [00:36<00:05, 89.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3193/3718 [00:36<00:05, 89.67it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3202/3718 [00:36<00:05, 87.84it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3212/3718 [00:36<00:05, 88.01it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3222/3718 [00:36<00:05, 88.31it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3232/3718 [00:37<00:05, 91.24it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3242/3718 [00:37<00:05, 90.47it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3252/3718 [00:37<00:05, 90.70it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3262/3718 [00:37<00:05, 89.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3271/3718 [00:37<00:05, 89.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3280/3718 [00:37<00:04, 88.81it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3289/3718 [00:37<00:05, 85.39it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3298/3718 [00:37<00:04, 86.65it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3308/3718 [00:37<00:04, 89.92it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3318/3718 [00:38<00:04, 89.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3327/3718 [00:38<00:04, 87.87it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3336/3718 [00:38<00:04, 87.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3345/3718 [00:38<00:04, 84.88it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3355/3718 [00:38<00:04, 88.03it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3365/3718 [00:38<00:03, 91.20it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3375/3718 [00:38<00:03, 90.85it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3385/3718 [00:38<00:03, 87.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3394/3718 [00:38<00:03, 87.33it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3404/3718 [00:39<00:03, 88.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3413/3718 [00:39<00:03, 87.93it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3423/3718 [00:39<00:03, 89.03it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3433/3718 [00:39<00:03, 89.25it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3443/3718 [00:39<00:02, 92.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3453/3718 [00:39<00:02, 89.33it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3462/3718 [00:39<00:02, 88.96it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3472/3718 [00:39<00:02, 89.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3482/3718 [00:39<00:02, 86.19it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3493/3718 [00:39<00:02, 91.80it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3503/3718 [00:40<00:02, 92.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3513/3718 [00:40<00:02, 89.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3522/3718 [00:40<00:02, 87.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3533/3718 [00:40<00:02, 92.21it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3544/3718 [00:40<00:01, 94.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3554/3718 [00:40<00:01, 92.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3564/3718 [00:40<00:01, 92.89it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3574/3718 [00:40<00:01, 92.75it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3584/3718 [00:41<00:01, 86.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3593/3718 [00:41<00:01, 86.80it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3602/3718 [00:41<00:01, 86.29it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3611/3718 [00:41<00:01, 82.93it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3621/3718 [00:41<00:01, 85.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3631/3718 [00:41<00:00, 87.98it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3640/3718 [00:41<00:00, 88.53it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3649/3718 [00:41<00:00, 86.39it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3658/3718 [00:41<00:00, 87.04it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3669/3718 [00:41<00:00, 88.59it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3679/3718 [00:42<00:00, 89.37it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3689/3718 [00:42<00:00, 90.03it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3699/3718 [00:42<00:00, 90.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3709/3718 [00:42<00:00, 92.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3718/3718 [00:42<00:00, 87.47it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 1.3657
Epoch 1 Step 51 Train Loss: 0.7157
Epoch 1 Step 101 Train Loss: 0.5783
Epoch 1 Step 151 Train Loss: 0.6287
Epoch 1 Step 201 Train Loss: 0.7402
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0077 Validation Top 20 DE MSE: 0.0187. 
Epoch 2 Step 1 Train Loss: 0.4461
Epoch 2 Step 51 Train Loss: 0.5224
Epoch 2 Step 101 Train Loss: 0.6407
Epoch 2 Step 151 Train Loss: 0.6045
Epoch 2 Step 201 Train Loss: 0.7218
Epoch 2: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0040. 
Epoch 3 Step 1 Train Loss: 0.4727
Epoch 3 Step 51 Train Loss: 0.6099
Epoch 3 Step 101 Train Loss: 0.6032
Epoch 3 Step 151 Train Loss: 0.6143
Epoch 3 Step 201 Train Loss: 0.5176
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0024. 
Epoch 4 Step 1 Train Loss: 0.6111
Epoch 4 Step 51 Train Loss: 0.5867
Epoch 4 Step 101 Train Loss: 0.6461
Epoch 4 Step 151 Train Loss: 0.7030
Epoch 4 Step 201 Train Loss: 0.6769
Epoch 4: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0048. 
Epoch 5 Step 1 Train Loss: 0.5359
Epoch 5 Step 51 Train Loss: 0.6012
Epoch 5 Step 101 Train Loss: 0.5529
Epoch 5 Step 151 Train Loss: 0.6152
Epoch 5 Step 201 Train Loss: 0.5214
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0024. 
Epoch 6 Step 1 Train Loss: 0.5544
Epoch 6 Step 51 Train Loss: 0.5773
Epoch 6 Step 101 Train Loss: 0.5628
Epoch 6 Step 151 Train Loss: 0.7671
Epoch 6 Step 201 Train Loss: 0.5226
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0028. 
Epoch 7 Step 1 Train Loss: 0.5009
Epoch 7 Step 51 Train Loss: 0.5840
Epoch 7 Step 101 Train Loss: 0.5510
Epoch 7 Step 151 Train Loss: 0.7916
Epoch 7 Step 201 Train Loss: 0.5631
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0030. 
Epoch 8 Step 1 Train Loss: 0.5185
Epoch 8 Step 51 Train Loss: 0.5926
Epoch 8 Step 101 Train Loss: 0.5365
Epoch 8 Step 151 Train Loss: 0.5165
Epoch 8 Step 201 Train Loss: 0.7134
Epoch 8: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0036. 
Epoch 9 Step 1 Train Loss: 0.5230
Epoch 9 Step 51 Train Loss: 0.6044
Epoch 9 Step 101 Train Loss: 0.5398
Epoch 9 Step 151 Train Loss: 0.5640
Epoch 9 Step 201 Train Loss: 0.6555
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0043. 
Epoch 10 Step 1 Train Loss: 0.5746
Epoch 10 Step 51 Train Loss: 0.5641
Epoch 10 Step 101 Train Loss: 0.6562
Epoch 10 Step 151 Train Loss: 0.6460
Epoch 10 Step 201 Train Loss: 0.5569
Epoch 10: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0034. 
Epoch 11 Step 1 Train Loss: 0.7359
Epoch 11 Step 51 Train Loss: 0.5783
Epoch 11 Step 101 Train Loss: 0.5711
Epoch 11 Step 151 Train Loss: 0.4951
Epoch 11 Step 201 Train Loss: 0.5736
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0027. 
Epoch 12 Step 1 Train Loss: 0.6551
Epoch 12 Step 51 Train Loss: 0.6151
Epoch 12 Step 101 Train Loss: 0.4720
Epoch 12 Step 151 Train Loss: 0.5011
Epoch 12 Step 201 Train Loss: 0.5768
Epoch 12: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0032. 
Epoch 13 Step 1 Train Loss: 0.5314
Epoch 13 Step 51 Train Loss: 0.5677
Epoch 13 Step 101 Train Loss: 0.5381
Epoch 13 Step 151 Train Loss: 0.5683
Epoch 13 Step 201 Train Loss: 0.6647
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0033. 
Epoch 14 Step 1 Train Loss: 0.5301
Epoch 14 Step 51 Train Loss: 0.7309
Epoch 14 Step 101 Train Loss: 0.6039
Epoch 14 Step 151 Train Loss: 0.6487
Epoch 14 Step 201 Train Loss: 0.5598
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Epoch 15 Step 1 Train Loss: 0.4927
Epoch 15 Step 51 Train Loss: 0.5975
Epoch 15 Step 101 Train Loss: 0.5336
Epoch 15 Step 151 Train Loss: 0.6330
Epoch 15 Step 201 Train Loss: 0.5263
Epoch 15: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0027
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0007564002
test_unseen_single_pearson: 0.9987768487772852
test_unseen_single_mse_de: 0.0026815461
test_unseen_single_pearson_de: 0.9994143665754143
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.44963428965210395
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.025
test_unseen_single_frac_sigma_below_1_non_dropout: 0.975
test_unseen_single_mse_top20_de_non_dropout: 0.0029475100932984787
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.001 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                             train_de_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–…â–‡â–‚â–„â–„â–‚â–‚â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–…â–„â–ƒâ–„â–„â–…â–â–„â–‚â–‡â–ƒâ–ƒâ–‚â–‚â–‚â–â–ƒâ–â–„â–ƒâ–‚â–â–‚
wandb:                                                   val_de_mse â–ˆâ–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–
wandb:                                                  val_pearson â–â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00268
wandb:                                              test_de_pearson 0.99941
wandb:               test_frac_opposite_direction_top20_non_dropout 0.025
wandb:                          test_frac_sigma_below_1_non_dropout 0.975
wandb:                                                     test_mse 0.00076
wandb:                                test_mse_top20_de_non_dropout 0.00295
wandb:                                                 test_pearson 0.99878
wandb:                                           test_pearson_delta 0.44963
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.025
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.975
wandb:                                       test_unseen_single_mse 0.00076
wandb:                                    test_unseen_single_mse_de 0.00268
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00295
wandb:                                   test_unseen_single_pearson 0.99878
wandb:                                test_unseen_single_pearson_de 0.99941
wandb:                             test_unseen_single_pearson_delta 0.44963
wandb:                                                 train_de_mse 0.00204
wandb:                                             train_de_pearson 0.99965
wandb:                                                    train_mse 0.00097
wandb:                                                train_pearson 0.99848
wandb:                                                training_loss 0.5182
wandb:                                                   val_de_mse 0.00308
wandb:                                               val_de_pearson 0.99937
wandb:                                                      val_mse 0.00047
wandb:                                                  val_pearson 0.99925
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396861_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/xd990f7v
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_084132-xd990f7v/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396861/splits/dixit_gsm2396861_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_085059-kcx1gp2s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396861_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/kcx1gp2s
Start Training...
Epoch 1 Step 1 Train Loss: 1.1659
Epoch 1 Step 51 Train Loss: 0.6336
Epoch 1 Step 101 Train Loss: 0.5723
Epoch 1 Step 151 Train Loss: 0.5931
Epoch 1 Step 201 Train Loss: 0.6198
Epoch 1: Train Overall MSE: 0.0029 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0111 Validation Top 20 DE MSE: 0.0083. 
Epoch 2 Step 1 Train Loss: 0.6645
Epoch 2 Step 51 Train Loss: 0.5420
Epoch 2 Step 101 Train Loss: 0.5096
Epoch 2 Step 151 Train Loss: 0.6175
Epoch 2 Step 201 Train Loss: 0.5169
Epoch 2: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0009. 
Epoch 3 Step 1 Train Loss: 0.5425
Epoch 3 Step 51 Train Loss: 0.5711
Epoch 3 Step 101 Train Loss: 0.5314
Epoch 3 Step 151 Train Loss: 0.5928
Epoch 3 Step 201 Train Loss: 0.6056
Epoch 3: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0013. 
Epoch 4 Step 1 Train Loss: 0.5604
Epoch 4 Step 51 Train Loss: 0.6236
Epoch 4 Step 101 Train Loss: 0.5597
Epoch 4 Step 151 Train Loss: 0.6283
Epoch 4 Step 201 Train Loss: 0.5346
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0011. 
Epoch 5 Step 1 Train Loss: 0.5298
Epoch 5 Step 51 Train Loss: 0.5717
Epoch 5 Step 101 Train Loss: 0.5443
Epoch 5 Step 151 Train Loss: 0.4731
Epoch 5 Step 201 Train Loss: 0.5238
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0012. 
Epoch 6 Step 1 Train Loss: 0.6304
Epoch 6 Step 51 Train Loss: 0.5448
Epoch 6 Step 101 Train Loss: 0.6250
Epoch 6 Step 151 Train Loss: 0.6070
Epoch 6 Step 201 Train Loss: 0.4849
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0020. 
Epoch 7 Step 1 Train Loss: 0.6040
Epoch 7 Step 51 Train Loss: 0.6522
Epoch 7 Step 101 Train Loss: 0.4874
Epoch 7 Step 151 Train Loss: 0.5244
Epoch 7 Step 201 Train Loss: 0.5218
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0020. 
Epoch 8 Step 1 Train Loss: 0.5322
Epoch 8 Step 51 Train Loss: 0.7082
Epoch 8 Step 101 Train Loss: 0.5814
Epoch 8 Step 151 Train Loss: 0.5816
Epoch 8 Step 201 Train Loss: 0.6008
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0020. 
Epoch 9 Step 1 Train Loss: 0.5545
Epoch 9 Step 51 Train Loss: 0.4850
Epoch 9 Step 101 Train Loss: 0.5025
Epoch 9 Step 151 Train Loss: 0.5669
Epoch 9 Step 201 Train Loss: 0.5557
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0021. 
Epoch 10 Step 1 Train Loss: 0.4653
Epoch 10 Step 51 Train Loss: 0.5293
Epoch 10 Step 101 Train Loss: 0.5883
Epoch 10 Step 151 Train Loss: 0.6367
Epoch 10 Step 201 Train Loss: 0.5463
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0019. 
Epoch 11 Step 1 Train Loss: 0.5215
Epoch 11 Step 51 Train Loss: 0.7359
Epoch 11 Step 101 Train Loss: 0.6080
Epoch 11 Step 151 Train Loss: 0.5948
Epoch 11 Step 201 Train Loss: 0.5255
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0022. 
Epoch 12 Step 1 Train Loss: 0.5871
Epoch 12 Step 51 Train Loss: 0.5462
Epoch 12 Step 101 Train Loss: 0.4892
Epoch 12 Step 151 Train Loss: 0.5205
Epoch 12 Step 201 Train Loss: 0.4980
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0019. 
Epoch 13 Step 1 Train Loss: 0.4864
Epoch 13 Step 51 Train Loss: 0.5488
Epoch 13 Step 101 Train Loss: 0.6160
Epoch 13 Step 151 Train Loss: 0.5244
Epoch 13 Step 201 Train Loss: 0.4750
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0022. 
Epoch 14 Step 1 Train Loss: 0.7426
Epoch 14 Step 51 Train Loss: 0.5129
Epoch 14 Step 101 Train Loss: 0.6043
Epoch 14 Step 151 Train Loss: 0.5505
Epoch 14 Step 201 Train Loss: 0.4987
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0022. 
Epoch 15 Step 1 Train Loss: 0.6417
Epoch 15 Step 51 Train Loss: 0.5546
Epoch 15 Step 101 Train Loss: 0.5711
Epoch 15 Step 151 Train Loss: 0.5704
Epoch 15 Step 201 Train Loss: 0.5273
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0023. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0019
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0005639266
test_unseen_single_pearson: 0.9990799468615257
test_unseen_single_mse_de: 0.001946456
test_unseen_single_pearson_de: 0.9995776160426821
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.464325869116106
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0625
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9875
test_unseen_single_mse_top20_de_non_dropout: 0.0026959795089518306
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.001 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–„â–†â–„â–„â–„â–â–‚â–ƒâ–„â–‚â–†â–…â–â–ƒâ–ƒâ–‚â–â–„â–ˆâ–„â–â–‚â–ƒâ–‡â–„â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–„â–ƒâ–„â–…â–ƒ
wandb:                                                   val_de_mse â–ˆâ–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                               val_de_pearson â–â–ˆâ–‡â–ˆâ–ˆâ–‡â–†â–‡â–†â–‡â–†â–‡â–†â–†â–†
wandb:                                                      val_mse â–ˆâ–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–„â–„
wandb:                                                  val_pearson â–â–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–†â–…â–†â–…â–…â–…
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00195
wandb:                                              test_de_pearson 0.99958
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0625
wandb:                          test_frac_sigma_below_1_non_dropout 0.9875
wandb:                                                     test_mse 0.00056
wandb:                                test_mse_top20_de_non_dropout 0.0027
wandb:                                                 test_pearson 0.99908
wandb:                                           test_pearson_delta 0.46433
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0625
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9875
wandb:                                       test_unseen_single_mse 0.00056
wandb:                                    test_unseen_single_mse_de 0.00195
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0027
wandb:                                   test_unseen_single_pearson 0.99908
wandb:                                test_unseen_single_pearson_de 0.99958
wandb:                             test_unseen_single_pearson_delta 0.46433
wandb:                                                 train_de_mse 0.00161
wandb:                                             train_de_pearson 0.99957
wandb:                                                    train_mse 0.00087
wandb:                                                train_pearson 0.99866
wandb:                                                training_loss 0.56432
wandb:                                                   val_de_mse 0.00232
wandb:                                               val_de_pearson 0.99953
wandb:                                                      val_mse 0.00098
wandb:                                                  val_pearson 0.99841
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396861_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/kcx1gp2s
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_085059-kcx1gp2s/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396861/splits/dixit_gsm2396861_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_085932-7aak4d3g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396861_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/7aak4d3g
Start Training...
Epoch 1 Step 1 Train Loss: 1.2241
Epoch 1 Step 51 Train Loss: 0.5886
Epoch 1 Step 101 Train Loss: 0.5944
Epoch 1 Step 151 Train Loss: 0.5536
Epoch 1 Step 201 Train Loss: 0.5123
Epoch 1: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0051. 
Epoch 2 Step 1 Train Loss: 0.6272
Epoch 2 Step 51 Train Loss: 0.4810
Epoch 2 Step 101 Train Loss: 0.5337
Epoch 2 Step 151 Train Loss: 0.5271
Epoch 2 Step 201 Train Loss: 0.6420
Epoch 2: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0024. 
Epoch 3 Step 1 Train Loss: 0.6115
Epoch 3 Step 51 Train Loss: 0.6404
Epoch 3 Step 101 Train Loss: 0.6831
Epoch 3 Step 151 Train Loss: 0.5502
Epoch 3 Step 201 Train Loss: 0.5694
Epoch 3: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0018. 
Epoch 4 Step 1 Train Loss: 0.6273
Epoch 4 Step 51 Train Loss: 0.5880
Epoch 4 Step 101 Train Loss: 0.6432
Epoch 4 Step 151 Train Loss: 0.5966
Epoch 4 Step 201 Train Loss: 0.5855
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0014. 
Epoch 5 Step 1 Train Loss: 0.5046
Epoch 5 Step 51 Train Loss: 0.6370
Epoch 5 Step 101 Train Loss: 0.5142
Epoch 5 Step 151 Train Loss: 0.6363
Epoch 5 Step 201 Train Loss: 0.5616
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0014. 
Epoch 6 Step 1 Train Loss: 0.7115
Epoch 6 Step 51 Train Loss: 0.5389
Epoch 6 Step 101 Train Loss: 0.6082
Epoch 6 Step 151 Train Loss: 0.6080
Epoch 6 Step 201 Train Loss: 0.6248
Epoch 6: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0016. 
Epoch 7 Step 1 Train Loss: 0.6001
Epoch 7 Step 51 Train Loss: 0.5821
Epoch 7 Step 101 Train Loss: 0.5936
Epoch 7 Step 151 Train Loss: 0.5757
Epoch 7 Step 201 Train Loss: 0.6314
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0016. 
Epoch 8 Step 1 Train Loss: 0.5432
Epoch 8 Step 51 Train Loss: 0.5739
Epoch 8 Step 101 Train Loss: 0.4916
Epoch 8 Step 151 Train Loss: 0.5586
Epoch 8 Step 201 Train Loss: 0.5374
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0015. 
Epoch 9 Step 1 Train Loss: 0.5276
Epoch 9 Step 51 Train Loss: 0.5613
Epoch 9 Step 101 Train Loss: 0.4927
Epoch 9 Step 151 Train Loss: 0.5479
Epoch 9 Step 201 Train Loss: 0.5456
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0016. 
Epoch 10 Step 1 Train Loss: 0.5960
Epoch 10 Step 51 Train Loss: 0.4988
Epoch 10 Step 101 Train Loss: 0.5687
Epoch 10 Step 151 Train Loss: 0.6936
Epoch 10 Step 201 Train Loss: 0.5881
Epoch 10: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0016. 
Epoch 11 Step 1 Train Loss: 0.5727
Epoch 11 Step 51 Train Loss: 0.6197
Epoch 11 Step 101 Train Loss: 0.6316
Epoch 11 Step 151 Train Loss: 0.5579
Epoch 11 Step 201 Train Loss: 0.6126
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0016. 
Epoch 12 Step 1 Train Loss: 0.5881
Epoch 12 Step 51 Train Loss: 0.5344
Epoch 12 Step 101 Train Loss: 0.5728
Epoch 12 Step 151 Train Loss: 0.5884
Epoch 12 Step 201 Train Loss: 0.7203
Epoch 12: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0015. 
Epoch 13 Step 1 Train Loss: 0.5521
Epoch 13 Step 51 Train Loss: 0.6204
Epoch 13 Step 101 Train Loss: 0.5095
Epoch 13 Step 151 Train Loss: 0.5876
Epoch 13 Step 201 Train Loss: 0.5913
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0016. 
Epoch 14 Step 1 Train Loss: 0.5439
Epoch 14 Step 51 Train Loss: 0.4990
Epoch 14 Step 101 Train Loss: 0.6491
Epoch 14 Step 151 Train Loss: 0.4799
Epoch 14 Step 201 Train Loss: 0.5210
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0015. 
Epoch 15 Step 1 Train Loss: 0.5992
Epoch 15 Step 51 Train Loss: 0.6651
Epoch 15 Step 101 Train Loss: 0.5623
Epoch 15 Step 151 Train Loss: 0.5441
Epoch 15 Step 201 Train Loss: 0.5269
Epoch 15: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0016. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0027
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0007369268
test_unseen_single_pearson: 0.9988182745465315
test_unseen_single_mse_de: 0.0027316061
test_unseen_single_pearson_de: 0.999138532001931
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4467237121299702
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.025
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9875
test_unseen_single_mse_top20_de_non_dropout: 0.0027316061993118146
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.014 MB of 0.019 MB uploadedwandb: / 0.014 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ƒâ–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                             train_de_pearson â–â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‡â–‚â–„â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                train_pearson â–â–‚â–‡â–…â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                training_loss â–†â–…â–„â–ƒâ–…â–ƒâ–„â–…â–„â–„â–ƒâ–‚â–‚â–…â–ˆâ–ƒâ–„â–ƒâ–ƒâ–„â–„â–…â–…â–ƒâ–„â–†â–ƒâ–â–ƒâ–„â–…â–ƒâ–ƒâ–†â–ƒâ–…â–„â–…â–†â–„
wandb:                                                   val_de_mse â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–‚â–ˆâ–ƒâ–ƒâ–‚â–â–â–‚â–â–‚â–â–â–‚â–‚â–
wandb:                                                  val_pearson â–†â–â–†â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00273
wandb:                                              test_de_pearson 0.99914
wandb:               test_frac_opposite_direction_top20_non_dropout 0.025
wandb:                          test_frac_sigma_below_1_non_dropout 0.9875
wandb:                                                     test_mse 0.00074
wandb:                                test_mse_top20_de_non_dropout 0.00273
wandb:                                                 test_pearson 0.99882
wandb:                                           test_pearson_delta 0.44672
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.025
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9875
wandb:                                       test_unseen_single_mse 0.00074
wandb:                                    test_unseen_single_mse_de 0.00273
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00273
wandb:                                   test_unseen_single_pearson 0.99882
wandb:                                test_unseen_single_pearson_de 0.99914
wandb:                             test_unseen_single_pearson_delta 0.44672
wandb:                                                 train_de_mse 0.00184
wandb:                                             train_de_pearson 0.99959
wandb:                                                    train_mse 0.00097
wandb:                                                train_pearson 0.99848
wandb:                                                training_loss 0.59583
wandb:                                                   val_de_mse 0.00163
wandb:                                               val_de_pearson 0.99981
wandb:                                                      val_mse 0.001
wandb:                                                  val_pearson 0.9984
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396861_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/7aak4d3g
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_085932-7aak4d3g/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396861/splits/dixit_gsm2396861_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_090741-xemt5xzz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396861_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/xemt5xzz
Start Training...
Epoch 1 Step 1 Train Loss: 1.4147
Epoch 1 Step 51 Train Loss: 0.7020
Epoch 1 Step 101 Train Loss: 0.7001
Epoch 1 Step 151 Train Loss: 0.6306
Epoch 1 Step 201 Train Loss: 0.6330
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0046 Validation Top 20 DE MSE: 0.0056. 
Epoch 2 Step 1 Train Loss: 0.6884
Epoch 2 Step 51 Train Loss: 0.7184
Epoch 2 Step 101 Train Loss: 0.5408
Epoch 2 Step 151 Train Loss: 0.5923
Epoch 2 Step 201 Train Loss: 0.5493
Epoch 2: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0022. 
Epoch 3 Step 1 Train Loss: 0.5720
Epoch 3 Step 51 Train Loss: 0.5618
Epoch 3 Step 101 Train Loss: 0.6601
Epoch 3 Step 151 Train Loss: 0.5498
Epoch 3 Step 201 Train Loss: 0.5715
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0017. 
Epoch 4 Step 1 Train Loss: 0.5980
Epoch 4 Step 51 Train Loss: 0.6039
Epoch 4 Step 101 Train Loss: 0.5731
Epoch 4 Step 151 Train Loss: 0.5505
Epoch 4 Step 201 Train Loss: 0.5555
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0017. 
Epoch 5 Step 1 Train Loss: 0.5693
Epoch 5 Step 51 Train Loss: 0.5733
Epoch 5 Step 101 Train Loss: 0.6069
Epoch 5 Step 151 Train Loss: 0.5981
Epoch 5 Step 201 Train Loss: 0.5979
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0017. 
Epoch 6 Step 1 Train Loss: 0.6184
Epoch 6 Step 51 Train Loss: 0.7253
Epoch 6 Step 101 Train Loss: 0.6030
Epoch 6 Step 151 Train Loss: 0.6289
Epoch 6 Step 201 Train Loss: 0.5605
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0017. 
Epoch 7 Step 1 Train Loss: 0.4869
Epoch 7 Step 51 Train Loss: 0.5839
Epoch 7 Step 101 Train Loss: 0.5423
Epoch 7 Step 151 Train Loss: 0.6502
Epoch 7 Step 201 Train Loss: 0.5808
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0017. 
Epoch 8 Step 1 Train Loss: 0.5223
Epoch 8 Step 51 Train Loss: 0.6133
Epoch 8 Step 101 Train Loss: 0.5881
Epoch 8 Step 151 Train Loss: 0.5193
Epoch 8 Step 201 Train Loss: 0.6687
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0017. 
Epoch 9 Step 1 Train Loss: 0.6258
Epoch 9 Step 51 Train Loss: 0.6177
Epoch 9 Step 101 Train Loss: 0.5896
Epoch 9 Step 151 Train Loss: 0.5560
Epoch 9 Step 201 Train Loss: 0.6310
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0016. 
Epoch 10 Step 1 Train Loss: 0.5261
Epoch 10 Step 51 Train Loss: 0.5934
Epoch 10 Step 101 Train Loss: 0.6145
Epoch 10 Step 151 Train Loss: 0.5650
Epoch 10 Step 201 Train Loss: 0.5435
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0017. 
Epoch 11 Step 1 Train Loss: 0.5503
Epoch 11 Step 51 Train Loss: 0.5796
Epoch 11 Step 101 Train Loss: 0.5073
Epoch 11 Step 151 Train Loss: 0.5747
Epoch 11 Step 201 Train Loss: 0.5927
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0016. 
Epoch 12 Step 1 Train Loss: 0.5812
Epoch 12 Step 51 Train Loss: 0.5618
Epoch 12 Step 101 Train Loss: 0.5849
Epoch 12 Step 151 Train Loss: 0.5530
Epoch 12 Step 201 Train Loss: 0.5524
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0017. 
Epoch 13 Step 1 Train Loss: 0.6393
Epoch 13 Step 51 Train Loss: 0.5289
Epoch 13 Step 101 Train Loss: 0.6555
Epoch 13 Step 151 Train Loss: 0.5825
Epoch 13 Step 201 Train Loss: 0.5419
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0017. 
Epoch 14 Step 1 Train Loss: 0.5891
Epoch 14 Step 51 Train Loss: 0.5908
Epoch 14 Step 101 Train Loss: 0.7163
Epoch 14 Step 151 Train Loss: 0.5437
Epoch 14 Step 201 Train Loss: 0.5118
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0016. 
Epoch 15 Step 1 Train Loss: 0.4713
Epoch 15 Step 51 Train Loss: 0.5083
Epoch 15 Step 101 Train Loss: 0.6249
Epoch 15 Step 151 Train Loss: 0.6311
Epoch 15 Step 201 Train Loss: 0.5628
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0016. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0021
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00077391055
test_unseen_single_pearson: 0.998742663552204
test_unseen_single_mse_de: 0.0020879295
test_unseen_single_pearson_de: 0.9994045900585071
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4513570458335755
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0625
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9875
test_unseen_single_mse_top20_de_non_dropout: 0.0034029522337449144
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.001 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–ƒâ–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                train_pearson â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                training_loss â–…â–„â–ƒâ–„â–â–ƒâ–ƒâ–ƒâ–ƒâ–†â–â–ˆâ–‚â–ƒâ–…â–‚â–ƒâ–‚â–‚â–„â–ˆâ–„â–…â–…â–„â–„â–ƒâ–‚â–…â–ƒâ–‚â–„â–…â–„â–ƒâ–‚â–„â–‚â–„â–„
wandb:                                                   val_de_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–â–‡â–†â–ˆâ–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:                                                  val_pearson â–â–ˆâ–ƒâ–„â–‚â–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00209
wandb:                                              test_de_pearson 0.9994
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0625
wandb:                          test_frac_sigma_below_1_non_dropout 0.9875
wandb:                                                     test_mse 0.00077
wandb:                                test_mse_top20_de_non_dropout 0.0034
wandb:                                                 test_pearson 0.99874
wandb:                                           test_pearson_delta 0.45136
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0625
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9875
wandb:                                       test_unseen_single_mse 0.00077
wandb:                                    test_unseen_single_mse_de 0.00209
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0034
wandb:                                   test_unseen_single_pearson 0.99874
wandb:                                test_unseen_single_pearson_de 0.9994
wandb:                             test_unseen_single_pearson_delta 0.45136
wandb:                                                 train_de_mse 0.00163
wandb:                                             train_de_pearson 0.99965
wandb:                                                    train_mse 0.00092
wandb:                                                train_pearson 0.99854
wandb:                                                training_loss 0.71601
wandb:                                                   val_de_mse 0.00164
wandb:                                               val_de_pearson 0.99966
wandb:                                                      val_mse 0.00085
wandb:                                                  val_pearson 0.99864
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396861_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/xemt5xzz
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_090741-xemt5xzz/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/dixit_gsm2396861/splits/dixit_gsm2396861_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/Z_gears_script/wandb/run-20240725_091538-wa4mnzua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Dixit_GSM2396861_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/wa4mnzua
Start Training...
Epoch 1 Step 1 Train Loss: 1.3097
Epoch 1 Step 51 Train Loss: 0.6615
Epoch 1 Step 101 Train Loss: 0.6547
Epoch 1 Step 151 Train Loss: 0.7193
Epoch 1: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0084. 
Epoch 2 Step 1 Train Loss: 0.5461
Epoch 2 Step 51 Train Loss: 0.5828
Epoch 2 Step 101 Train Loss: 0.5726
Epoch 2 Step 151 Train Loss: 0.5395
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0074. 
Epoch 3 Step 1 Train Loss: 0.5925
Epoch 3 Step 51 Train Loss: 0.6044
Epoch 3 Step 101 Train Loss: 0.6304
Epoch 3 Step 151 Train Loss: 0.6408
Epoch 3: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0044. 
Epoch 4 Step 1 Train Loss: 0.5285
Epoch 4 Step 51 Train Loss: 0.6166
Epoch 4 Step 101 Train Loss: 0.5744
Epoch 4 Step 151 Train Loss: 0.5557
Epoch 4: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0037. 
Epoch 5 Step 1 Train Loss: 0.4854
Epoch 5 Step 51 Train Loss: 0.5938
Epoch 5 Step 101 Train Loss: 0.5571
Epoch 5 Step 151 Train Loss: 0.5138
Epoch 5: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0029. 
Epoch 6 Step 1 Train Loss: 0.5595
Epoch 6 Step 51 Train Loss: 0.6455
Epoch 6 Step 101 Train Loss: 0.6858
Epoch 6 Step 151 Train Loss: 0.6146
Epoch 6: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0029. 
Epoch 7 Step 1 Train Loss: 0.5461
Epoch 7 Step 51 Train Loss: 0.5655
Epoch 7 Step 101 Train Loss: 0.7073
Epoch 7 Step 151 Train Loss: 0.5356
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0032. 
Epoch 8 Step 1 Train Loss: 0.5419
Epoch 8 Step 51 Train Loss: 0.5087
Epoch 8 Step 101 Train Loss: 0.5720
Epoch 8 Step 151 Train Loss: 0.6597
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0032. 
Epoch 9 Step 1 Train Loss: 0.6072
Epoch 9 Step 51 Train Loss: 0.5296
Epoch 9 Step 101 Train Loss: 0.5281
Epoch 9 Step 151 Train Loss: 0.6880
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Epoch 10 Step 1 Train Loss: 0.6037
Epoch 10 Step 51 Train Loss: 0.6222
Epoch 10 Step 101 Train Loss: 0.5657
Epoch 10 Step 151 Train Loss: 0.5379
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0032. 
Epoch 11 Step 1 Train Loss: 0.5435
Epoch 11 Step 51 Train Loss: 0.6123
Epoch 11 Step 101 Train Loss: 0.6675
Epoch 11 Step 151 Train Loss: 0.5822
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0033. 
Epoch 12 Step 1 Train Loss: 0.8061
Epoch 12 Step 51 Train Loss: 0.5431
Epoch 12 Step 101 Train Loss: 0.6526
Epoch 12 Step 151 Train Loss: 0.5727
Epoch 12: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Epoch 13 Step 1 Train Loss: 0.5128
Epoch 13 Step 51 Train Loss: 0.5973
Epoch 13 Step 101 Train Loss: 0.5267
Epoch 13 Step 151 Train Loss: 0.7021
Epoch 13: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Epoch 14 Step 1 Train Loss: 0.6030
Epoch 14 Step 51 Train Loss: 0.6255
Epoch 14 Step 101 Train Loss: 0.6370
Epoch 14 Step 151 Train Loss: 0.5156
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Epoch 15 Step 1 Train Loss: 0.5269
Epoch 15 Step 51 Train Loss: 0.6003
Epoch 15 Step 101 Train Loss: 0.5021
Epoch 15 Step 151 Train Loss: 0.5644
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0031. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0028
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0009143797
test_unseen_single_pearson: 0.9985371815457963
test_unseen_single_mse_de: 0.0027934057
test_unseen_single_pearson_de: 0.9993501911241143
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4470632966197368
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.037500000000000006
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9625
test_unseen_single_mse_top20_de_non_dropout: 0.003104858454101089
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.001 MB of 0.019 MB uploadedwandb: / 0.013 MB of 0.019 MB uploadedwandb: - 0.013 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–„â–â–â–â–â–â–‚â–â–â–â–‚â–‚â–
wandb:                                             train_de_pearson â–â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‚â–„â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‡â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–†â–ƒâ–‚â–‚â–„â–ˆâ–„â–ƒâ–‚â–…â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–„â–‚â–‚â–ƒâ–‚â–â–„â–ƒâ–ƒâ–„â–â–‚â–ƒâ–ƒâ–‚â–…â–ƒâ–ƒâ–â–‚â–‚â–„â–„
wandb:                                                   val_de_mse â–ˆâ–‡â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–„â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–„â–â–ˆâ–ƒâ–…â–†â–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                  val_pearson â–…â–ˆâ–â–†â–„â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–‚
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00279
wandb:                                              test_de_pearson 0.99935
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0375
wandb:                          test_frac_sigma_below_1_non_dropout 0.9625
wandb:                                                     test_mse 0.00091
wandb:                                test_mse_top20_de_non_dropout 0.0031
wandb:                                                 test_pearson 0.99854
wandb:                                           test_pearson_delta 0.44706
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9625
wandb:                                       test_unseen_single_mse 0.00091
wandb:                                    test_unseen_single_mse_de 0.00279
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0031
wandb:                                   test_unseen_single_pearson 0.99854
wandb:                                test_unseen_single_pearson_de 0.99935
wandb:                             test_unseen_single_pearson_delta 0.44706
wandb:                                                 train_de_mse 0.00197
wandb:                                             train_de_pearson 0.99955
wandb:                                                    train_mse 0.00106
wandb:                                                train_pearson 0.99835
wandb:                                                training_loss 0.74609
wandb:                                                   val_de_mse 0.00315
wandb:                                               val_de_pearson 0.99881
wandb:                                                      val_mse 0.00082
wandb:                                                  val_pearson 0.99865
wandb: 
wandb: ðŸš€ View run Dixit_GSM2396861_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/wa4mnzua
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_091538-wa4mnzua/logs
