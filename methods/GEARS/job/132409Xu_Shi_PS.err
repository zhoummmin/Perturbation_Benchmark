cuda-11.8.0 loaded successful
gcc-12.2.0 loaded successful
cmake-3.27.0 loaded successful
openmpi-4.1.2 loaded successful
Openblas-0.3.25 loaded successful
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/203 [00:00<?, ?it/s]  0%|          | 1/203 [00:19<1:05:54, 19.58s/it]  1%|          | 2/203 [01:01<1:49:33, 32.70s/it]  1%|â–         | 3/203 [02:10<2:45:02, 49.51s/it]  2%|â–         | 4/203 [02:55<2:38:06, 47.67s/it]  2%|â–         | 5/203 [04:00<2:57:55, 53.92s/it]  3%|â–Ž         | 6/203 [04:44<2:45:11, 50.31s/it]  3%|â–Ž         | 7/203 [06:37<3:51:57, 71.01s/it]  4%|â–         | 8/203 [07:02<3:03:19, 56.41s/it]  4%|â–         | 9/203 [08:37<3:41:27, 68.49s/it]  5%|â–         | 10/203 [08:49<2:43:40, 50.88s/it]  5%|â–Œ         | 11/203 [08:54<1:57:37, 36.76s/it]  6%|â–Œ         | 12/203 [09:28<1:54:48, 36.06s/it]  6%|â–‹         | 13/203 [09:57<1:46:58, 33.78s/it]  7%|â–‹         | 14/203 [11:10<2:23:49, 45.66s/it]  7%|â–‹         | 15/203 [12:08<2:34:34, 49.33s/it]  8%|â–Š         | 16/203 [12:48<2:25:31, 46.69s/it]  8%|â–Š         | 17/203 [13:27<2:17:36, 44.39s/it]  9%|â–‰         | 18/203 [13:29<1:37:52, 31.74s/it]  9%|â–‰         | 19/203 [14:08<1:43:31, 33.76s/it] 10%|â–‰         | 20/203 [14:37<1:39:04, 32.48s/it] 10%|â–ˆ         | 21/203 [15:11<1:39:23, 32.77s/it] 11%|â–ˆ         | 22/203 [15:37<1:33:09, 30.88s/it] 11%|â–ˆâ–        | 23/203 [16:03<1:27:46, 29.26s/it] 12%|â–ˆâ–        | 24/203 [16:25<1:21:04, 27.17s/it] 12%|â–ˆâ–        | 25/203 [16:36<1:06:03, 22.26s/it] 13%|â–ˆâ–Ž        | 26/203 [16:54<1:01:34, 20.87s/it] 13%|â–ˆâ–Ž        | 27/203 [17:42<1:25:40, 29.21s/it] 14%|â–ˆâ–        | 28/203 [18:41<1:50:44, 37.97s/it] 14%|â–ˆâ–        | 29/203 [19:10<1:42:26, 35.33s/it] 15%|â–ˆâ–        | 30/203 [19:18<1:18:40, 27.29s/it] 15%|â–ˆâ–Œ        | 31/203 [19:39<1:12:21, 25.24s/it] 16%|â–ˆâ–Œ        | 32/203 [20:37<1:39:49, 35.03s/it] 16%|â–ˆâ–‹        | 33/203 [20:39<1:11:36, 25.27s/it] 17%|â–ˆâ–‹        | 34/203 [21:33<1:34:58, 33.72s/it] 17%|â–ˆâ–‹        | 35/203 [21:34<1:07:24, 24.08s/it] 18%|â–ˆâ–Š        | 36/203 [21:58<1:07:08, 24.12s/it] 18%|â–ˆâ–Š        | 37/203 [22:29<1:12:07, 26.07s/it] 19%|â–ˆâ–Š        | 38/203 [23:11<1:24:41, 30.80s/it] 19%|â–ˆâ–‰        | 39/203 [24:07<1:44:38, 38.28s/it] 20%|â–ˆâ–‰        | 40/203 [24:14<1:18:38, 28.95s/it] 20%|â–ˆâ–ˆ        | 41/203 [24:18<57:50, 21.42s/it]   21%|â–ˆâ–ˆ        | 42/203 [24:44<1:01:12, 22.81s/it] 21%|â–ˆâ–ˆ        | 43/203 [25:14<1:06:33, 24.96s/it] 22%|â–ˆâ–ˆâ–       | 44/203 [25:26<56:25, 21.29s/it]   22%|â–ˆâ–ˆâ–       | 45/203 [26:11<1:14:13, 28.19s/it] 23%|â–ˆâ–ˆâ–Ž       | 46/203 [26:57<1:28:02, 33.65s/it] 23%|â–ˆâ–ˆâ–Ž       | 47/203 [28:03<1:52:51, 43.41s/it] 24%|â–ˆâ–ˆâ–Ž       | 48/203 [28:22<1:32:50, 35.94s/it] 24%|â–ˆâ–ˆâ–       | 49/203 [28:55<1:30:23, 35.22s/it] 25%|â–ˆâ–ˆâ–       | 50/203 [29:37<1:34:32, 37.07s/it] 25%|â–ˆâ–ˆâ–Œ       | 51/203 [29:39<1:07:16, 26.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 52/203 [30:06<1:07:06, 26.67s/it] 26%|â–ˆâ–ˆâ–Œ       | 53/203 [30:26<1:02:18, 24.92s/it] 27%|â–ˆâ–ˆâ–‹       | 54/203 [30:57<1:06:21, 26.72s/it] 27%|â–ˆâ–ˆâ–‹       | 55/203 [31:51<1:25:46, 34.77s/it] 28%|â–ˆâ–ˆâ–Š       | 56/203 [32:36<1:32:41, 37.83s/it] 28%|â–ˆâ–ˆâ–Š       | 57/203 [33:01<1:22:34, 33.93s/it] 29%|â–ˆâ–ˆâ–Š       | 58/203 [33:32<1:19:50, 33.04s/it] 29%|â–ˆâ–ˆâ–‰       | 59/203 [33:56<1:12:49, 30.34s/it] 30%|â–ˆâ–ˆâ–‰       | 60/203 [34:25<1:11:22, 29.95s/it] 30%|â–ˆâ–ˆâ–ˆ       | 61/203 [35:24<1:31:48, 38.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 62/203 [35:50<1:21:40, 34.75s/it] 31%|â–ˆâ–ˆâ–ˆ       | 63/203 [36:38<1:30:32, 38.80s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 64/203 [37:04<1:20:54, 34.93s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 65/203 [38:01<1:35:49, 41.66s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/203 [38:01<1:06:50, 29.28s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/203 [38:50<1:19:10, 34.93s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 68/203 [39:38<1:27:55, 39.08s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 69/203 [40:16<1:26:09, 38.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 70/203 [40:50<1:22:53, 37.39s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 71/203 [41:44<1:33:07, 42.33s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/203 [42:20<1:27:53, 40.26s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 73/203 [42:44<1:16:51, 35.47s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 74/203 [43:13<1:12:20, 33.65s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/203 [43:18<53:28, 25.07s/it]   37%|â–ˆâ–ˆâ–ˆâ–‹      | 76/203 [44:08<1:08:48, 32.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/203 [44:51<1:14:38, 35.55s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 78/203 [45:22<1:11:09, 34.15s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/203 [45:40<1:00:47, 29.41s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 80/203 [45:58<53:25, 26.06s/it]   40%|â–ˆâ–ˆâ–ˆâ–‰      | 81/203 [46:18<49:15, 24.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/203 [47:10<1:05:09, 32.31s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 83/203 [47:38<1:02:22, 31.19s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/203 [48:05<59:35, 30.05s/it]   42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/203 [48:39<1:01:04, 31.06s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 86/203 [48:57<53:14, 27.30s/it]   43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/203 [49:04<40:40, 21.04s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 88/203 [49:42<50:06, 26.14s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/203 [50:02<46:09, 24.29s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/203 [50:49<58:31, 31.08s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 91/203 [52:10<1:25:58, 46.06s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/203 [52:33<1:12:35, 39.24s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 93/203 [52:35<51:10, 27.91s/it]   46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/203 [53:09<54:03, 29.75s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/203 [53:53<1:01:27, 34.15s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 96/203 [54:08<50:44, 28.46s/it]   48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/203 [54:10<36:08, 20.46s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 98/203 [54:40<40:33, 23.18s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/203 [54:49<32:57, 19.01s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/203 [55:47<53:00, 30.88s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 101/203 [56:23<54:47, 32.23s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/203 [56:48<50:52, 30.22s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/203 [56:55<38:22, 23.03s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 104/203 [57:25<41:38, 25.24s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/203 [57:35<33:39, 20.60s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 106/203 [58:02<36:45, 22.73s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/203 [58:49<47:47, 29.87s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/203 [59:01<38:48, 24.52s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 109/203 [59:56<52:49, 33.72s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/203 [1:00:21<48:07, 31.05s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 111/203 [1:00:43<43:32, 28.40s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/203 [1:01:03<39:20, 25.94s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/203 [1:01:29<38:57, 25.98s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 114/203 [1:01:56<38:33, 26.00s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/203 [1:02:09<32:42, 22.30s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 116/203 [1:02:52<41:04, 28.33s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/203 [1:03:50<53:29, 37.32s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/203 [1:04:03<42:47, 30.20s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 119/203 [1:04:09<31:54, 22.79s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/203 [1:04:35<32:56, 23.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 121/203 [1:04:53<29:52, 21.86s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/203 [1:05:12<28:44, 21.29s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/203 [1:05:43<32:10, 24.13s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 124/203 [1:06:02<29:48, 22.64s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/203 [1:06:22<28:21, 21.82s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 126/203 [1:06:50<30:15, 23.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/203 [1:07:30<36:07, 28.51s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/203 [1:08:09<39:24, 31.53s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 129/203 [1:08:27<34:01, 27.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/203 [1:08:50<31:58, 26.28s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 131/203 [1:08:55<23:46, 19.82s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/203 [1:09:15<23:23, 19.76s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/203 [1:09:30<21:34, 18.49s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 134/203 [1:09:49<21:23, 18.60s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/203 [1:09:56<17:15, 15.23s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 136/203 [1:10:02<13:57, 12.51s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 137/203 [1:10:16<14:08, 12.86s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/203 [1:10:26<12:49, 11.83s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 139/203 [1:11:11<23:19, 21.87s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/203 [1:11:30<22:04, 21.02s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 141/203 [1:11:54<22:49, 22.09s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 142/203 [1:12:11<20:40, 20.34s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/203 [1:12:19<16:45, 16.76s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 144/203 [1:12:35<16:20, 16.62s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/203 [1:12:43<13:28, 13.94s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 146/203 [1:12:52<11:39, 12.27s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/203 [1:13:05<11:50, 12.69s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/203 [1:13:37<17:01, 18.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 149/203 [1:13:44<13:29, 14.99s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/203 [1:13:47<10:07, 11.47s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 151/203 [1:13:56<09:18, 10.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 152/203 [1:14:47<19:14, 22.63s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/203 [1:14:54<15:05, 18.12s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 154/203 [1:15:17<15:54, 19.48s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/203 [1:15:25<12:44, 15.92s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 156/203 [1:15:31<10:10, 12.99s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 157/203 [1:15:38<08:32, 11.13s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/203 [1:15:54<09:29, 12.66s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 159/203 [1:15:55<06:50,  9.34s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/203 [1:16:17<09:17, 12.96s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 161/203 [1:16:30<09:01, 12.89s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 162/203 [1:16:46<09:35, 14.03s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/203 [1:16:58<08:55, 13.40s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 164/203 [1:17:07<07:53, 12.13s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/203 [1:17:14<06:38, 10.47s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/203 [1:17:26<06:47, 11.02s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/203 [1:17:32<05:39,  9.42s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/203 [1:17:39<05:07,  8.79s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 169/203 [1:17:51<05:32,  9.78s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 170/203 [1:17:55<04:23,  7.97s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 171/203 [1:18:16<06:18, 11.84s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 172/203 [1:18:23<05:24, 10.46s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/203 [1:18:30<04:39,  9.32s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 174/203 [1:18:39<04:29,  9.30s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 175/203 [1:18:48<04:15,  9.14s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 176/203 [1:18:49<03:03,  6.78s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 177/203 [1:18:53<02:33,  5.89s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/203 [1:19:00<02:38,  6.33s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 179/203 [1:19:04<02:14,  5.59s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 180/203 [1:19:13<02:30,  6.56s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 181/203 [1:19:24<02:54,  7.94s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 182/203 [1:19:27<02:17,  6.54s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/203 [1:19:29<01:39,  4.95s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 184/203 [1:19:45<02:41,  8.51s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 185/203 [1:19:47<01:58,  6.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/203 [1:19:54<01:52,  6.63s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/203 [1:20:07<02:16,  8.53s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/203 [1:20:08<01:33,  6.21s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 189/203 [1:20:10<01:07,  4.82s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 190/203 [1:20:14<01:01,  4.71s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 191/203 [1:20:15<00:41,  3.45s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 192/203 [1:20:16<00:30,  2.80s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/203 [1:20:17<00:22,  2.29s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 194/203 [1:20:23<00:31,  3.54s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 195/203 [1:20:24<00:20,  2.62s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 196/203 [1:20:26<00:16,  2.39s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 197/203 [1:20:33<00:22,  3.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/203 [1:20:35<00:15,  3.16s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 199/203 [1:20:36<00:10,  2.52s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 200/203 [1:20:37<00:06,  2.29s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 201/203 [1:20:40<00:04,  2.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 202/203 [1:20:41<00:01,  1.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [1:20:41<00:00,  1.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [1:20:41<00:00, 23.85s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/xucao2023/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/xucao2023/splits/xucao2023_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_030822-c32f876g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/c32f876g
  0%|          | 0/2098 [00:00<?, ?it/s]  1%|          | 11/2098 [00:00<00:20, 103.00it/s]  1%|          | 26/2098 [00:00<00:17, 121.49it/s]  2%|â–         | 44/2098 [00:00<00:14, 146.04it/s]  3%|â–Ž         | 61/2098 [00:00<00:13, 151.81it/s]  4%|â–Ž         | 77/2098 [00:00<00:13, 154.36it/s]  4%|â–         | 94/2098 [00:00<00:12, 156.70it/s]  5%|â–Œ         | 110/2098 [00:00<00:12, 154.42it/s]  6%|â–Œ         | 126/2098 [00:00<00:12, 155.31it/s]  7%|â–‹         | 143/2098 [00:00<00:12, 158.83it/s]  8%|â–Š         | 160/2098 [00:01<00:12, 159.06it/s]  8%|â–Š         | 176/2098 [00:01<00:12, 155.78it/s]  9%|â–‰         | 193/2098 [00:01<00:12, 156.86it/s] 10%|â–ˆ         | 210/2098 [00:01<00:11, 160.63it/s] 11%|â–ˆ         | 227/2098 [00:01<00:11, 157.35it/s] 12%|â–ˆâ–        | 244/2098 [00:01<00:11, 157.92it/s] 12%|â–ˆâ–        | 261/2098 [00:01<00:11, 161.16it/s] 13%|â–ˆâ–Ž        | 278/2098 [00:01<00:11, 160.43it/s] 14%|â–ˆâ–        | 295/2098 [00:01<00:11, 160.25it/s] 15%|â–ˆâ–        | 312/2098 [00:02<00:11, 159.78it/s] 16%|â–ˆâ–Œ        | 328/2098 [00:02<00:11, 158.90it/s] 16%|â–ˆâ–‹        | 344/2098 [00:02<00:11, 159.04it/s] 17%|â–ˆâ–‹        | 361/2098 [00:02<00:10, 159.34it/s] 18%|â–ˆâ–Š        | 377/2098 [00:02<00:11, 156.13it/s] 19%|â–ˆâ–‰        | 394/2098 [00:02<00:10, 159.18it/s] 20%|â–ˆâ–‰        | 410/2098 [00:02<00:10, 158.84it/s] 20%|â–ˆâ–ˆ        | 426/2098 [00:02<00:10, 158.76it/s] 21%|â–ˆâ–ˆ        | 442/2098 [00:02<00:10, 158.79it/s] 22%|â–ˆâ–ˆâ–       | 459/2098 [00:02<00:10, 159.58it/s] 23%|â–ˆâ–ˆâ–Ž       | 475/2098 [00:03<00:10, 159.52it/s] 23%|â–ˆâ–ˆâ–Ž       | 492/2098 [00:03<00:10, 159.78it/s] 24%|â–ˆâ–ˆâ–       | 508/2098 [00:03<00:09, 159.26it/s] 25%|â–ˆâ–ˆâ–       | 524/2098 [00:03<00:09, 159.15it/s] 26%|â–ˆâ–ˆâ–Œ       | 541/2098 [00:03<00:09, 158.80it/s] 27%|â–ˆâ–ˆâ–‹       | 557/2098 [00:03<00:10, 147.31it/s] 27%|â–ˆâ–ˆâ–‹       | 572/2098 [00:03<00:11, 138.24it/s] 28%|â–ˆâ–ˆâ–Š       | 587/2098 [00:03<00:10, 137.63it/s] 29%|â–ˆâ–ˆâ–Š       | 601/2098 [00:03<00:11, 135.27it/s] 29%|â–ˆâ–ˆâ–‰       | 615/2098 [00:04<00:11, 131.01it/s] 30%|â–ˆâ–ˆâ–‰       | 629/2098 [00:04<00:11, 133.40it/s] 31%|â–ˆâ–ˆâ–ˆ       | 643/2098 [00:04<00:10, 132.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 657/2098 [00:04<00:10, 131.52it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 671/2098 [00:04<00:10, 131.00it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 685/2098 [00:04<00:10, 130.27it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 699/2098 [00:04<00:10, 131.80it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 713/2098 [00:04<00:10, 133.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 730/2098 [00:04<00:09, 141.01it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 747/2098 [00:04<00:09, 149.11it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 763/2098 [00:05<00:08, 150.61it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 779/2098 [00:05<00:08, 151.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 795/2098 [00:05<00:08, 151.32it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 811/2098 [00:05<00:08, 146.54it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 829/2098 [00:05<00:08, 154.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 845/2098 [00:05<00:08, 154.59it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 861/2098 [00:05<00:08, 154.37it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 877/2098 [00:05<00:07, 154.19it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 893/2098 [00:05<00:07, 153.86it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 909/2098 [00:06<00:07, 154.64it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 925/2098 [00:06<00:07, 155.40it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 941/2098 [00:06<00:07, 154.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 957/2098 [00:06<00:07, 151.89it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 974/2098 [00:06<00:07, 152.92it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 990/2098 [00:06<00:07, 153.54it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1007/2098 [00:06<00:07, 151.13it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1024/2098 [00:06<00:06, 154.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1040/2098 [00:06<00:06, 155.03it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1057/2098 [00:06<00:06, 157.37it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1073/2098 [00:07<00:06, 156.67it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1089/2098 [00:07<00:06, 152.86it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1105/2098 [00:07<00:06, 152.33it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1122/2098 [00:07<00:06, 155.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1138/2098 [00:07<00:06, 155.89it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1154/2098 [00:07<00:06, 155.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1170/2098 [00:07<00:06, 154.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1186/2098 [00:07<00:05, 154.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1202/2098 [00:07<00:05, 154.54it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1218/2098 [00:08<00:05, 150.97it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1235/2098 [00:08<00:05, 152.24it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1251/2098 [00:08<00:05, 153.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1268/2098 [00:08<00:05, 157.41it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1284/2098 [00:08<00:05, 157.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1300/2098 [00:08<00:05, 157.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1316/2098 [00:08<00:04, 157.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1332/2098 [00:08<00:04, 156.86it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1348/2098 [00:08<00:04, 157.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1364/2098 [00:08<00:04, 157.84it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1380/2098 [00:09<00:04, 155.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1396/2098 [00:09<00:04, 156.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1413/2098 [00:09<00:04, 157.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1430/2098 [00:09<00:04, 152.24it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1448/2098 [00:09<00:04, 158.27it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1464/2098 [00:09<00:04, 155.97it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1481/2098 [00:09<00:03, 156.36it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1499/2098 [00:09<00:03, 161.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1516/2098 [00:09<00:03, 156.84it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1533/2098 [00:10<00:03, 157.94it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1550/2098 [00:10<00:03, 160.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1567/2098 [00:10<00:03, 159.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1583/2098 [00:10<00:03, 155.87it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1600/2098 [00:10<00:03, 158.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1616/2098 [00:10<00:03, 157.35it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1632/2098 [00:10<00:02, 156.05it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1648/2098 [00:10<00:02, 156.98it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1664/2098 [00:10<00:02, 155.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1680/2098 [00:10<00:02, 154.03it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1696/2098 [00:11<00:02, 151.51it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1713/2098 [00:11<00:02, 152.84it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1730/2098 [00:11<00:02, 157.39it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1746/2098 [00:11<00:02, 154.28it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1762/2098 [00:11<00:02, 155.30it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1779/2098 [00:11<00:02, 158.65it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1795/2098 [00:11<00:01, 154.84it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1812/2098 [00:11<00:01, 158.54it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1828/2098 [00:11<00:01, 155.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1844/2098 [00:12<00:01, 156.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1861/2098 [00:12<00:01, 156.10it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1878/2098 [00:12<00:01, 159.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1895/2098 [00:12<00:01, 159.41it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1911/2098 [00:12<00:01, 159.01it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1927/2098 [00:12<00:01, 158.24it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1943/2098 [00:12<00:00, 158.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1959/2098 [00:12<00:00, 157.95it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1975/2098 [00:12<00:00, 158.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1992/2098 [00:12<00:00, 159.09it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2008/2098 [00:13<00:00, 158.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2025/2098 [00:13<00:00, 156.53it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2042/2098 [00:13<00:00, 157.15it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2059/2098 [00:13<00:00, 159.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2075/2098 [00:13<00:00, 159.09it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2091/2098 [00:13<00:00, 158.40it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2098/2098 [00:13<00:00, 153.88it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.4553
Epoch 1 Step 51 Train Loss: 0.2768
Epoch 1 Step 101 Train Loss: 0.2729
Epoch 1 Step 151 Train Loss: 0.2838
Epoch 1 Step 201 Train Loss: 0.2682
Epoch 1 Step 251 Train Loss: 0.2717
Epoch 1 Step 301 Train Loss: 0.2762
Epoch 1 Step 351 Train Loss: 0.2667
Epoch 1 Step 401 Train Loss: 0.2744
Epoch 1 Step 451 Train Loss: 0.2767
Epoch 1 Step 501 Train Loss: 0.2571
Epoch 1 Step 551 Train Loss: 0.2706
Epoch 1 Step 601 Train Loss: 0.2544
Epoch 1 Step 651 Train Loss: 0.2697
Epoch 1 Step 701 Train Loss: 0.2644
Epoch 1 Step 751 Train Loss: 0.2593
Epoch 1 Step 801 Train Loss: 0.2406
Epoch 1 Step 851 Train Loss: 0.2601
Epoch 1 Step 901 Train Loss: 0.2396
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0083. 
Epoch 2 Step 1 Train Loss: 0.2605
Epoch 2 Step 51 Train Loss: 0.2676
Epoch 2 Step 101 Train Loss: 0.2583
Epoch 2 Step 151 Train Loss: 0.2456
Epoch 2 Step 201 Train Loss: 0.2770
Epoch 2 Step 251 Train Loss: 0.2406
Epoch 2 Step 301 Train Loss: 0.2633
Epoch 2 Step 351 Train Loss: 0.2577
Epoch 2 Step 401 Train Loss: 0.2565
Epoch 2 Step 451 Train Loss: 0.2634
Epoch 2 Step 501 Train Loss: 0.2748
Epoch 2 Step 551 Train Loss: 0.2685
Epoch 2 Step 601 Train Loss: 0.2547
Epoch 2 Step 651 Train Loss: 0.2671
Epoch 2 Step 701 Train Loss: 0.2762
Epoch 2 Step 751 Train Loss: 0.2485
Epoch 2 Step 801 Train Loss: 0.2523
Epoch 2 Step 851 Train Loss: 0.2624
Epoch 2 Step 901 Train Loss: 0.2694
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0081. 
Epoch 3 Step 1 Train Loss: 0.2530
Epoch 3 Step 51 Train Loss: 0.2530
Epoch 3 Step 101 Train Loss: 0.2523
Epoch 3 Step 151 Train Loss: 0.2932
Epoch 3 Step 201 Train Loss: 0.2558
Epoch 3 Step 251 Train Loss: 0.2691
Epoch 3 Step 301 Train Loss: 0.2494
Epoch 3 Step 351 Train Loss: 0.2936
Epoch 3 Step 401 Train Loss: 0.2749
Epoch 3 Step 451 Train Loss: 0.2518
Epoch 3 Step 501 Train Loss: 0.2639
Epoch 3 Step 551 Train Loss: 0.2376
Epoch 3 Step 601 Train Loss: 0.2686
Epoch 3 Step 651 Train Loss: 0.2551
Epoch 3 Step 701 Train Loss: 0.2781
Epoch 3 Step 751 Train Loss: 0.2660
Epoch 3 Step 801 Train Loss: 0.2584
Epoch 3 Step 851 Train Loss: 0.2559
Epoch 3 Step 901 Train Loss: 0.2610
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 4 Step 1 Train Loss: 0.2615
Epoch 4 Step 51 Train Loss: 0.2641
Epoch 4 Step 101 Train Loss: 0.2666
Epoch 4 Step 151 Train Loss: 0.2665
Epoch 4 Step 201 Train Loss: 0.2563
Epoch 4 Step 251 Train Loss: 0.2711
Epoch 4 Step 301 Train Loss: 0.2538
Epoch 4 Step 351 Train Loss: 0.2585
Epoch 4 Step 401 Train Loss: 0.2591
Epoch 4 Step 451 Train Loss: 0.2745
Epoch 4 Step 501 Train Loss: 0.2621
Epoch 4 Step 551 Train Loss: 0.2582
Epoch 4 Step 601 Train Loss: 0.2732
Epoch 4 Step 651 Train Loss: 0.2793
Epoch 4 Step 701 Train Loss: 0.2561
Epoch 4 Step 751 Train Loss: 0.2585
Epoch 4 Step 801 Train Loss: 0.2559
Epoch 4 Step 851 Train Loss: 0.2620
Epoch 4 Step 901 Train Loss: 0.2556
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 5 Step 1 Train Loss: 0.2650
Epoch 5 Step 51 Train Loss: 0.2578
Epoch 5 Step 101 Train Loss: 0.2641
Epoch 5 Step 151 Train Loss: 0.2558
Epoch 5 Step 201 Train Loss: 0.2626
Epoch 5 Step 251 Train Loss: 0.2623
Epoch 5 Step 301 Train Loss: 0.2624
Epoch 5 Step 351 Train Loss: 0.2449
Epoch 5 Step 401 Train Loss: 0.2519
Epoch 5 Step 451 Train Loss: 0.2516
Epoch 5 Step 501 Train Loss: 0.2611
Epoch 5 Step 551 Train Loss: 0.2550
Epoch 5 Step 601 Train Loss: 0.2673
Epoch 5 Step 651 Train Loss: 0.2734
Epoch 5 Step 701 Train Loss: 0.2541
Epoch 5 Step 751 Train Loss: 0.2593
Epoch 5 Step 801 Train Loss: 0.2548
Epoch 5 Step 851 Train Loss: 0.2800
Epoch 5 Step 901 Train Loss: 0.2602
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 6 Step 1 Train Loss: 0.2713
Epoch 6 Step 51 Train Loss: 0.2636
Epoch 6 Step 101 Train Loss: 0.2521
Epoch 6 Step 151 Train Loss: 0.2566
Epoch 6 Step 201 Train Loss: 0.2561
Epoch 6 Step 251 Train Loss: 0.2487
Epoch 6 Step 301 Train Loss: 0.2728
Epoch 6 Step 351 Train Loss: 0.2668
Epoch 6 Step 401 Train Loss: 0.2541
Epoch 6 Step 451 Train Loss: 0.2622
Epoch 6 Step 501 Train Loss: 0.2591
Epoch 6 Step 551 Train Loss: 0.2639
Epoch 6 Step 601 Train Loss: 0.2728
Epoch 6 Step 651 Train Loss: 0.2697
Epoch 6 Step 701 Train Loss: 0.2619
Epoch 6 Step 751 Train Loss: 0.2434
Epoch 6 Step 801 Train Loss: 0.2725
Epoch 6 Step 851 Train Loss: 0.2582
Epoch 6 Step 901 Train Loss: 0.2738
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 7 Step 1 Train Loss: 0.2703
Epoch 7 Step 51 Train Loss: 0.2579
Epoch 7 Step 101 Train Loss: 0.2627
Epoch 7 Step 151 Train Loss: 0.2550
Epoch 7 Step 201 Train Loss: 0.2836
Epoch 7 Step 251 Train Loss: 0.2490
Epoch 7 Step 301 Train Loss: 0.2576
Epoch 7 Step 351 Train Loss: 0.2522
Epoch 7 Step 401 Train Loss: 0.2624
Epoch 7 Step 451 Train Loss: 0.2514
Epoch 7 Step 501 Train Loss: 0.2669
Epoch 7 Step 551 Train Loss: 0.2660
Epoch 7 Step 601 Train Loss: 0.2636
Epoch 7 Step 651 Train Loss: 0.2558
Epoch 7 Step 701 Train Loss: 0.2632
Epoch 7 Step 751 Train Loss: 0.2540
Epoch 7 Step 801 Train Loss: 0.2747
Epoch 7 Step 851 Train Loss: 0.2543
Epoch 7 Step 901 Train Loss: 0.2635
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 8 Step 1 Train Loss: 0.2549
Epoch 8 Step 51 Train Loss: 0.2809
Epoch 8 Step 101 Train Loss: 0.2582
Epoch 8 Step 151 Train Loss: 0.2512
Epoch 8 Step 201 Train Loss: 0.2598
Epoch 8 Step 251 Train Loss: 0.2609
Epoch 8 Step 301 Train Loss: 0.2540
Epoch 8 Step 351 Train Loss: 0.2754
Epoch 8 Step 401 Train Loss: 0.2684
Epoch 8 Step 451 Train Loss: 0.2624
Epoch 8 Step 501 Train Loss: 0.2452
Epoch 8 Step 551 Train Loss: 0.2758
Epoch 8 Step 601 Train Loss: 0.2500
Epoch 8 Step 651 Train Loss: 0.2649
Epoch 8 Step 701 Train Loss: 0.2637
Epoch 8 Step 751 Train Loss: 0.2588
Epoch 8 Step 801 Train Loss: 0.2627
Epoch 8 Step 851 Train Loss: 0.2530
Epoch 8 Step 901 Train Loss: 0.2814
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 9 Step 1 Train Loss: 0.2531
Epoch 9 Step 51 Train Loss: 0.2539
Epoch 9 Step 101 Train Loss: 0.2621
Epoch 9 Step 151 Train Loss: 0.2515
Epoch 9 Step 201 Train Loss: 0.2592
Epoch 9 Step 251 Train Loss: 0.2712
Epoch 9 Step 301 Train Loss: 0.2710
Epoch 9 Step 351 Train Loss: 0.2659
Epoch 9 Step 401 Train Loss: 0.2588
Epoch 9 Step 451 Train Loss: 0.2617
Epoch 9 Step 501 Train Loss: 0.2618
Epoch 9 Step 551 Train Loss: 0.2743
Epoch 9 Step 601 Train Loss: 0.2563
Epoch 9 Step 651 Train Loss: 0.2686
Epoch 9 Step 701 Train Loss: 0.2501
Epoch 9 Step 751 Train Loss: 0.2680
Epoch 9 Step 801 Train Loss: 0.2727
Epoch 9 Step 851 Train Loss: 0.2663
Epoch 9 Step 901 Train Loss: 0.2678
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 10 Step 1 Train Loss: 0.2646
Epoch 10 Step 51 Train Loss: 0.2714
Epoch 10 Step 101 Train Loss: 0.2480
Epoch 10 Step 151 Train Loss: 0.2822
Epoch 10 Step 201 Train Loss: 0.2591
Epoch 10 Step 251 Train Loss: 0.2656
Epoch 10 Step 301 Train Loss: 0.2722
Epoch 10 Step 351 Train Loss: 0.2671
Epoch 10 Step 401 Train Loss: 0.2742
Epoch 10 Step 451 Train Loss: 0.2669
Epoch 10 Step 501 Train Loss: 0.2476
Epoch 10 Step 551 Train Loss: 0.2601
Epoch 10 Step 601 Train Loss: 0.2716
Epoch 10 Step 651 Train Loss: 0.2574
Epoch 10 Step 701 Train Loss: 0.2553
Epoch 10 Step 751 Train Loss: 0.2811
Epoch 10 Step 801 Train Loss: 0.2474
Epoch 10 Step 851 Train Loss: 0.2469
Epoch 10 Step 901 Train Loss: 0.2487
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 11 Step 1 Train Loss: 0.2827
Epoch 11 Step 51 Train Loss: 0.2578
Epoch 11 Step 101 Train Loss: 0.2568
Epoch 11 Step 151 Train Loss: 0.2633
Epoch 11 Step 201 Train Loss: 0.2661
Epoch 11 Step 251 Train Loss: 0.2689
Epoch 11 Step 301 Train Loss: 0.2593
Epoch 11 Step 351 Train Loss: 0.2738
Epoch 11 Step 401 Train Loss: 0.2659
Epoch 11 Step 451 Train Loss: 0.2646
Epoch 11 Step 501 Train Loss: 0.2580
Epoch 11 Step 551 Train Loss: 0.2666
Epoch 11 Step 601 Train Loss: 0.2600
Epoch 11 Step 651 Train Loss: 0.2542
Epoch 11 Step 701 Train Loss: 0.2672
Epoch 11 Step 751 Train Loss: 0.2521
Epoch 11 Step 801 Train Loss: 0.2600
Epoch 11 Step 851 Train Loss: 0.2679
Epoch 11 Step 901 Train Loss: 0.2684
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 12 Step 1 Train Loss: 0.2555
Epoch 12 Step 51 Train Loss: 0.2565
Epoch 12 Step 101 Train Loss: 0.2660
Epoch 12 Step 151 Train Loss: 0.2571
Epoch 12 Step 201 Train Loss: 0.2502
Epoch 12 Step 251 Train Loss: 0.2527
Epoch 12 Step 301 Train Loss: 0.2595
Epoch 12 Step 351 Train Loss: 0.2501
Epoch 12 Step 401 Train Loss: 0.2611
Epoch 12 Step 451 Train Loss: 0.2633
Epoch 12 Step 501 Train Loss: 0.2650
Epoch 12 Step 551 Train Loss: 0.2621
Epoch 12 Step 601 Train Loss: 0.2568
Epoch 12 Step 651 Train Loss: 0.2721
Epoch 12 Step 701 Train Loss: 0.2536
Epoch 12 Step 751 Train Loss: 0.2748
Epoch 12 Step 801 Train Loss: 0.2605
Epoch 12 Step 851 Train Loss: 0.2566
Epoch 12 Step 901 Train Loss: 0.2557
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 13 Step 1 Train Loss: 0.2696
Epoch 13 Step 51 Train Loss: 0.2551
Epoch 13 Step 101 Train Loss: 0.2527
Epoch 13 Step 151 Train Loss: 0.2666
Epoch 13 Step 201 Train Loss: 0.2593
Epoch 13 Step 251 Train Loss: 0.2390
Epoch 13 Step 301 Train Loss: 0.2589
Epoch 13 Step 351 Train Loss: 0.2856
Epoch 13 Step 401 Train Loss: 0.2540
Epoch 13 Step 451 Train Loss: 0.2665
Epoch 13 Step 501 Train Loss: 0.2545
Epoch 13 Step 551 Train Loss: 0.2562
Epoch 13 Step 601 Train Loss: 0.2559
Epoch 13 Step 651 Train Loss: 0.2572
Epoch 13 Step 701 Train Loss: 0.2635
Epoch 13 Step 751 Train Loss: 0.2629
Epoch 13 Step 801 Train Loss: 0.2591
Epoch 13 Step 851 Train Loss: 0.2737
Epoch 13 Step 901 Train Loss: 0.2549
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 14 Step 1 Train Loss: 0.2578
Epoch 14 Step 51 Train Loss: 0.2800
Epoch 14 Step 101 Train Loss: 0.2746
Epoch 14 Step 151 Train Loss: 0.2361
Epoch 14 Step 201 Train Loss: 0.2627
Epoch 14 Step 251 Train Loss: 0.2687
Epoch 14 Step 301 Train Loss: 0.2715
Epoch 14 Step 351 Train Loss: 0.2500
Epoch 14 Step 401 Train Loss: 0.2533
Epoch 14 Step 451 Train Loss: 0.2648
Epoch 14 Step 501 Train Loss: 0.2592
Epoch 14 Step 551 Train Loss: 0.2540
Epoch 14 Step 601 Train Loss: 0.2534
Epoch 14 Step 651 Train Loss: 0.2719
Epoch 14 Step 701 Train Loss: 0.2653
Epoch 14 Step 751 Train Loss: 0.2617
Epoch 14 Step 801 Train Loss: 0.2568
Epoch 14 Step 851 Train Loss: 0.2626
Epoch 14 Step 901 Train Loss: 0.2608
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 15 Step 1 Train Loss: 0.2559
Epoch 15 Step 51 Train Loss: 0.2491
Epoch 15 Step 101 Train Loss: 0.2550
Epoch 15 Step 151 Train Loss: 0.2555
Epoch 15 Step 201 Train Loss: 0.2542
Epoch 15 Step 251 Train Loss: 0.2495
Epoch 15 Step 301 Train Loss: 0.2561
Epoch 15 Step 351 Train Loss: 0.2479
Epoch 15 Step 401 Train Loss: 0.2460
Epoch 15 Step 451 Train Loss: 0.2722
Epoch 15 Step 501 Train Loss: 0.2722
Epoch 15 Step 551 Train Loss: 0.2721
Epoch 15 Step 601 Train Loss: 0.2595
Epoch 15 Step 651 Train Loss: 0.2620
Epoch 15 Step 701 Train Loss: 0.2736
Epoch 15 Step 751 Train Loss: 0.2441
Epoch 15 Step 801 Train Loss: 0.2705
Epoch 15 Step 851 Train Loss: 0.2522
Epoch 15 Step 901 Train Loss: 0.2385
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0044
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00034321725
test_unseen_single_pearson: 0.9926012692785807
test_unseen_single_mse_de: 0.004419386
test_unseen_single_pearson_de: 0.7997727759641851
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.15561165360272933
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.33627450980392154
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9323529411764707
test_unseen_single_mse_top20_de_non_dropout: 0.0074174618900368205
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.026 MB of 0.027 MB uploadedwandb: / 0.026 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                                             train_de_pearson â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–„â–‡â–â–„â–‡â–ƒâ–ˆâ–„â–†â–ƒâ–‡â–‚â–â–â–ˆâ–†â–„â–„â–ƒâ–…â–ƒâ–ƒâ–â–†â–ƒâ–ƒâ–‡â–‚â–‚â–…â–‡â–‚â–…â–ƒâ–„â–†â–†â–‚â–â–…
wandb:                                                   val_de_mse â–ˆâ–â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                                               val_de_pearson â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                      val_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00442
wandb:                                              test_de_pearson 0.79977
wandb:               test_frac_opposite_direction_top20_non_dropout 0.33627
wandb:                          test_frac_sigma_below_1_non_dropout 0.93235
wandb:                                                     test_mse 0.00034
wandb:                                test_mse_top20_de_non_dropout 0.00742
wandb:                                                 test_pearson 0.9926
wandb:                                           test_pearson_delta 0.15561
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.33627
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93235
wandb:                                       test_unseen_single_mse 0.00034
wandb:                                    test_unseen_single_mse_de 0.00442
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00742
wandb:                                   test_unseen_single_pearson 0.9926
wandb:                                test_unseen_single_pearson_de 0.79977
wandb:                             test_unseen_single_pearson_delta 0.15561
wandb:                                                 train_de_mse 0.00544
wandb:                                             train_de_pearson 0.79006
wandb:                                                    train_mse 0.00033
wandb:                                                train_pearson 0.99278
wandb:                                                training_loss 0.26165
wandb:                                                   val_de_mse 0.00822
wandb:                                               val_de_pearson 0.74861
wandb:                                                      val_mse 0.00055
wandb:                                                  val_pearson 0.9886
wandb: 
wandb: ðŸš€ View run XuCao2023_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/c32f876g
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_030822-c32f876g/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/xucao2023/splits/xucao2023_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_040440-g43m4kre
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/g43m4kre
Start Training...
Epoch 1 Step 1 Train Loss: 0.4513
Epoch 1 Step 51 Train Loss: 0.2895
Epoch 1 Step 101 Train Loss: 0.2661
Epoch 1 Step 151 Train Loss: 0.2828
Epoch 1 Step 201 Train Loss: 0.2650
Epoch 1 Step 251 Train Loss: 0.2779
Epoch 1 Step 301 Train Loss: 0.2795
Epoch 1 Step 351 Train Loss: 0.2835
Epoch 1 Step 401 Train Loss: 0.2700
Epoch 1 Step 451 Train Loss: 0.2742
Epoch 1 Step 501 Train Loss: 0.2635
Epoch 1 Step 551 Train Loss: 0.2564
Epoch 1 Step 601 Train Loss: 0.2569
Epoch 1 Step 651 Train Loss: 0.2665
Epoch 1 Step 701 Train Loss: 0.2563
Epoch 1 Step 751 Train Loss: 0.2670
Epoch 1 Step 801 Train Loss: 0.2454
Epoch 1 Step 851 Train Loss: 0.2489
Epoch 1 Step 901 Train Loss: 0.2433
Epoch 1 Step 951 Train Loss: 0.2572
Epoch 1 Step 1001 Train Loss: 0.2704
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0039. 
Epoch 2 Step 1 Train Loss: 0.2725
Epoch 2 Step 51 Train Loss: 0.2817
Epoch 2 Step 101 Train Loss: 0.2691
Epoch 2 Step 151 Train Loss: 0.2451
Epoch 2 Step 201 Train Loss: 0.2387
Epoch 2 Step 251 Train Loss: 0.2618
Epoch 2 Step 301 Train Loss: 0.2448
Epoch 2 Step 351 Train Loss: 0.2776
Epoch 2 Step 401 Train Loss: 0.2417
Epoch 2 Step 451 Train Loss: 0.2676
Epoch 2 Step 501 Train Loss: 0.2488
Epoch 2 Step 551 Train Loss: 0.2982
Epoch 2 Step 601 Train Loss: 0.2584
Epoch 2 Step 651 Train Loss: 0.2441
Epoch 2 Step 701 Train Loss: 0.2523
Epoch 2 Step 751 Train Loss: 0.2355
Epoch 2 Step 801 Train Loss: 0.2391
Epoch 2 Step 851 Train Loss: 0.2403
Epoch 2 Step 901 Train Loss: 0.2494
Epoch 2 Step 951 Train Loss: 0.2669
Epoch 2 Step 1001 Train Loss: 0.2472
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 3 Step 1 Train Loss: 0.2662
Epoch 3 Step 51 Train Loss: 0.2756
Epoch 3 Step 101 Train Loss: 0.2569
Epoch 3 Step 151 Train Loss: 0.2531
Epoch 3 Step 201 Train Loss: 0.2561
Epoch 3 Step 251 Train Loss: 0.2500
Epoch 3 Step 301 Train Loss: 0.2709
Epoch 3 Step 351 Train Loss: 0.2447
Epoch 3 Step 401 Train Loss: 0.2637
Epoch 3 Step 451 Train Loss: 0.2565
Epoch 3 Step 501 Train Loss: 0.2398
Epoch 3 Step 551 Train Loss: 0.2548
Epoch 3 Step 601 Train Loss: 0.2550
Epoch 3 Step 651 Train Loss: 0.2597
Epoch 3 Step 701 Train Loss: 0.2565
Epoch 3 Step 751 Train Loss: 0.2481
Epoch 3 Step 801 Train Loss: 0.2637
Epoch 3 Step 851 Train Loss: 0.2432
Epoch 3 Step 901 Train Loss: 0.2493
Epoch 3 Step 951 Train Loss: 0.2559
Epoch 3 Step 1001 Train Loss: 0.2493
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 4 Step 1 Train Loss: 0.2668
Epoch 4 Step 51 Train Loss: 0.2589
Epoch 4 Step 101 Train Loss: 0.2535
Epoch 4 Step 151 Train Loss: 0.2491
Epoch 4 Step 201 Train Loss: 0.2540
Epoch 4 Step 251 Train Loss: 0.2610
Epoch 4 Step 301 Train Loss: 0.2492
Epoch 4 Step 351 Train Loss: 0.2495
Epoch 4 Step 401 Train Loss: 0.2617
Epoch 4 Step 451 Train Loss: 0.2593
Epoch 4 Step 501 Train Loss: 0.2743
Epoch 4 Step 551 Train Loss: 0.2547
Epoch 4 Step 601 Train Loss: 0.2570
Epoch 4 Step 651 Train Loss: 0.2552
Epoch 4 Step 701 Train Loss: 0.2530
Epoch 4 Step 751 Train Loss: 0.2594
Epoch 4 Step 801 Train Loss: 0.2462
Epoch 4 Step 851 Train Loss: 0.2601
Epoch 4 Step 901 Train Loss: 0.2574
Epoch 4 Step 951 Train Loss: 0.2581
Epoch 4 Step 1001 Train Loss: 0.2488
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 5 Step 1 Train Loss: 0.2574
Epoch 5 Step 51 Train Loss: 0.2698
Epoch 5 Step 101 Train Loss: 0.2722
Epoch 5 Step 151 Train Loss: 0.2735
Epoch 5 Step 201 Train Loss: 0.2501
Epoch 5 Step 251 Train Loss: 0.2485
Epoch 5 Step 301 Train Loss: 0.2497
Epoch 5 Step 351 Train Loss: 0.2529
Epoch 5 Step 401 Train Loss: 0.2674
Epoch 5 Step 451 Train Loss: 0.2511
Epoch 5 Step 501 Train Loss: 0.2646
Epoch 5 Step 551 Train Loss: 0.2728
Epoch 5 Step 601 Train Loss: 0.2491
Epoch 5 Step 651 Train Loss: 0.2551
Epoch 5 Step 701 Train Loss: 0.2601
Epoch 5 Step 751 Train Loss: 0.2669
Epoch 5 Step 801 Train Loss: 0.2658
Epoch 5 Step 851 Train Loss: 0.2592
Epoch 5 Step 901 Train Loss: 0.2571
Epoch 5 Step 951 Train Loss: 0.2671
Epoch 5 Step 1001 Train Loss: 0.2549
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 6 Step 1 Train Loss: 0.2580
Epoch 6 Step 51 Train Loss: 0.2576
Epoch 6 Step 101 Train Loss: 0.2557
Epoch 6 Step 151 Train Loss: 0.2560
Epoch 6 Step 201 Train Loss: 0.2516
Epoch 6 Step 251 Train Loss: 0.2637
Epoch 6 Step 301 Train Loss: 0.2617
Epoch 6 Step 351 Train Loss: 0.2662
Epoch 6 Step 401 Train Loss: 0.2589
Epoch 6 Step 451 Train Loss: 0.2505
Epoch 6 Step 501 Train Loss: 0.2630
Epoch 6 Step 551 Train Loss: 0.2632
Epoch 6 Step 601 Train Loss: 0.2511
Epoch 6 Step 651 Train Loss: 0.2597
Epoch 6 Step 701 Train Loss: 0.2753
Epoch 6 Step 751 Train Loss: 0.2674
Epoch 6 Step 801 Train Loss: 0.2572
Epoch 6 Step 851 Train Loss: 0.2492
Epoch 6 Step 901 Train Loss: 0.2632
Epoch 6 Step 951 Train Loss: 0.2714
Epoch 6 Step 1001 Train Loss: 0.2539
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 7 Step 1 Train Loss: 0.2610
Epoch 7 Step 51 Train Loss: 0.2642
Epoch 7 Step 101 Train Loss: 0.2548
Epoch 7 Step 151 Train Loss: 0.2761
Epoch 7 Step 201 Train Loss: 0.2694
Epoch 7 Step 251 Train Loss: 0.2757
Epoch 7 Step 301 Train Loss: 0.2638
Epoch 7 Step 351 Train Loss: 0.2730
Epoch 7 Step 401 Train Loss: 0.2663
Epoch 7 Step 451 Train Loss: 0.2463
Epoch 7 Step 501 Train Loss: 0.2713
Epoch 7 Step 551 Train Loss: 0.2627
Epoch 7 Step 601 Train Loss: 0.2546
Epoch 7 Step 651 Train Loss: 0.2530
Epoch 7 Step 701 Train Loss: 0.2593
Epoch 7 Step 751 Train Loss: 0.2511
Epoch 7 Step 801 Train Loss: 0.2604
Epoch 7 Step 851 Train Loss: 0.2459
Epoch 7 Step 901 Train Loss: 0.2562
Epoch 7 Step 951 Train Loss: 0.2720
Epoch 7 Step 1001 Train Loss: 0.2551
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 8 Step 1 Train Loss: 0.2748
Epoch 8 Step 51 Train Loss: 0.2595
Epoch 8 Step 101 Train Loss: 0.2573
Epoch 8 Step 151 Train Loss: 0.2688
Epoch 8 Step 201 Train Loss: 0.2760
Epoch 8 Step 251 Train Loss: 0.2385
Epoch 8 Step 301 Train Loss: 0.2406
Epoch 8 Step 351 Train Loss: 0.2608
Epoch 8 Step 401 Train Loss: 0.2617
Epoch 8 Step 451 Train Loss: 0.2507
Epoch 8 Step 501 Train Loss: 0.2531
Epoch 8 Step 551 Train Loss: 0.2443
Epoch 8 Step 601 Train Loss: 0.2420
Epoch 8 Step 651 Train Loss: 0.2785
Epoch 8 Step 701 Train Loss: 0.2744
Epoch 8 Step 751 Train Loss: 0.2543
Epoch 8 Step 801 Train Loss: 0.2562
Epoch 8 Step 851 Train Loss: 0.2524
Epoch 8 Step 901 Train Loss: 0.2687
Epoch 8 Step 951 Train Loss: 0.2522
Epoch 8 Step 1001 Train Loss: 0.2642
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 9 Step 1 Train Loss: 0.2558
Epoch 9 Step 51 Train Loss: 0.2459
Epoch 9 Step 101 Train Loss: 0.2656
Epoch 9 Step 151 Train Loss: 0.2636
Epoch 9 Step 201 Train Loss: 0.2642
Epoch 9 Step 251 Train Loss: 0.2668
Epoch 9 Step 301 Train Loss: 0.2583
Epoch 9 Step 351 Train Loss: 0.2515
Epoch 9 Step 401 Train Loss: 0.2628
Epoch 9 Step 451 Train Loss: 0.2587
Epoch 9 Step 501 Train Loss: 0.2582
Epoch 9 Step 551 Train Loss: 0.2540
Epoch 9 Step 601 Train Loss: 0.2481
Epoch 9 Step 651 Train Loss: 0.2581
Epoch 9 Step 701 Train Loss: 0.2482
Epoch 9 Step 751 Train Loss: 0.2576
Epoch 9 Step 801 Train Loss: 0.2636
Epoch 9 Step 851 Train Loss: 0.2632
Epoch 9 Step 901 Train Loss: 0.2678
Epoch 9 Step 951 Train Loss: 0.2573
Epoch 9 Step 1001 Train Loss: 0.2763
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 10 Step 1 Train Loss: 0.2593
Epoch 10 Step 51 Train Loss: 0.2693
Epoch 10 Step 101 Train Loss: 0.2623
Epoch 10 Step 151 Train Loss: 0.2548
Epoch 10 Step 201 Train Loss: 0.2569
Epoch 10 Step 251 Train Loss: 0.2525
Epoch 10 Step 301 Train Loss: 0.2684
Epoch 10 Step 351 Train Loss: 0.2659
Epoch 10 Step 401 Train Loss: 0.2624
Epoch 10 Step 451 Train Loss: 0.2606
Epoch 10 Step 501 Train Loss: 0.2614
Epoch 10 Step 551 Train Loss: 0.2698
Epoch 10 Step 601 Train Loss: 0.2723
Epoch 10 Step 651 Train Loss: 0.2502
Epoch 10 Step 701 Train Loss: 0.2553
Epoch 10 Step 751 Train Loss: 0.2806
Epoch 10 Step 801 Train Loss: 0.2467
Epoch 10 Step 851 Train Loss: 0.2607
Epoch 10 Step 901 Train Loss: 0.2753
Epoch 10 Step 951 Train Loss: 0.2611
Epoch 10 Step 1001 Train Loss: 0.2685
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 11 Step 1 Train Loss: 0.2711
Epoch 11 Step 51 Train Loss: 0.2612
Epoch 11 Step 101 Train Loss: 0.2709
Epoch 11 Step 151 Train Loss: 0.2619
Epoch 11 Step 201 Train Loss: 0.2509
Epoch 11 Step 251 Train Loss: 0.2523
Epoch 11 Step 301 Train Loss: 0.2480
Epoch 11 Step 351 Train Loss: 0.2610
Epoch 11 Step 401 Train Loss: 0.2597
Epoch 11 Step 451 Train Loss: 0.2550
Epoch 11 Step 501 Train Loss: 0.2520
Epoch 11 Step 551 Train Loss: 0.2601
Epoch 11 Step 601 Train Loss: 0.2520
Epoch 11 Step 651 Train Loss: 0.2467
Epoch 11 Step 701 Train Loss: 0.2671
Epoch 11 Step 751 Train Loss: 0.2629
Epoch 11 Step 801 Train Loss: 0.2502
Epoch 11 Step 851 Train Loss: 0.2629
Epoch 11 Step 901 Train Loss: 0.2698
Epoch 11 Step 951 Train Loss: 0.2586
Epoch 11 Step 1001 Train Loss: 0.2535
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 12 Step 1 Train Loss: 0.2542
Epoch 12 Step 51 Train Loss: 0.2568
Epoch 12 Step 101 Train Loss: 0.2610
Epoch 12 Step 151 Train Loss: 0.2648
Epoch 12 Step 201 Train Loss: 0.2789
Epoch 12 Step 251 Train Loss: 0.2640
Epoch 12 Step 301 Train Loss: 0.2564
Epoch 12 Step 351 Train Loss: 0.2541
Epoch 12 Step 401 Train Loss: 0.2496
Epoch 12 Step 451 Train Loss: 0.2519
Epoch 12 Step 501 Train Loss: 0.2684
Epoch 12 Step 551 Train Loss: 0.2576
Epoch 12 Step 601 Train Loss: 0.2578
Epoch 12 Step 651 Train Loss: 0.2524
Epoch 12 Step 701 Train Loss: 0.2632
Epoch 12 Step 751 Train Loss: 0.2723
Epoch 12 Step 801 Train Loss: 0.2537
Epoch 12 Step 851 Train Loss: 0.2655
Epoch 12 Step 901 Train Loss: 0.2503
Epoch 12 Step 951 Train Loss: 0.2684
Epoch 12 Step 1001 Train Loss: 0.2667
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 13 Step 1 Train Loss: 0.2623
Epoch 13 Step 51 Train Loss: 0.2531
Epoch 13 Step 101 Train Loss: 0.2529
Epoch 13 Step 151 Train Loss: 0.2577
Epoch 13 Step 201 Train Loss: 0.2601
Epoch 13 Step 251 Train Loss: 0.2506
Epoch 13 Step 301 Train Loss: 0.2539
Epoch 13 Step 351 Train Loss: 0.2508
Epoch 13 Step 401 Train Loss: 0.2660
Epoch 13 Step 451 Train Loss: 0.2617
Epoch 13 Step 501 Train Loss: 0.2677
Epoch 13 Step 551 Train Loss: 0.2543
Epoch 13 Step 601 Train Loss: 0.2568
Epoch 13 Step 651 Train Loss: 0.2608
Epoch 13 Step 701 Train Loss: 0.2585
Epoch 13 Step 751 Train Loss: 0.2688
Epoch 13 Step 801 Train Loss: 0.2752
Epoch 13 Step 851 Train Loss: 0.2534
Epoch 13 Step 901 Train Loss: 0.2511
Epoch 13 Step 951 Train Loss: 0.2629
Epoch 13 Step 1001 Train Loss: 0.2627
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 14 Step 1 Train Loss: 0.2670
Epoch 14 Step 51 Train Loss: 0.2529
Epoch 14 Step 101 Train Loss: 0.2475
Epoch 14 Step 151 Train Loss: 0.2594
Epoch 14 Step 201 Train Loss: 0.2555
Epoch 14 Step 251 Train Loss: 0.2658
Epoch 14 Step 301 Train Loss: 0.2546
Epoch 14 Step 351 Train Loss: 0.2457
Epoch 14 Step 401 Train Loss: 0.2545
Epoch 14 Step 451 Train Loss: 0.2686
Epoch 14 Step 501 Train Loss: 0.2535
Epoch 14 Step 551 Train Loss: 0.2586
Epoch 14 Step 601 Train Loss: 0.2846
Epoch 14 Step 651 Train Loss: 0.2605
Epoch 14 Step 701 Train Loss: 0.2552
Epoch 14 Step 751 Train Loss: 0.2628
Epoch 14 Step 801 Train Loss: 0.2546
Epoch 14 Step 851 Train Loss: 0.2601
Epoch 14 Step 901 Train Loss: 0.2509
Epoch 14 Step 951 Train Loss: 0.2548
Epoch 14 Step 1001 Train Loss: 0.2667
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 15 Step 1 Train Loss: 0.2665
Epoch 15 Step 51 Train Loss: 0.2633
Epoch 15 Step 101 Train Loss: 0.2572
Epoch 15 Step 151 Train Loss: 0.2468
Epoch 15 Step 201 Train Loss: 0.2558
Epoch 15 Step 251 Train Loss: 0.2554
Epoch 15 Step 301 Train Loss: 0.2618
Epoch 15 Step 351 Train Loss: 0.2725
Epoch 15 Step 401 Train Loss: 0.2685
Epoch 15 Step 451 Train Loss: 0.2531
Epoch 15 Step 501 Train Loss: 0.2501
Epoch 15 Step 551 Train Loss: 0.2586
Epoch 15 Step 601 Train Loss: 0.2779
Epoch 15 Step 651 Train Loss: 0.2672
Epoch 15 Step 701 Train Loss: 0.2502
Epoch 15 Step 751 Train Loss: 0.2589
Epoch 15 Step 801 Train Loss: 0.2549
Epoch 15 Step 851 Train Loss: 0.2659
Epoch 15 Step 901 Train Loss: 0.2503
Epoch 15 Step 951 Train Loss: 0.2782
Epoch 15 Step 1001 Train Loss: 0.2713
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0040
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00036726918
test_unseen_single_pearson: 0.9922209052256744
test_unseen_single_mse_de: 0.004043177
test_unseen_single_pearson_de: 0.8018400037700735
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1535082621806089
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.34607843137254907
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9274509803921568
test_unseen_single_mse_top20_de_non_dropout: 0.008612488991010981
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.001 MB of 0.028 MB uploadedwandb: / 0.027 MB of 0.028 MB uploadedwandb: - 0.027 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:                                             train_de_pearson â–â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡
wandb:                                                    train_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–‡â–„â–„â–ˆâ–ƒâ–ƒâ–„â–„â–â–ˆâ–ƒâ–„â–…â–ƒâ–ƒâ–„â–‡â–‚â–„â–„â–ƒâ–…â–…â–„â–ƒâ–„â–…â–„â–ƒâ–‚â–„â–‚â–„â–†â–…â–„â–†â–ˆâ–†â–ƒ
wandb:                                                   val_de_mse â–ˆâ–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                               val_de_pearson â–â–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00404
wandb:                                              test_de_pearson 0.80184
wandb:               test_frac_opposite_direction_top20_non_dropout 0.34608
wandb:                          test_frac_sigma_below_1_non_dropout 0.92745
wandb:                                                     test_mse 0.00037
wandb:                                test_mse_top20_de_non_dropout 0.00861
wandb:                                                 test_pearson 0.99222
wandb:                                           test_pearson_delta 0.15351
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.34608
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.92745
wandb:                                       test_unseen_single_mse 0.00037
wandb:                                    test_unseen_single_mse_de 0.00404
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00861
wandb:                                   test_unseen_single_pearson 0.99222
wandb:                                test_unseen_single_pearson_de 0.80184
wandb:                             test_unseen_single_pearson_delta 0.15351
wandb:                                                 train_de_mse 0.00609
wandb:                                             train_de_pearson 0.79356
wandb:                                                    train_mse 0.00032
wandb:                                                train_pearson 0.99303
wandb:                                                training_loss 0.27088
wandb:                                                   val_de_mse 0.00379
wandb:                                               val_de_pearson 0.70048
wandb:                                                      val_mse 0.00056
wandb:                                                  val_pearson 0.98818
wandb: 
wandb: ðŸš€ View run XuCao2023_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/g43m4kre
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_040440-g43m4kre/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/xucao2023/splits/xucao2023_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_050422-dt97itum
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/dt97itum
Start Training...
Epoch 1 Step 1 Train Loss: 0.4448
Epoch 1 Step 51 Train Loss: 0.2864
Epoch 1 Step 101 Train Loss: 0.2641
Epoch 1 Step 151 Train Loss: 0.2560
Epoch 1 Step 201 Train Loss: 0.2751
Epoch 1 Step 251 Train Loss: 0.2621
Epoch 1 Step 301 Train Loss: 0.2824
Epoch 1 Step 351 Train Loss: 0.2680
Epoch 1 Step 401 Train Loss: 0.2665
Epoch 1 Step 451 Train Loss: 0.2629
Epoch 1 Step 501 Train Loss: 0.2531
Epoch 1 Step 551 Train Loss: 0.2504
Epoch 1 Step 601 Train Loss: 0.2636
Epoch 1 Step 651 Train Loss: 0.2632
Epoch 1 Step 701 Train Loss: 0.2545
Epoch 1 Step 751 Train Loss: 0.2681
Epoch 1 Step 801 Train Loss: 0.2474
Epoch 1 Step 851 Train Loss: 0.2557
Epoch 1 Step 901 Train Loss: 0.2551
Epoch 1 Step 951 Train Loss: 0.2659
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0150. 
Epoch 2 Step 1 Train Loss: 0.2701
Epoch 2 Step 51 Train Loss: 0.2517
Epoch 2 Step 101 Train Loss: 0.2552
Epoch 2 Step 151 Train Loss: 0.2561
Epoch 2 Step 201 Train Loss: 0.2619
Epoch 2 Step 251 Train Loss: 0.2501
Epoch 2 Step 301 Train Loss: 0.2446
Epoch 2 Step 351 Train Loss: 0.2651
Epoch 2 Step 401 Train Loss: 0.2625
Epoch 2 Step 451 Train Loss: 0.2481
Epoch 2 Step 501 Train Loss: 0.2758
Epoch 2 Step 551 Train Loss: 0.2586
Epoch 2 Step 601 Train Loss: 0.2610
Epoch 2 Step 651 Train Loss: 0.2718
Epoch 2 Step 701 Train Loss: 0.2467
Epoch 2 Step 751 Train Loss: 0.2376
Epoch 2 Step 801 Train Loss: 0.2638
Epoch 2 Step 851 Train Loss: 0.2677
Epoch 2 Step 901 Train Loss: 0.2527
Epoch 2 Step 951 Train Loss: 0.2679
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0153. 
Epoch 3 Step 1 Train Loss: 0.2535
Epoch 3 Step 51 Train Loss: 0.2630
Epoch 3 Step 101 Train Loss: 0.2457
Epoch 3 Step 151 Train Loss: 0.2764
Epoch 3 Step 201 Train Loss: 0.2570
Epoch 3 Step 251 Train Loss: 0.2695
Epoch 3 Step 301 Train Loss: 0.2590
Epoch 3 Step 351 Train Loss: 0.2616
Epoch 3 Step 401 Train Loss: 0.2731
Epoch 3 Step 451 Train Loss: 0.2418
Epoch 3 Step 501 Train Loss: 0.2618
Epoch 3 Step 551 Train Loss: 0.2492
Epoch 3 Step 601 Train Loss: 0.2636
Epoch 3 Step 651 Train Loss: 0.2564
Epoch 3 Step 701 Train Loss: 0.2609
Epoch 3 Step 751 Train Loss: 0.2556
Epoch 3 Step 801 Train Loss: 0.2657
Epoch 3 Step 851 Train Loss: 0.2811
Epoch 3 Step 901 Train Loss: 0.2378
Epoch 3 Step 951 Train Loss: 0.2624
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 4 Step 1 Train Loss: 0.2417
Epoch 4 Step 51 Train Loss: 0.2466
Epoch 4 Step 101 Train Loss: 0.2528
Epoch 4 Step 151 Train Loss: 0.2740
Epoch 4 Step 201 Train Loss: 0.2613
Epoch 4 Step 251 Train Loss: 0.2621
Epoch 4 Step 301 Train Loss: 0.2609
Epoch 4 Step 351 Train Loss: 0.2567
Epoch 4 Step 401 Train Loss: 0.2563
Epoch 4 Step 451 Train Loss: 0.2606
Epoch 4 Step 501 Train Loss: 0.2625
Epoch 4 Step 551 Train Loss: 0.2638
Epoch 4 Step 601 Train Loss: 0.2642
Epoch 4 Step 651 Train Loss: 0.2680
Epoch 4 Step 701 Train Loss: 0.2672
Epoch 4 Step 751 Train Loss: 0.2683
Epoch 4 Step 801 Train Loss: 0.2620
Epoch 4 Step 851 Train Loss: 0.2632
Epoch 4 Step 901 Train Loss: 0.2538
Epoch 4 Step 951 Train Loss: 0.2515
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 5 Step 1 Train Loss: 0.2645
Epoch 5 Step 51 Train Loss: 0.2543
Epoch 5 Step 101 Train Loss: 0.2619
Epoch 5 Step 151 Train Loss: 0.2542
Epoch 5 Step 201 Train Loss: 0.2519
Epoch 5 Step 251 Train Loss: 0.2540
Epoch 5 Step 301 Train Loss: 0.2620
Epoch 5 Step 351 Train Loss: 0.2562
Epoch 5 Step 401 Train Loss: 0.2597
Epoch 5 Step 451 Train Loss: 0.2557
Epoch 5 Step 501 Train Loss: 0.2746
Epoch 5 Step 551 Train Loss: 0.2892
Epoch 5 Step 601 Train Loss: 0.2577
Epoch 5 Step 651 Train Loss: 0.2712
Epoch 5 Step 701 Train Loss: 0.2553
Epoch 5 Step 751 Train Loss: 0.2755
Epoch 5 Step 801 Train Loss: 0.2653
Epoch 5 Step 851 Train Loss: 0.2731
Epoch 5 Step 901 Train Loss: 0.2644
Epoch 5 Step 951 Train Loss: 0.2510
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 6 Step 1 Train Loss: 0.2637
Epoch 6 Step 51 Train Loss: 0.2583
Epoch 6 Step 101 Train Loss: 0.2713
Epoch 6 Step 151 Train Loss: 0.2681
Epoch 6 Step 201 Train Loss: 0.2583
Epoch 6 Step 251 Train Loss: 0.2759
Epoch 6 Step 301 Train Loss: 0.2561
Epoch 6 Step 351 Train Loss: 0.2534
Epoch 6 Step 401 Train Loss: 0.2514
Epoch 6 Step 451 Train Loss: 0.2510
Epoch 6 Step 501 Train Loss: 0.2675
Epoch 6 Step 551 Train Loss: 0.2659
Epoch 6 Step 601 Train Loss: 0.2595
Epoch 6 Step 651 Train Loss: 0.2646
Epoch 6 Step 701 Train Loss: 0.2451
Epoch 6 Step 751 Train Loss: 0.2555
Epoch 6 Step 801 Train Loss: 0.2611
Epoch 6 Step 851 Train Loss: 0.2716
Epoch 6 Step 901 Train Loss: 0.2693
Epoch 6 Step 951 Train Loss: 0.2492
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 7 Step 1 Train Loss: 0.2583
Epoch 7 Step 51 Train Loss: 0.2571
Epoch 7 Step 101 Train Loss: 0.2562
Epoch 7 Step 151 Train Loss: 0.2593
Epoch 7 Step 201 Train Loss: 0.2548
Epoch 7 Step 251 Train Loss: 0.2606
Epoch 7 Step 301 Train Loss: 0.2585
Epoch 7 Step 351 Train Loss: 0.2557
Epoch 7 Step 401 Train Loss: 0.2493
Epoch 7 Step 451 Train Loss: 0.2702
Epoch 7 Step 501 Train Loss: 0.2483
Epoch 7 Step 551 Train Loss: 0.2609
Epoch 7 Step 601 Train Loss: 0.2607
Epoch 7 Step 651 Train Loss: 0.2554
Epoch 7 Step 701 Train Loss: 0.2555
Epoch 7 Step 751 Train Loss: 0.2477
Epoch 7 Step 801 Train Loss: 0.2551
Epoch 7 Step 851 Train Loss: 0.2673
Epoch 7 Step 901 Train Loss: 0.2597
Epoch 7 Step 951 Train Loss: 0.2515
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 8 Step 1 Train Loss: 0.2687
Epoch 8 Step 51 Train Loss: 0.2720
Epoch 8 Step 101 Train Loss: 0.2409
Epoch 8 Step 151 Train Loss: 0.2490
Epoch 8 Step 201 Train Loss: 0.2723
Epoch 8 Step 251 Train Loss: 0.2597
Epoch 8 Step 301 Train Loss: 0.2624
Epoch 8 Step 351 Train Loss: 0.2553
Epoch 8 Step 401 Train Loss: 0.2555
Epoch 8 Step 451 Train Loss: 0.2563
Epoch 8 Step 501 Train Loss: 0.2511
Epoch 8 Step 551 Train Loss: 0.2576
Epoch 8 Step 601 Train Loss: 0.2693
Epoch 8 Step 651 Train Loss: 0.2497
Epoch 8 Step 701 Train Loss: 0.2547
Epoch 8 Step 751 Train Loss: 0.2809
Epoch 8 Step 801 Train Loss: 0.2631
Epoch 8 Step 851 Train Loss: 0.2495
Epoch 8 Step 901 Train Loss: 0.2537
Epoch 8 Step 951 Train Loss: 0.2497
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 9 Step 1 Train Loss: 0.2653
Epoch 9 Step 51 Train Loss: 0.2577
Epoch 9 Step 101 Train Loss: 0.2712
Epoch 9 Step 151 Train Loss: 0.2627
Epoch 9 Step 201 Train Loss: 0.2487
Epoch 9 Step 251 Train Loss: 0.2516
Epoch 9 Step 301 Train Loss: 0.2614
Epoch 9 Step 351 Train Loss: 0.2676
Epoch 9 Step 401 Train Loss: 0.2646
Epoch 9 Step 451 Train Loss: 0.2590
Epoch 9 Step 501 Train Loss: 0.2512
Epoch 9 Step 551 Train Loss: 0.2508
Epoch 9 Step 601 Train Loss: 0.2651
Epoch 9 Step 651 Train Loss: 0.2475
Epoch 9 Step 701 Train Loss: 0.2717
Epoch 9 Step 751 Train Loss: 0.2488
Epoch 9 Step 801 Train Loss: 0.2499
Epoch 9 Step 851 Train Loss: 0.2533
Epoch 9 Step 901 Train Loss: 0.2569
Epoch 9 Step 951 Train Loss: 0.2653
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 10 Step 1 Train Loss: 0.2526
Epoch 10 Step 51 Train Loss: 0.2510
Epoch 10 Step 101 Train Loss: 0.2487
Epoch 10 Step 151 Train Loss: 0.2535
Epoch 10 Step 201 Train Loss: 0.2623
Epoch 10 Step 251 Train Loss: 0.2589
Epoch 10 Step 301 Train Loss: 0.2703
Epoch 10 Step 351 Train Loss: 0.2503
Epoch 10 Step 401 Train Loss: 0.2610
Epoch 10 Step 451 Train Loss: 0.2459
Epoch 10 Step 501 Train Loss: 0.2579
Epoch 10 Step 551 Train Loss: 0.2629
Epoch 10 Step 601 Train Loss: 0.2891
Epoch 10 Step 651 Train Loss: 0.2564
Epoch 10 Step 701 Train Loss: 0.2558
Epoch 10 Step 751 Train Loss: 0.2536
Epoch 10 Step 801 Train Loss: 0.2592
Epoch 10 Step 851 Train Loss: 0.2589
Epoch 10 Step 901 Train Loss: 0.2543
Epoch 10 Step 951 Train Loss: 0.2643
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 11 Step 1 Train Loss: 0.2563
Epoch 11 Step 51 Train Loss: 0.2496
Epoch 11 Step 101 Train Loss: 0.2702
Epoch 11 Step 151 Train Loss: 0.2586
Epoch 11 Step 201 Train Loss: 0.2689
Epoch 11 Step 251 Train Loss: 0.2499
Epoch 11 Step 301 Train Loss: 0.2430
Epoch 11 Step 351 Train Loss: 0.2603
Epoch 11 Step 401 Train Loss: 0.2614
Epoch 11 Step 451 Train Loss: 0.2451
Epoch 11 Step 501 Train Loss: 0.2612
Epoch 11 Step 551 Train Loss: 0.2664
Epoch 11 Step 601 Train Loss: 0.2664
Epoch 11 Step 651 Train Loss: 0.2508
Epoch 11 Step 701 Train Loss: 0.2510
Epoch 11 Step 751 Train Loss: 0.2633
Epoch 11 Step 801 Train Loss: 0.2596
Epoch 11 Step 851 Train Loss: 0.2635
Epoch 11 Step 901 Train Loss: 0.2553
Epoch 11 Step 951 Train Loss: 0.2420
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 12 Step 1 Train Loss: 0.2571
Epoch 12 Step 51 Train Loss: 0.2567
Epoch 12 Step 101 Train Loss: 0.2639
Epoch 12 Step 151 Train Loss: 0.2717
Epoch 12 Step 201 Train Loss: 0.2514
Epoch 12 Step 251 Train Loss: 0.2507
Epoch 12 Step 301 Train Loss: 0.2610
Epoch 12 Step 351 Train Loss: 0.2740
Epoch 12 Step 401 Train Loss: 0.2534
Epoch 12 Step 451 Train Loss: 0.2586
Epoch 12 Step 501 Train Loss: 0.2519
Epoch 12 Step 551 Train Loss: 0.2573
Epoch 12 Step 601 Train Loss: 0.2654
Epoch 12 Step 651 Train Loss: 0.2602
Epoch 12 Step 701 Train Loss: 0.2617
Epoch 12 Step 751 Train Loss: 0.2599
Epoch 12 Step 801 Train Loss: 0.2605
Epoch 12 Step 851 Train Loss: 0.2627
Epoch 12 Step 901 Train Loss: 0.2597
Epoch 12 Step 951 Train Loss: 0.2333
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 13 Step 1 Train Loss: 0.2570
Epoch 13 Step 51 Train Loss: 0.2533
Epoch 13 Step 101 Train Loss: 0.2538
Epoch 13 Step 151 Train Loss: 0.2604
Epoch 13 Step 201 Train Loss: 0.2633
Epoch 13 Step 251 Train Loss: 0.2634
Epoch 13 Step 301 Train Loss: 0.2500
Epoch 13 Step 351 Train Loss: 0.2647
Epoch 13 Step 401 Train Loss: 0.2620
Epoch 13 Step 451 Train Loss: 0.2593
Epoch 13 Step 501 Train Loss: 0.2668
Epoch 13 Step 551 Train Loss: 0.2606
Epoch 13 Step 601 Train Loss: 0.2544
Epoch 13 Step 651 Train Loss: 0.2666
Epoch 13 Step 701 Train Loss: 0.2597
Epoch 13 Step 751 Train Loss: 0.2545
Epoch 13 Step 801 Train Loss: 0.2507
Epoch 13 Step 851 Train Loss: 0.2459
Epoch 13 Step 901 Train Loss: 0.2541
Epoch 13 Step 951 Train Loss: 0.2473
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 14 Step 1 Train Loss: 0.2499
Epoch 14 Step 51 Train Loss: 0.2753
Epoch 14 Step 101 Train Loss: 0.2653
Epoch 14 Step 151 Train Loss: 0.2429
Epoch 14 Step 201 Train Loss: 0.2529
Epoch 14 Step 251 Train Loss: 0.2530
Epoch 14 Step 301 Train Loss: 0.2468
Epoch 14 Step 351 Train Loss: 0.2635
Epoch 14 Step 401 Train Loss: 0.2760
Epoch 14 Step 451 Train Loss: 0.2593
Epoch 14 Step 501 Train Loss: 0.2591
Epoch 14 Step 551 Train Loss: 0.2675
Epoch 14 Step 601 Train Loss: 0.2769
Epoch 14 Step 651 Train Loss: 0.2463
Epoch 14 Step 701 Train Loss: 0.2526
Epoch 14 Step 751 Train Loss: 0.2595
Epoch 14 Step 801 Train Loss: 0.2563
Epoch 14 Step 851 Train Loss: 0.2615
Epoch 14 Step 901 Train Loss: 0.2635
Epoch 14 Step 951 Train Loss: 0.2814
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 15 Step 1 Train Loss: 0.2547
Epoch 15 Step 51 Train Loss: 0.2550
Epoch 15 Step 101 Train Loss: 0.2723
Epoch 15 Step 151 Train Loss: 0.2601
Epoch 15 Step 201 Train Loss: 0.2551
Epoch 15 Step 251 Train Loss: 0.2577
Epoch 15 Step 301 Train Loss: 0.2737
Epoch 15 Step 351 Train Loss: 0.2513
Epoch 15 Step 401 Train Loss: 0.2423
Epoch 15 Step 451 Train Loss: 0.2497
Epoch 15 Step 501 Train Loss: 0.2439
Epoch 15 Step 551 Train Loss: 0.2550
Epoch 15 Step 601 Train Loss: 0.2517
Epoch 15 Step 651 Train Loss: 0.2512
Epoch 15 Step 701 Train Loss: 0.2605
Epoch 15 Step 751 Train Loss: 0.2591
Epoch 15 Step 801 Train Loss: 0.2625
Epoch 15 Step 851 Train Loss: 0.2492
Epoch 15 Step 901 Train Loss: 0.2620
Epoch 15 Step 951 Train Loss: 0.2682
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0041
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0004941128
test_unseen_single_pearson: 0.9894085321242234
test_unseen_single_mse_de: 0.004141147
test_unseen_single_pearson_de: 0.6964634901397078
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.10492747133808365
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.39705882352941174
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8823529411764706
test_unseen_single_mse_top20_de_non_dropout: 0.009322370413826392
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.026 MB of 0.027 MB uploadedwandb: / 0.026 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–„â–‚â–â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                             train_de_pearson â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–†â–…â–‡â–‚â–†â–…â–„â–ƒâ–„â–…â–…â–ˆâ–‡â–…â–ˆâ–ƒâ–‡â–‡â–„â–†â–†â–„â–…â–†â–â–…â–†â–†â–…â–†â–‡â–†â–ƒâ–†â–†â–„â–†â–†â–…â–…
wandb:                                                   val_de_mse â–â–ˆâ–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:                                               val_de_pearson â–â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00414
wandb:                                              test_de_pearson 0.69646
wandb:               test_frac_opposite_direction_top20_non_dropout 0.39706
wandb:                          test_frac_sigma_below_1_non_dropout 0.88235
wandb:                                                     test_mse 0.00049
wandb:                                test_mse_top20_de_non_dropout 0.00932
wandb:                                                 test_pearson 0.98941
wandb:                                           test_pearson_delta 0.10493
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.39706
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.88235
wandb:                                       test_unseen_single_mse 0.00049
wandb:                                    test_unseen_single_mse_de 0.00414
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00932
wandb:                                   test_unseen_single_pearson 0.98941
wandb:                                test_unseen_single_pearson_de 0.69646
wandb:                             test_unseen_single_pearson_delta 0.10493
wandb:                                                 train_de_mse 0.00478
wandb:                                             train_de_pearson 0.80222
wandb:                                                    train_mse 0.00033
wandb:                                                train_pearson 0.99286
wandb:                                                training_loss 0.25735
wandb:                                                   val_de_mse 0.01521
wandb:                                               val_de_pearson 0.92351
wandb:                                                      val_mse 0.0003
wandb:                                                  val_pearson 0.99345
wandb: 
wandb: ðŸš€ View run XuCao2023_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/dt97itum
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_050422-dt97itum/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/xucao2023/splits/xucao2023_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_060236-0b0xpqx5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/0b0xpqx5
Start Training...
Epoch 1 Step 1 Train Loss: 0.4840
Epoch 1 Step 51 Train Loss: 0.2775
Epoch 1 Step 101 Train Loss: 0.2761
Epoch 1 Step 151 Train Loss: 0.2639
Epoch 1 Step 201 Train Loss: 0.2738
Epoch 1 Step 251 Train Loss: 0.2654
Epoch 1 Step 301 Train Loss: 0.2682
Epoch 1 Step 351 Train Loss: 0.2764
Epoch 1 Step 401 Train Loss: 0.2691
Epoch 1 Step 451 Train Loss: 0.2652
Epoch 1 Step 501 Train Loss: 0.2610
Epoch 1 Step 551 Train Loss: 0.2474
Epoch 1 Step 601 Train Loss: 0.2603
Epoch 1 Step 651 Train Loss: 0.2551
Epoch 1 Step 701 Train Loss: 0.2705
Epoch 1 Step 751 Train Loss: 0.2754
Epoch 1 Step 801 Train Loss: 0.2789
Epoch 1 Step 851 Train Loss: 0.2461
Epoch 1 Step 901 Train Loss: 0.2572
Epoch 1 Step 951 Train Loss: 0.2612
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0063 Validation Top 20 DE MSE: 0.0053. 
Epoch 2 Step 1 Train Loss: 0.2340
Epoch 2 Step 51 Train Loss: 0.2570
Epoch 2 Step 101 Train Loss: 0.2698
Epoch 2 Step 151 Train Loss: 0.2701
Epoch 2 Step 201 Train Loss: 0.2482
Epoch 2 Step 251 Train Loss: 0.2574
Epoch 2 Step 301 Train Loss: 0.2647
Epoch 2 Step 351 Train Loss: 0.2577
Epoch 2 Step 401 Train Loss: 0.2562
Epoch 2 Step 451 Train Loss: 0.2735
Epoch 2 Step 501 Train Loss: 0.2630
Epoch 2 Step 551 Train Loss: 0.2538
Epoch 2 Step 601 Train Loss: 0.2789
Epoch 2 Step 651 Train Loss: 0.2653
Epoch 2 Step 701 Train Loss: 0.2617
Epoch 2 Step 751 Train Loss: 0.2620
Epoch 2 Step 801 Train Loss: 0.2637
Epoch 2 Step 851 Train Loss: 0.2606
Epoch 2 Step 901 Train Loss: 0.2541
Epoch 2 Step 951 Train Loss: 0.2519
Epoch 2: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0052. 
Epoch 3 Step 1 Train Loss: 0.2441
Epoch 3 Step 51 Train Loss: 0.2673
Epoch 3 Step 101 Train Loss: 0.2588
Epoch 3 Step 151 Train Loss: 0.2732
Epoch 3 Step 201 Train Loss: 0.2587
Epoch 3 Step 251 Train Loss: 0.2578
Epoch 3 Step 301 Train Loss: 0.2488
Epoch 3 Step 351 Train Loss: 0.2420
Epoch 3 Step 401 Train Loss: 0.2555
Epoch 3 Step 451 Train Loss: 0.2571
Epoch 3 Step 501 Train Loss: 0.2586
Epoch 3 Step 551 Train Loss: 0.2531
Epoch 3 Step 601 Train Loss: 0.2726
Epoch 3 Step 651 Train Loss: 0.2521
Epoch 3 Step 701 Train Loss: 0.2594
Epoch 3 Step 751 Train Loss: 0.2548
Epoch 3 Step 801 Train Loss: 0.2729
Epoch 3 Step 851 Train Loss: 0.2593
Epoch 3 Step 901 Train Loss: 0.2607
Epoch 3 Step 951 Train Loss: 0.2537
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0052. 
Epoch 4 Step 1 Train Loss: 0.2462
Epoch 4 Step 51 Train Loss: 0.2701
Epoch 4 Step 101 Train Loss: 0.2504
Epoch 4 Step 151 Train Loss: 0.2625
Epoch 4 Step 201 Train Loss: 0.2548
Epoch 4 Step 251 Train Loss: 0.2685
Epoch 4 Step 301 Train Loss: 0.2493
Epoch 4 Step 351 Train Loss: 0.2638
Epoch 4 Step 401 Train Loss: 0.2379
Epoch 4 Step 451 Train Loss: 0.2596
Epoch 4 Step 501 Train Loss: 0.2588
Epoch 4 Step 551 Train Loss: 0.2326
Epoch 4 Step 601 Train Loss: 0.2576
Epoch 4 Step 651 Train Loss: 0.2729
Epoch 4 Step 701 Train Loss: 0.2541
Epoch 4 Step 751 Train Loss: 0.2717
Epoch 4 Step 801 Train Loss: 0.2554
Epoch 4 Step 851 Train Loss: 0.2631
Epoch 4 Step 901 Train Loss: 0.2638
Epoch 4 Step 951 Train Loss: 0.2657
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 5 Step 1 Train Loss: 0.2429
Epoch 5 Step 51 Train Loss: 0.2766
Epoch 5 Step 101 Train Loss: 0.2609
Epoch 5 Step 151 Train Loss: 0.2558
Epoch 5 Step 201 Train Loss: 0.2497
Epoch 5 Step 251 Train Loss: 0.2575
Epoch 5 Step 301 Train Loss: 0.2660
Epoch 5 Step 351 Train Loss: 0.2593
Epoch 5 Step 401 Train Loss: 0.2697
Epoch 5 Step 451 Train Loss: 0.2571
Epoch 5 Step 501 Train Loss: 0.2764
Epoch 5 Step 551 Train Loss: 0.2687
Epoch 5 Step 601 Train Loss: 0.2551
Epoch 5 Step 651 Train Loss: 0.2473
Epoch 5 Step 701 Train Loss: 0.2486
Epoch 5 Step 751 Train Loss: 0.2615
Epoch 5 Step 801 Train Loss: 0.2559
Epoch 5 Step 851 Train Loss: 0.2531
Epoch 5 Step 901 Train Loss: 0.2466
Epoch 5 Step 951 Train Loss: 0.2775
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 6 Step 1 Train Loss: 0.2472
Epoch 6 Step 51 Train Loss: 0.2443
Epoch 6 Step 101 Train Loss: 0.2624
Epoch 6 Step 151 Train Loss: 0.2539
Epoch 6 Step 201 Train Loss: 0.2532
Epoch 6 Step 251 Train Loss: 0.2460
Epoch 6 Step 301 Train Loss: 0.2663
Epoch 6 Step 351 Train Loss: 0.2612
Epoch 6 Step 401 Train Loss: 0.2596
Epoch 6 Step 451 Train Loss: 0.2543
Epoch 6 Step 501 Train Loss: 0.2653
Epoch 6 Step 551 Train Loss: 0.2483
Epoch 6 Step 601 Train Loss: 0.2623
Epoch 6 Step 651 Train Loss: 0.2665
Epoch 6 Step 701 Train Loss: 0.2753
Epoch 6 Step 751 Train Loss: 0.2590
Epoch 6 Step 801 Train Loss: 0.2610
Epoch 6 Step 851 Train Loss: 0.2446
Epoch 6 Step 901 Train Loss: 0.2547
Epoch 6 Step 951 Train Loss: 0.2487
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 7 Step 1 Train Loss: 0.2488
Epoch 7 Step 51 Train Loss: 0.2650
Epoch 7 Step 101 Train Loss: 0.2527
Epoch 7 Step 151 Train Loss: 0.2469
Epoch 7 Step 201 Train Loss: 0.2504
Epoch 7 Step 251 Train Loss: 0.2868
Epoch 7 Step 301 Train Loss: 0.2631
Epoch 7 Step 351 Train Loss: 0.2601
Epoch 7 Step 401 Train Loss: 0.2596
Epoch 7 Step 451 Train Loss: 0.2759
Epoch 7 Step 501 Train Loss: 0.2681
Epoch 7 Step 551 Train Loss: 0.2732
Epoch 7 Step 601 Train Loss: 0.2651
Epoch 7 Step 651 Train Loss: 0.2603
Epoch 7 Step 701 Train Loss: 0.2603
Epoch 7 Step 751 Train Loss: 0.2540
Epoch 7 Step 801 Train Loss: 0.2535
Epoch 7 Step 851 Train Loss: 0.2542
Epoch 7 Step 901 Train Loss: 0.2666
Epoch 7 Step 951 Train Loss: 0.2653
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 8 Step 1 Train Loss: 0.2585
Epoch 8 Step 51 Train Loss: 0.2382
Epoch 8 Step 101 Train Loss: 0.2554
Epoch 8 Step 151 Train Loss: 0.2598
Epoch 8 Step 201 Train Loss: 0.2646
Epoch 8 Step 251 Train Loss: 0.2540
Epoch 8 Step 301 Train Loss: 0.2428
Epoch 8 Step 351 Train Loss: 0.2604
Epoch 8 Step 401 Train Loss: 0.2591
Epoch 8 Step 451 Train Loss: 0.2566
Epoch 8 Step 501 Train Loss: 0.2522
Epoch 8 Step 551 Train Loss: 0.2551
Epoch 8 Step 601 Train Loss: 0.2500
Epoch 8 Step 651 Train Loss: 0.2601
Epoch 8 Step 701 Train Loss: 0.2485
Epoch 8 Step 751 Train Loss: 0.2583
Epoch 8 Step 801 Train Loss: 0.2426
Epoch 8 Step 851 Train Loss: 0.2653
Epoch 8 Step 901 Train Loss: 0.2602
Epoch 8 Step 951 Train Loss: 0.2795
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 9 Step 1 Train Loss: 0.2513
Epoch 9 Step 51 Train Loss: 0.2478
Epoch 9 Step 101 Train Loss: 0.2793
Epoch 9 Step 151 Train Loss: 0.2686
Epoch 9 Step 201 Train Loss: 0.2586
Epoch 9 Step 251 Train Loss: 0.2594
Epoch 9 Step 301 Train Loss: 0.2585
Epoch 9 Step 351 Train Loss: 0.2528
Epoch 9 Step 401 Train Loss: 0.2747
Epoch 9 Step 451 Train Loss: 0.2455
Epoch 9 Step 501 Train Loss: 0.2727
Epoch 9 Step 551 Train Loss: 0.2519
Epoch 9 Step 601 Train Loss: 0.2672
Epoch 9 Step 651 Train Loss: 0.2487
Epoch 9 Step 701 Train Loss: 0.2543
Epoch 9 Step 751 Train Loss: 0.2550
Epoch 9 Step 801 Train Loss: 0.2556
Epoch 9 Step 851 Train Loss: 0.2638
Epoch 9 Step 901 Train Loss: 0.2636
Epoch 9 Step 951 Train Loss: 0.2555
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 10 Step 1 Train Loss: 0.2592
Epoch 10 Step 51 Train Loss: 0.2784
Epoch 10 Step 101 Train Loss: 0.2664
Epoch 10 Step 151 Train Loss: 0.2548
Epoch 10 Step 201 Train Loss: 0.2629
Epoch 10 Step 251 Train Loss: 0.2571
Epoch 10 Step 301 Train Loss: 0.2565
Epoch 10 Step 351 Train Loss: 0.2488
Epoch 10 Step 401 Train Loss: 0.2464
Epoch 10 Step 451 Train Loss: 0.2479
Epoch 10 Step 501 Train Loss: 0.2529
Epoch 10 Step 551 Train Loss: 0.2540
Epoch 10 Step 601 Train Loss: 0.2622
Epoch 10 Step 651 Train Loss: 0.2511
Epoch 10 Step 701 Train Loss: 0.2591
Epoch 10 Step 751 Train Loss: 0.2667
Epoch 10 Step 801 Train Loss: 0.2530
Epoch 10 Step 851 Train Loss: 0.2417
Epoch 10 Step 901 Train Loss: 0.2640
Epoch 10 Step 951 Train Loss: 0.2671
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 11 Step 1 Train Loss: 0.2524
Epoch 11 Step 51 Train Loss: 0.2552
Epoch 11 Step 101 Train Loss: 0.2648
Epoch 11 Step 151 Train Loss: 0.2698
Epoch 11 Step 201 Train Loss: 0.2461
Epoch 11 Step 251 Train Loss: 0.2636
Epoch 11 Step 301 Train Loss: 0.2543
Epoch 11 Step 351 Train Loss: 0.2641
Epoch 11 Step 401 Train Loss: 0.2516
Epoch 11 Step 451 Train Loss: 0.2413
Epoch 11 Step 501 Train Loss: 0.2619
Epoch 11 Step 551 Train Loss: 0.2657
Epoch 11 Step 601 Train Loss: 0.2588
Epoch 11 Step 651 Train Loss: 0.2439
Epoch 11 Step 701 Train Loss: 0.2606
Epoch 11 Step 751 Train Loss: 0.2625
Epoch 11 Step 801 Train Loss: 0.2599
Epoch 11 Step 851 Train Loss: 0.2571
Epoch 11 Step 901 Train Loss: 0.2518
Epoch 11 Step 951 Train Loss: 0.2737
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 12 Step 1 Train Loss: 0.2651
Epoch 12 Step 51 Train Loss: 0.2646
Epoch 12 Step 101 Train Loss: 0.2635
Epoch 12 Step 151 Train Loss: 0.2509
Epoch 12 Step 201 Train Loss: 0.2479
Epoch 12 Step 251 Train Loss: 0.2619
Epoch 12 Step 301 Train Loss: 0.2424
Epoch 12 Step 351 Train Loss: 0.2599
Epoch 12 Step 401 Train Loss: 0.2636
Epoch 12 Step 451 Train Loss: 0.2447
Epoch 12 Step 501 Train Loss: 0.2738
Epoch 12 Step 551 Train Loss: 0.2675
Epoch 12 Step 601 Train Loss: 0.2509
Epoch 12 Step 651 Train Loss: 0.2553
Epoch 12 Step 701 Train Loss: 0.2530
Epoch 12 Step 751 Train Loss: 0.2609
Epoch 12 Step 801 Train Loss: 0.2627
Epoch 12 Step 851 Train Loss: 0.2612
Epoch 12 Step 901 Train Loss: 0.2501
Epoch 12 Step 951 Train Loss: 0.2563
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 13 Step 1 Train Loss: 0.2616
Epoch 13 Step 51 Train Loss: 0.2525
Epoch 13 Step 101 Train Loss: 0.2682
Epoch 13 Step 151 Train Loss: 0.2380
Epoch 13 Step 201 Train Loss: 0.2565
Epoch 13 Step 251 Train Loss: 0.2520
Epoch 13 Step 301 Train Loss: 0.2693
Epoch 13 Step 351 Train Loss: 0.2515
Epoch 13 Step 401 Train Loss: 0.2547
Epoch 13 Step 451 Train Loss: 0.2513
Epoch 13 Step 501 Train Loss: 0.2643
Epoch 13 Step 551 Train Loss: 0.2541
Epoch 13 Step 601 Train Loss: 0.2691
Epoch 13 Step 651 Train Loss: 0.2613
Epoch 13 Step 701 Train Loss: 0.2687
Epoch 13 Step 751 Train Loss: 0.2378
Epoch 13 Step 801 Train Loss: 0.2603
Epoch 13 Step 851 Train Loss: 0.2767
Epoch 13 Step 901 Train Loss: 0.2623
Epoch 13 Step 951 Train Loss: 0.2603
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 14 Step 1 Train Loss: 0.2627
Epoch 14 Step 51 Train Loss: 0.2674
Epoch 14 Step 101 Train Loss: 0.2522
Epoch 14 Step 151 Train Loss: 0.2544
Epoch 14 Step 201 Train Loss: 0.2608
Epoch 14 Step 251 Train Loss: 0.2546
Epoch 14 Step 301 Train Loss: 0.2671
Epoch 14 Step 351 Train Loss: 0.2552
Epoch 14 Step 401 Train Loss: 0.2568
Epoch 14 Step 451 Train Loss: 0.2645
Epoch 14 Step 501 Train Loss: 0.2582
Epoch 14 Step 551 Train Loss: 0.2585
Epoch 14 Step 601 Train Loss: 0.2428
Epoch 14 Step 651 Train Loss: 0.2577
Epoch 14 Step 701 Train Loss: 0.2576
Epoch 14 Step 751 Train Loss: 0.2646
Epoch 14 Step 801 Train Loss: 0.2551
Epoch 14 Step 851 Train Loss: 0.2638
Epoch 14 Step 901 Train Loss: 0.2575
Epoch 14 Step 951 Train Loss: 0.2534
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 15 Step 1 Train Loss: 0.2607
Epoch 15 Step 51 Train Loss: 0.2578
Epoch 15 Step 101 Train Loss: 0.2450
Epoch 15 Step 151 Train Loss: 0.2511
Epoch 15 Step 201 Train Loss: 0.2547
Epoch 15 Step 251 Train Loss: 0.2561
Epoch 15 Step 301 Train Loss: 0.2444
Epoch 15 Step 351 Train Loss: 0.2634
Epoch 15 Step 401 Train Loss: 0.2473
Epoch 15 Step 451 Train Loss: 0.2484
Epoch 15 Step 501 Train Loss: 0.2502
Epoch 15 Step 551 Train Loss: 0.2746
Epoch 15 Step 601 Train Loss: 0.2512
Epoch 15 Step 651 Train Loss: 0.2537
Epoch 15 Step 701 Train Loss: 0.2450
Epoch 15 Step 751 Train Loss: 0.2408
Epoch 15 Step 801 Train Loss: 0.2513
Epoch 15 Step 851 Train Loss: 0.2653
Epoch 15 Step 901 Train Loss: 0.2580
Epoch 15 Step 951 Train Loss: 0.2584
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0033
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00034498167
test_unseen_single_pearson: 0.9925201930057
test_unseen_single_mse_de: 0.0033181757
test_unseen_single_pearson_de: 0.7501562142857824
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1431237435494637
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.36372549019607836
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9284313725490195
test_unseen_single_mse_top20_de_non_dropout: 0.007401959220161176
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.026 MB of 0.027 MB uploadedwandb: / 0.026 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:                                             train_de_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–‡â–ƒâ–â–â–„â–„â–„â–â–…â–†â–†â–„â–„â–ƒâ–„â–ƒâ–‚â–‚â–…â–„â–„â–†â–„â–…â–„â–†â–…â–ˆâ–‚â–â–ˆâ–…â–‚â–†â–â–„â–ƒâ–†â–‚â–‚
wandb:                                                   val_de_mse â–ˆâ–â–‚â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:                                               val_de_pearson â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00332
wandb:                                              test_de_pearson 0.75016
wandb:               test_frac_opposite_direction_top20_non_dropout 0.36373
wandb:                          test_frac_sigma_below_1_non_dropout 0.92843
wandb:                                                     test_mse 0.00034
wandb:                                test_mse_top20_de_non_dropout 0.0074
wandb:                                                 test_pearson 0.99252
wandb:                                           test_pearson_delta 0.14312
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.36373
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.92843
wandb:                                       test_unseen_single_mse 0.00034
wandb:                                    test_unseen_single_mse_de 0.00332
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0074
wandb:                                   test_unseen_single_pearson 0.99252
wandb:                                test_unseen_single_pearson_de 0.75016
wandb:                             test_unseen_single_pearson_delta 0.14312
wandb:                                                 train_de_mse 0.0062
wandb:                                             train_de_pearson 0.80179
wandb:                                                    train_mse 0.00035
wandb:                                                train_pearson 0.99247
wandb:                                                training_loss 0.23869
wandb:                                                   val_de_mse 0.00526
wandb:                                               val_de_pearson 0.80675
wandb:                                                      val_mse 0.0004
wandb:                                                  val_pearson 0.99149
wandb: 
wandb: ðŸš€ View run XuCao2023_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/0b0xpqx5
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_060236-0b0xpqx5/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/xucao2023/splits/xucao2023_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_070151-c1ej4ys5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/c1ej4ys5
Start Training...
Epoch 1 Step 1 Train Loss: 0.3722
Epoch 1 Step 51 Train Loss: 0.2913
Epoch 1 Step 101 Train Loss: 0.2647
Epoch 1 Step 151 Train Loss: 0.2601
Epoch 1 Step 201 Train Loss: 0.2562
Epoch 1 Step 251 Train Loss: 0.2597
Epoch 1 Step 301 Train Loss: 0.2876
Epoch 1 Step 351 Train Loss: 0.2771
Epoch 1 Step 401 Train Loss: 0.2557
Epoch 1 Step 451 Train Loss: 0.2591
Epoch 1 Step 501 Train Loss: 0.2727
Epoch 1 Step 551 Train Loss: 0.2634
Epoch 1 Step 601 Train Loss: 0.2516
Epoch 1 Step 651 Train Loss: 0.2663
Epoch 1 Step 701 Train Loss: 0.2590
Epoch 1 Step 751 Train Loss: 0.2727
Epoch 1 Step 801 Train Loss: 0.2374
Epoch 1 Step 851 Train Loss: 0.2398
Epoch 1 Step 901 Train Loss: 0.2619
Epoch 1: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0065. 
Epoch 2 Step 1 Train Loss: 0.2571
Epoch 2 Step 51 Train Loss: 0.2776
Epoch 2 Step 101 Train Loss: 0.2550
Epoch 2 Step 151 Train Loss: 0.2468
Epoch 2 Step 201 Train Loss: 0.2535
Epoch 2 Step 251 Train Loss: 0.2548
Epoch 2 Step 301 Train Loss: 0.2657
Epoch 2 Step 351 Train Loss: 0.2594
Epoch 2 Step 401 Train Loss: 0.2529
Epoch 2 Step 451 Train Loss: 0.2335
Epoch 2 Step 501 Train Loss: 0.2423
Epoch 2 Step 551 Train Loss: 0.2631
Epoch 2 Step 601 Train Loss: 0.2477
Epoch 2 Step 651 Train Loss: 0.2656
Epoch 2 Step 701 Train Loss: 0.2391
Epoch 2 Step 751 Train Loss: 0.2631
Epoch 2 Step 801 Train Loss: 0.2426
Epoch 2 Step 851 Train Loss: 0.2517
Epoch 2 Step 901 Train Loss: 0.2609
Epoch 2: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0066. 
Epoch 3 Step 1 Train Loss: 0.2642
Epoch 3 Step 51 Train Loss: 0.2727
Epoch 3 Step 101 Train Loss: 0.2626
Epoch 3 Step 151 Train Loss: 0.2582
Epoch 3 Step 201 Train Loss: 0.2560
Epoch 3 Step 251 Train Loss: 0.2579
Epoch 3 Step 301 Train Loss: 0.2579
Epoch 3 Step 351 Train Loss: 0.2562
Epoch 3 Step 401 Train Loss: 0.2762
Epoch 3 Step 451 Train Loss: 0.2666
Epoch 3 Step 501 Train Loss: 0.2606
Epoch 3 Step 551 Train Loss: 0.2470
Epoch 3 Step 601 Train Loss: 0.2630
Epoch 3 Step 651 Train Loss: 0.2755
Epoch 3 Step 701 Train Loss: 0.2654
Epoch 3 Step 751 Train Loss: 0.2688
Epoch 3 Step 801 Train Loss: 0.2614
Epoch 3 Step 851 Train Loss: 0.2481
Epoch 3 Step 901 Train Loss: 0.2602
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 4 Step 1 Train Loss: 0.2837
Epoch 4 Step 51 Train Loss: 0.2599
Epoch 4 Step 101 Train Loss: 0.2561
Epoch 4 Step 151 Train Loss: 0.2559
Epoch 4 Step 201 Train Loss: 0.2702
Epoch 4 Step 251 Train Loss: 0.2579
Epoch 4 Step 301 Train Loss: 0.2628
Epoch 4 Step 351 Train Loss: 0.2723
Epoch 4 Step 401 Train Loss: 0.2744
Epoch 4 Step 451 Train Loss: 0.2552
Epoch 4 Step 501 Train Loss: 0.2711
Epoch 4 Step 551 Train Loss: 0.2509
Epoch 4 Step 601 Train Loss: 0.2515
Epoch 4 Step 651 Train Loss: 0.2529
Epoch 4 Step 701 Train Loss: 0.2708
Epoch 4 Step 751 Train Loss: 0.2655
Epoch 4 Step 801 Train Loss: 0.2672
Epoch 4 Step 851 Train Loss: 0.2634
Epoch 4 Step 901 Train Loss: 0.2764
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 5 Step 1 Train Loss: 0.2599
Epoch 5 Step 51 Train Loss: 0.2574
Epoch 5 Step 101 Train Loss: 0.2526
Epoch 5 Step 151 Train Loss: 0.2719
Epoch 5 Step 201 Train Loss: 0.2792
Epoch 5 Step 251 Train Loss: 0.2576
Epoch 5 Step 301 Train Loss: 0.2416
Epoch 5 Step 351 Train Loss: 0.2558
Epoch 5 Step 401 Train Loss: 0.2610
Epoch 5 Step 451 Train Loss: 0.2760
Epoch 5 Step 501 Train Loss: 0.2494
Epoch 5 Step 551 Train Loss: 0.2561
Epoch 5 Step 601 Train Loss: 0.2549
Epoch 5 Step 651 Train Loss: 0.2705
Epoch 5 Step 701 Train Loss: 0.2598
Epoch 5 Step 751 Train Loss: 0.2583
Epoch 5 Step 801 Train Loss: 0.2593
Epoch 5 Step 851 Train Loss: 0.2541
Epoch 5 Step 901 Train Loss: 0.2582
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 6 Step 1 Train Loss: 0.2702
Epoch 6 Step 51 Train Loss: 0.2658
Epoch 6 Step 101 Train Loss: 0.2594
Epoch 6 Step 151 Train Loss: 0.2650
Epoch 6 Step 201 Train Loss: 0.2577
Epoch 6 Step 251 Train Loss: 0.2472
Epoch 6 Step 301 Train Loss: 0.2495
Epoch 6 Step 351 Train Loss: 0.2604
Epoch 6 Step 401 Train Loss: 0.2533
Epoch 6 Step 451 Train Loss: 0.2508
Epoch 6 Step 501 Train Loss: 0.2602
Epoch 6 Step 551 Train Loss: 0.2622
Epoch 6 Step 601 Train Loss: 0.2641
Epoch 6 Step 651 Train Loss: 0.2548
Epoch 6 Step 701 Train Loss: 0.2571
Epoch 6 Step 751 Train Loss: 0.2753
Epoch 6 Step 801 Train Loss: 0.2605
Epoch 6 Step 851 Train Loss: 0.2512
Epoch 6 Step 901 Train Loss: 0.2626
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 7 Step 1 Train Loss: 0.2491
Epoch 7 Step 51 Train Loss: 0.2920
Epoch 7 Step 101 Train Loss: 0.2639
Epoch 7 Step 151 Train Loss: 0.2547
Epoch 7 Step 201 Train Loss: 0.2575
Epoch 7 Step 251 Train Loss: 0.2662
Epoch 7 Step 301 Train Loss: 0.2582
Epoch 7 Step 351 Train Loss: 0.2632
Epoch 7 Step 401 Train Loss: 0.2790
Epoch 7 Step 451 Train Loss: 0.2598
Epoch 7 Step 501 Train Loss: 0.2568
Epoch 7 Step 551 Train Loss: 0.2473
Epoch 7 Step 601 Train Loss: 0.2671
Epoch 7 Step 651 Train Loss: 0.2686
Epoch 7 Step 701 Train Loss: 0.2510
Epoch 7 Step 751 Train Loss: 0.2493
Epoch 7 Step 801 Train Loss: 0.2769
Epoch 7 Step 851 Train Loss: 0.2632
Epoch 7 Step 901 Train Loss: 0.2673
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 8 Step 1 Train Loss: 0.2714
Epoch 8 Step 51 Train Loss: 0.2508
Epoch 8 Step 101 Train Loss: 0.2595
Epoch 8 Step 151 Train Loss: 0.2450
Epoch 8 Step 201 Train Loss: 0.2609
Epoch 8 Step 251 Train Loss: 0.2707
Epoch 8 Step 301 Train Loss: 0.2740
Epoch 8 Step 351 Train Loss: 0.2477
Epoch 8 Step 401 Train Loss: 0.2581
Epoch 8 Step 451 Train Loss: 0.2679
Epoch 8 Step 501 Train Loss: 0.2712
Epoch 8 Step 551 Train Loss: 0.2681
Epoch 8 Step 601 Train Loss: 0.2511
Epoch 8 Step 651 Train Loss: 0.2639
Epoch 8 Step 701 Train Loss: 0.2601
Epoch 8 Step 751 Train Loss: 0.2707
Epoch 8 Step 801 Train Loss: 0.2664
Epoch 8 Step 851 Train Loss: 0.2570
Epoch 8 Step 901 Train Loss: 0.2593
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 9 Step 1 Train Loss: 0.2765
Epoch 9 Step 51 Train Loss: 0.2602
Epoch 9 Step 101 Train Loss: 0.2616
Epoch 9 Step 151 Train Loss: 0.2510
Epoch 9 Step 201 Train Loss: 0.2801
Epoch 9 Step 251 Train Loss: 0.2619
Epoch 9 Step 301 Train Loss: 0.2603
Epoch 9 Step 351 Train Loss: 0.2542
Epoch 9 Step 401 Train Loss: 0.2821
Epoch 9 Step 451 Train Loss: 0.2731
Epoch 9 Step 501 Train Loss: 0.2540
Epoch 9 Step 551 Train Loss: 0.2665
Epoch 9 Step 601 Train Loss: 0.2637
Epoch 9 Step 651 Train Loss: 0.2596
Epoch 9 Step 701 Train Loss: 0.2559
Epoch 9 Step 751 Train Loss: 0.2450
Epoch 9 Step 801 Train Loss: 0.2610
Epoch 9 Step 851 Train Loss: 0.2519
Epoch 9 Step 901 Train Loss: 0.2552
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 10 Step 1 Train Loss: 0.2625
Epoch 10 Step 51 Train Loss: 0.2652
Epoch 10 Step 101 Train Loss: 0.2529
Epoch 10 Step 151 Train Loss: 0.2583
Epoch 10 Step 201 Train Loss: 0.2574
Epoch 10 Step 251 Train Loss: 0.2602
Epoch 10 Step 301 Train Loss: 0.2539
Epoch 10 Step 351 Train Loss: 0.2607
Epoch 10 Step 401 Train Loss: 0.2681
Epoch 10 Step 451 Train Loss: 0.2648
Epoch 10 Step 501 Train Loss: 0.2633
Epoch 10 Step 551 Train Loss: 0.2738
Epoch 10 Step 601 Train Loss: 0.2532
Epoch 10 Step 651 Train Loss: 0.2668
Epoch 10 Step 701 Train Loss: 0.2619
Epoch 10 Step 751 Train Loss: 0.2656
Epoch 10 Step 801 Train Loss: 0.2709
Epoch 10 Step 851 Train Loss: 0.2489
Epoch 10 Step 901 Train Loss: 0.2580
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 11 Step 1 Train Loss: 0.2507
Epoch 11 Step 51 Train Loss: 0.2686
Epoch 11 Step 101 Train Loss: 0.2434
Epoch 11 Step 151 Train Loss: 0.2603
Epoch 11 Step 201 Train Loss: 0.2806
Epoch 11 Step 251 Train Loss: 0.2592
Epoch 11 Step 301 Train Loss: 0.2522
Epoch 11 Step 351 Train Loss: 0.2633
Epoch 11 Step 401 Train Loss: 0.2636
Epoch 11 Step 451 Train Loss: 0.2572
Epoch 11 Step 501 Train Loss: 0.2595
Epoch 11 Step 551 Train Loss: 0.2741
Epoch 11 Step 601 Train Loss: 0.2499
Epoch 11 Step 651 Train Loss: 0.2597
Epoch 11 Step 701 Train Loss: 0.2718
Epoch 11 Step 751 Train Loss: 0.2415
Epoch 11 Step 801 Train Loss: 0.2519
Epoch 11 Step 851 Train Loss: 0.2670
Epoch 11 Step 901 Train Loss: 0.2404
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 12 Step 1 Train Loss: 0.2672
Epoch 12 Step 51 Train Loss: 0.2662
Epoch 12 Step 101 Train Loss: 0.2691
Epoch 12 Step 151 Train Loss: 0.2496
Epoch 12 Step 201 Train Loss: 0.2737
Epoch 12 Step 251 Train Loss: 0.2540
Epoch 12 Step 301 Train Loss: 0.2549
Epoch 12 Step 351 Train Loss: 0.2551
Epoch 12 Step 401 Train Loss: 0.2577
Epoch 12 Step 451 Train Loss: 0.2635
Epoch 12 Step 501 Train Loss: 0.2609
Epoch 12 Step 551 Train Loss: 0.2619
Epoch 12 Step 601 Train Loss: 0.2542
Epoch 12 Step 651 Train Loss: 0.2625
Epoch 12 Step 701 Train Loss: 0.2547
Epoch 12 Step 751 Train Loss: 0.2778
Epoch 12 Step 801 Train Loss: 0.2573
Epoch 12 Step 851 Train Loss: 0.2707
Epoch 12 Step 901 Train Loss: 0.2553
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 13 Step 1 Train Loss: 0.2376
Epoch 13 Step 51 Train Loss: 0.2784
Epoch 13 Step 101 Train Loss: 0.2830
Epoch 13 Step 151 Train Loss: 0.2549
Epoch 13 Step 201 Train Loss: 0.2556
Epoch 13 Step 251 Train Loss: 0.2704
Epoch 13 Step 301 Train Loss: 0.2741
Epoch 13 Step 351 Train Loss: 0.2662
Epoch 13 Step 401 Train Loss: 0.2707
Epoch 13 Step 451 Train Loss: 0.2762
Epoch 13 Step 501 Train Loss: 0.2585
Epoch 13 Step 551 Train Loss: 0.2673
Epoch 13 Step 601 Train Loss: 0.2641
Epoch 13 Step 651 Train Loss: 0.2658
Epoch 13 Step 701 Train Loss: 0.2747
Epoch 13 Step 751 Train Loss: 0.2581
Epoch 13 Step 801 Train Loss: 0.2646
Epoch 13 Step 851 Train Loss: 0.2453
Epoch 13 Step 901 Train Loss: 0.2615
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 14 Step 1 Train Loss: 0.2606
Epoch 14 Step 51 Train Loss: 0.2462
Epoch 14 Step 101 Train Loss: 0.2602
Epoch 14 Step 151 Train Loss: 0.2424
Epoch 14 Step 201 Train Loss: 0.2711
Epoch 14 Step 251 Train Loss: 0.2560
Epoch 14 Step 301 Train Loss: 0.2629
Epoch 14 Step 351 Train Loss: 0.2656
Epoch 14 Step 401 Train Loss: 0.2667
Epoch 14 Step 451 Train Loss: 0.2502
Epoch 14 Step 501 Train Loss: 0.2541
Epoch 14 Step 551 Train Loss: 0.2508
Epoch 14 Step 601 Train Loss: 0.2726
Epoch 14 Step 651 Train Loss: 0.2751
Epoch 14 Step 701 Train Loss: 0.2537
Epoch 14 Step 751 Train Loss: 0.2676
Epoch 14 Step 801 Train Loss: 0.2618
Epoch 14 Step 851 Train Loss: 0.2604
Epoch 14 Step 901 Train Loss: 0.2463
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 15 Step 1 Train Loss: 0.2468
Epoch 15 Step 51 Train Loss: 0.2589
Epoch 15 Step 101 Train Loss: 0.2605
Epoch 15 Step 151 Train Loss: 0.2626
Epoch 15 Step 201 Train Loss: 0.2579
Epoch 15 Step 251 Train Loss: 0.2574
Epoch 15 Step 301 Train Loss: 0.2474
Epoch 15 Step 351 Train Loss: 0.2566
Epoch 15 Step 401 Train Loss: 0.2527
Epoch 15 Step 451 Train Loss: 0.2688
Epoch 15 Step 501 Train Loss: 0.2512
Epoch 15 Step 551 Train Loss: 0.2631
Epoch 15 Step 601 Train Loss: 0.2682
Epoch 15 Step 651 Train Loss: 0.2545
Epoch 15 Step 701 Train Loss: 0.2667
Epoch 15 Step 751 Train Loss: 0.2575
Epoch 15 Step 801 Train Loss: 0.2609
Epoch 15 Step 851 Train Loss: 0.2637
Epoch 15 Step 901 Train Loss: 0.2679
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0040
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0002989883
test_unseen_single_pearson: 0.9935623244782219
test_unseen_single_mse_de: 0.0039726086
test_unseen_single_pearson_de: 0.8061704589578904
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.16456608154923022
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3686274509803922
test_unseen_single_frac_sigma_below_1_non_dropout: 0.930392156862745
test_unseen_single_mse_top20_de_non_dropout: 0.0066831553453148
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.017 MB of 0.027 MB uploadedwandb: / 0.017 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–†â–‡â–†â–…â–…â–†â–ƒâ–„â–â–†â–ƒâ–„â–ƒâ–…â–…â–ƒâ–ƒâ–„â–„â–â–†â–‡â–…â–‡â–…â–ƒâ–ˆâ–ˆâ–ƒâ–„â–ˆâ–„â–„â–…â–‚â–â–‡â–†â–ƒâ–‚
wandb:                                                   val_de_mse â–„â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00397
wandb:                                              test_de_pearson 0.80617
wandb:               test_frac_opposite_direction_top20_non_dropout 0.36863
wandb:                          test_frac_sigma_below_1_non_dropout 0.93039
wandb:                                                     test_mse 0.0003
wandb:                                test_mse_top20_de_non_dropout 0.00668
wandb:                                                 test_pearson 0.99356
wandb:                                           test_pearson_delta 0.16457
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.36863
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93039
wandb:                                       test_unseen_single_mse 0.0003
wandb:                                    test_unseen_single_mse_de 0.00397
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00668
wandb:                                   test_unseen_single_pearson 0.99356
wandb:                                test_unseen_single_pearson_de 0.80617
wandb:                             test_unseen_single_pearson_delta 0.16457
wandb:                                                 train_de_mse 0.00575
wandb:                                             train_de_pearson 0.77789
wandb:                                                    train_mse 0.00038
wandb:                                                train_pearson 0.99188
wandb:                                                training_loss 0.27512
wandb:                                                   val_de_mse 0.00647
wandb:                                               val_de_pearson 0.82894
wandb:                                                      val_mse 0.00029
wandb:                                                  val_pearson 0.99371
wandb: 
wandb: ðŸš€ View run XuCao2023_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/c1ej4ys5
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_070151-c1ej4ys5/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/21 [00:00<?, ?it/s]  5%|â–         | 1/21 [00:04<01:28,  4.40s/it] 10%|â–‰         | 2/21 [00:09<01:26,  4.56s/it] 14%|â–ˆâ–        | 3/21 [00:16<01:41,  5.65s/it] 19%|â–ˆâ–‰        | 4/21 [00:18<01:16,  4.47s/it] 24%|â–ˆâ–ˆâ–       | 5/21 [00:25<01:24,  5.28s/it] 29%|â–ˆâ–ˆâ–Š       | 6/21 [00:30<01:19,  5.32s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:34<01:06,  4.77s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:44<01:25,  6.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:49<01:11,  5.99s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:55<01:06,  6.03s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [01:00<00:55,  5.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [01:06<00:52,  5.81s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [01:10<00:42,  5.27s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [01:16<00:38,  5.50s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [01:20<00:29,  4.89s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [01:24<00:23,  4.77s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [01:31<00:21,  5.47s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [01:35<00:14,  4.98s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [01:38<00:08,  4.45s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [01:42<00:04,  4.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [01:46<00:00,  4.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [01:46<00:00,  5.05s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/shifrutmarson2018/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/shifrutmarson2018/splits/shifrutmarson2018_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_075945-u7eejd48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u7eejd48
  0%|          | 0/3286 [00:00<?, ?it/s]  0%|          | 7/3286 [00:00<00:49, 66.13it/s]  0%|          | 15/3286 [00:00<00:44, 74.09it/s]  1%|          | 24/3286 [00:00<00:43, 75.42it/s]  1%|          | 37/3286 [00:00<00:34, 93.82it/s]  1%|â–         | 49/3286 [00:00<00:32, 100.54it/s]  2%|â–         | 60/3286 [00:00<00:31, 101.65it/s]  2%|â–         | 71/3286 [00:00<00:31, 101.54it/s]  2%|â–         | 82/3286 [00:00<00:31, 100.66it/s]  3%|â–Ž         | 93/3286 [00:00<00:31, 100.65it/s]  3%|â–Ž         | 104/3286 [00:01<00:32, 97.70it/s]  4%|â–Ž         | 116/3286 [00:01<00:32, 99.03it/s]  4%|â–         | 127/3286 [00:01<00:31, 99.41it/s]  4%|â–         | 138/3286 [00:01<00:30, 101.98it/s]  5%|â–         | 149/3286 [00:01<00:30, 101.43it/s]  5%|â–         | 160/3286 [00:01<00:31, 98.40it/s]   5%|â–Œ         | 170/3286 [00:01<00:31, 98.78it/s]  6%|â–Œ         | 181/3286 [00:01<00:31, 98.95it/s]  6%|â–Œ         | 192/3286 [00:01<00:30, 102.05it/s]  6%|â–Œ         | 203/3286 [00:02<00:30, 101.66it/s]  7%|â–‹         | 214/3286 [00:02<00:30, 100.58it/s]  7%|â–‹         | 225/3286 [00:02<00:30, 100.51it/s]  7%|â–‹         | 236/3286 [00:02<00:31, 97.77it/s]   8%|â–Š         | 247/3286 [00:02<00:30, 100.79it/s]  8%|â–Š         | 258/3286 [00:02<00:29, 101.20it/s]  8%|â–Š         | 269/3286 [00:02<00:29, 101.52it/s]  9%|â–Š         | 280/3286 [00:02<00:29, 100.69it/s]  9%|â–‰         | 291/3286 [00:02<00:30, 98.07it/s]   9%|â–‰         | 302/3286 [00:03<00:30, 99.30it/s] 10%|â–‰         | 313/3286 [00:03<00:29, 102.24it/s] 10%|â–‰         | 324/3286 [00:03<00:29, 101.81it/s] 10%|â–ˆ         | 335/3286 [00:03<00:29, 101.39it/s] 11%|â–ˆ         | 346/3286 [00:03<00:29, 98.84it/s]  11%|â–ˆ         | 358/3286 [00:03<00:28, 102.62it/s] 11%|â–ˆ         | 369/3286 [00:03<00:28, 102.22it/s] 12%|â–ˆâ–        | 380/3286 [00:03<00:28, 101.39it/s] 12%|â–ˆâ–        | 391/3286 [00:03<00:28, 101.15it/s] 12%|â–ˆâ–        | 402/3286 [00:04<00:28, 101.25it/s] 13%|â–ˆâ–Ž        | 413/3286 [00:04<00:28, 100.07it/s] 13%|â–ˆâ–Ž        | 424/3286 [00:04<00:29, 96.63it/s]  13%|â–ˆâ–Ž        | 434/3286 [00:04<00:29, 97.29it/s] 14%|â–ˆâ–Ž        | 445/3286 [00:04<00:28, 99.94it/s] 14%|â–ˆâ–        | 456/3286 [00:04<00:28, 98.99it/s] 14%|â–ˆâ–        | 466/3286 [00:04<00:28, 98.37it/s] 14%|â–ˆâ–        | 476/3286 [00:04<00:28, 97.92it/s] 15%|â–ˆâ–        | 486/3286 [00:04<00:28, 98.25it/s] 15%|â–ˆâ–Œ        | 496/3286 [00:05<00:28, 98.16it/s] 15%|â–ˆâ–Œ        | 506/3286 [00:05<00:28, 97.74it/s] 16%|â–ˆâ–Œ        | 516/3286 [00:05<00:28, 97.51it/s] 16%|â–ˆâ–Œ        | 526/3286 [00:05<00:28, 97.67it/s] 16%|â–ˆâ–‹        | 536/3286 [00:05<00:29, 94.54it/s] 17%|â–ˆâ–‹        | 547/3286 [00:05<00:28, 95.38it/s] 17%|â–ˆâ–‹        | 558/3286 [00:05<00:27, 99.01it/s] 17%|â–ˆâ–‹        | 568/3286 [00:05<00:27, 98.60it/s] 18%|â–ˆâ–Š        | 578/3286 [00:05<00:28, 96.32it/s] 18%|â–ˆâ–Š        | 588/3286 [00:05<00:28, 96.27it/s] 18%|â–ˆâ–Š        | 598/3286 [00:06<00:28, 92.95it/s] 19%|â–ˆâ–Š        | 608/3286 [00:06<00:28, 93.88it/s] 19%|â–ˆâ–‰        | 618/3286 [00:06<00:28, 92.33it/s] 19%|â–ˆâ–‰        | 628/3286 [00:06<00:28, 93.39it/s] 19%|â–ˆâ–‰        | 639/3286 [00:06<00:27, 96.31it/s] 20%|â–ˆâ–‰        | 649/3286 [00:06<00:27, 94.94it/s] 20%|â–ˆâ–ˆ        | 659/3286 [00:06<00:29, 88.66it/s] 20%|â–ˆâ–ˆ        | 670/3286 [00:06<00:27, 93.57it/s] 21%|â–ˆâ–ˆ        | 680/3286 [00:06<00:28, 92.22it/s] 21%|â–ˆâ–ˆ        | 690/3286 [00:07<00:28, 91.06it/s] 21%|â–ˆâ–ˆâ–       | 701/3286 [00:07<00:28, 91.46it/s] 22%|â–ˆâ–ˆâ–       | 711/3286 [00:07<00:27, 92.28it/s] 22%|â–ˆâ–ˆâ–       | 721/3286 [00:07<00:27, 93.29it/s] 22%|â–ˆâ–ˆâ–       | 732/3286 [00:07<00:26, 95.82it/s] 23%|â–ˆâ–ˆâ–Ž       | 742/3286 [00:07<00:26, 96.79it/s] 23%|â–ˆâ–ˆâ–Ž       | 752/3286 [00:07<00:27, 92.09it/s] 23%|â–ˆâ–ˆâ–Ž       | 762/3286 [00:07<00:26, 93.91it/s] 24%|â–ˆâ–ˆâ–Ž       | 773/3286 [00:07<00:26, 93.28it/s] 24%|â–ˆâ–ˆâ–       | 784/3286 [00:08<00:26, 95.93it/s] 24%|â–ˆâ–ˆâ–       | 794/3286 [00:08<00:25, 96.48it/s] 24%|â–ˆâ–ˆâ–       | 804/3286 [00:08<00:25, 95.80it/s] 25%|â–ˆâ–ˆâ–       | 814/3286 [00:08<00:26, 91.90it/s] 25%|â–ˆâ–ˆâ–Œ       | 824/3286 [00:08<00:27, 88.54it/s] 25%|â–ˆâ–ˆâ–Œ       | 834/3286 [00:08<00:27, 90.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 844/3286 [00:08<00:27, 89.68it/s] 26%|â–ˆâ–ˆâ–Œ       | 854/3286 [00:08<00:27, 88.52it/s] 26%|â–ˆâ–ˆâ–‹       | 863/3286 [00:08<00:27, 87.88it/s] 27%|â–ˆâ–ˆâ–‹       | 873/3286 [00:09<00:27, 86.44it/s] 27%|â–ˆâ–ˆâ–‹       | 883/3286 [00:09<00:26, 89.25it/s] 27%|â–ˆâ–ˆâ–‹       | 892/3286 [00:09<00:26, 89.12it/s] 27%|â–ˆâ–ˆâ–‹       | 902/3286 [00:09<00:26, 89.58it/s] 28%|â–ˆâ–ˆâ–Š       | 911/3286 [00:09<00:26, 88.47it/s] 28%|â–ˆâ–ˆâ–Š       | 920/3286 [00:09<00:27, 85.94it/s] 28%|â–ˆâ–ˆâ–Š       | 930/3286 [00:09<00:26, 89.57it/s] 29%|â–ˆâ–ˆâ–Š       | 939/3286 [00:09<00:26, 88.20it/s] 29%|â–ˆâ–ˆâ–‰       | 948/3286 [00:09<00:27, 86.48it/s] 29%|â–ˆâ–ˆâ–‰       | 957/3286 [00:10<00:26, 86.32it/s] 29%|â–ˆâ–ˆâ–‰       | 967/3286 [00:10<00:26, 86.77it/s] 30%|â–ˆâ–ˆâ–‰       | 976/3286 [00:10<00:26, 87.47it/s] 30%|â–ˆâ–ˆâ–‰       | 985/3286 [00:10<00:26, 86.46it/s] 30%|â–ˆâ–ˆâ–ˆ       | 995/3286 [00:10<00:25, 89.93it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1005/3286 [00:10<00:25, 89.31it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1014/3286 [00:10<00:25, 88.50it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1023/3286 [00:10<00:26, 86.89it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1032/3286 [00:10<00:26, 84.01it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1042/3286 [00:10<00:25, 88.04it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1051/3286 [00:11<00:25, 88.34it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1061/3286 [00:11<00:24, 89.31it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1070/3286 [00:11<00:25, 88.22it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1079/3286 [00:11<00:25, 88.10it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1088/3286 [00:11<00:24, 87.96it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1097/3286 [00:11<00:25, 86.23it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1107/3286 [00:11<00:24, 89.96it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1117/3286 [00:11<00:24, 89.04it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1126/3286 [00:11<00:24, 87.94it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1135/3286 [00:12<00:24, 88.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1144/3286 [00:12<00:24, 88.61it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1153/3286 [00:12<00:24, 88.73it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1162/3286 [00:12<00:24, 88.21it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1172/3286 [00:12<00:24, 87.37it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1182/3286 [00:12<00:23, 90.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1192/3286 [00:12<00:23, 89.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1202/3286 [00:12<00:23, 89.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1211/3286 [00:12<00:23, 89.19it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1220/3286 [00:12<00:23, 89.37it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1229/3286 [00:13<00:23, 89.05it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1238/3286 [00:13<00:22, 89.24it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1248/3286 [00:13<00:24, 84.77it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1258/3286 [00:13<00:22, 88.79it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1268/3286 [00:13<00:21, 91.89it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1278/3286 [00:13<00:21, 91.76it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1288/3286 [00:13<00:21, 91.05it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1298/3286 [00:13<00:22, 88.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1307/3286 [00:13<00:22, 88.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1318/3286 [00:14<00:21, 92.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1328/3286 [00:14<00:21, 92.05it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1338/3286 [00:14<00:21, 91.14it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1348/3286 [00:14<00:21, 91.49it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1358/3286 [00:14<00:21, 90.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1368/3286 [00:14<00:21, 89.98it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1378/3286 [00:14<00:21, 90.55it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1388/3286 [00:14<00:21, 90.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1398/3286 [00:14<00:20, 90.70it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1408/3286 [00:15<00:20, 91.27it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1418/3286 [00:15<00:20, 90.97it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1428/3286 [00:15<00:20, 91.50it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1438/3286 [00:15<00:20, 90.91it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1448/3286 [00:15<00:20, 91.11it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1458/3286 [00:15<00:19, 92.29it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1468/3286 [00:15<00:19, 91.36it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1478/3286 [00:15<00:19, 92.48it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1488/3286 [00:15<00:19, 91.85it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1498/3286 [00:16<00:19, 92.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1508/3286 [00:16<00:19, 91.79it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1518/3286 [00:16<00:19, 90.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1528/3286 [00:16<00:19, 90.97it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1538/3286 [00:16<00:19, 90.51it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1548/3286 [00:16<00:19, 91.04it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1558/3286 [00:16<00:18, 91.41it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1568/3286 [00:16<00:19, 90.32it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1578/3286 [00:16<00:18, 91.05it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1588/3286 [00:17<00:18, 90.88it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1598/3286 [00:17<00:18, 89.14it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1609/3286 [00:17<00:18, 92.37it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1619/3286 [00:17<00:18, 91.34it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1629/3286 [00:17<00:18, 91.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1639/3286 [00:17<00:18, 88.83it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1650/3286 [00:17<00:18, 90.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1660/3286 [00:17<00:17, 92.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1670/3286 [00:17<00:17, 92.04it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1680/3286 [00:18<00:17, 92.05it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1690/3286 [00:18<00:17, 91.93it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1700/3286 [00:18<00:17, 89.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1710/3286 [00:18<00:17, 89.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1721/3286 [00:18<00:16, 92.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1731/3286 [00:18<00:17, 90.23it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1741/3286 [00:18<00:16, 92.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1751/3286 [00:18<00:16, 92.52it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1761/3286 [00:18<00:16, 92.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1771/3286 [00:19<00:16, 92.04it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1781/3286 [00:19<00:16, 92.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1791/3286 [00:19<00:16, 92.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1801/3286 [00:19<00:16, 92.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1811/3286 [00:19<00:15, 92.40it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1821/3286 [00:19<00:16, 91.48it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1831/3286 [00:19<00:15, 91.57it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1841/3286 [00:19<00:15, 91.41it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1851/3286 [00:19<00:15, 92.32it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1861/3286 [00:20<00:15, 92.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1871/3286 [00:20<00:15, 91.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1881/3286 [00:20<00:15, 91.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1891/3286 [00:20<00:15, 90.79it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1901/3286 [00:20<00:15, 91.70it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1911/3286 [00:20<00:14, 92.03it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1921/3286 [00:20<00:15, 89.52it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1932/3286 [00:20<00:14, 93.34it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1942/3286 [00:20<00:14, 92.37it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1952/3286 [00:21<00:14, 92.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1962/3286 [00:21<00:14, 89.35it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1973/3286 [00:21<00:14, 92.68it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1983/3286 [00:21<00:14, 91.84it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1993/3286 [00:21<00:14, 92.33it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2003/3286 [00:21<00:13, 92.50it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2013/3286 [00:21<00:13, 92.56it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2023/3286 [00:21<00:13, 92.05it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2033/3286 [00:21<00:13, 91.30it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2043/3286 [00:22<00:13, 92.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2053/3286 [00:22<00:13, 92.30it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2063/3286 [00:22<00:13, 92.72it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2073/3286 [00:22<00:13, 91.90it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2083/3286 [00:22<00:13, 91.77it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2093/3286 [00:22<00:13, 91.51it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2103/3286 [00:22<00:12, 92.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2113/3286 [00:22<00:12, 91.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2123/3286 [00:22<00:12, 91.10it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2133/3286 [00:22<00:12, 91.95it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2143/3286 [00:23<00:12, 91.91it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2153/3286 [00:23<00:12, 92.79it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2163/3286 [00:23<00:12, 89.49it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2172/3286 [00:23<00:12, 88.12it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2181/3286 [00:23<00:12, 85.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2190/3286 [00:23<00:12, 84.50it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2199/3286 [00:23<00:13, 83.23it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2208/3286 [00:23<00:13, 81.54it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2217/3286 [00:23<00:13, 82.15it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2226/3286 [00:24<00:12, 82.57it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2235/3286 [00:24<00:12, 82.43it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2246/3286 [00:24<00:11, 87.37it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2255/3286 [00:24<00:11, 86.43it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2264/3286 [00:24<00:11, 86.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2274/3286 [00:24<00:11, 87.82it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2285/3286 [00:24<00:10, 92.33it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2295/3286 [00:24<00:10, 92.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2308/3286 [00:24<00:09, 98.70it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2320/3286 [00:25<00:09, 99.62it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2331/3286 [00:25<00:09, 100.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2342/3286 [00:25<00:09, 101.04it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2354/3286 [00:25<00:09, 101.61it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2366/3286 [00:25<00:08, 105.37it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2377/3286 [00:25<00:08, 101.99it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2389/3286 [00:25<00:08, 105.27it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2400/3286 [00:25<00:08, 101.66it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2411/3286 [00:25<00:08, 102.51it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2423/3286 [00:26<00:08, 102.13it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2434/3286 [00:26<00:08, 102.77it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2445/3286 [00:26<00:08, 101.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2456/3286 [00:26<00:08, 103.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2467/3286 [00:26<00:07, 103.09it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2478/3286 [00:26<00:08, 100.26it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2489/3286 [00:26<00:10, 78.95it/s]  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2500/3286 [00:26<00:09, 81.15it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2509/3286 [00:27<00:09, 79.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2518/3286 [00:27<00:09, 78.23it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2527/3286 [00:27<00:09, 76.70it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2535/3286 [00:27<00:09, 75.25it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2543/3286 [00:27<00:10, 74.03it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2551/3286 [00:27<00:09, 73.92it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2559/3286 [00:27<00:09, 74.28it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2567/3286 [00:27<00:09, 72.47it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2576/3286 [00:27<00:09, 75.75it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2584/3286 [00:28<00:09, 74.72it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2592/3286 [00:28<00:09, 75.21it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2601/3286 [00:28<00:08, 77.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2609/3286 [00:28<00:08, 75.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2617/3286 [00:28<00:09, 73.90it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2625/3286 [00:28<00:08, 74.24it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2633/3286 [00:28<00:09, 72.39it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2641/3286 [00:28<00:09, 71.28it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2650/3286 [00:29<00:08, 72.10it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2661/3286 [00:29<00:08, 77.42it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2673/3286 [00:29<00:06, 88.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2682/3286 [00:29<00:06, 87.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2691/3286 [00:29<00:06, 87.90it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2704/3286 [00:29<00:05, 97.42it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2714/3286 [00:29<00:06, 94.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2725/3286 [00:29<00:06, 93.13it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2736/3286 [00:29<00:05, 94.29it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2749/3286 [00:30<00:05, 99.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2759/3286 [00:30<00:05, 99.09it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2771/3286 [00:30<00:05, 102.15it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2782/3286 [00:30<00:05, 98.82it/s]  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2793/3286 [00:30<00:04, 99.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2804/3286 [00:30<00:04, 102.01it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2815/3286 [00:30<00:04, 100.01it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2826/3286 [00:30<00:04, 99.97it/s]  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2837/3286 [00:30<00:04, 100.46it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2848/3286 [00:31<00:04, 98.81it/s]  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2860/3286 [00:31<00:04, 102.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2871/3286 [00:31<00:04, 102.46it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2882/3286 [00:31<00:04, 98.81it/s]  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2893/3286 [00:31<00:03, 100.52it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2905/3286 [00:31<00:03, 100.67it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2916/3286 [00:31<00:03, 101.93it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2927/3286 [00:31<00:03, 102.04it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2938/3286 [00:31<00:03, 99.32it/s]  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2950/3286 [00:32<00:03, 100.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2962/3286 [00:32<00:03, 103.39it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2973/3286 [00:32<00:03, 99.22it/s]  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2986/3286 [00:32<00:02, 102.53it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2998/3286 [00:32<00:02, 106.00it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3009/3286 [00:32<00:02, 103.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3020/3286 [00:32<00:02, 103.73it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3032/3286 [00:32<00:02, 104.52it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3044/3286 [00:32<00:02, 104.48it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3055/3286 [00:33<00:02, 105.01it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3066/3286 [00:33<00:02, 104.90it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3078/3286 [00:33<00:01, 107.08it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3089/3286 [00:33<00:02, 90.67it/s]  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3105/3286 [00:33<00:01, 104.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3116/3286 [00:33<00:01, 105.20it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3127/3286 [00:33<00:01, 97.58it/s]  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3138/3286 [00:33<00:01, 87.95it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3148/3286 [00:34<00:01, 85.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3158/3286 [00:34<00:01, 85.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3169/3286 [00:34<00:01, 91.39it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3181/3286 [00:34<00:01, 94.37it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3192/3286 [00:34<00:00, 96.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3203/3286 [00:34<00:00, 98.41it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3213/3286 [00:34<00:00, 96.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3224/3286 [00:34<00:00, 99.19it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3234/3286 [00:34<00:00, 97.56it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3244/3286 [00:35<00:00, 92.75it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3256/3286 [00:35<00:00, 96.78it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3267/3286 [00:35<00:00, 97.28it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3278/3286 [00:35<00:00, 97.97it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3286/3286 [00:35<00:00, 92.78it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.9234
Epoch 1 Step 51 Train Loss: 0.3794
Epoch 1 Step 101 Train Loss: 0.3326
Epoch 1 Step 151 Train Loss: 0.3606
Epoch 1 Step 201 Train Loss: 0.3194
Epoch 1 Step 251 Train Loss: 0.3161
Epoch 1 Step 301 Train Loss: 0.2922
Epoch 1 Step 351 Train Loss: 0.3476
Epoch 1 Step 401 Train Loss: 0.3027
Epoch 1 Step 451 Train Loss: 0.3299
Epoch 1 Step 501 Train Loss: 0.3219
Epoch 1 Step 551 Train Loss: 0.3291
Epoch 1 Step 601 Train Loss: 0.2746
Epoch 1 Step 651 Train Loss: 0.2668
Epoch 1 Step 701 Train Loss: 0.3374
Epoch 1: Train Overall MSE: 0.0043 Validation Overall MSE: 0.0085. 
Train Top 20 DE MSE: 0.0187 Validation Top 20 DE MSE: 0.0233. 
Epoch 2 Step 1 Train Loss: 0.2692
Epoch 2 Step 51 Train Loss: 0.3151
Epoch 2 Step 101 Train Loss: 0.2982
Epoch 2 Step 151 Train Loss: 0.2672
Epoch 2 Step 201 Train Loss: 0.3186
Epoch 2 Step 251 Train Loss: 0.3274
Epoch 2 Step 301 Train Loss: 0.2898
Epoch 2 Step 351 Train Loss: 0.2834
Epoch 2 Step 401 Train Loss: 0.3532
Epoch 2 Step 451 Train Loss: 0.3534
Epoch 2 Step 501 Train Loss: 0.3257
Epoch 2 Step 551 Train Loss: 0.2991
Epoch 2 Step 601 Train Loss: 0.2882
Epoch 2 Step 651 Train Loss: 0.2784
Epoch 2 Step 701 Train Loss: 0.3204
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0117. 
Epoch 3 Step 1 Train Loss: 0.2698
Epoch 3 Step 51 Train Loss: 0.2780
Epoch 3 Step 101 Train Loss: 0.2787
Epoch 3 Step 151 Train Loss: 0.2860
Epoch 3 Step 201 Train Loss: 0.3334
Epoch 3 Step 251 Train Loss: 0.3010
Epoch 3 Step 301 Train Loss: 0.2634
Epoch 3 Step 351 Train Loss: 0.2956
Epoch 3 Step 401 Train Loss: 0.2621
Epoch 3 Step 451 Train Loss: 0.3434
Epoch 3 Step 501 Train Loss: 0.2417
Epoch 3 Step 551 Train Loss: 0.3243
Epoch 3 Step 601 Train Loss: 0.2833
Epoch 3 Step 651 Train Loss: 0.2776
Epoch 3 Step 701 Train Loss: 0.3736
Epoch 3: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0121 Validation Top 20 DE MSE: 0.0160. 
Epoch 4 Step 1 Train Loss: 0.2729
Epoch 4 Step 51 Train Loss: 0.3126
Epoch 4 Step 101 Train Loss: 0.3108
Epoch 4 Step 151 Train Loss: 0.2742
Epoch 4 Step 201 Train Loss: 0.2945
Epoch 4 Step 251 Train Loss: 0.3873
Epoch 4 Step 301 Train Loss: 0.3052
Epoch 4 Step 351 Train Loss: 0.3190
Epoch 4 Step 401 Train Loss: 0.3256
Epoch 4 Step 451 Train Loss: 0.3016
Epoch 4 Step 501 Train Loss: 0.2619
Epoch 4 Step 551 Train Loss: 0.2859
Epoch 4 Step 601 Train Loss: 0.3507
Epoch 4 Step 651 Train Loss: 0.3196
Epoch 4 Step 701 Train Loss: 0.3234
Epoch 4: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0242 Validation Top 20 DE MSE: 0.0133. 
Epoch 5 Step 1 Train Loss: 0.2597
Epoch 5 Step 51 Train Loss: 0.2953
Epoch 5 Step 101 Train Loss: 0.2937
Epoch 5 Step 151 Train Loss: 0.2676
Epoch 5 Step 201 Train Loss: 0.3007
Epoch 5 Step 251 Train Loss: 0.2449
Epoch 5 Step 301 Train Loss: 0.2948
Epoch 5 Step 351 Train Loss: 0.3437
Epoch 5 Step 401 Train Loss: 0.3677
Epoch 5 Step 451 Train Loss: 0.3065
Epoch 5 Step 501 Train Loss: 0.3440
Epoch 5 Step 551 Train Loss: 0.3188
Epoch 5 Step 601 Train Loss: 0.3171
Epoch 5 Step 651 Train Loss: 0.2988
Epoch 5 Step 701 Train Loss: 0.2642
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0116 Validation Top 20 DE MSE: 0.0111. 
Epoch 6 Step 1 Train Loss: 0.2781
Epoch 6 Step 51 Train Loss: 0.3174
Epoch 6 Step 101 Train Loss: 0.2647
Epoch 6 Step 151 Train Loss: 0.2829
Epoch 6 Step 201 Train Loss: 0.2818
Epoch 6 Step 251 Train Loss: 0.2783
Epoch 6 Step 301 Train Loss: 0.2809
Epoch 6 Step 351 Train Loss: 0.3956
Epoch 6 Step 401 Train Loss: 0.2794
Epoch 6 Step 451 Train Loss: 0.2526
Epoch 6 Step 501 Train Loss: 0.3178
Epoch 6 Step 551 Train Loss: 0.2871
Epoch 6 Step 601 Train Loss: 0.3309
Epoch 6 Step 651 Train Loss: 0.3137
Epoch 6 Step 701 Train Loss: 0.3278
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0109 Validation Top 20 DE MSE: 0.0118. 
Epoch 7 Step 1 Train Loss: 0.2640
Epoch 7 Step 51 Train Loss: 0.3036
Epoch 7 Step 101 Train Loss: 0.3118
Epoch 7 Step 151 Train Loss: 0.2606
Epoch 7 Step 201 Train Loss: 0.3090
Epoch 7 Step 251 Train Loss: 0.2696
Epoch 7 Step 301 Train Loss: 0.3129
Epoch 7 Step 351 Train Loss: 0.2913
Epoch 7 Step 401 Train Loss: 0.3014
Epoch 7 Step 451 Train Loss: 0.3563
Epoch 7 Step 501 Train Loss: 0.3988
Epoch 7 Step 551 Train Loss: 0.2505
Epoch 7 Step 601 Train Loss: 0.3093
Epoch 7 Step 651 Train Loss: 0.3128
Epoch 7 Step 701 Train Loss: 0.2774
Epoch 7: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0108. 
Epoch 8 Step 1 Train Loss: 0.3096
Epoch 8 Step 51 Train Loss: 0.3220
Epoch 8 Step 101 Train Loss: 0.3148
Epoch 8 Step 151 Train Loss: 0.3013
Epoch 8 Step 201 Train Loss: 0.3176
Epoch 8 Step 251 Train Loss: 0.2912
Epoch 8 Step 301 Train Loss: 0.2954
Epoch 8 Step 351 Train Loss: 0.2405
Epoch 8 Step 401 Train Loss: 0.3041
Epoch 8 Step 451 Train Loss: 0.2656
Epoch 8 Step 501 Train Loss: 0.2931
Epoch 8 Step 551 Train Loss: 0.3030
Epoch 8 Step 601 Train Loss: 0.2911
Epoch 8 Step 651 Train Loss: 0.2358
Epoch 8 Step 701 Train Loss: 0.3005
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0118. 
Epoch 9 Step 1 Train Loss: 0.2565
Epoch 9 Step 51 Train Loss: 0.2334
Epoch 9 Step 101 Train Loss: 0.3005
Epoch 9 Step 151 Train Loss: 0.2722
Epoch 9 Step 201 Train Loss: 0.3104
Epoch 9 Step 251 Train Loss: 0.3474
Epoch 9 Step 301 Train Loss: 0.2918
Epoch 9 Step 351 Train Loss: 0.2926
Epoch 9 Step 401 Train Loss: 0.2990
Epoch 9 Step 451 Train Loss: 0.2647
Epoch 9 Step 501 Train Loss: 0.2878
Epoch 9 Step 551 Train Loss: 0.3169
Epoch 9 Step 601 Train Loss: 0.2193
Epoch 9 Step 651 Train Loss: 0.2645
Epoch 9 Step 701 Train Loss: 0.3210
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0106. 
Epoch 10 Step 1 Train Loss: 0.2936
Epoch 10 Step 51 Train Loss: 0.3181
Epoch 10 Step 101 Train Loss: 0.2440
Epoch 10 Step 151 Train Loss: 0.2561
Epoch 10 Step 201 Train Loss: 0.3322
Epoch 10 Step 251 Train Loss: 0.3019
Epoch 10 Step 301 Train Loss: 0.3236
Epoch 10 Step 351 Train Loss: 0.2892
Epoch 10 Step 401 Train Loss: 0.2939
Epoch 10 Step 451 Train Loss: 0.2421
Epoch 10 Step 501 Train Loss: 0.3392
Epoch 10 Step 551 Train Loss: 0.2825
Epoch 10 Step 601 Train Loss: 0.2625
Epoch 10 Step 651 Train Loss: 0.2752
Epoch 10 Step 701 Train Loss: 0.2886
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0109. 
Epoch 11 Step 1 Train Loss: 0.2500
Epoch 11 Step 51 Train Loss: 0.2680
Epoch 11 Step 101 Train Loss: 0.2935
Epoch 11 Step 151 Train Loss: 0.3113
Epoch 11 Step 201 Train Loss: 0.2605
Epoch 11 Step 251 Train Loss: 0.2826
Epoch 11 Step 301 Train Loss: 0.2613
Epoch 11 Step 351 Train Loss: 0.2888
Epoch 11 Step 401 Train Loss: 0.2927
Epoch 11 Step 451 Train Loss: 0.2938
Epoch 11 Step 501 Train Loss: 0.3138
Epoch 11 Step 551 Train Loss: 0.3216
Epoch 11 Step 601 Train Loss: 0.2642
Epoch 11 Step 651 Train Loss: 0.3866
Epoch 11 Step 701 Train Loss: 0.2680
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0106. 
Epoch 12 Step 1 Train Loss: 0.3749
Epoch 12 Step 51 Train Loss: 0.2737
Epoch 12 Step 101 Train Loss: 0.2896
Epoch 12 Step 151 Train Loss: 0.3362
Epoch 12 Step 201 Train Loss: 0.2453
Epoch 12 Step 251 Train Loss: 0.2701
Epoch 12 Step 301 Train Loss: 0.3224
Epoch 12 Step 351 Train Loss: 0.2860
Epoch 12 Step 401 Train Loss: 0.2948
Epoch 12 Step 451 Train Loss: 0.2772
Epoch 12 Step 501 Train Loss: 0.2465
Epoch 12 Step 551 Train Loss: 0.3387
Epoch 12 Step 601 Train Loss: 0.2658
Epoch 12 Step 651 Train Loss: 0.2990
Epoch 12 Step 701 Train Loss: 0.3174
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0106. 
Epoch 13 Step 1 Train Loss: 0.2914
Epoch 13 Step 51 Train Loss: 0.2739
Epoch 13 Step 101 Train Loss: 0.3007
Epoch 13 Step 151 Train Loss: 0.2997
Epoch 13 Step 201 Train Loss: 0.3212
Epoch 13 Step 251 Train Loss: 0.3346
Epoch 13 Step 301 Train Loss: 0.2843
Epoch 13 Step 351 Train Loss: 0.3535
Epoch 13 Step 401 Train Loss: 0.2786
Epoch 13 Step 451 Train Loss: 0.2756
Epoch 13 Step 501 Train Loss: 0.3043
Epoch 13 Step 551 Train Loss: 0.3177
Epoch 13 Step 601 Train Loss: 0.2646
Epoch 13 Step 651 Train Loss: 0.3106
Epoch 13 Step 701 Train Loss: 0.3033
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0110. 
Epoch 14 Step 1 Train Loss: 0.2962
Epoch 14 Step 51 Train Loss: 0.3412
Epoch 14 Step 101 Train Loss: 0.2748
Epoch 14 Step 151 Train Loss: 0.2952
Epoch 14 Step 201 Train Loss: 0.2907
Epoch 14 Step 251 Train Loss: 0.3257
Epoch 14 Step 301 Train Loss: 0.2415
Epoch 14 Step 351 Train Loss: 0.2797
Epoch 14 Step 401 Train Loss: 0.3346
Epoch 14 Step 451 Train Loss: 0.2908
Epoch 14 Step 501 Train Loss: 0.3240
Epoch 14 Step 551 Train Loss: 0.2596
Epoch 14 Step 601 Train Loss: 0.3213
Epoch 14 Step 651 Train Loss: 0.2867
Epoch 14 Step 701 Train Loss: 0.2531
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0105. 
Epoch 15 Step 1 Train Loss: 0.2859
Epoch 15 Step 51 Train Loss: 0.2925
Epoch 15 Step 101 Train Loss: 0.2494
Epoch 15 Step 151 Train Loss: 0.2916
Epoch 15 Step 201 Train Loss: 0.2840
Epoch 15 Step 251 Train Loss: 0.3549
Epoch 15 Step 301 Train Loss: 0.2769
Epoch 15 Step 351 Train Loss: 0.2809
Epoch 15 Step 401 Train Loss: 0.2443
Epoch 15 Step 451 Train Loss: 0.3030
Epoch 15 Step 501 Train Loss: 0.2814
Epoch 15 Step 551 Train Loss: 0.2889
Epoch 15 Step 601 Train Loss: 0.2717
Epoch 15 Step 651 Train Loss: 0.3442
Epoch 15 Step 701 Train Loss: 0.2441
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0109. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0109
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002225279
test_unseen_single_pearson: 0.9900891633277575
test_unseen_single_mse_de: 0.010918593
test_unseen_single_pearson_de: 0.9981676593863297
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.866926627428698
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9
test_unseen_single_mse_top20_de_non_dropout: 0.011357439
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–…â–‚â–‚â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–„â–‡â–‡â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–„â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–„â–†â–ƒâ–…â–„â–„â–‚â–†â–…â–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–‚â–„â–ƒâ–…â–ƒâ–‡â–â–„â–‡â–ƒâ–ˆâ–‚â–„â–ƒâ–â–‡â–„
wandb:                                                   val_de_mse â–ˆâ–‚â–„â–ƒâ–â–‚â–â–‚â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–‚â–†â–â–ˆâ–‡â–…â–†â–…â–…â–…â–…â–…â–…â–†â–…
wandb:                                                      val_mse â–ˆâ–„â–„â–â–‚â–‚â–â–‚â–â–â–â–â–‚â–â–‚
wandb:                                                  val_pearson â–â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01092
wandb:                                              test_de_pearson 0.99817
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.9
wandb:                                                     test_mse 0.00223
wandb:                                test_mse_top20_de_non_dropout 0.01136
wandb:                                                 test_pearson 0.99009
wandb:                                           test_pearson_delta 0.86693
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9
wandb:                                       test_unseen_single_mse 0.00223
wandb:                                    test_unseen_single_mse_de 0.01092
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01136
wandb:                                   test_unseen_single_pearson 0.99009
wandb:                                test_unseen_single_pearson_de 0.99817
wandb:                             test_unseen_single_pearson_delta 0.86693
wandb:                                                 train_de_mse 0.00957
wandb:                                             train_de_pearson 0.99803
wandb:                                                    train_mse 0.00165
wandb:                                                train_pearson 0.99262
wandb:                                                training_loss 0.34124
wandb:                                                   val_de_mse 0.01094
wandb:                                               val_de_pearson 0.79353
wandb:                                                      val_mse 0.00235
wandb:                                                  val_pearson 0.98946
wandb: 
wandb: ðŸš€ View run ShifrutMarson2018_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u7eejd48
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_075945-u7eejd48/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/shifrutmarson2018/splits/shifrutmarson2018_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_082038-czrq02dj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/czrq02dj
Start Training...
Epoch 1 Step 1 Train Loss: 1.7955
Epoch 1 Step 51 Train Loss: 0.3976
Epoch 1 Step 101 Train Loss: 0.3775
Epoch 1 Step 151 Train Loss: 0.4100
Epoch 1 Step 201 Train Loss: 0.3289
Epoch 1 Step 251 Train Loss: 0.3128
Epoch 1 Step 301 Train Loss: 0.3435
Epoch 1 Step 351 Train Loss: 0.3144
Epoch 1 Step 401 Train Loss: 0.3623
Epoch 1 Step 451 Train Loss: 0.3308
Epoch 1 Step 501 Train Loss: 0.2893
Epoch 1 Step 551 Train Loss: 0.3786
Epoch 1 Step 601 Train Loss: 0.3118
Epoch 1 Step 651 Train Loss: 0.2373
Epoch 1 Step 701 Train Loss: 0.3336
Epoch 1: Train Overall MSE: 0.0064 Validation Overall MSE: 0.0052. 
Train Top 20 DE MSE: 0.0150 Validation Top 20 DE MSE: 0.0248. 
Epoch 2 Step 1 Train Loss: 0.3031
Epoch 2 Step 51 Train Loss: 0.3260
Epoch 2 Step 101 Train Loss: 0.2674
Epoch 2 Step 151 Train Loss: 0.2994
Epoch 2 Step 201 Train Loss: 0.3296
Epoch 2 Step 251 Train Loss: 0.2786
Epoch 2 Step 301 Train Loss: 0.3011
Epoch 2 Step 351 Train Loss: 0.3080
Epoch 2 Step 401 Train Loss: 0.2485
Epoch 2 Step 451 Train Loss: 0.3400
Epoch 2 Step 501 Train Loss: 0.3500
Epoch 2 Step 551 Train Loss: 0.3412
Epoch 2 Step 601 Train Loss: 0.3498
Epoch 2 Step 651 Train Loss: 0.3383
Epoch 2 Step 701 Train Loss: 0.3449
Epoch 2: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0146 Validation Top 20 DE MSE: 0.0263. 
Epoch 3 Step 1 Train Loss: 0.2776
Epoch 3 Step 51 Train Loss: 0.2795
Epoch 3 Step 101 Train Loss: 0.3019
Epoch 3 Step 151 Train Loss: 0.3078
Epoch 3 Step 201 Train Loss: 0.2865
Epoch 3 Step 251 Train Loss: 0.2763
Epoch 3 Step 301 Train Loss: 0.2703
Epoch 3 Step 351 Train Loss: 0.2657
Epoch 3 Step 401 Train Loss: 0.2788
Epoch 3 Step 451 Train Loss: 0.3008
Epoch 3 Step 501 Train Loss: 0.3566
Epoch 3 Step 551 Train Loss: 0.3163
Epoch 3 Step 601 Train Loss: 0.2846
Epoch 3 Step 651 Train Loss: 0.2710
Epoch 3 Step 701 Train Loss: 0.2914
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0136 Validation Top 20 DE MSE: 0.0282. 
Epoch 4 Step 1 Train Loss: 0.2919
Epoch 4 Step 51 Train Loss: 0.3196
Epoch 4 Step 101 Train Loss: 0.3018
Epoch 4 Step 151 Train Loss: 0.2691
Epoch 4 Step 201 Train Loss: 0.2855
Epoch 4 Step 251 Train Loss: 0.2911
Epoch 4 Step 301 Train Loss: 0.3254
Epoch 4 Step 351 Train Loss: 0.2920
Epoch 4 Step 401 Train Loss: 0.3800
Epoch 4 Step 451 Train Loss: 0.2882
Epoch 4 Step 501 Train Loss: 0.2672
Epoch 4 Step 551 Train Loss: 0.2450
Epoch 4 Step 601 Train Loss: 0.2758
Epoch 4 Step 651 Train Loss: 0.2721
Epoch 4 Step 701 Train Loss: 0.3112
Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.0234. 
Epoch 5 Step 1 Train Loss: 0.2323
Epoch 5 Step 51 Train Loss: 0.2505
Epoch 5 Step 101 Train Loss: 0.2782
Epoch 5 Step 151 Train Loss: 0.2993
Epoch 5 Step 201 Train Loss: 0.3356
Epoch 5 Step 251 Train Loss: 0.3606
Epoch 5 Step 301 Train Loss: 0.2806
Epoch 5 Step 351 Train Loss: 0.3199
Epoch 5 Step 401 Train Loss: 0.2824
Epoch 5 Step 451 Train Loss: 0.3541
Epoch 5 Step 501 Train Loss: 0.2835
Epoch 5 Step 551 Train Loss: 0.2545
Epoch 5 Step 601 Train Loss: 0.2644
Epoch 5 Step 651 Train Loss: 0.2839
Epoch 5 Step 701 Train Loss: 0.2198
Epoch 5: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0116 Validation Top 20 DE MSE: 0.0320. 
Epoch 6 Step 1 Train Loss: 0.2503
Epoch 6 Step 51 Train Loss: 0.3125
Epoch 6 Step 101 Train Loss: 0.2898
Epoch 6 Step 151 Train Loss: 0.2982
Epoch 6 Step 201 Train Loss: 0.2988
Epoch 6 Step 251 Train Loss: 0.2838
Epoch 6 Step 301 Train Loss: 0.3158
Epoch 6 Step 351 Train Loss: 0.2969
Epoch 6 Step 401 Train Loss: 0.2926
Epoch 6 Step 451 Train Loss: 0.2790
Epoch 6 Step 501 Train Loss: 0.4040
Epoch 6 Step 551 Train Loss: 0.2873
Epoch 6 Step 601 Train Loss: 0.3054
Epoch 6 Step 651 Train Loss: 0.2688
Epoch 6 Step 701 Train Loss: 0.2793
Epoch 6: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0227. 
Epoch 7 Step 1 Train Loss: 0.2803
Epoch 7 Step 51 Train Loss: 0.2619
Epoch 7 Step 101 Train Loss: 0.2922
Epoch 7 Step 151 Train Loss: 0.2926
Epoch 7 Step 201 Train Loss: 0.3265
Epoch 7 Step 251 Train Loss: 0.3825
Epoch 7 Step 301 Train Loss: 0.2701
Epoch 7 Step 351 Train Loss: 0.2686
Epoch 7 Step 401 Train Loss: 0.2547
Epoch 7 Step 451 Train Loss: 0.2548
Epoch 7 Step 501 Train Loss: 0.2447
Epoch 7 Step 551 Train Loss: 0.3031
Epoch 7 Step 601 Train Loss: 0.2800
Epoch 7 Step 651 Train Loss: 0.3253
Epoch 7 Step 701 Train Loss: 0.3147
Epoch 7: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0241. 
Epoch 8 Step 1 Train Loss: 0.2949
Epoch 8 Step 51 Train Loss: 0.2859
Epoch 8 Step 101 Train Loss: 0.3181
Epoch 8 Step 151 Train Loss: 0.2602
Epoch 8 Step 201 Train Loss: 0.3043
Epoch 8 Step 251 Train Loss: 0.3236
Epoch 8 Step 301 Train Loss: 0.2582
Epoch 8 Step 351 Train Loss: 0.2873
Epoch 8 Step 401 Train Loss: 0.2443
Epoch 8 Step 451 Train Loss: 0.3245
Epoch 8 Step 501 Train Loss: 0.2504
Epoch 8 Step 551 Train Loss: 0.2881
Epoch 8 Step 601 Train Loss: 0.2535
Epoch 8 Step 651 Train Loss: 0.3065
Epoch 8 Step 701 Train Loss: 0.2684
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.0213. 
Epoch 9 Step 1 Train Loss: 0.3315
Epoch 9 Step 51 Train Loss: 0.2683
Epoch 9 Step 101 Train Loss: 0.2790
Epoch 9 Step 151 Train Loss: 0.2469
Epoch 9 Step 201 Train Loss: 0.2523
Epoch 9 Step 251 Train Loss: 0.2929
Epoch 9 Step 301 Train Loss: 0.2879
Epoch 9 Step 351 Train Loss: 0.2469
Epoch 9 Step 401 Train Loss: 0.3225
Epoch 9 Step 451 Train Loss: 0.3154
Epoch 9 Step 501 Train Loss: 0.2852
Epoch 9 Step 551 Train Loss: 0.2884
Epoch 9 Step 601 Train Loss: 0.2583
Epoch 9 Step 651 Train Loss: 0.3643
Epoch 9 Step 701 Train Loss: 0.2528
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0229. 
Epoch 10 Step 1 Train Loss: 0.2903
Epoch 10 Step 51 Train Loss: 0.3024
Epoch 10 Step 101 Train Loss: 0.3353
Epoch 10 Step 151 Train Loss: 0.2770
Epoch 10 Step 201 Train Loss: 0.3264
Epoch 10 Step 251 Train Loss: 0.2845
Epoch 10 Step 301 Train Loss: 0.2951
Epoch 10 Step 351 Train Loss: 0.2971
Epoch 10 Step 401 Train Loss: 0.2681
Epoch 10 Step 451 Train Loss: 0.2782
Epoch 10 Step 501 Train Loss: 0.3731
Epoch 10 Step 551 Train Loss: 0.3123
Epoch 10 Step 601 Train Loss: 0.2812
Epoch 10 Step 651 Train Loss: 0.3062
Epoch 10 Step 701 Train Loss: 0.3949
Epoch 10: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.0218. 
Epoch 11 Step 1 Train Loss: 0.2781
Epoch 11 Step 51 Train Loss: 0.2448
Epoch 11 Step 101 Train Loss: 0.2377
Epoch 11 Step 151 Train Loss: 0.2681
Epoch 11 Step 201 Train Loss: 0.2879
Epoch 11 Step 251 Train Loss: 0.2644
Epoch 11 Step 301 Train Loss: 0.3012
Epoch 11 Step 351 Train Loss: 0.2849
Epoch 11 Step 401 Train Loss: 0.2763
Epoch 11 Step 451 Train Loss: 0.2978
Epoch 11 Step 501 Train Loss: 0.3535
Epoch 11 Step 551 Train Loss: 0.2994
Epoch 11 Step 601 Train Loss: 0.2879
Epoch 11 Step 651 Train Loss: 0.2842
Epoch 11 Step 701 Train Loss: 0.3069
Epoch 11: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0087 Validation Top 20 DE MSE: 0.0230. 
Epoch 12 Step 1 Train Loss: 0.2960
Epoch 12 Step 51 Train Loss: 0.2923
Epoch 12 Step 101 Train Loss: 0.3881
Epoch 12 Step 151 Train Loss: 0.3430
Epoch 12 Step 201 Train Loss: 0.3301
Epoch 12 Step 251 Train Loss: 0.3399
Epoch 12 Step 301 Train Loss: 0.2824
Epoch 12 Step 351 Train Loss: 0.3542
Epoch 12 Step 401 Train Loss: 0.2687
Epoch 12 Step 451 Train Loss: 0.2821
Epoch 12 Step 501 Train Loss: 0.2960
Epoch 12 Step 551 Train Loss: 0.3080
Epoch 12 Step 601 Train Loss: 0.2788
Epoch 12 Step 651 Train Loss: 0.2764
Epoch 12 Step 701 Train Loss: 0.3146
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.0233. 
Epoch 13 Step 1 Train Loss: 0.2776
Epoch 13 Step 51 Train Loss: 0.2562
Epoch 13 Step 101 Train Loss: 0.2937
Epoch 13 Step 151 Train Loss: 0.2574
Epoch 13 Step 201 Train Loss: 0.2631
Epoch 13 Step 251 Train Loss: 0.3454
Epoch 13 Step 301 Train Loss: 0.2840
Epoch 13 Step 351 Train Loss: 0.3124
Epoch 13 Step 401 Train Loss: 0.2855
Epoch 13 Step 451 Train Loss: 0.3144
Epoch 13 Step 501 Train Loss: 0.3296
Epoch 13 Step 551 Train Loss: 0.2737
Epoch 13 Step 601 Train Loss: 0.2718
Epoch 13 Step 651 Train Loss: 0.2912
Epoch 13 Step 701 Train Loss: 0.2874
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.0230. 
Epoch 14 Step 1 Train Loss: 0.2811
Epoch 14 Step 51 Train Loss: 0.3385
Epoch 14 Step 101 Train Loss: 0.3064
Epoch 14 Step 151 Train Loss: 0.2748
Epoch 14 Step 201 Train Loss: 0.3361
Epoch 14 Step 251 Train Loss: 0.2480
Epoch 14 Step 301 Train Loss: 0.3825
Epoch 14 Step 351 Train Loss: 0.2694
Epoch 14 Step 401 Train Loss: 0.3807
Epoch 14 Step 451 Train Loss: 0.2833
Epoch 14 Step 501 Train Loss: 0.3527
Epoch 14 Step 551 Train Loss: 0.3085
Epoch 14 Step 601 Train Loss: 0.2782
Epoch 14 Step 651 Train Loss: 0.2633
Epoch 14 Step 701 Train Loss: 0.3246
Epoch 14: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.0233. 
Epoch 15 Step 1 Train Loss: 0.3225
Epoch 15 Step 51 Train Loss: 0.3020
Epoch 15 Step 101 Train Loss: 0.2721
Epoch 15 Step 151 Train Loss: 0.2946
Epoch 15 Step 201 Train Loss: 0.3211
Epoch 15 Step 251 Train Loss: 0.2524
Epoch 15 Step 301 Train Loss: 0.2806
Epoch 15 Step 351 Train Loss: 0.3449
Epoch 15 Step 401 Train Loss: 0.2745
Epoch 15 Step 451 Train Loss: 0.2679
Epoch 15 Step 501 Train Loss: 0.2823
Epoch 15 Step 551 Train Loss: 0.3121
Epoch 15 Step 601 Train Loss: 0.2478
Epoch 15 Step 651 Train Loss: 0.3370
Epoch 15 Step 701 Train Loss: 0.3187
Epoch 15: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.0242. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0115
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0023272529
test_unseen_single_pearson: 0.9896833843849384
test_unseen_single_mse_de: 0.011509789
test_unseen_single_pearson_de: 0.9980532378368897
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8482232770943525
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.96
test_unseen_single_mse_top20_de_non_dropout: 0.01219776
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.021 MB uploadedwandb: / 0.001 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb: | 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ˆâ–‡â–…â–…â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–…â–â–…â–„â–ˆâ–†â–ˆâ–…â–‡â–†â–‡â–‡â–‡â–‡â–‡
wandb:                                                    train_mse â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–†â–„â–…â–†â–ƒâ–‚â–ˆâ–†â–„â–â–‡â–†â–‚â–„â–†â–„â–ˆâ–‚â–â–‚â–ƒâ–‚â–…â–…â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–‡
wandb:                                                   val_de_mse â–ƒâ–„â–…â–‚â–ˆâ–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–ƒ
wandb:                                               val_de_pearson â–…â–†â–„â–†â–â–‡â–†â–ˆâ–‡â–‡â–‡â–†â–‡â–†â–†
wandb:                                                      val_mse â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01151
wandb:                                              test_de_pearson 0.99805
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.96
wandb:                                                     test_mse 0.00233
wandb:                                test_mse_top20_de_non_dropout 0.0122
wandb:                                                 test_pearson 0.98968
wandb:                                           test_pearson_delta 0.84822
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.96
wandb:                                       test_unseen_single_mse 0.00233
wandb:                                    test_unseen_single_mse_de 0.01151
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0122
wandb:                                   test_unseen_single_pearson 0.98968
wandb:                                test_unseen_single_pearson_de 0.99805
wandb:                             test_unseen_single_pearson_delta 0.84822
wandb:                                                 train_de_mse 0.00852
wandb:                                             train_de_pearson 0.97923
wandb:                                                    train_mse 0.00154
wandb:                                                train_pearson 0.99308
wandb:                                                training_loss 0.25415
wandb:                                                   val_de_mse 0.02424
wandb:                                               val_de_pearson 0.9954
wandb:                                                      val_mse 0.00158
wandb:                                                  val_pearson 0.99277
wandb: 
wandb: ðŸš€ View run ShifrutMarson2018_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/czrq02dj
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_082038-czrq02dj/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/shifrutmarson2018/splits/shifrutmarson2018_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_084007-t083mzsi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/t083mzsi
Start Training...
Epoch 1 Step 1 Train Loss: 1.0025
Epoch 1 Step 51 Train Loss: 0.3720
Epoch 1 Step 101 Train Loss: 0.5177
Epoch 1 Step 151 Train Loss: 0.2945
Epoch 1 Step 201 Train Loss: 0.3314
Epoch 1 Step 251 Train Loss: 0.4180
Epoch 1 Step 301 Train Loss: 0.3147
Epoch 1 Step 351 Train Loss: 0.3525
Epoch 1 Step 401 Train Loss: 0.3158
Epoch 1 Step 451 Train Loss: 0.3274
Epoch 1 Step 501 Train Loss: 0.2593
Epoch 1 Step 551 Train Loss: 0.3258
Epoch 1 Step 601 Train Loss: 0.4244
Epoch 1 Step 651 Train Loss: 0.3217
Epoch 1: Train Overall MSE: 0.0055 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0298 Validation Top 20 DE MSE: 0.0395. 
Epoch 2 Step 1 Train Loss: 0.2731
Epoch 2 Step 51 Train Loss: 0.2818
Epoch 2 Step 101 Train Loss: 0.3565
Epoch 2 Step 151 Train Loss: 0.2866
Epoch 2 Step 201 Train Loss: 0.3121
Epoch 2 Step 251 Train Loss: 0.2960
Epoch 2 Step 301 Train Loss: 0.3314
Epoch 2 Step 351 Train Loss: 0.2856
Epoch 2 Step 401 Train Loss: 0.3459
Epoch 2 Step 451 Train Loss: 0.3023
Epoch 2 Step 501 Train Loss: 0.3137
Epoch 2 Step 551 Train Loss: 0.3456
Epoch 2 Step 601 Train Loss: 0.3064
Epoch 2 Step 651 Train Loss: 0.2849
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0136 Validation Top 20 DE MSE: 0.0160. 
Epoch 3 Step 1 Train Loss: 0.2951
Epoch 3 Step 51 Train Loss: 0.2827
Epoch 3 Step 101 Train Loss: 0.3427
Epoch 3 Step 151 Train Loss: 0.2965
Epoch 3 Step 201 Train Loss: 0.3149
Epoch 3 Step 251 Train Loss: 0.2649
Epoch 3 Step 301 Train Loss: 0.3151
Epoch 3 Step 351 Train Loss: 0.3368
Epoch 3 Step 401 Train Loss: 0.3402
Epoch 3 Step 451 Train Loss: 0.2653
Epoch 3 Step 501 Train Loss: 0.2912
Epoch 3 Step 551 Train Loss: 0.2790
Epoch 3 Step 601 Train Loss: 0.3755
Epoch 3 Step 651 Train Loss: 0.2660
Epoch 3: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0073. 
Epoch 4 Step 1 Train Loss: 0.3253
Epoch 4 Step 51 Train Loss: 0.2983
Epoch 4 Step 101 Train Loss: 0.2925
Epoch 4 Step 151 Train Loss: 0.3062
Epoch 4 Step 201 Train Loss: 0.3056
Epoch 4 Step 251 Train Loss: 0.3265
Epoch 4 Step 301 Train Loss: 0.2969
Epoch 4 Step 351 Train Loss: 0.3586
Epoch 4 Step 401 Train Loss: 0.3144
Epoch 4 Step 451 Train Loss: 0.2273
Epoch 4 Step 501 Train Loss: 0.3144
Epoch 4 Step 551 Train Loss: 0.3403
Epoch 4 Step 601 Train Loss: 0.2687
Epoch 4 Step 651 Train Loss: 0.3101
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.0181. 
Epoch 5 Step 1 Train Loss: 0.2794
Epoch 5 Step 51 Train Loss: 0.2952
Epoch 5 Step 101 Train Loss: 0.2805
Epoch 5 Step 151 Train Loss: 0.2998
Epoch 5 Step 201 Train Loss: 0.2852
Epoch 5 Step 251 Train Loss: 0.2656
Epoch 5 Step 301 Train Loss: 0.2959
Epoch 5 Step 351 Train Loss: 0.2832
Epoch 5 Step 401 Train Loss: 0.3101
Epoch 5 Step 451 Train Loss: 0.2816
Epoch 5 Step 501 Train Loss: 0.2709
Epoch 5 Step 551 Train Loss: 0.3044
Epoch 5 Step 601 Train Loss: 0.2729
Epoch 5 Step 651 Train Loss: 0.3238
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0134 Validation Top 20 DE MSE: 0.0183. 
Epoch 6 Step 1 Train Loss: 0.2840
Epoch 6 Step 51 Train Loss: 0.2763
Epoch 6 Step 101 Train Loss: 0.2766
Epoch 6 Step 151 Train Loss: 0.2990
Epoch 6 Step 201 Train Loss: 0.3445
Epoch 6 Step 251 Train Loss: 0.3040
Epoch 6 Step 301 Train Loss: 0.2972
Epoch 6 Step 351 Train Loss: 0.2543
Epoch 6 Step 401 Train Loss: 0.2530
Epoch 6 Step 451 Train Loss: 0.3078
Epoch 6 Step 501 Train Loss: 0.3238
Epoch 6 Step 551 Train Loss: 0.2787
Epoch 6 Step 601 Train Loss: 0.2957
Epoch 6 Step 651 Train Loss: 0.3399
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0111 Validation Top 20 DE MSE: 0.0146. 
Epoch 7 Step 1 Train Loss: 0.2630
Epoch 7 Step 51 Train Loss: 0.2899
Epoch 7 Step 101 Train Loss: 0.3369
Epoch 7 Step 151 Train Loss: 0.3328
Epoch 7 Step 201 Train Loss: 0.2934
Epoch 7 Step 251 Train Loss: 0.3052
Epoch 7 Step 301 Train Loss: 0.2841
Epoch 7 Step 351 Train Loss: 0.3118
Epoch 7 Step 401 Train Loss: 0.2502
Epoch 7 Step 451 Train Loss: 0.2700
Epoch 7 Step 501 Train Loss: 0.3732
Epoch 7 Step 551 Train Loss: 0.3067
Epoch 7 Step 601 Train Loss: 0.3048
Epoch 7 Step 651 Train Loss: 0.3519
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0163. 
Epoch 8 Step 1 Train Loss: 0.3365
Epoch 8 Step 51 Train Loss: 0.3144
Epoch 8 Step 101 Train Loss: 0.3197
Epoch 8 Step 151 Train Loss: 0.3130
Epoch 8 Step 201 Train Loss: 0.3616
Epoch 8 Step 251 Train Loss: 0.2701
Epoch 8 Step 301 Train Loss: 0.3094
Epoch 8 Step 351 Train Loss: 0.3204
Epoch 8 Step 401 Train Loss: 0.2683
Epoch 8 Step 451 Train Loss: 0.3149
Epoch 8 Step 501 Train Loss: 0.3451
Epoch 8 Step 551 Train Loss: 0.2558
Epoch 8 Step 601 Train Loss: 0.2833
Epoch 8 Step 651 Train Loss: 0.2990
Epoch 8: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0153. 
Epoch 9 Step 1 Train Loss: 0.3142
Epoch 9 Step 51 Train Loss: 0.3361
Epoch 9 Step 101 Train Loss: 0.2500
Epoch 9 Step 151 Train Loss: 0.3086
Epoch 9 Step 201 Train Loss: 0.3006
Epoch 9 Step 251 Train Loss: 0.3193
Epoch 9 Step 301 Train Loss: 0.3003
Epoch 9 Step 351 Train Loss: 0.2967
Epoch 9 Step 401 Train Loss: 0.2829
Epoch 9 Step 451 Train Loss: 0.2585
Epoch 9 Step 501 Train Loss: 0.3075
Epoch 9 Step 551 Train Loss: 0.2524
Epoch 9 Step 601 Train Loss: 0.2783
Epoch 9 Step 651 Train Loss: 0.2469
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.0141. 
Epoch 10 Step 1 Train Loss: 0.3024
Epoch 10 Step 51 Train Loss: 0.2863
Epoch 10 Step 101 Train Loss: 0.2528
Epoch 10 Step 151 Train Loss: 0.2377
Epoch 10 Step 201 Train Loss: 0.2985
Epoch 10 Step 251 Train Loss: 0.3112
Epoch 10 Step 301 Train Loss: 0.3116
Epoch 10 Step 351 Train Loss: 0.3095
Epoch 10 Step 401 Train Loss: 0.2994
Epoch 10 Step 451 Train Loss: 0.2764
Epoch 10 Step 501 Train Loss: 0.2720
Epoch 10 Step 551 Train Loss: 0.3362
Epoch 10 Step 601 Train Loss: 0.2459
Epoch 10 Step 651 Train Loss: 0.2716
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.0150. 
Epoch 11 Step 1 Train Loss: 0.2653
Epoch 11 Step 51 Train Loss: 0.2797
Epoch 11 Step 101 Train Loss: 0.2402
Epoch 11 Step 151 Train Loss: 0.2736
Epoch 11 Step 201 Train Loss: 0.2967
Epoch 11 Step 251 Train Loss: 0.3188
Epoch 11 Step 301 Train Loss: 0.3481
Epoch 11 Step 351 Train Loss: 0.2749
Epoch 11 Step 401 Train Loss: 0.2858
Epoch 11 Step 451 Train Loss: 0.3368
Epoch 11 Step 501 Train Loss: 0.2947
Epoch 11 Step 551 Train Loss: 0.3348
Epoch 11 Step 601 Train Loss: 0.3076
Epoch 11 Step 651 Train Loss: 0.2930
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.0140. 
Epoch 12 Step 1 Train Loss: 0.3168
Epoch 12 Step 51 Train Loss: 0.2802
Epoch 12 Step 101 Train Loss: 0.3518
Epoch 12 Step 151 Train Loss: 0.2868
Epoch 12 Step 201 Train Loss: 0.2693
Epoch 12 Step 251 Train Loss: 0.3445
Epoch 12 Step 301 Train Loss: 0.3452
Epoch 12 Step 351 Train Loss: 0.3301
Epoch 12 Step 401 Train Loss: 0.3052
Epoch 12 Step 451 Train Loss: 0.2935
Epoch 12 Step 501 Train Loss: 0.2584
Epoch 12 Step 551 Train Loss: 0.2448
Epoch 12 Step 601 Train Loss: 0.2893
Epoch 12 Step 651 Train Loss: 0.3217
Epoch 12: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0139. 
Epoch 13 Step 1 Train Loss: 0.2869
Epoch 13 Step 51 Train Loss: 0.2933
Epoch 13 Step 101 Train Loss: 0.2660
Epoch 13 Step 151 Train Loss: 0.2796
Epoch 13 Step 201 Train Loss: 0.2827
Epoch 13 Step 251 Train Loss: 0.3166
Epoch 13 Step 301 Train Loss: 0.2759
Epoch 13 Step 351 Train Loss: 0.3322
Epoch 13 Step 401 Train Loss: 0.2733
Epoch 13 Step 451 Train Loss: 0.3455
Epoch 13 Step 501 Train Loss: 0.3204
Epoch 13 Step 551 Train Loss: 0.2937
Epoch 13 Step 601 Train Loss: 0.3258
Epoch 13 Step 651 Train Loss: 0.3007
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0168. 
Epoch 14 Step 1 Train Loss: 0.2766
Epoch 14 Step 51 Train Loss: 0.2619
Epoch 14 Step 101 Train Loss: 0.2873
Epoch 14 Step 151 Train Loss: 0.3183
Epoch 14 Step 201 Train Loss: 0.3116
Epoch 14 Step 251 Train Loss: 0.2850
Epoch 14 Step 301 Train Loss: 0.2797
Epoch 14 Step 351 Train Loss: 0.2351
Epoch 14 Step 401 Train Loss: 0.2975
Epoch 14 Step 451 Train Loss: 0.3483
Epoch 14 Step 501 Train Loss: 0.3715
Epoch 14 Step 551 Train Loss: 0.2337
Epoch 14 Step 601 Train Loss: 0.3356
Epoch 14 Step 651 Train Loss: 0.3166
Epoch 14: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0164. 
Epoch 15 Step 1 Train Loss: 0.3121
Epoch 15 Step 51 Train Loss: 0.3386
Epoch 15 Step 101 Train Loss: 0.2617
Epoch 15 Step 151 Train Loss: 0.2507
Epoch 15 Step 201 Train Loss: 0.2854
Epoch 15 Step 251 Train Loss: 0.2909
Epoch 15 Step 301 Train Loss: 0.2638
Epoch 15 Step 351 Train Loss: 0.2937
Epoch 15 Step 401 Train Loss: 0.3187
Epoch 15 Step 451 Train Loss: 0.3487
Epoch 15 Step 501 Train Loss: 0.3237
Epoch 15 Step 551 Train Loss: 0.2739
Epoch 15 Step 601 Train Loss: 0.2741
Epoch 15 Step 651 Train Loss: 0.2799
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.0146. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0086
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0033646345
test_unseen_single_pearson: 0.9838572272206608
test_unseen_single_mse_de: 0.008646364
test_unseen_single_pearson_de: 0.9987762724244424
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8253368691021183
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9400000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.0075895963
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.003 MB of 0.024 MB uploadedwandb: / 0.023 MB of 0.024 MB uploadedwandb: - 0.023 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–
wandb:                                             train_de_pearson â–â–…â–‚â–†â–ˆâ–†â–†â–†â–†â–†â–…â–…â–‡â–‡â–†
wandb:                                                    train_mse â–ˆâ–ƒâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–†â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–…â–ˆâ–†â–†â–…â–…â–ˆâ–„â–†â–ƒâ–ƒâ–„â–„â–‡â–…â–ƒâ–â–‡â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–„â–‚â–ƒâ–„â–ƒâ–ƒâ–…â–ƒâ–…â–„â–‚â–„â–‚
wandb:                                                   val_de_mse â–ˆâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒ
wandb:                                               val_de_pearson â–â–†â–ˆâ–†â–†â–‡â–†â–†â–‡â–†â–‡â–‡â–†â–†â–‡
wandb:                                                      val_mse â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00865
wandb:                                              test_de_pearson 0.99878
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.94
wandb:                                                     test_mse 0.00336
wandb:                                test_mse_top20_de_non_dropout 0.00759
wandb:                                                 test_pearson 0.98386
wandb:                                           test_pearson_delta 0.82534
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.94
wandb:                                       test_unseen_single_mse 0.00336
wandb:                                    test_unseen_single_mse_de 0.00865
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00759
wandb:                                   test_unseen_single_pearson 0.98386
wandb:                                test_unseen_single_pearson_de 0.99878
wandb:                             test_unseen_single_pearson_delta 0.82534
wandb:                                                 train_de_mse 0.01082
wandb:                                             train_de_pearson 0.9798
wandb:                                                    train_mse 0.00162
wandb:                                                train_pearson 0.99277
wandb:                                                training_loss 0.29311
wandb:                                                   val_de_mse 0.01456
wandb:                                               val_de_pearson 0.9967
wandb:                                                      val_mse 0.00193
wandb:                                                  val_pearson 0.99146
wandb: 
wandb: ðŸš€ View run ShifrutMarson2018_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/t083mzsi
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_084007-t083mzsi/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/shifrutmarson2018/splits/shifrutmarson2018_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_085926-o2k40i40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/o2k40i40
Start Training...
Epoch 1 Step 1 Train Loss: 0.7344
Epoch 1 Step 51 Train Loss: 0.3247
Epoch 1 Step 101 Train Loss: 0.3709
Epoch 1 Step 151 Train Loss: 0.3389
Epoch 1 Step 201 Train Loss: 0.3444
Epoch 1 Step 251 Train Loss: 0.3421
Epoch 1 Step 301 Train Loss: 0.3620
Epoch 1 Step 351 Train Loss: 0.3220
Epoch 1 Step 401 Train Loss: 0.3307
Epoch 1 Step 451 Train Loss: 0.3255
Epoch 1 Step 501 Train Loss: 0.3425
Epoch 1 Step 551 Train Loss: 0.3399
Epoch 1 Step 601 Train Loss: 0.3560
Epoch 1 Step 651 Train Loss: 0.2868
Epoch 1: Train Overall MSE: 0.0049 Validation Overall MSE: 0.0080. 
Train Top 20 DE MSE: 0.0179 Validation Top 20 DE MSE: 0.0180. 
Epoch 2 Step 1 Train Loss: 0.3633
Epoch 2 Step 51 Train Loss: 0.2971
Epoch 2 Step 101 Train Loss: 0.3791
Epoch 2 Step 151 Train Loss: 0.2708
Epoch 2 Step 201 Train Loss: 0.3092
Epoch 2 Step 251 Train Loss: 0.2203
Epoch 2 Step 301 Train Loss: 0.3736
Epoch 2 Step 351 Train Loss: 0.3099
Epoch 2 Step 401 Train Loss: 0.3066
Epoch 2 Step 451 Train Loss: 0.2639
Epoch 2 Step 501 Train Loss: 0.3824
Epoch 2 Step 551 Train Loss: 0.2741
Epoch 2 Step 601 Train Loss: 0.2749
Epoch 2 Step 651 Train Loss: 0.3930
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0517 Validation Top 20 DE MSE: 0.0420. 
Epoch 3 Step 1 Train Loss: 0.3345
Epoch 3 Step 51 Train Loss: 0.3087
Epoch 3 Step 101 Train Loss: 0.2812
Epoch 3 Step 151 Train Loss: 0.2768
Epoch 3 Step 201 Train Loss: 0.2714
Epoch 3 Step 251 Train Loss: 0.3435
Epoch 3 Step 301 Train Loss: 0.3170
Epoch 3 Step 351 Train Loss: 0.3115
Epoch 3 Step 401 Train Loss: 0.2969
Epoch 3 Step 451 Train Loss: 0.3008
Epoch 3 Step 501 Train Loss: 0.3268
Epoch 3 Step 551 Train Loss: 0.3013
Epoch 3 Step 601 Train Loss: 0.3101
Epoch 3 Step 651 Train Loss: 0.2665
Epoch 3: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.0126. 
Epoch 4 Step 1 Train Loss: 0.3036
Epoch 4 Step 51 Train Loss: 0.3376
Epoch 4 Step 101 Train Loss: 0.3804
Epoch 4 Step 151 Train Loss: 0.2942
Epoch 4 Step 201 Train Loss: 0.2630
Epoch 4 Step 251 Train Loss: 0.2831
Epoch 4 Step 301 Train Loss: 0.3481
Epoch 4 Step 351 Train Loss: 0.2871
Epoch 4 Step 401 Train Loss: 0.2833
Epoch 4 Step 451 Train Loss: 0.3528
Epoch 4 Step 501 Train Loss: 0.3353
Epoch 4 Step 551 Train Loss: 0.3153
Epoch 4 Step 601 Train Loss: 0.3021
Epoch 4 Step 651 Train Loss: 0.2383
Epoch 4: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0041. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0125. 
Epoch 5 Step 1 Train Loss: 0.3131
Epoch 5 Step 51 Train Loss: 0.3034
Epoch 5 Step 101 Train Loss: 0.2817
Epoch 5 Step 151 Train Loss: 0.3463
Epoch 5 Step 201 Train Loss: 0.2742
Epoch 5 Step 251 Train Loss: 0.3134
Epoch 5 Step 301 Train Loss: 0.3076
Epoch 5 Step 351 Train Loss: 0.2960
Epoch 5 Step 401 Train Loss: 0.3080
Epoch 5 Step 451 Train Loss: 0.3236
Epoch 5 Step 501 Train Loss: 0.2540
Epoch 5 Step 551 Train Loss: 0.3059
Epoch 5 Step 601 Train Loss: 0.2387
Epoch 5 Step 651 Train Loss: 0.2585
Epoch 5: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0099. 
Epoch 6 Step 1 Train Loss: 0.2850
Epoch 6 Step 51 Train Loss: 0.2880
Epoch 6 Step 101 Train Loss: 0.3285
Epoch 6 Step 151 Train Loss: 0.3205
Epoch 6 Step 201 Train Loss: 0.2864
Epoch 6 Step 251 Train Loss: 0.3337
Epoch 6 Step 301 Train Loss: 0.3121
Epoch 6 Step 351 Train Loss: 0.3268
Epoch 6 Step 401 Train Loss: 0.2718
Epoch 6 Step 451 Train Loss: 0.3615
Epoch 6 Step 501 Train Loss: 0.3177
Epoch 6 Step 551 Train Loss: 0.3452
Epoch 6 Step 601 Train Loss: 0.2535
Epoch 6 Step 651 Train Loss: 0.2582
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0106. 
Epoch 7 Step 1 Train Loss: 0.3567
Epoch 7 Step 51 Train Loss: 0.2955
Epoch 7 Step 101 Train Loss: 0.3015
Epoch 7 Step 151 Train Loss: 0.2848
Epoch 7 Step 201 Train Loss: 0.3075
Epoch 7 Step 251 Train Loss: 0.2904
Epoch 7 Step 301 Train Loss: 0.2795
Epoch 7 Step 351 Train Loss: 0.3445
Epoch 7 Step 401 Train Loss: 0.2505
Epoch 7 Step 451 Train Loss: 0.3108
Epoch 7 Step 501 Train Loss: 0.2242
Epoch 7 Step 551 Train Loss: 0.2243
Epoch 7 Step 601 Train Loss: 0.3032
Epoch 7 Step 651 Train Loss: 0.2847
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0098. 
Epoch 8 Step 1 Train Loss: 0.2653
Epoch 8 Step 51 Train Loss: 0.3083
Epoch 8 Step 101 Train Loss: 0.2563
Epoch 8 Step 151 Train Loss: 0.2579
Epoch 8 Step 201 Train Loss: 0.3642
Epoch 8 Step 251 Train Loss: 0.2987
Epoch 8 Step 301 Train Loss: 0.2739
Epoch 8 Step 351 Train Loss: 0.2979
Epoch 8 Step 401 Train Loss: 0.3150
Epoch 8 Step 451 Train Loss: 0.2449
Epoch 8 Step 501 Train Loss: 0.2234
Epoch 8 Step 551 Train Loss: 0.2754
Epoch 8 Step 601 Train Loss: 0.2513
Epoch 8 Step 651 Train Loss: 0.2946
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0108. 
Epoch 9 Step 1 Train Loss: 0.2981
Epoch 9 Step 51 Train Loss: 0.3046
Epoch 9 Step 101 Train Loss: 0.2727
Epoch 9 Step 151 Train Loss: 0.3308
Epoch 9 Step 201 Train Loss: 0.3266
Epoch 9 Step 251 Train Loss: 0.3359
Epoch 9 Step 301 Train Loss: 0.2721
Epoch 9 Step 351 Train Loss: 0.2748
Epoch 9 Step 401 Train Loss: 0.3005
Epoch 9 Step 451 Train Loss: 0.2813
Epoch 9 Step 501 Train Loss: 0.2711
Epoch 9 Step 551 Train Loss: 0.3113
Epoch 9 Step 601 Train Loss: 0.3219
Epoch 9 Step 651 Train Loss: 0.3087
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0104. 
Epoch 10 Step 1 Train Loss: 0.3100
Epoch 10 Step 51 Train Loss: 0.3003
Epoch 10 Step 101 Train Loss: 0.2667
Epoch 10 Step 151 Train Loss: 0.2759
Epoch 10 Step 201 Train Loss: 0.2952
Epoch 10 Step 251 Train Loss: 0.2628
Epoch 10 Step 301 Train Loss: 0.3141
Epoch 10 Step 351 Train Loss: 0.2758
Epoch 10 Step 401 Train Loss: 0.3595
Epoch 10 Step 451 Train Loss: 0.3161
Epoch 10 Step 501 Train Loss: 0.3248
Epoch 10 Step 551 Train Loss: 0.2746
Epoch 10 Step 601 Train Loss: 0.2456
Epoch 10 Step 651 Train Loss: 0.2054
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0103. 
Epoch 11 Step 1 Train Loss: 0.2935
Epoch 11 Step 51 Train Loss: 0.3640
Epoch 11 Step 101 Train Loss: 0.2968
Epoch 11 Step 151 Train Loss: 0.3239
Epoch 11 Step 201 Train Loss: 0.2351
Epoch 11 Step 251 Train Loss: 0.2941
Epoch 11 Step 301 Train Loss: 0.3300
Epoch 11 Step 351 Train Loss: 0.3245
Epoch 11 Step 401 Train Loss: 0.3066
Epoch 11 Step 451 Train Loss: 0.3136
Epoch 11 Step 501 Train Loss: 0.2807
Epoch 11 Step 551 Train Loss: 0.2997
Epoch 11 Step 601 Train Loss: 0.2958
Epoch 11 Step 651 Train Loss: 0.2746
Epoch 11: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0104. 
Epoch 12 Step 1 Train Loss: 0.3129
Epoch 12 Step 51 Train Loss: 0.3054
Epoch 12 Step 101 Train Loss: 0.2879
Epoch 12 Step 151 Train Loss: 0.2806
Epoch 12 Step 201 Train Loss: 0.2914
Epoch 12 Step 251 Train Loss: 0.3131
Epoch 12 Step 301 Train Loss: 0.2574
Epoch 12 Step 351 Train Loss: 0.2486
Epoch 12 Step 401 Train Loss: 0.2848
Epoch 12 Step 451 Train Loss: 0.2762
Epoch 12 Step 501 Train Loss: 0.3311
Epoch 12 Step 551 Train Loss: 0.2962
Epoch 12 Step 601 Train Loss: 0.3415
Epoch 12 Step 651 Train Loss: 0.2576
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.0103. 
Epoch 13 Step 1 Train Loss: 0.3186
Epoch 13 Step 51 Train Loss: 0.2667
Epoch 13 Step 101 Train Loss: 0.3393
Epoch 13 Step 151 Train Loss: 0.2985
Epoch 13 Step 201 Train Loss: 0.2770
Epoch 13 Step 251 Train Loss: 0.2522
Epoch 13 Step 301 Train Loss: 0.2756
Epoch 13 Step 351 Train Loss: 0.3148
Epoch 13 Step 401 Train Loss: 0.2472
Epoch 13 Step 451 Train Loss: 0.3153
Epoch 13 Step 501 Train Loss: 0.2751
Epoch 13 Step 551 Train Loss: 0.2520
Epoch 13 Step 601 Train Loss: 0.3601
Epoch 13 Step 651 Train Loss: 0.2402
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0102. 
Epoch 14 Step 1 Train Loss: 0.3434
Epoch 14 Step 51 Train Loss: 0.3028
Epoch 14 Step 101 Train Loss: 0.3230
Epoch 14 Step 151 Train Loss: 0.3338
Epoch 14 Step 201 Train Loss: 0.2802
Epoch 14 Step 251 Train Loss: 0.3240
Epoch 14 Step 301 Train Loss: 0.3429
Epoch 14 Step 351 Train Loss: 0.3368
Epoch 14 Step 401 Train Loss: 0.2574
Epoch 14 Step 451 Train Loss: 0.2901
Epoch 14 Step 501 Train Loss: 0.2907
Epoch 14 Step 551 Train Loss: 0.2722
Epoch 14 Step 601 Train Loss: 0.3386
Epoch 14 Step 651 Train Loss: 0.2877
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0101. 
Epoch 15 Step 1 Train Loss: 0.3028
Epoch 15 Step 51 Train Loss: 0.2379
Epoch 15 Step 101 Train Loss: 0.2850
Epoch 15 Step 151 Train Loss: 0.3355
Epoch 15 Step 201 Train Loss: 0.3341
Epoch 15 Step 251 Train Loss: 0.2216
Epoch 15 Step 301 Train Loss: 0.2426
Epoch 15 Step 351 Train Loss: 0.2876
Epoch 15 Step 401 Train Loss: 0.3090
Epoch 15 Step 451 Train Loss: 0.3327
Epoch 15 Step 501 Train Loss: 0.2687
Epoch 15 Step 551 Train Loss: 0.2976
Epoch 15 Step 601 Train Loss: 0.2934
Epoch 15 Step 651 Train Loss: 0.2923
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.0104. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0123
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.001901042
test_unseen_single_pearson: 0.9912013166600261
test_unseen_single_mse_de: 0.01226957
test_unseen_single_pearson_de: 0.9966712390393202
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8576614869711818
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9400000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.012109871
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.001 MB of 0.024 MB uploadedwandb: / 0.018 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–‚â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–ˆâ–…â–…â–ˆâ–‡â–‡â–†â–†â–†â–‡â–‡â–‡â–†
wandb:                                                    train_mse â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–…â–„â–„â–â–„â–…â–„â–‡â–‚â–†â–…â–†â–„â–„â–ƒâ–…â–â–ƒâ–ƒâ–…â–‡â–â–„â–„â–â–…â–ˆâ–†â–‡â–‚â–â–ƒâ–â–„â–†â–‡â–…â–‡â–†â–†
wandb:                                                   val_de_mse â–ƒâ–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–‡â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–‚â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–‡â–†â–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01227
wandb:                                              test_de_pearson 0.99667
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.94
wandb:                                                     test_mse 0.0019
wandb:                                test_mse_top20_de_non_dropout 0.01211
wandb:                                                 test_pearson 0.9912
wandb:                                           test_pearson_delta 0.85766
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.94
wandb:                                       test_unseen_single_mse 0.0019
wandb:                                    test_unseen_single_mse_de 0.01227
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01211
wandb:                                   test_unseen_single_pearson 0.9912
wandb:                                test_unseen_single_pearson_de 0.99667
wandb:                             test_unseen_single_pearson_delta 0.85766
wandb:                                                 train_de_mse 0.00972
wandb:                                             train_de_pearson 0.98494
wandb:                                                    train_mse 0.00165
wandb:                                                train_pearson 0.99234
wandb:                                                training_loss 0.30886
wandb:                                                   val_de_mse 0.01041
wandb:                                               val_de_pearson 0.99866
wandb:                                                      val_mse 0.00231
wandb:                                                  val_pearson 0.98934
wandb: 
wandb: ðŸš€ View run ShifrutMarson2018_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/o2k40i40
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_085926-o2k40i40/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/shifrutmarson2018/splits/shifrutmarson2018_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_091824-u42memjq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u42memjq
Start Training...
Epoch 1 Step 1 Train Loss: 1.1718
Epoch 1 Step 51 Train Loss: 0.3978
Epoch 1 Step 101 Train Loss: 0.3918
Epoch 1 Step 151 Train Loss: 0.3045
Epoch 1 Step 201 Train Loss: 0.4173
Epoch 1 Step 251 Train Loss: 0.3700
Epoch 1 Step 301 Train Loss: 0.4273
Epoch 1 Step 351 Train Loss: 0.3496
Epoch 1 Step 401 Train Loss: 0.3318
Epoch 1 Step 451 Train Loss: 0.2902
Epoch 1 Step 501 Train Loss: 0.3648
Epoch 1 Step 551 Train Loss: 0.3303
Epoch 1 Step 601 Train Loss: 0.3300
Epoch 1 Step 651 Train Loss: 0.2878
Epoch 1 Step 701 Train Loss: 0.3238
Epoch 1: Train Overall MSE: 0.0040 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0189 Validation Top 20 DE MSE: 0.0137. 
Epoch 2 Step 1 Train Loss: 0.2906
Epoch 2 Step 51 Train Loss: 0.2921
Epoch 2 Step 101 Train Loss: 0.3149
Epoch 2 Step 151 Train Loss: 0.3126
Epoch 2 Step 201 Train Loss: 0.3089
Epoch 2 Step 251 Train Loss: 0.3161
Epoch 2 Step 301 Train Loss: 0.3123
Epoch 2 Step 351 Train Loss: 0.2755
Epoch 2 Step 401 Train Loss: 0.3157
Epoch 2 Step 451 Train Loss: 0.3127
Epoch 2 Step 501 Train Loss: 0.3137
Epoch 2 Step 551 Train Loss: 0.3603
Epoch 2 Step 601 Train Loss: 0.3091
Epoch 2 Step 651 Train Loss: 0.3415
Epoch 2 Step 701 Train Loss: 0.2787
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.0155 Validation Top 20 DE MSE: 0.0141. 
Epoch 3 Step 1 Train Loss: 0.2967
Epoch 3 Step 51 Train Loss: 0.3184
Epoch 3 Step 101 Train Loss: 0.2727
Epoch 3 Step 151 Train Loss: 0.2811
Epoch 3 Step 201 Train Loss: 0.2873
Epoch 3 Step 251 Train Loss: 0.2713
Epoch 3 Step 301 Train Loss: 0.2991
Epoch 3 Step 351 Train Loss: 0.3220
Epoch 3 Step 401 Train Loss: 0.2491
Epoch 3 Step 451 Train Loss: 0.3518
Epoch 3 Step 501 Train Loss: 0.2891
Epoch 3 Step 551 Train Loss: 0.2959
Epoch 3 Step 601 Train Loss: 0.3067
Epoch 3 Step 651 Train Loss: 0.2757
Epoch 3 Step 701 Train Loss: 0.2927
Epoch 3: Train Overall MSE: 0.0092 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.1419 Validation Top 20 DE MSE: 0.0510. 
Epoch 4 Step 1 Train Loss: 0.3721
Epoch 4 Step 51 Train Loss: 0.2802
Epoch 4 Step 101 Train Loss: 0.3216
Epoch 4 Step 151 Train Loss: 0.2780
Epoch 4 Step 201 Train Loss: 0.2752
Epoch 4 Step 251 Train Loss: 0.3167
Epoch 4 Step 301 Train Loss: 0.3538
Epoch 4 Step 351 Train Loss: 0.2589
Epoch 4 Step 401 Train Loss: 0.3640
Epoch 4 Step 451 Train Loss: 0.3280
Epoch 4 Step 501 Train Loss: 0.3267
Epoch 4 Step 551 Train Loss: 0.3857
Epoch 4 Step 601 Train Loss: 0.2760
Epoch 4 Step 651 Train Loss: 0.3061
Epoch 4 Step 701 Train Loss: 0.3445
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0174 Validation Top 20 DE MSE: 0.0163. 
Epoch 5 Step 1 Train Loss: 0.2844
Epoch 5 Step 51 Train Loss: 0.2832
Epoch 5 Step 101 Train Loss: 0.3125
Epoch 5 Step 151 Train Loss: 0.3218
Epoch 5 Step 201 Train Loss: 0.2356
Epoch 5 Step 251 Train Loss: 0.2439
Epoch 5 Step 301 Train Loss: 0.3456
Epoch 5 Step 351 Train Loss: 0.3294
Epoch 5 Step 401 Train Loss: 0.2630
Epoch 5 Step 451 Train Loss: 0.3856
Epoch 5 Step 501 Train Loss: 0.3231
Epoch 5 Step 551 Train Loss: 0.3165
Epoch 5 Step 601 Train Loss: 0.2817
Epoch 5 Step 651 Train Loss: 0.3036
Epoch 5 Step 701 Train Loss: 0.2802
Epoch 5: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0171 Validation Top 20 DE MSE: 0.0268. 
Epoch 6 Step 1 Train Loss: 0.3124
Epoch 6 Step 51 Train Loss: 0.2574
Epoch 6 Step 101 Train Loss: 0.2484
Epoch 6 Step 151 Train Loss: 0.3286
Epoch 6 Step 201 Train Loss: 0.2508
Epoch 6 Step 251 Train Loss: 0.2565
Epoch 6 Step 301 Train Loss: 0.3293
Epoch 6 Step 351 Train Loss: 0.3019
Epoch 6 Step 401 Train Loss: 0.2780
Epoch 6 Step 451 Train Loss: 0.3053
Epoch 6 Step 501 Train Loss: 0.2785
Epoch 6 Step 551 Train Loss: 0.2862
Epoch 6 Step 601 Train Loss: 0.2373
Epoch 6 Step 651 Train Loss: 0.2907
Epoch 6 Step 701 Train Loss: 0.2541
Epoch 6: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0169. 
Epoch 7 Step 1 Train Loss: 0.2711
Epoch 7 Step 51 Train Loss: 0.3342
Epoch 7 Step 101 Train Loss: 0.3316
Epoch 7 Step 151 Train Loss: 0.3031
Epoch 7 Step 201 Train Loss: 0.2866
Epoch 7 Step 251 Train Loss: 0.2703
Epoch 7 Step 301 Train Loss: 0.2555
Epoch 7 Step 351 Train Loss: 0.2862
Epoch 7 Step 401 Train Loss: 0.2606
Epoch 7 Step 451 Train Loss: 0.2600
Epoch 7 Step 501 Train Loss: 0.2806
Epoch 7 Step 551 Train Loss: 0.2335
Epoch 7 Step 601 Train Loss: 0.3142
Epoch 7 Step 651 Train Loss: 0.3189
Epoch 7 Step 701 Train Loss: 0.3073
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0116 Validation Top 20 DE MSE: 0.0176. 
Epoch 8 Step 1 Train Loss: 0.2917
Epoch 8 Step 51 Train Loss: 0.2600
Epoch 8 Step 101 Train Loss: 0.2954
Epoch 8 Step 151 Train Loss: 0.3205
Epoch 8 Step 201 Train Loss: 0.3063
Epoch 8 Step 251 Train Loss: 0.3068
Epoch 8 Step 301 Train Loss: 0.2255
Epoch 8 Step 351 Train Loss: 0.2814
Epoch 8 Step 401 Train Loss: 0.2773
Epoch 8 Step 451 Train Loss: 0.3250
Epoch 8 Step 501 Train Loss: 0.2688
Epoch 8 Step 551 Train Loss: 0.2670
Epoch 8 Step 601 Train Loss: 0.2381
Epoch 8 Step 651 Train Loss: 0.2836
Epoch 8 Step 701 Train Loss: 0.3189
Epoch 8: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0135. 
Epoch 9 Step 1 Train Loss: 0.2608
Epoch 9 Step 51 Train Loss: 0.3083
Epoch 9 Step 101 Train Loss: 0.3332
Epoch 9 Step 151 Train Loss: 0.2833
Epoch 9 Step 201 Train Loss: 0.3130
Epoch 9 Step 251 Train Loss: 0.2690
Epoch 9 Step 301 Train Loss: 0.2811
Epoch 9 Step 351 Train Loss: 0.3120
Epoch 9 Step 401 Train Loss: 0.3127
Epoch 9 Step 451 Train Loss: 0.2686
Epoch 9 Step 501 Train Loss: 0.3269
Epoch 9 Step 551 Train Loss: 0.2684
Epoch 9 Step 601 Train Loss: 0.3198
Epoch 9 Step 651 Train Loss: 0.2723
Epoch 9 Step 701 Train Loss: 0.3094
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0124. 
Epoch 10 Step 1 Train Loss: 0.2928
Epoch 10 Step 51 Train Loss: 0.3148
Epoch 10 Step 101 Train Loss: 0.2697
Epoch 10 Step 151 Train Loss: 0.3099
Epoch 10 Step 201 Train Loss: 0.2749
Epoch 10 Step 251 Train Loss: 0.3023
Epoch 10 Step 301 Train Loss: 0.3021
Epoch 10 Step 351 Train Loss: 0.2924
Epoch 10 Step 401 Train Loss: 0.2725
Epoch 10 Step 451 Train Loss: 0.2996
Epoch 10 Step 501 Train Loss: 0.3333
Epoch 10 Step 551 Train Loss: 0.2622
Epoch 10 Step 601 Train Loss: 0.3154
Epoch 10 Step 651 Train Loss: 0.3080
Epoch 10 Step 701 Train Loss: 0.2911
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0133. 
Epoch 11 Step 1 Train Loss: 0.2392
Epoch 11 Step 51 Train Loss: 0.3378
Epoch 11 Step 101 Train Loss: 0.3084
Epoch 11 Step 151 Train Loss: 0.3538
Epoch 11 Step 201 Train Loss: 0.2849
Epoch 11 Step 251 Train Loss: 0.2943
Epoch 11 Step 301 Train Loss: 0.3188
Epoch 11 Step 351 Train Loss: 0.2955
Epoch 11 Step 401 Train Loss: 0.2396
Epoch 11 Step 451 Train Loss: 0.3403
Epoch 11 Step 501 Train Loss: 0.3227
Epoch 11 Step 551 Train Loss: 0.3567
Epoch 11 Step 601 Train Loss: 0.3345
Epoch 11 Step 651 Train Loss: 0.3171
Epoch 11 Step 701 Train Loss: 0.2939
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0121. 
Epoch 12 Step 1 Train Loss: 0.2840
Epoch 12 Step 51 Train Loss: 0.2928
Epoch 12 Step 101 Train Loss: 0.2625
Epoch 12 Step 151 Train Loss: 0.2932
Epoch 12 Step 201 Train Loss: 0.2474
Epoch 12 Step 251 Train Loss: 0.2725
Epoch 12 Step 301 Train Loss: 0.2858
Epoch 12 Step 351 Train Loss: 0.2852
Epoch 12 Step 401 Train Loss: 0.3013
Epoch 12 Step 451 Train Loss: 0.2867
Epoch 12 Step 501 Train Loss: 0.3084
Epoch 12 Step 551 Train Loss: 0.2998
Epoch 12 Step 601 Train Loss: 0.2749
Epoch 12 Step 651 Train Loss: 0.3317
Epoch 12 Step 701 Train Loss: 0.2810
Epoch 12: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.0139. 
Epoch 13 Step 1 Train Loss: 0.3228
Epoch 13 Step 51 Train Loss: 0.3016
Epoch 13 Step 101 Train Loss: 0.3489
Epoch 13 Step 151 Train Loss: 0.3370
Epoch 13 Step 201 Train Loss: 0.3068
Epoch 13 Step 251 Train Loss: 0.2959
Epoch 13 Step 301 Train Loss: 0.2698
Epoch 13 Step 351 Train Loss: 0.2987
Epoch 13 Step 401 Train Loss: 0.3198
Epoch 13 Step 451 Train Loss: 0.2751
Epoch 13 Step 501 Train Loss: 0.2864
Epoch 13 Step 551 Train Loss: 0.3030
Epoch 13 Step 601 Train Loss: 0.2839
Epoch 13 Step 651 Train Loss: 0.3323
Epoch 13 Step 701 Train Loss: 0.2918
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0156. 
Epoch 14 Step 1 Train Loss: 0.3312
Epoch 14 Step 51 Train Loss: 0.3355
Epoch 14 Step 101 Train Loss: 0.2923
Epoch 14 Step 151 Train Loss: 0.3126
Epoch 14 Step 201 Train Loss: 0.2832
Epoch 14 Step 251 Train Loss: 0.3370
Epoch 14 Step 301 Train Loss: 0.2666
Epoch 14 Step 351 Train Loss: 0.3427
Epoch 14 Step 401 Train Loss: 0.2608
Epoch 14 Step 451 Train Loss: 0.3126
Epoch 14 Step 501 Train Loss: 0.2420
Epoch 14 Step 551 Train Loss: 0.3122
Epoch 14 Step 601 Train Loss: 0.2888
Epoch 14 Step 651 Train Loss: 0.3011
Epoch 14 Step 701 Train Loss: 0.3074
Epoch 14: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0111 Validation Top 20 DE MSE: 0.0129. 
Epoch 15 Step 1 Train Loss: 0.3446
Epoch 15 Step 51 Train Loss: 0.2942
Epoch 15 Step 101 Train Loss: 0.2733
Epoch 15 Step 151 Train Loss: 0.3421
Epoch 15 Step 201 Train Loss: 0.2929
Epoch 15 Step 251 Train Loss: 0.2522
Epoch 15 Step 301 Train Loss: 0.3244
Epoch 15 Step 351 Train Loss: 0.2751
Epoch 15 Step 401 Train Loss: 0.2883
Epoch 15 Step 451 Train Loss: 0.3157
Epoch 15 Step 501 Train Loss: 0.2991
Epoch 15 Step 551 Train Loss: 0.2579
Epoch 15 Step 601 Train Loss: 0.2900
Epoch 15 Step 651 Train Loss: 0.2750
Epoch 15 Step 701 Train Loss: 0.3251
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.0132. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0193
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0020268455
test_unseen_single_pearson: 0.9905413999113509
test_unseen_single_mse_de: 0.019306991
test_unseen_single_pearson_de: 0.9642062324017109
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.83199674435616
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9400000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.023856923
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–‡â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ƒâ–‚â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–…â–†â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–„â–ˆâ–†â–ƒâ–‡â–†â–†â–„â–…â–ƒâ–„â–„â–†â–‚â–„â–ƒâ–…â–„â–‡â–…â–„â–â–‚â–…â–ƒâ–†â–†â–ƒâ–…â–„â–„â–„â–„â–‚â–â–ƒâ–…â–ƒâ–„â–ƒ
wandb:                                                   val_de_mse â–â–â–ˆâ–‚â–„â–‚â–‚â–â–â–â–â–â–‚â–â–
wandb:                                               val_de_pearson â–‡â–‡â–â–‡â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–…â–„â–‚â–â–‚â–â–‚â–â–â–â–â–‚â–â–
wandb:                                                  val_pearson â–â–„â–„â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01931
wandb:                                              test_de_pearson 0.96421
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.94
wandb:                                                     test_mse 0.00203
wandb:                                test_mse_top20_de_non_dropout 0.02386
wandb:                                                 test_pearson 0.99054
wandb:                                           test_pearson_delta 0.832
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.94
wandb:                                       test_unseen_single_mse 0.00203
wandb:                                    test_unseen_single_mse_de 0.01931
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.02386
wandb:                                   test_unseen_single_pearson 0.99054
wandb:                                test_unseen_single_pearson_de 0.96421
wandb:                             test_unseen_single_pearson_delta 0.832
wandb:                                                 train_de_mse 0.01019
wandb:                                             train_de_pearson 0.99807
wandb:                                                    train_mse 0.00163
wandb:                                                train_pearson 0.99265
wandb:                                                training_loss 0.28813
wandb:                                                   val_de_mse 0.01317
wandb:                                               val_de_pearson 0.99611
wandb:                                                      val_mse 0.002
wandb:                                                  val_pearson 0.99067
wandb: 
wandb: ðŸš€ View run ShifrutMarson2018_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u42memjq
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_091824-u42memjq/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/9 [00:00<?, ?it/s] 11%|â–ˆ         | 1/9 [00:09<01:19,  9.92s/it] 22%|â–ˆâ–ˆâ–       | 2/9 [00:22<01:21, 11.70s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:36<01:14, 12.37s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:36<00:37,  7.53s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:42<00:28,  7.14s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:52<00:23,  7.99s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:14<00:25, 12.50s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:14<00:08,  8.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:26<00:00,  9.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:26<00:00,  9.57s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_arrayed_rna/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_094005-cjolcgyl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/cjolcgyl
  0%|          | 0/4744 [00:00<?, ?it/s]  0%|          | 4/4744 [00:00<02:19, 33.96it/s]  0%|          | 10/4744 [00:00<01:44, 45.47it/s]  0%|          | 16/4744 [00:00<01:35, 49.73it/s]  0%|          | 22/4744 [00:00<01:30, 51.94it/s]  1%|          | 29/4744 [00:00<01:23, 56.79it/s]  1%|          | 35/4744 [00:00<01:23, 56.64it/s]  1%|          | 41/4744 [00:00<01:22, 56.83it/s]  1%|          | 47/4744 [00:00<01:21, 57.52it/s]  1%|          | 53/4744 [00:00<01:21, 57.53it/s]  1%|          | 59/4744 [00:01<01:21, 57.32it/s]  1%|â–         | 65/4744 [00:01<01:21, 57.56it/s]  1%|â–         | 71/4744 [00:01<01:21, 57.17it/s]  2%|â–         | 77/4744 [00:01<01:21, 57.01it/s]  2%|â–         | 83/4744 [00:01<01:21, 57.19it/s]  2%|â–         | 89/4744 [00:01<01:20, 57.58it/s]  2%|â–         | 95/4744 [00:01<01:21, 57.36it/s]  2%|â–         | 101/4744 [00:01<01:21, 57.18it/s]  2%|â–         | 107/4744 [00:01<01:21, 56.98it/s]  2%|â–         | 113/4744 [00:02<01:20, 57.22it/s]  3%|â–Ž         | 119/4744 [00:02<01:21, 56.94it/s]  3%|â–Ž         | 125/4744 [00:02<01:21, 56.82it/s]  3%|â–Ž         | 131/4744 [00:02<01:20, 56.96it/s]  3%|â–Ž         | 137/4744 [00:02<01:20, 57.24it/s]  3%|â–Ž         | 143/4744 [00:02<01:20, 57.32it/s]  3%|â–Ž         | 149/4744 [00:02<01:19, 57.48it/s]  3%|â–Ž         | 155/4744 [00:02<01:19, 57.47it/s]  3%|â–Ž         | 161/4744 [00:02<01:20, 57.25it/s]  4%|â–Ž         | 167/4744 [00:02<01:20, 57.03it/s]  4%|â–Ž         | 173/4744 [00:03<01:20, 56.65it/s]  4%|â–         | 179/4744 [00:03<01:20, 56.89it/s]  4%|â–         | 185/4744 [00:03<01:19, 57.11it/s]  4%|â–         | 191/4744 [00:03<01:19, 57.36it/s]  4%|â–         | 197/4744 [00:03<01:19, 56.87it/s]  4%|â–         | 203/4744 [00:04<02:57, 25.52it/s]  4%|â–         | 208/4744 [00:04<03:09, 23.97it/s]  4%|â–         | 213/4744 [00:04<02:46, 27.20it/s]  5%|â–         | 218/4744 [00:04<02:28, 30.41it/s]  5%|â–         | 225/4744 [00:04<02:01, 37.30it/s]  5%|â–Œ         | 246/4744 [00:04<01:01, 73.48it/s]  5%|â–Œ         | 256/4744 [00:04<01:06, 67.95it/s]  6%|â–Œ         | 265/4744 [00:05<01:09, 64.79it/s]  6%|â–Œ         | 273/4744 [00:05<01:11, 62.39it/s]  6%|â–Œ         | 280/4744 [00:05<01:12, 61.36it/s]  6%|â–Œ         | 287/4744 [00:05<01:14, 60.10it/s]  6%|â–Œ         | 294/4744 [00:05<01:15, 58.98it/s]  6%|â–‹         | 301/4744 [00:05<01:15, 58.57it/s]  6%|â–‹         | 307/4744 [00:05<01:16, 58.31it/s]  7%|â–‹         | 313/4744 [00:05<01:15, 58.38it/s]  7%|â–‹         | 319/4744 [00:05<01:16, 57.87it/s]  7%|â–‹         | 325/4744 [00:06<01:16, 57.63it/s]  7%|â–‹         | 331/4744 [00:06<01:16, 57.76it/s]  7%|â–‹         | 337/4744 [00:06<01:16, 57.56it/s]  7%|â–‹         | 343/4744 [00:06<01:16, 57.35it/s]  7%|â–‹         | 349/4744 [00:06<01:16, 57.10it/s]  7%|â–‹         | 355/4744 [00:06<01:16, 57.28it/s]  8%|â–Š         | 361/4744 [00:06<01:19, 55.29it/s]  8%|â–Š         | 367/4744 [00:06<01:17, 56.38it/s]  8%|â–Š         | 373/4744 [00:06<01:16, 57.01it/s]  8%|â–Š         | 379/4744 [00:07<01:15, 57.54it/s]  8%|â–Š         | 385/4744 [00:07<01:15, 57.46it/s]  8%|â–Š         | 391/4744 [00:07<01:16, 57.23it/s]  8%|â–Š         | 397/4744 [00:07<01:15, 57.30it/s]  8%|â–Š         | 403/4744 [00:07<01:16, 57.12it/s]  9%|â–Š         | 409/4744 [00:07<01:15, 57.37it/s]  9%|â–Š         | 415/4744 [00:07<01:15, 57.38it/s]  9%|â–‰         | 421/4744 [00:07<01:15, 57.39it/s]  9%|â–‰         | 427/4744 [00:07<01:15, 57.03it/s]  9%|â–‰         | 433/4744 [00:07<01:14, 57.61it/s]  9%|â–‰         | 439/4744 [00:08<01:15, 56.69it/s]  9%|â–‰         | 445/4744 [00:08<01:23, 51.60it/s] 10%|â–‰         | 451/4744 [00:08<01:27, 48.96it/s] 10%|â–‰         | 456/4744 [00:08<01:29, 48.08it/s] 10%|â–‰         | 461/4744 [00:08<01:33, 45.59it/s] 10%|â–‰         | 467/4744 [00:08<01:27, 48.69it/s] 10%|â–‰         | 473/4744 [00:08<01:25, 50.15it/s] 10%|â–ˆ         | 479/4744 [00:08<01:24, 50.62it/s] 10%|â–ˆ         | 485/4744 [00:09<01:23, 50.75it/s] 10%|â–ˆ         | 491/4744 [00:09<01:23, 50.82it/s] 10%|â–ˆ         | 497/4744 [00:09<01:21, 51.97it/s] 11%|â–ˆ         | 503/4744 [00:09<01:19, 53.63it/s] 11%|â–ˆ         | 509/4744 [00:09<01:21, 52.01it/s] 11%|â–ˆ         | 515/4744 [00:09<01:22, 51.35it/s] 11%|â–ˆ         | 521/4744 [00:09<01:23, 50.61it/s] 11%|â–ˆ         | 527/4744 [00:09<01:23, 50.22it/s] 11%|â–ˆ         | 533/4744 [00:10<01:23, 50.33it/s] 11%|â–ˆâ–        | 539/4744 [00:10<01:24, 49.92it/s] 11%|â–ˆâ–        | 544/4744 [00:10<01:24, 49.90it/s] 12%|â–ˆâ–        | 549/4744 [00:10<01:24, 49.86it/s] 12%|â–ˆâ–        | 555/4744 [00:10<01:23, 50.14it/s] 12%|â–ˆâ–        | 561/4744 [00:10<01:22, 50.55it/s] 12%|â–ˆâ–        | 567/4744 [00:10<01:26, 48.46it/s] 12%|â–ˆâ–        | 574/4744 [00:10<01:18, 53.28it/s] 12%|â–ˆâ–        | 580/4744 [00:10<01:16, 54.40it/s] 12%|â–ˆâ–        | 586/4744 [00:11<01:15, 55.23it/s] 12%|â–ˆâ–        | 592/4744 [00:11<01:15, 54.87it/s] 13%|â–ˆâ–Ž        | 598/4744 [00:11<01:14, 55.32it/s] 13%|â–ˆâ–Ž        | 604/4744 [00:11<01:15, 54.85it/s] 13%|â–ˆâ–Ž        | 610/4744 [00:11<01:14, 55.57it/s] 13%|â–ˆâ–Ž        | 616/4744 [00:11<01:13, 56.49it/s] 13%|â–ˆâ–Ž        | 622/4744 [00:11<01:13, 56.40it/s] 13%|â–ˆâ–Ž        | 628/4744 [00:11<01:12, 56.88it/s] 13%|â–ˆâ–Ž        | 634/4744 [00:11<01:12, 56.58it/s] 13%|â–ˆâ–Ž        | 640/4744 [00:11<01:11, 57.24it/s] 14%|â–ˆâ–Ž        | 646/4744 [00:12<01:13, 55.89it/s] 14%|â–ˆâ–Ž        | 652/4744 [00:12<01:16, 53.81it/s] 14%|â–ˆâ–        | 658/4744 [00:12<01:15, 53.86it/s] 14%|â–ˆâ–        | 664/4744 [00:12<01:13, 55.19it/s] 14%|â–ˆâ–        | 670/4744 [00:12<01:12, 55.89it/s] 14%|â–ˆâ–        | 676/4744 [00:12<01:13, 55.72it/s] 14%|â–ˆâ–        | 682/4744 [00:12<01:13, 55.39it/s] 15%|â–ˆâ–        | 688/4744 [00:12<01:12, 55.66it/s] 15%|â–ˆâ–        | 694/4744 [00:12<01:12, 56.20it/s] 15%|â–ˆâ–        | 700/4744 [00:13<01:12, 55.73it/s] 15%|â–ˆâ–        | 706/4744 [00:13<01:16, 53.01it/s] 15%|â–ˆâ–Œ        | 713/4744 [00:13<01:11, 56.49it/s] 15%|â–ˆâ–Œ        | 719/4744 [00:13<01:11, 56.38it/s] 15%|â–ˆâ–Œ        | 725/4744 [00:13<01:11, 55.96it/s] 15%|â–ˆâ–Œ        | 731/4744 [00:13<01:11, 56.01it/s] 16%|â–ˆâ–Œ        | 737/4744 [00:13<01:12, 55.57it/s] 16%|â–ˆâ–Œ        | 743/4744 [00:13<01:11, 55.59it/s] 16%|â–ˆâ–Œ        | 749/4744 [00:13<01:11, 55.75it/s] 16%|â–ˆâ–Œ        | 755/4744 [00:14<01:11, 56.08it/s] 16%|â–ˆâ–Œ        | 761/4744 [00:14<01:11, 55.82it/s] 16%|â–ˆâ–Œ        | 767/4744 [00:14<01:11, 55.77it/s] 16%|â–ˆâ–‹        | 773/4744 [00:14<01:10, 56.19it/s] 16%|â–ˆâ–‹        | 779/4744 [00:14<01:10, 56.08it/s] 17%|â–ˆâ–‹        | 785/4744 [00:14<01:10, 55.95it/s] 17%|â–ˆâ–‹        | 791/4744 [00:14<01:10, 55.99it/s] 17%|â–ˆâ–‹        | 797/4744 [00:14<01:10, 56.15it/s] 17%|â–ˆâ–‹        | 803/4744 [00:14<01:10, 55.87it/s] 17%|â–ˆâ–‹        | 809/4744 [00:15<01:10, 55.91it/s] 17%|â–ˆâ–‹        | 815/4744 [00:15<01:10, 56.03it/s] 17%|â–ˆâ–‹        | 821/4744 [00:15<01:10, 55.99it/s] 17%|â–ˆâ–‹        | 827/4744 [00:15<01:09, 56.09it/s] 18%|â–ˆâ–Š        | 833/4744 [00:15<01:10, 55.84it/s] 18%|â–ˆâ–Š        | 839/4744 [00:15<01:09, 55.96it/s] 18%|â–ˆâ–Š        | 845/4744 [00:15<01:09, 55.82it/s] 18%|â–ˆâ–Š        | 851/4744 [00:15<01:09, 55.91it/s] 18%|â–ˆâ–Š        | 857/4744 [00:15<01:09, 55.84it/s] 18%|â–ˆâ–Š        | 863/4744 [00:15<01:09, 55.82it/s] 18%|â–ˆâ–Š        | 869/4744 [00:16<01:09, 55.93it/s] 18%|â–ˆâ–Š        | 875/4744 [00:16<01:09, 55.95it/s] 19%|â–ˆâ–Š        | 881/4744 [00:16<01:08, 56.09it/s] 19%|â–ˆâ–Š        | 887/4744 [00:16<01:09, 55.83it/s] 19%|â–ˆâ–‰        | 893/4744 [00:16<01:09, 55.75it/s] 19%|â–ˆâ–‰        | 899/4744 [00:16<01:08, 55.81it/s] 19%|â–ˆâ–‰        | 905/4744 [00:16<01:08, 56.15it/s] 19%|â–ˆâ–‰        | 911/4744 [00:16<01:10, 54.51it/s] 19%|â–ˆâ–‰        | 917/4744 [00:16<01:08, 55.54it/s] 19%|â–ˆâ–‰        | 923/4744 [00:17<01:11, 53.50it/s] 20%|â–ˆâ–‰        | 929/4744 [00:17<01:14, 51.19it/s] 20%|â–ˆâ–‰        | 935/4744 [00:17<01:13, 52.06it/s] 20%|â–ˆâ–‰        | 942/4744 [00:17<01:08, 55.65it/s] 20%|â–ˆâ–ˆ        | 949/4744 [00:17<01:05, 58.36it/s] 20%|â–ˆâ–ˆ        | 956/4744 [00:17<01:05, 57.68it/s] 20%|â–ˆâ–ˆ        | 964/4744 [00:17<01:00, 62.28it/s] 20%|â–ˆâ–ˆ        | 971/4744 [00:17<00:59, 63.06it/s] 21%|â–ˆâ–ˆ        | 978/4744 [00:17<00:59, 63.67it/s] 21%|â–ˆâ–ˆ        | 985/4744 [00:18<01:00, 61.85it/s] 21%|â–ˆâ–ˆ        | 992/4744 [00:18<01:00, 62.38it/s] 21%|â–ˆâ–ˆ        | 999/4744 [00:18<00:59, 62.70it/s] 21%|â–ˆâ–ˆ        | 1006/4744 [00:18<00:59, 63.15it/s] 21%|â–ˆâ–ˆâ–       | 1013/4744 [00:18<00:59, 63.18it/s] 22%|â–ˆâ–ˆâ–       | 1020/4744 [00:18<00:58, 63.37it/s] 22%|â–ˆâ–ˆâ–       | 1027/4744 [00:18<01:01, 60.55it/s] 22%|â–ˆâ–ˆâ–       | 1034/4744 [00:18<01:00, 61.40it/s] 22%|â–ˆâ–ˆâ–       | 1041/4744 [00:18<00:59, 62.16it/s] 22%|â–ˆâ–ˆâ–       | 1048/4744 [00:19<01:01, 59.79it/s] 22%|â–ˆâ–ˆâ–       | 1056/4744 [00:19<00:58, 63.09it/s] 22%|â–ˆâ–ˆâ–       | 1063/4744 [00:19<00:58, 63.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 1070/4744 [00:19<00:59, 62.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 1077/4744 [00:19<01:04, 56.44it/s] 23%|â–ˆâ–ˆâ–Ž       | 1083/4744 [00:19<01:16, 47.97it/s] 23%|â–ˆâ–ˆâ–Ž       | 1089/4744 [00:19<01:20, 45.66it/s] 23%|â–ˆâ–ˆâ–Ž       | 1094/4744 [00:20<01:18, 46.47it/s] 23%|â–ˆâ–ˆâ–Ž       | 1099/4744 [00:20<01:22, 44.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 1104/4744 [00:20<01:24, 43.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 1109/4744 [00:20<01:31, 39.81it/s] 23%|â–ˆâ–ˆâ–Ž       | 1114/4744 [00:20<01:26, 42.15it/s] 24%|â–ˆâ–ˆâ–Ž       | 1119/4744 [00:20<01:32, 39.16it/s] 24%|â–ˆâ–ˆâ–Ž       | 1124/4744 [00:20<01:32, 39.17it/s] 24%|â–ˆâ–ˆâ–       | 1128/4744 [00:20<01:32, 39.20it/s] 24%|â–ˆâ–ˆâ–       | 1133/4744 [00:21<01:31, 39.30it/s] 24%|â–ˆâ–ˆâ–       | 1137/4744 [00:21<01:31, 39.39it/s] 24%|â–ˆâ–ˆâ–       | 1141/4744 [00:21<01:31, 39.49it/s] 24%|â–ˆâ–ˆâ–       | 1145/4744 [00:21<01:30, 39.61it/s] 24%|â–ˆâ–ˆâ–       | 1149/4744 [00:21<01:30, 39.54it/s] 24%|â–ˆâ–ˆâ–       | 1154/4744 [00:21<01:30, 39.67it/s] 24%|â–ˆâ–ˆâ–       | 1158/4744 [00:21<01:30, 39.63it/s] 25%|â–ˆâ–ˆâ–       | 1163/4744 [00:21<01:30, 39.60it/s] 25%|â–ˆâ–ˆâ–       | 1168/4744 [00:21<01:29, 39.76it/s] 25%|â–ˆâ–ˆâ–       | 1172/4744 [00:22<01:30, 39.64it/s] 25%|â–ˆâ–ˆâ–       | 1176/4744 [00:22<01:30, 39.62it/s] 25%|â–ˆâ–ˆâ–       | 1181/4744 [00:22<01:29, 39.88it/s] 25%|â–ˆâ–ˆâ–       | 1185/4744 [00:22<01:29, 39.90it/s] 25%|â–ˆâ–ˆâ–Œ       | 1191/4744 [00:22<01:28, 40.09it/s] 25%|â–ˆâ–ˆâ–Œ       | 1196/4744 [00:22<01:23, 42.49it/s] 25%|â–ˆâ–ˆâ–Œ       | 1201/4744 [00:22<01:30, 39.11it/s] 25%|â–ˆâ–ˆâ–Œ       | 1206/4744 [00:22<01:30, 39.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 1210/4744 [00:22<01:29, 39.39it/s] 26%|â–ˆâ–ˆâ–Œ       | 1215/4744 [00:23<01:29, 39.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 1219/4744 [00:23<01:29, 39.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 1223/4744 [00:23<01:29, 39.55it/s] 26%|â–ˆâ–ˆâ–Œ       | 1227/4744 [00:23<01:28, 39.66it/s] 26%|â–ˆâ–ˆâ–Œ       | 1232/4744 [00:23<01:28, 39.75it/s] 26%|â–ˆâ–ˆâ–Œ       | 1237/4744 [00:23<01:28, 39.66it/s] 26%|â–ˆâ–ˆâ–Œ       | 1243/4744 [00:23<01:27, 39.88it/s] 26%|â–ˆâ–ˆâ–‹       | 1247/4744 [00:23<01:27, 39.88it/s] 26%|â–ˆâ–ˆâ–‹       | 1251/4744 [00:24<01:27, 39.86it/s] 26%|â–ˆâ–ˆâ–‹       | 1255/4744 [00:24<01:27, 39.84it/s] 27%|â–ˆâ–ˆâ–‹       | 1260/4744 [00:24<01:27, 39.83it/s] 27%|â–ˆâ–ˆâ–‹       | 1266/4744 [00:24<01:17, 44.66it/s] 27%|â–ˆâ–ˆâ–‹       | 1273/4744 [00:24<01:11, 48.28it/s] 27%|â–ˆâ–ˆâ–‹       | 1281/4744 [00:24<01:02, 55.34it/s] 27%|â–ˆâ–ˆâ–‹       | 1288/4744 [00:24<01:00, 56.88it/s] 27%|â–ˆâ–ˆâ–‹       | 1294/4744 [00:24<01:01, 56.31it/s] 27%|â–ˆâ–ˆâ–‹       | 1300/4744 [00:24<01:02, 55.42it/s] 28%|â–ˆâ–ˆâ–Š       | 1306/4744 [00:25<01:02, 54.87it/s] 28%|â–ˆâ–ˆâ–Š       | 1312/4744 [00:25<01:02, 54.99it/s] 28%|â–ˆâ–ˆâ–Š       | 1318/4744 [00:25<01:01, 55.55it/s] 28%|â–ˆâ–ˆâ–Š       | 1324/4744 [00:25<01:01, 55.91it/s] 28%|â–ˆâ–ˆâ–Š       | 1330/4744 [00:25<01:01, 55.13it/s] 28%|â–ˆâ–ˆâ–Š       | 1336/4744 [00:25<01:06, 51.34it/s] 28%|â–ˆâ–ˆâ–Š       | 1342/4744 [00:25<01:11, 47.31it/s] 28%|â–ˆâ–ˆâ–Š       | 1348/4744 [00:25<01:07, 50.08it/s] 29%|â–ˆâ–ˆâ–Š       | 1354/4744 [00:25<01:04, 52.45it/s] 29%|â–ˆâ–ˆâ–Š       | 1361/4744 [00:26<01:01, 55.00it/s] 29%|â–ˆâ–ˆâ–‰       | 1368/4744 [00:26<00:58, 57.52it/s] 29%|â–ˆâ–ˆâ–‰       | 1375/4744 [00:26<00:56, 59.52it/s] 29%|â–ˆâ–ˆâ–‰       | 1382/4744 [00:26<00:55, 60.35it/s] 29%|â–ˆâ–ˆâ–‰       | 1389/4744 [00:26<00:55, 60.97it/s] 29%|â–ˆâ–ˆâ–‰       | 1396/4744 [00:26<00:54, 61.72it/s] 30%|â–ˆâ–ˆâ–‰       | 1403/4744 [00:26<00:53, 62.54it/s] 30%|â–ˆâ–ˆâ–‰       | 1410/4744 [00:26<00:53, 62.16it/s] 30%|â–ˆâ–ˆâ–‰       | 1417/4744 [00:26<00:56, 58.80it/s] 30%|â–ˆâ–ˆâ–‰       | 1423/4744 [00:27<00:58, 56.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1429/4744 [00:27<00:59, 55.65it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1435/4744 [00:27<01:00, 54.82it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1441/4744 [00:27<01:10, 46.71it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1446/4744 [00:27<01:13, 44.61it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1451/4744 [00:27<01:16, 43.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1456/4744 [00:27<01:17, 42.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1461/4744 [00:27<01:19, 41.40it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1466/4744 [00:28<01:15, 43.40it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1471/4744 [00:28<01:27, 37.57it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1477/4744 [00:28<01:16, 42.69it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1482/4744 [00:28<01:22, 39.53it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1487/4744 [00:28<01:22, 39.62it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1492/4744 [00:28<01:21, 39.83it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1497/4744 [00:28<01:21, 39.98it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1502/4744 [00:29<01:21, 40.00it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1507/4744 [00:29<01:21, 39.93it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1512/4744 [00:29<01:21, 39.89it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1517/4744 [00:29<01:20, 39.95it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1522/4744 [00:29<01:16, 42.37it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1527/4744 [00:29<01:21, 39.30it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1532/4744 [00:29<01:21, 39.45it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1537/4744 [00:29<01:20, 39.74it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1542/4744 [00:30<01:20, 39.85it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1547/4744 [00:30<01:20, 39.83it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1552/4744 [00:30<01:20, 39.79it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1557/4744 [00:30<01:19, 39.86it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1562/4744 [00:30<01:15, 42.33it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1567/4744 [00:30<01:21, 39.11it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1571/4744 [00:30<01:21, 39.16it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1575/4744 [00:30<01:20, 39.34it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1580/4744 [00:30<01:20, 39.51it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1585/4744 [00:31<01:19, 39.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1590/4744 [00:31<01:19, 39.85it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1595/4744 [00:31<01:19, 39.79it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1600/4744 [00:31<01:19, 39.71it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1604/4744 [00:31<01:19, 39.72it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1608/4744 [00:31<01:19, 39.69it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1612/4744 [00:31<01:19, 39.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1617/4744 [00:31<01:18, 39.73it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1622/4744 [00:32<01:13, 42.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1627/4744 [00:32<01:15, 41.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1632/4744 [00:32<01:20, 38.48it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1636/4744 [00:32<01:20, 38.73it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1642/4744 [00:32<01:14, 41.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1647/4744 [00:32<01:20, 38.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1652/4744 [00:32<01:19, 39.03it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1656/4744 [00:32<01:19, 39.04it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1661/4744 [00:33<01:18, 39.37it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1666/4744 [00:33<01:17, 39.67it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1670/4744 [00:33<01:17, 39.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1674/4744 [00:33<01:17, 39.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1678/4744 [00:33<01:22, 37.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1683/4744 [00:33<01:15, 40.54it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1688/4744 [00:33<01:15, 40.33it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1694/4744 [00:33<01:11, 42.79it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1699/4744 [00:33<01:17, 39.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1704/4744 [00:34<01:17, 39.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1708/4744 [00:34<01:16, 39.51it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1713/4744 [00:34<01:16, 39.73it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1718/4744 [00:34<01:15, 40.00it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1723/4744 [00:34<01:15, 39.82it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1728/4744 [00:34<01:15, 39.98it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1733/4744 [00:34<01:15, 39.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1737/4744 [00:34<01:15, 39.83it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1742/4744 [00:35<01:14, 40.09it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1747/4744 [00:35<01:14, 39.98it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1751/4744 [00:35<01:15, 39.89it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1756/4744 [00:35<01:14, 39.96it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1760/4744 [00:35<01:15, 39.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1764/4744 [00:35<01:15, 39.60it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1768/4744 [00:35<01:15, 39.68it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1773/4744 [00:35<01:15, 39.41it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1777/4744 [00:35<01:15, 39.28it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1781/4744 [00:36<01:15, 39.07it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1785/4744 [00:36<01:15, 39.14it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1790/4744 [00:36<01:10, 42.06it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1797/4744 [00:36<01:00, 48.92it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1803/4744 [00:36<00:57, 50.76it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1811/4744 [00:36<00:51, 57.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1818/4744 [00:36<00:49, 58.97it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1825/4744 [00:36<00:48, 60.52it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1832/4744 [00:36<00:47, 61.74it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1839/4744 [00:37<00:46, 62.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1846/4744 [00:37<00:46, 62.68it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1853/4744 [00:37<00:45, 63.06it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1860/4744 [00:37<00:45, 63.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1867/4744 [00:37<00:45, 63.82it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1874/4744 [00:37<00:44, 64.04it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1881/4744 [00:37<00:45, 63.56it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1888/4744 [00:37<00:46, 61.23it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1896/4744 [00:37<00:43, 64.74it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1903/4744 [00:38<00:43, 64.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1910/4744 [00:38<00:44, 64.22it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1917/4744 [00:38<00:43, 64.34it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1924/4744 [00:38<00:43, 64.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1931/4744 [00:38<00:43, 64.41it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1938/4744 [00:38<00:43, 64.50it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1945/4744 [00:38<00:43, 63.90it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1952/4744 [00:38<00:45, 60.96it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1959/4744 [00:38<00:47, 58.46it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1965/4744 [00:39<00:48, 57.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1971/4744 [00:39<00:49, 56.50it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1977/4744 [00:39<00:49, 55.77it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1983/4744 [00:39<00:50, 54.55it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1989/4744 [00:39<00:55, 49.95it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1995/4744 [00:39<00:58, 46.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2000/4744 [00:39<01:01, 44.41it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2005/4744 [00:39<00:59, 45.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2010/4744 [00:40<01:06, 41.26it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2015/4744 [00:40<01:06, 40.87it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2020/4744 [00:40<01:07, 40.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2025/4744 [00:40<01:03, 42.74it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2030/4744 [00:40<01:08, 39.39it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2035/4744 [00:40<01:08, 39.38it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2040/4744 [00:40<01:04, 41.95it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2045/4744 [00:40<01:09, 38.86it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2050/4744 [00:41<01:08, 39.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2054/4744 [00:41<01:08, 39.33it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2058/4744 [00:41<01:08, 39.50it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2063/4744 [00:41<01:07, 39.67it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2068/4744 [00:41<01:07, 39.85it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2073/4744 [00:41<01:07, 39.67it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2078/4744 [00:41<01:07, 39.78it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2083/4744 [00:41<01:06, 39.85it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2087/4744 [00:41<01:06, 39.81it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2092/4744 [00:42<01:10, 37.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2097/4744 [00:42<01:05, 40.38it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2102/4744 [00:42<01:05, 40.15it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2107/4744 [00:42<01:05, 40.06it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2112/4744 [00:42<01:05, 40.05it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2117/4744 [00:42<01:05, 40.01it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2122/4744 [00:42<01:06, 39.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2126/4744 [00:43<01:31, 28.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2136/4744 [00:43<01:03, 41.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2141/4744 [00:43<01:03, 40.77it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2146/4744 [00:43<01:03, 40.63it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2151/4744 [00:43<01:04, 40.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2156/4744 [00:43<01:04, 40.40it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2161/4744 [00:43<01:04, 40.21it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2166/4744 [00:43<01:04, 40.07it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2171/4744 [00:44<01:04, 39.89it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2176/4744 [00:44<01:00, 42.30it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2181/4744 [00:44<01:05, 39.24it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2186/4744 [00:44<01:04, 39.42it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2191/4744 [00:44<01:04, 39.47it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2195/4744 [00:44<01:04, 39.47it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2199/4744 [00:44<01:04, 39.56it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2204/4744 [00:44<01:04, 39.61it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2209/4744 [00:45<01:03, 39.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2214/4744 [00:45<00:59, 42.26it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2219/4744 [00:45<01:04, 38.92it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2223/4744 [00:45<01:04, 39.05it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2227/4744 [00:45<01:04, 39.19it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2231/4744 [00:45<01:03, 39.31it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2236/4744 [00:45<00:59, 42.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2241/4744 [00:45<01:00, 41.41it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2246/4744 [00:45<01:01, 40.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2251/4744 [00:46<01:01, 40.50it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2256/4744 [00:46<01:05, 37.93it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2262/4744 [00:46<00:57, 43.11it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2269/4744 [00:46<00:50, 48.87it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2276/4744 [00:46<00:46, 53.33it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2283/4744 [00:46<00:43, 56.14it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2290/4744 [00:46<00:41, 58.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2297/4744 [00:46<00:40, 60.31it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2304/4744 [00:47<00:39, 61.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2311/4744 [00:47<00:39, 62.06it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2318/4744 [00:47<00:38, 62.71it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2325/4744 [00:47<00:38, 63.30it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2332/4744 [00:47<00:37, 63.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2339/4744 [00:47<00:37, 64.00it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2346/4744 [00:47<00:37, 63.86it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2353/4744 [00:47<00:37, 64.21it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2360/4744 [00:47<00:37, 64.28it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2367/4744 [00:47<00:36, 64.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2374/4744 [00:48<00:36, 64.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2381/4744 [00:48<00:36, 64.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2388/4744 [00:48<00:36, 64.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2395/4744 [00:48<00:36, 64.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2402/4744 [00:48<00:36, 64.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2409/4744 [00:48<00:36, 64.35it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2416/4744 [00:48<00:36, 64.54it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2423/4744 [00:48<00:35, 64.59it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2430/4744 [00:48<00:35, 64.75it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2437/4744 [00:49<00:35, 64.88it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2444/4744 [00:49<00:37, 60.55it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2451/4744 [00:49<00:39, 57.94it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2457/4744 [00:49<00:40, 56.39it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2463/4744 [00:49<00:41, 55.13it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2469/4744 [00:49<00:42, 53.10it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2475/4744 [00:49<00:47, 48.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2480/4744 [00:50<00:52, 43.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2485/4744 [00:50<00:50, 44.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2490/4744 [00:50<00:52, 43.02it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2495/4744 [00:50<00:53, 42.06it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2500/4744 [00:50<00:54, 41.29it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2505/4744 [00:50<00:54, 40.76it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2510/4744 [00:50<00:52, 42.88it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2515/4744 [00:50<00:56, 39.38it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2520/4744 [00:50<00:56, 39.40it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2525/4744 [00:51<00:56, 39.40it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2529/4744 [00:51<00:56, 39.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2533/4744 [00:51<00:55, 39.53it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2537/4744 [00:51<00:55, 39.54it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2541/4744 [00:51<00:55, 39.35it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2545/4744 [00:51<00:55, 39.53it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2549/4744 [00:51<00:55, 39.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2555/4744 [00:51<00:49, 44.29it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2562/4744 [00:51<00:43, 49.93it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2569/4744 [00:52<00:40, 53.70it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2575/4744 [00:52<00:57, 37.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2593/4744 [00:52<00:32, 66.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2602/4744 [00:52<00:32, 65.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2610/4744 [00:52<00:33, 64.55it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2618/4744 [00:52<00:32, 64.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2625/4744 [00:52<00:32, 64.30it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2632/4744 [00:53<00:32, 64.08it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2639/4744 [00:53<00:32, 63.91it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2646/4744 [00:53<00:32, 63.86it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2653/4744 [00:53<00:33, 61.95it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2660/4744 [00:53<00:35, 59.00it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2666/4744 [00:53<00:36, 57.15it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2672/4744 [00:53<00:37, 55.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2678/4744 [00:53<00:37, 55.07it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2684/4744 [00:53<00:37, 54.81it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2690/4744 [00:54<00:37, 54.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2696/4744 [00:54<00:38, 53.85it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2702/4744 [00:54<00:38, 53.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2708/4744 [00:54<00:37, 53.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2714/4744 [00:54<00:38, 53.16it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2720/4744 [00:54<00:38, 53.07it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2726/4744 [00:54<00:38, 52.32it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2732/4744 [00:54<00:43, 46.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2738/4744 [00:55<00:40, 49.69it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2745/4744 [00:55<00:37, 52.77it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2752/4744 [00:55<00:35, 55.35it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2759/4744 [00:55<00:34, 57.60it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2766/4744 [00:55<00:33, 59.09it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2772/4744 [00:55<00:34, 57.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2778/4744 [00:55<00:35, 55.00it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2784/4744 [00:55<00:36, 54.20it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2790/4744 [00:55<00:36, 53.11it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2796/4744 [00:56<00:37, 52.31it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2802/4744 [00:56<00:37, 51.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2808/4744 [00:56<00:37, 51.59it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2814/4744 [00:56<00:39, 49.00it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2820/4744 [00:56<00:39, 48.99it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2827/4744 [00:56<00:36, 51.98it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2835/4744 [00:56<00:33, 57.31it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2842/4744 [00:56<00:32, 59.15it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2849/4744 [00:57<00:31, 60.53it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2856/4744 [00:57<00:30, 60.98it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2863/4744 [00:57<00:30, 61.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2870/4744 [00:57<00:29, 62.62it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2877/4744 [00:57<00:29, 63.14it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2884/4744 [00:57<00:29, 63.10it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2891/4744 [00:57<00:29, 63.27it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2898/4744 [00:57<00:30, 60.72it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2905/4744 [00:57<00:31, 57.81it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2911/4744 [00:58<00:33, 54.63it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2917/4744 [00:58<00:36, 49.71it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2923/4744 [00:58<00:37, 48.74it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2928/4744 [00:58<00:37, 48.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2935/4744 [00:58<00:33, 53.38it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2942/4744 [00:58<00:32, 55.94it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2949/4744 [00:58<00:30, 57.98it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2956/4744 [00:58<00:30, 59.13it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2963/4744 [00:59<00:29, 59.59it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2970/4744 [00:59<00:29, 60.56it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2977/4744 [00:59<00:28, 61.28it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2984/4744 [00:59<00:28, 61.87it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2991/4744 [00:59<00:28, 62.30it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2998/4744 [00:59<00:27, 62.68it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3005/4744 [00:59<00:27, 62.72it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3012/4744 [00:59<00:28, 61.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3019/4744 [00:59<00:29, 59.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3025/4744 [01:00<00:30, 56.98it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3031/4744 [01:00<00:30, 55.84it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3037/4744 [01:00<00:33, 51.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3043/4744 [01:00<00:35, 47.45it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3048/4744 [01:00<00:35, 47.72it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3053/4744 [01:00<00:39, 42.78it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3058/4744 [01:00<00:38, 44.33it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3063/4744 [01:00<00:41, 40.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3068/4744 [01:01<00:41, 40.36it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3073/4744 [01:01<00:41, 40.20it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3078/4744 [01:01<00:41, 40.16it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3083/4744 [01:01<00:41, 40.09it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3088/4744 [01:01<00:41, 40.15it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3093/4744 [01:01<00:41, 39.97it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3098/4744 [01:01<00:41, 39.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3103/4744 [01:01<00:41, 39.93it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3108/4744 [01:02<00:38, 42.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3113/4744 [01:02<00:41, 39.07it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3117/4744 [01:02<00:41, 39.25it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3121/4744 [01:02<00:41, 39.42it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3125/4744 [01:02<00:58, 27.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3133/4744 [01:02<00:41, 38.82it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3138/4744 [01:02<00:38, 41.39it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3143/4744 [01:02<00:39, 40.95it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3148/4744 [01:03<00:41, 38.37it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3154/4744 [01:03<00:36, 43.40it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3159/4744 [01:03<00:39, 39.73it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3164/4744 [01:03<00:37, 42.07it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3169/4744 [01:03<00:40, 39.07it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3174/4744 [01:03<00:42, 36.92it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3179/4744 [01:03<00:39, 39.90it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3184/4744 [01:04<00:39, 39.78it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3189/4744 [01:04<00:36, 42.24it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3194/4744 [01:04<00:37, 41.34it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3199/4744 [01:04<00:40, 38.40it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3203/4744 [01:04<00:39, 38.69it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3208/4744 [01:04<00:39, 38.97it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3213/4744 [01:04<00:41, 37.06it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3218/4744 [01:04<00:40, 37.91it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3223/4744 [01:05<00:37, 40.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3228/4744 [01:05<00:37, 40.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3233/4744 [01:05<00:37, 40.09it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3238/4744 [01:05<00:37, 40.00it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3243/4744 [01:05<00:35, 42.40it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3248/4744 [01:05<00:38, 39.13it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3253/4744 [01:05<00:37, 39.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3257/4744 [01:05<00:37, 39.33it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3262/4744 [01:05<00:37, 39.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3267/4744 [01:06<00:37, 39.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3271/4744 [01:06<00:37, 39.66it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3276/4744 [01:06<00:35, 41.37it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3283/4744 [01:06<00:30, 48.05it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3290/4744 [01:06<00:27, 52.99it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3297/4744 [01:06<00:25, 56.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3303/4744 [01:06<00:25, 55.86it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3311/4744 [01:06<00:23, 61.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3318/4744 [01:06<00:22, 62.14it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3325/4744 [01:07<00:23, 61.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3332/4744 [01:07<00:22, 62.32it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3339/4744 [01:07<00:22, 63.15it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3346/4744 [01:07<00:21, 63.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3353/4744 [01:07<00:31, 43.60it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3365/4744 [01:07<00:30, 45.24it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3387/4744 [01:08<00:17, 76.21it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3397/4744 [01:08<00:19, 70.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3406/4744 [01:08<00:19, 68.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3414/4744 [01:08<00:19, 69.68it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3422/4744 [01:08<00:19, 66.21it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3430/4744 [01:08<00:19, 67.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3438/4744 [01:08<00:20, 65.05it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3445/4744 [01:09<00:21, 60.79it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3452/4744 [01:09<00:22, 58.23it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3458/4744 [01:09<00:22, 57.37it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3464/4744 [01:09<00:22, 56.33it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3470/4744 [01:09<00:22, 56.76it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3476/4744 [01:09<00:23, 55.12it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3483/4744 [01:09<00:22, 56.98it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3489/4744 [01:09<00:22, 55.59it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3497/4744 [01:09<00:21, 57.19it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3505/4744 [01:10<00:20, 61.22it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3512/4744 [01:10<00:19, 61.60it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3519/4744 [01:10<00:19, 61.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3526/4744 [01:10<00:19, 61.95it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3533/4744 [01:10<00:19, 62.40it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3540/4744 [01:10<00:19, 62.96it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3547/4744 [01:10<00:19, 60.37it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3555/4744 [01:10<00:18, 64.01it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3562/4744 [01:10<00:18, 64.21it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3569/4744 [01:11<00:18, 61.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3577/4744 [01:11<00:18, 64.75it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3584/4744 [01:11<00:18, 63.91it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3591/4744 [01:11<00:17, 64.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3598/4744 [01:11<00:18, 61.68it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3606/4744 [01:11<00:18, 62.03it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3614/4744 [01:11<00:17, 65.25it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3621/4744 [01:11<00:17, 65.03it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3628/4744 [01:11<00:17, 64.65it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3635/4744 [01:12<00:17, 63.91it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3642/4744 [01:12<00:17, 63.74it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3649/4744 [01:12<00:17, 63.99it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3656/4744 [01:12<00:16, 64.38it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3663/4744 [01:12<00:16, 64.72it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3670/4744 [01:12<00:16, 64.55it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3677/4744 [01:12<00:16, 64.51it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3684/4744 [01:12<00:16, 64.72it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3691/4744 [01:12<00:16, 64.82it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3698/4744 [01:13<00:16, 64.79it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3705/4744 [01:13<00:16, 64.60it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3712/4744 [01:13<00:16, 63.27it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3719/4744 [01:13<00:17, 59.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3726/4744 [01:13<00:17, 58.95it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3733/4744 [01:13<00:16, 60.22it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3740/4744 [01:13<00:17, 58.07it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3747/4744 [01:13<00:16, 58.92it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3755/4744 [01:14<00:15, 62.94it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3762/4744 [01:14<00:15, 63.38it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3769/4744 [01:14<00:15, 62.98it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3776/4744 [01:14<00:16, 58.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3782/4744 [01:14<00:16, 57.40it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3789/4744 [01:14<00:16, 58.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3796/4744 [01:14<00:16, 59.21it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3802/4744 [01:14<00:15, 59.34it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3809/4744 [01:14<00:15, 59.99it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3816/4744 [01:15<00:15, 61.14it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3823/4744 [01:15<00:14, 61.67it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3830/4744 [01:15<00:14, 61.52it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3837/4744 [01:15<00:14, 61.79it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3844/4744 [01:15<00:14, 62.60it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3851/4744 [01:15<00:14, 62.95it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3858/4744 [01:15<00:14, 62.24it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3865/4744 [01:15<00:14, 62.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3872/4744 [01:15<00:14, 60.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3880/4744 [01:16<00:13, 64.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3887/4744 [01:16<00:13, 64.65it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3894/4744 [01:16<00:13, 62.20it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3902/4744 [01:16<00:12, 65.67it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3909/4744 [01:16<00:12, 65.51it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3916/4744 [01:16<00:12, 65.39it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3923/4744 [01:16<00:12, 64.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3930/4744 [01:16<00:12, 62.80it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3937/4744 [01:16<00:13, 58.86it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3944/4744 [01:17<00:13, 60.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3951/4744 [01:17<00:13, 60.99it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3958/4744 [01:17<00:12, 60.92it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3965/4744 [01:17<00:12, 61.52it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3972/4744 [01:17<00:12, 62.07it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3979/4744 [01:17<00:12, 59.21it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3985/4744 [01:17<00:13, 56.60it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3991/4744 [01:17<00:13, 55.03it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3997/4744 [01:18<00:13, 53.74it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4003/4744 [01:18<00:14, 52.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4009/4744 [01:18<00:14, 52.08it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4015/4744 [01:18<00:14, 51.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4021/4744 [01:18<00:13, 51.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4027/4744 [01:18<00:15, 47.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4032/4744 [01:18<00:15, 45.24it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4037/4744 [01:18<00:17, 41.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4043/4744 [01:19<00:16, 43.21it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4048/4744 [01:19<00:16, 42.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4053/4744 [01:19<00:16, 41.92it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4058/4744 [01:19<00:15, 42.92it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4065/4744 [01:19<00:13, 48.96it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4072/4744 [01:19<00:12, 53.33it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4078/4744 [01:19<00:12, 53.24it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4086/4744 [01:19<00:11, 59.00it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4093/4744 [01:19<00:10, 60.67it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4100/4744 [01:20<00:10, 61.37it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4107/4744 [01:20<00:10, 61.98it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4114/4744 [01:20<00:10, 62.86it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4121/4744 [01:20<00:09, 63.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4128/4744 [01:20<00:09, 62.41it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4135/4744 [01:20<00:10, 59.00it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4141/4744 [01:20<00:10, 56.62it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4147/4744 [01:20<00:10, 54.96it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4153/4744 [01:21<00:19, 29.86it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4170/4744 [01:21<00:10, 53.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4178/4744 [01:21<00:11, 50.77it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4185/4744 [01:21<00:12, 45.56it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4191/4744 [01:21<00:12, 44.50it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4197/4744 [01:22<00:12, 45.20it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4203/4744 [01:22<00:12, 41.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4208/4744 [01:22<00:12, 41.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4213/4744 [01:22<00:12, 40.96it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4218/4744 [01:22<00:12, 41.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4223/4744 [01:22<00:12, 40.71it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4228/4744 [01:22<00:12, 40.52it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4233/4744 [01:22<00:12, 40.24it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4238/4744 [01:23<00:12, 40.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4243/4744 [01:23<00:12, 40.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4248/4744 [01:23<00:12, 40.33it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4253/4744 [01:23<00:12, 40.26it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4258/4744 [01:23<00:16, 29.00it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4268/4744 [01:23<00:11, 41.39it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4273/4744 [01:24<00:11, 40.99it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4278/4744 [01:24<00:11, 40.74it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4283/4744 [01:24<00:11, 40.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4288/4744 [01:24<00:11, 40.22it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4293/4744 [01:24<00:11, 38.41it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4298/4744 [01:24<00:10, 41.02it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4303/4744 [01:24<00:10, 40.56it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4308/4744 [01:24<00:10, 42.92it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4313/4744 [01:25<00:10, 39.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4318/4744 [01:25<00:10, 40.12it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4323/4744 [01:25<00:10, 40.05it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4328/4744 [01:25<00:10, 39.96it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4333/4744 [01:25<00:10, 37.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4338/4744 [01:25<00:10, 40.53it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4344/4744 [01:25<00:09, 43.04it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4349/4744 [01:25<00:09, 39.74it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4354/4744 [01:26<00:09, 39.83it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4359/4744 [01:26<00:09, 39.96it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4364/4744 [01:26<00:09, 39.87it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4370/4744 [01:26<00:08, 44.06it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4377/4744 [01:26<00:07, 49.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4384/4744 [01:26<00:06, 53.41it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4390/4744 [01:26<00:06, 53.27it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4397/4744 [01:26<00:06, 55.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4404/4744 [01:26<00:05, 57.95it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4411/4744 [01:27<00:05, 59.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4417/4744 [01:27<00:05, 56.94it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4425/4744 [01:27<00:05, 61.39it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4432/4744 [01:27<00:05, 61.99it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4439/4744 [01:27<00:05, 60.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4446/4744 [01:27<00:05, 56.24it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4452/4744 [01:27<00:05, 50.94it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4458/4744 [01:27<00:05, 50.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4464/4744 [01:28<00:05, 47.55it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4470/4744 [01:28<00:05, 48.26it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4478/4744 [01:28<00:04, 54.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4485/4744 [01:28<00:04, 57.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4492/4744 [01:28<00:04, 58.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4499/4744 [01:28<00:04, 59.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4506/4744 [01:28<00:03, 60.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4513/4744 [01:28<00:03, 61.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4520/4744 [01:28<00:03, 61.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4527/4744 [01:29<00:03, 61.99it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4534/4744 [01:29<00:03, 62.36it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4541/4744 [01:29<00:03, 62.85it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4548/4744 [01:29<00:03, 62.36it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4555/4744 [01:29<00:03, 62.98it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4562/4744 [01:29<00:02, 63.45it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4569/4744 [01:29<00:02, 63.00it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4576/4744 [01:29<00:02, 62.97it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4583/4744 [01:29<00:02, 63.31it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4590/4744 [01:30<00:02, 63.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4597/4744 [01:30<00:02, 62.72it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4604/4744 [01:30<00:02, 63.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4611/4744 [01:30<00:02, 58.98it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4617/4744 [01:30<00:02, 57.28it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4623/4744 [01:30<00:02, 56.06it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4629/4744 [01:30<00:02, 54.90it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4635/4744 [01:30<00:02, 51.29it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4642/4744 [01:31<00:01, 53.61it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4648/4744 [01:31<00:01, 53.03it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4654/4744 [01:31<00:01, 52.70it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4660/4744 [01:31<00:01, 52.14it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4666/4744 [01:31<00:01, 49.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4671/4744 [01:31<00:01, 47.12it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4676/4744 [01:31<00:01, 47.52it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4681/4744 [01:31<00:01, 45.03it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4686/4744 [01:32<00:01, 40.74it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4691/4744 [01:32<00:01, 40.23it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4697/4744 [01:32<00:01, 40.60it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4702/4744 [01:32<00:00, 42.66it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4707/4744 [01:32<00:00, 39.28it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4712/4744 [01:32<00:00, 39.31it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4717/4744 [01:32<00:00, 39.33it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4722/4744 [01:32<00:00, 37.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4727/4744 [01:33<00:00, 40.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4732/4744 [01:33<00:00, 40.15it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4737/4744 [01:33<00:00, 39.94it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4742/4744 [01:33<00:00, 39.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4744/4744 [01:33<00:00, 50.75it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.9393
Epoch 1 Step 51 Train Loss: 0.3828
Epoch 1: Train Overall MSE: 0.0098 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.1320 Validation Top 20 DE MSE: 0.1907. 
Epoch 2 Step 1 Train Loss: 0.3776
Epoch 2 Step 51 Train Loss: 0.4088
Epoch 2: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0522 Validation Top 20 DE MSE: 0.1487. 
Epoch 3 Step 1 Train Loss: 0.3766
Epoch 3 Step 51 Train Loss: 0.3645
Epoch 3: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0479 Validation Top 20 DE MSE: 0.1024. 
Epoch 4 Step 1 Train Loss: 0.3436
Epoch 4 Step 51 Train Loss: 0.3488
Epoch 4: Train Overall MSE: 0.0073 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0395 Validation Top 20 DE MSE: 0.0740. 
Epoch 5 Step 1 Train Loss: 0.3489
Epoch 5 Step 51 Train Loss: 0.3899
Epoch 5: Train Overall MSE: 0.0070 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0426 Validation Top 20 DE MSE: 0.0677. 
Epoch 6 Step 1 Train Loss: 0.3558
Epoch 6 Step 51 Train Loss: 0.3596
Epoch 6: Train Overall MSE: 0.0069 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0398 Validation Top 20 DE MSE: 0.0682. 
Epoch 7 Step 1 Train Loss: 0.3891
Epoch 7 Step 51 Train Loss: 0.3260
Epoch 7: Train Overall MSE: 0.0068 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0369 Validation Top 20 DE MSE: 0.0622. 
Epoch 8 Step 1 Train Loss: 0.3536
Epoch 8 Step 51 Train Loss: 0.3580
Epoch 8: Train Overall MSE: 0.0090 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0506 Validation Top 20 DE MSE: 0.0634. 
Epoch 9 Step 1 Train Loss: 0.3392
Epoch 9 Step 51 Train Loss: 0.3642
Epoch 9: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0361 Validation Top 20 DE MSE: 0.0595. 
Epoch 10 Step 1 Train Loss: 0.3543
Epoch 10 Step 51 Train Loss: 0.3691
Epoch 10: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0368 Validation Top 20 DE MSE: 0.0616. 
Epoch 11 Step 1 Train Loss: 0.3763
Epoch 11 Step 51 Train Loss: 0.3768
Epoch 11: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0358 Validation Top 20 DE MSE: 0.0567. 
Epoch 12 Step 1 Train Loss: 0.3716
Epoch 12 Step 51 Train Loss: 0.3850
Epoch 12: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0358 Validation Top 20 DE MSE: 0.0565. 
Epoch 13 Step 1 Train Loss: 0.3529
Epoch 13 Step 51 Train Loss: 0.3527
Epoch 13: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0364 Validation Top 20 DE MSE: 0.0616. 
Epoch 14 Step 1 Train Loss: 0.3434
Epoch 14 Step 51 Train Loss: 0.3711
Epoch 14: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0367 Validation Top 20 DE MSE: 0.0616. 
Epoch 15 Step 1 Train Loss: 0.3769
Epoch 15 Step 51 Train Loss: 0.3420
Epoch 15: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0362 Validation Top 20 DE MSE: 0.0610. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.7668
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.015906954
test_unseen_single_pearson: 0.9452505704841109
test_unseen_single_mse_de: 0.76684153
test_unseen_single_pearson_de: 0.86694220749806
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.22087426050804732
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.175
test_unseen_single_frac_sigma_below_1_non_dropout: 0.525
test_unseen_single_mse_top20_de_non_dropout: 0.7668415
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.001 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb: \ 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–†â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–…â–†â–‡â–‡â–ˆâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–ˆâ–…â–„â–ƒâ–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:                                                   val_de_mse â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.76684
wandb:                                              test_de_pearson 0.86694
wandb:               test_frac_opposite_direction_top20_non_dropout 0.175
wandb:                          test_frac_sigma_below_1_non_dropout 0.525
wandb:                                                     test_mse 0.01591
wandb:                                test_mse_top20_de_non_dropout 0.76684
wandb:                                                 test_pearson 0.94525
wandb:                                           test_pearson_delta 0.22087
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.175
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.525
wandb:                                       test_unseen_single_mse 0.01591
wandb:                                    test_unseen_single_mse_de 0.76684
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.76684
wandb:                                   test_unseen_single_pearson 0.94525
wandb:                                test_unseen_single_pearson_de 0.86694
wandb:                             test_unseen_single_pearson_delta 0.22087
wandb:                                                 train_de_mse 0.0362
wandb:                                             train_de_pearson 0.7987
wandb:                                                    train_mse 0.00662
wandb:                                                train_pearson 0.97903
wandb:                                                training_loss 0.32739
wandb:                                                   val_de_mse 0.061
wandb:                                               val_de_pearson 0.98602
wandb:                                                      val_mse 0.00267
wandb:                                                  val_pearson 0.99135
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_arrayed_RNA_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/cjolcgyl
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_094005-cjolcgyl/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_094544-zc9qk6px
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/zc9qk6px
Start Training...
Epoch 1 Step 1 Train Loss: 0.8192
Epoch 1 Step 51 Train Loss: 0.3854
Epoch 1: Train Overall MSE: 0.0108 Validation Overall MSE: 0.0044. 
Train Top 20 DE MSE: 0.3911 Validation Top 20 DE MSE: 0.1685. 
Epoch 2 Step 1 Train Loss: 0.3577
Epoch 2 Step 51 Train Loss: 0.3813
Epoch 2: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0044. 
Train Top 20 DE MSE: 0.1098 Validation Top 20 DE MSE: 0.1478. 
Epoch 3 Step 1 Train Loss: 0.3604
Epoch 3 Step 51 Train Loss: 0.3691
Epoch 3: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0599 Validation Top 20 DE MSE: 0.0646. 
Epoch 4 Step 1 Train Loss: 0.3473
Epoch 4 Step 51 Train Loss: 0.3663
Epoch 4: Train Overall MSE: 0.0064 Validation Overall MSE: 0.0106. 
Train Top 20 DE MSE: 0.0755 Validation Top 20 DE MSE: 0.0738. 
Epoch 5 Step 1 Train Loss: 0.3628
Epoch 5 Step 51 Train Loss: 0.3613
Epoch 5: Train Overall MSE: 0.0055 Validation Overall MSE: 0.0095. 
Train Top 20 DE MSE: 0.0471 Validation Top 20 DE MSE: 0.0764. 
Epoch 6 Step 1 Train Loss: 0.3727
Epoch 6 Step 51 Train Loss: 0.3577
Epoch 6: Train Overall MSE: 0.0060 Validation Overall MSE: 0.0109. 
Train Top 20 DE MSE: 0.0609 Validation Top 20 DE MSE: 0.0641. 
Epoch 7 Step 1 Train Loss: 0.3600
Epoch 7 Step 51 Train Loss: 0.3499
Epoch 7: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0125. 
Train Top 20 DE MSE: 0.0546 Validation Top 20 DE MSE: 0.0689. 
Epoch 8 Step 1 Train Loss: 0.3495
Epoch 8 Step 51 Train Loss: 0.3428
Epoch 8: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0127. 
Train Top 20 DE MSE: 0.0563 Validation Top 20 DE MSE: 0.0624. 
Epoch 9 Step 1 Train Loss: 0.3460
Epoch 9 Step 51 Train Loss: 0.3508
Epoch 9: Train Overall MSE: 0.0074 Validation Overall MSE: 0.0136. 
Train Top 20 DE MSE: 0.0475 Validation Top 20 DE MSE: 0.0560. 
Epoch 10 Step 1 Train Loss: 0.3786
Epoch 10 Step 51 Train Loss: 0.3629
Epoch 10: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0151. 
Train Top 20 DE MSE: 0.0515 Validation Top 20 DE MSE: 0.0597. 
Epoch 11 Step 1 Train Loss: 0.3675
Epoch 11 Step 51 Train Loss: 0.3406
Epoch 11: Train Overall MSE: 0.0062 Validation Overall MSE: 0.0120. 
Train Top 20 DE MSE: 0.0514 Validation Top 20 DE MSE: 0.0613. 
Epoch 12 Step 1 Train Loss: 0.3643
Epoch 12 Step 51 Train Loss: 0.3598
Epoch 12: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0146. 
Train Top 20 DE MSE: 0.0513 Validation Top 20 DE MSE: 0.0575. 
Epoch 13 Step 1 Train Loss: 0.3330
Epoch 13 Step 51 Train Loss: 0.3687
Epoch 13: Train Overall MSE: 0.0069 Validation Overall MSE: 0.0136. 
Train Top 20 DE MSE: 0.0481 Validation Top 20 DE MSE: 0.0613. 
Epoch 14 Step 1 Train Loss: 0.3762
Epoch 14 Step 51 Train Loss: 0.3574
Epoch 14: Train Overall MSE: 0.0075 Validation Overall MSE: 0.0143. 
Train Top 20 DE MSE: 0.0583 Validation Top 20 DE MSE: 0.0591. 
Epoch 15 Step 1 Train Loss: 0.3795
Epoch 15 Step 51 Train Loss: 0.3518
Epoch 15: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0150. 
Train Top 20 DE MSE: 0.0558 Validation Top 20 DE MSE: 0.0610. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1080
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00682023
test_unseen_single_pearson: 0.9762489784806826
test_unseen_single_mse_de: 0.10797941
test_unseen_single_pearson_de: 0.9553987896025307
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.30470839216532175
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.15
test_unseen_single_frac_sigma_below_1_non_dropout: 0.975
test_unseen_single_mse_top20_de_non_dropout: 0.1079794
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.009 MB of 0.018 MB uploadedwandb: / 0.009 MB of 0.018 MB uploadedwandb: - 0.009 MB of 0.018 MB uploadedwandb: \ 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–‚â–‚â–â–‚â–‚â–‚â–„â–„â–‚â–„â–ƒâ–„â–„
wandb:                                                train_pearson â–â–…â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–†â–†â–‡â–†â–‡â–†â–†
wandb:                                                training_loss â–ˆâ–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚
wandb:                                                   val_de_mse â–ˆâ–‡â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–â–â–â–…â–„â–…â–†â–†â–‡â–ˆâ–†â–ˆâ–‡â–‡â–ˆ
wandb:                                                  val_pearson â–ˆâ–ˆâ–‡â–„â–„â–„â–ƒâ–‚â–‚â–â–ƒâ–â–‚â–â–
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.10798
wandb:                                              test_de_pearson 0.9554
wandb:               test_frac_opposite_direction_top20_non_dropout 0.15
wandb:                          test_frac_sigma_below_1_non_dropout 0.975
wandb:                                                     test_mse 0.00682
wandb:                                test_mse_top20_de_non_dropout 0.10798
wandb:                                                 test_pearson 0.97625
wandb:                                           test_pearson_delta 0.30471
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.15
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.975
wandb:                                       test_unseen_single_mse 0.00682
wandb:                                    test_unseen_single_mse_de 0.10798
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.10798
wandb:                                   test_unseen_single_pearson 0.97625
wandb:                                test_unseen_single_pearson_de 0.9554
wandb:                             test_unseen_single_pearson_delta 0.30471
wandb:                                                 train_de_mse 0.05581
wandb:                                             train_de_pearson 0.79783
wandb:                                                    train_mse 0.00804
wandb:                                                train_pearson 0.97709
wandb:                                                training_loss 0.34217
wandb:                                                   val_de_mse 0.06103
wandb:                                               val_de_pearson 0.98176
wandb:                                                      val_mse 0.01501
wandb:                                                  val_pearson 0.94825
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_arrayed_RNA_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/zc9qk6px
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_094544-zc9qk6px/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_094828-pqnrlxkj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/pqnrlxkj
Start Training...
Epoch 1 Step 1 Train Loss: 0.8129
Epoch 1 Step 51 Train Loss: 0.4158
Epoch 1: Train Overall MSE: 0.0047 Validation Overall MSE: 0.0443. 
Train Top 20 DE MSE: 0.3539 Validation Top 20 DE MSE: 0.1849. 
Epoch 2 Step 1 Train Loss: 0.4113
Epoch 2 Step 51 Train Loss: 0.4010
Epoch 2: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0463. 
Train Top 20 DE MSE: 0.0585 Validation Top 20 DE MSE: 0.1571. 
Epoch 3 Step 1 Train Loss: 0.3705
Epoch 3 Step 51 Train Loss: 0.3688
Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0439. 
Train Top 20 DE MSE: 0.0499 Validation Top 20 DE MSE: 0.1579. 
Epoch 4 Step 1 Train Loss: 0.3671
Epoch 4 Step 51 Train Loss: 0.3658
Epoch 4: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0449. 
Train Top 20 DE MSE: 0.0277 Validation Top 20 DE MSE: 0.1554. 
Epoch 5 Step 1 Train Loss: 0.3968
Epoch 5 Step 51 Train Loss: 0.3566
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0457. 
Train Top 20 DE MSE: 0.0250 Validation Top 20 DE MSE: 0.1553. 
Epoch 6 Step 1 Train Loss: 0.3423
Epoch 6 Step 51 Train Loss: 0.3611
Epoch 6: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0451. 
Train Top 20 DE MSE: 0.0229 Validation Top 20 DE MSE: 0.1552. 
Epoch 7 Step 1 Train Loss: 0.3786
Epoch 7 Step 51 Train Loss: 0.3679
Epoch 7: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0458. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.1544. 
Epoch 8 Step 1 Train Loss: 0.3708
Epoch 8 Step 51 Train Loss: 0.3587
Epoch 8: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0459. 
Train Top 20 DE MSE: 0.0165 Validation Top 20 DE MSE: 0.1543. 
Epoch 9 Step 1 Train Loss: 0.3667
Epoch 9 Step 51 Train Loss: 0.3735
Epoch 9: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0461. 
Train Top 20 DE MSE: 0.0166 Validation Top 20 DE MSE: 0.1545. 
Epoch 10 Step 1 Train Loss: 0.3645
Epoch 10 Step 51 Train Loss: 0.3585
Epoch 10: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0463. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.1534. 
Epoch 11 Step 1 Train Loss: 0.3743
Epoch 11 Step 51 Train Loss: 0.3603
Epoch 11: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0456. 
Train Top 20 DE MSE: 0.0220 Validation Top 20 DE MSE: 0.1563. 
Epoch 12 Step 1 Train Loss: 0.3796
Epoch 12 Step 51 Train Loss: 0.3633
Epoch 12: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0460. 
Train Top 20 DE MSE: 0.0170 Validation Top 20 DE MSE: 0.1551. 
Epoch 13 Step 1 Train Loss: 0.3820
Epoch 13 Step 51 Train Loss: 0.3799
Epoch 13: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0463. 
Train Top 20 DE MSE: 0.0151 Validation Top 20 DE MSE: 0.1539. 
Epoch 14 Step 1 Train Loss: 0.3791
Epoch 14 Step 51 Train Loss: 0.4020
Epoch 14: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0456. 
Train Top 20 DE MSE: 0.0196 Validation Top 20 DE MSE: 0.1558. 
Epoch 15 Step 1 Train Loss: 0.3623
Epoch 15 Step 51 Train Loss: 0.3707
Epoch 15: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0458. 
Train Top 20 DE MSE: 0.0191 Validation Top 20 DE MSE: 0.1553. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0437
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0018973529
test_unseen_single_pearson: 0.993526743777618
test_unseen_single_mse_de: 0.04366693
test_unseen_single_pearson_de: 0.9823760096284018
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3270809390523306
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.07500000000000001
test_unseen_single_frac_sigma_below_1_non_dropout: 0.975
test_unseen_single_mse_top20_de_non_dropout: 0.04366693
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.002 MB of 0.018 MB uploadedwandb: | 0.013 MB of 0.018 MB uploadedwandb: / 0.013 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–â–‚â–â–â–â–â–â–â–â–‚â–â–â–‚â–‚
wandb:                                                train_pearson â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–„â–„â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–â–â–ƒâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–ƒ
wandb:                                                   val_de_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–‚â–
wandb:                                               val_de_pearson â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                      val_mse â–‚â–ˆâ–â–„â–†â–…â–†â–‡â–‡â–ˆâ–†â–‡â–ˆâ–†â–‡
wandb:                                                  val_pearson â–ˆâ–â–†â–„â–‚â–„â–‚â–‚â–‚â–â–ƒâ–‚â–â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.04367
wandb:                                              test_de_pearson 0.98238
wandb:               test_frac_opposite_direction_top20_non_dropout 0.075
wandb:                          test_frac_sigma_below_1_non_dropout 0.975
wandb:                                                     test_mse 0.0019
wandb:                                test_mse_top20_de_non_dropout 0.04367
wandb:                                                 test_pearson 0.99353
wandb:                                           test_pearson_delta 0.32708
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.075
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.975
wandb:                                       test_unseen_single_mse 0.0019
wandb:                                    test_unseen_single_mse_de 0.04367
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.04367
wandb:                                   test_unseen_single_pearson 0.99353
wandb:                                test_unseen_single_pearson_de 0.98238
wandb:                             test_unseen_single_pearson_delta 0.32708
wandb:                                                 train_de_mse 0.01914
wandb:                                             train_de_pearson 0.99611
wandb:                                                    train_mse 0.00226
wandb:                                                train_pearson 0.99328
wandb:                                                training_loss 0.36144
wandb:                                                   val_de_mse 0.15528
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.04577
wandb:                                                  val_pearson 0.84367
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_arrayed_RNA_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/pqnrlxkj
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_094828-pqnrlxkj/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_095112-jq1jm2g6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/jq1jm2g6
Start Training...
Epoch 1 Step 1 Train Loss: 0.6298
Epoch 1: Train Overall MSE: 0.0109 Validation Overall MSE: 0.0157. 
Train Top 20 DE MSE: 0.2142 Validation Top 20 DE MSE: 1.5870. 
Epoch 2 Step 1 Train Loss: 0.3984
Epoch 2: Train Overall MSE: 0.0084 Validation Overall MSE: 0.0151. 
Train Top 20 DE MSE: 0.0685 Validation Top 20 DE MSE: 1.4659. 
Epoch 3 Step 1 Train Loss: 0.3459
Epoch 3: Train Overall MSE: 0.0075 Validation Overall MSE: 0.0157. 
Train Top 20 DE MSE: 0.0506 Validation Top 20 DE MSE: 1.4869. 
Epoch 4 Step 1 Train Loss: 0.4384
Epoch 4: Train Overall MSE: 0.0065 Validation Overall MSE: 0.0241. 
Train Top 20 DE MSE: 0.0478 Validation Top 20 DE MSE: 1.4307. 
Epoch 5 Step 1 Train Loss: 0.3786
Epoch 5: Train Overall MSE: 0.0062 Validation Overall MSE: 0.0317. 
Train Top 20 DE MSE: 0.0396 Validation Top 20 DE MSE: 1.4268. 
Epoch 6 Step 1 Train Loss: 0.3611
Epoch 6: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0355. 
Train Top 20 DE MSE: 0.0429 Validation Top 20 DE MSE: 1.4147. 
Epoch 7 Step 1 Train Loss: 0.4190
Epoch 7: Train Overall MSE: 0.0061 Validation Overall MSE: 0.0352. 
Train Top 20 DE MSE: 0.0384 Validation Top 20 DE MSE: 1.3937. 
Epoch 8 Step 1 Train Loss: 0.3761
Epoch 8: Train Overall MSE: 0.0065 Validation Overall MSE: 0.0409. 
Train Top 20 DE MSE: 0.0356 Validation Top 20 DE MSE: 1.3944. 
Epoch 9 Step 1 Train Loss: 0.3368
Epoch 9: Train Overall MSE: 0.0067 Validation Overall MSE: 0.0432. 
Train Top 20 DE MSE: 0.0360 Validation Top 20 DE MSE: 1.3937. 
Epoch 10 Step 1 Train Loss: 0.3781
Epoch 10: Train Overall MSE: 0.0067 Validation Overall MSE: 0.0427. 
Train Top 20 DE MSE: 0.0367 Validation Top 20 DE MSE: 1.3969. 
Epoch 11 Step 1 Train Loss: 0.3741
Epoch 11: Train Overall MSE: 0.0072 Validation Overall MSE: 0.0478. 
Train Top 20 DE MSE: 0.0345 Validation Top 20 DE MSE: 1.3993. 
Epoch 12 Step 1 Train Loss: 0.3618
Epoch 12: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0423. 
Train Top 20 DE MSE: 0.0358 Validation Top 20 DE MSE: 1.3918. 
Epoch 13 Step 1 Train Loss: 0.3538
Epoch 13: Train Overall MSE: 0.0064 Validation Overall MSE: 0.0399. 
Train Top 20 DE MSE: 0.0367 Validation Top 20 DE MSE: 1.3927. 
Epoch 14 Step 1 Train Loss: 0.3542
Epoch 14: Train Overall MSE: 0.0068 Validation Overall MSE: 0.0436. 
Train Top 20 DE MSE: 0.0360 Validation Top 20 DE MSE: 1.3962. 
Epoch 15 Step 1 Train Loss: 0.3701
Epoch 15: Train Overall MSE: 0.0063 Validation Overall MSE: 0.0392. 
Train Top 20 DE MSE: 0.0362 Validation Top 20 DE MSE: 1.3912. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0208
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0026583518
test_unseen_single_pearson: 0.9907358425828123
test_unseen_single_mse_de: 0.020761881
test_unseen_single_pearson_de: 0.9901705124562438
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.456474682828833
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.025
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.020761881
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.017 MB uploadedwandb: | 0.001 MB of 0.017 MB uploadedwandb: / 0.007 MB of 0.017 MB uploadedwandb: - 0.007 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–ƒâ–‚â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–
wandb:                                                train_pearson â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ
wandb:                                                training_loss â–ˆâ–†â–†â–…â–„â–ƒâ–‚â–‚â–…â–‚â–ƒâ–„â–…â–„â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–ƒâ–„â–‚â–ƒâ–…â–ƒâ–ƒâ–â–…â–„â–‚â–ƒâ–â–‚â–ƒâ–„â–„â–ƒ
wandb:                                                   val_de_mse â–ˆâ–„â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–…â–ˆâ–‡â–…â–„â–ƒâ–„â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–ƒ
wandb:                                                      val_mse â–â–â–â–ƒâ–…â–…â–…â–‡â–‡â–‡â–ˆâ–‡â–†â–‡â–†
wandb:                                                  val_pearson â–ˆâ–ˆâ–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–ƒâ–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.02076
wandb:                                              test_de_pearson 0.99017
wandb:               test_frac_opposite_direction_top20_non_dropout 0.025
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00266
wandb:                                test_mse_top20_de_non_dropout 0.02076
wandb:                                                 test_pearson 0.99074
wandb:                                           test_pearson_delta 0.45647
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.025
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00266
wandb:                                    test_unseen_single_mse_de 0.02076
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.02076
wandb:                                   test_unseen_single_pearson 0.99074
wandb:                                test_unseen_single_pearson_de 0.99017
wandb:                             test_unseen_single_pearson_delta 0.45647
wandb:                                                 train_de_mse 0.03625
wandb:                                             train_de_pearson 0.79781
wandb:                                                    train_mse 0.0063
wandb:                                                train_pearson 0.97995
wandb:                                                training_loss 0.33776
wandb:                                                   val_de_mse 1.39125
wandb:                                               val_de_pearson 0.74639
wandb:                                                      val_mse 0.03921
wandb:                                                  val_pearson 0.86986
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_arrayed_RNA_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/jq1jm2g6
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_095112-jq1jm2g6/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_095343-vyvhnzom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/vyvhnzom
Start Training...
Epoch 1 Step 1 Train Loss: 0.7596
Epoch 1 Step 51 Train Loss: 0.4078
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0421. 
Train Top 20 DE MSE: 0.0835 Validation Top 20 DE MSE: 0.1838. 
Epoch 2 Step 1 Train Loss: 0.4315
Epoch 2 Step 51 Train Loss: 0.3798
Epoch 2: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0420. 
Train Top 20 DE MSE: 0.0260 Validation Top 20 DE MSE: 0.1824. 
Epoch 3 Step 1 Train Loss: 0.4053
Epoch 3 Step 51 Train Loss: 0.4046
Epoch 3: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0131 Validation Top 20 DE MSE: 0.1802. 
Epoch 4 Step 1 Train Loss: 0.3566
Epoch 4 Step 51 Train Loss: 0.3759
Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0415. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.1778. 
Epoch 5 Step 1 Train Loss: 0.3922
Epoch 5 Step 51 Train Loss: 0.3931
Epoch 5: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0414. 
Train Top 20 DE MSE: 0.0069 Validation Top 20 DE MSE: 0.1806. 
Epoch 6 Step 1 Train Loss: 0.3894
Epoch 6 Step 51 Train Loss: 0.3830
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0412. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.1769. 
Epoch 7 Step 1 Train Loss: 0.3695
Epoch 7 Step 51 Train Loss: 0.3882
Epoch 7: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0056 Validation Top 20 DE MSE: 0.1792. 
Epoch 8 Step 1 Train Loss: 0.3905
Epoch 8 Step 51 Train Loss: 0.3894
Epoch 8: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0414. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.1795. 
Epoch 9 Step 1 Train Loss: 0.3958
Epoch 9 Step 51 Train Loss: 0.3685
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.1783. 
Epoch 10 Step 1 Train Loss: 0.3810
Epoch 10 Step 51 Train Loss: 0.3792
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.1786. 
Epoch 11 Step 1 Train Loss: 0.3881
Epoch 11 Step 51 Train Loss: 0.3842
Epoch 11: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0415. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.1811. 
Epoch 12 Step 1 Train Loss: 0.4099
Epoch 12 Step 51 Train Loss: 0.3967
Epoch 12: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.1784. 
Epoch 13 Step 1 Train Loss: 0.3705
Epoch 13 Step 51 Train Loss: 0.3716
Epoch 13: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.1790. 
Epoch 14 Step 1 Train Loss: 0.3872
Epoch 14 Step 51 Train Loss: 0.3833
Epoch 14: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0413. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.1782. 
Epoch 15 Step 1 Train Loss: 0.3405
Epoch 15 Step 51 Train Loss: 0.3874
Epoch 15: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0414. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.1805. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.7047
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00803935
test_unseen_single_pearson: 0.9713398080542622
test_unseen_single_mse_de: 0.7046528
test_unseen_single_pearson_de: 0.8966108255912029
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4292305332511376
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.175
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5
test_unseen_single_mse_top20_de_non_dropout: 0.70465285
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.003 MB of 0.018 MB uploadedwandb: / 0.003 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–ƒâ–â–ƒâ–ƒâ–„â–„â–„â–„â–†â–„â–„â–„â–†
wandb:                                                train_pearson â–â–…â–†â–ˆâ–‡â–‡â–†â–†â–†â–†â–…â–†â–†â–†â–…
wandb:                                                training_loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                   val_de_mse â–ˆâ–‡â–„â–‚â–…â–â–ƒâ–„â–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–…
wandb:                                               val_de_pearson â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                      val_mse â–ˆâ–‡â–â–ƒâ–ƒâ–â–‚â–‚â–â–â–ƒâ–â–‚â–â–‚
wandb:                                                  val_pearson â–â–â–‡â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.70465
wandb:                                              test_de_pearson 0.89661
wandb:               test_frac_opposite_direction_top20_non_dropout 0.175
wandb:                          test_frac_sigma_below_1_non_dropout 0.5
wandb:                                                     test_mse 0.00804
wandb:                                test_mse_top20_de_non_dropout 0.70465
wandb:                                                 test_pearson 0.97134
wandb:                                           test_pearson_delta 0.42923
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.175
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.5
wandb:                                       test_unseen_single_mse 0.00804
wandb:                                    test_unseen_single_mse_de 0.70465
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.70465
wandb:                                   test_unseen_single_pearson 0.97134
wandb:                                test_unseen_single_pearson_de 0.89661
wandb:                             test_unseen_single_pearson_delta 0.42923
wandb:                                                 train_de_mse 0.00719
wandb:                                             train_de_pearson 0.99752
wandb:                                                    train_mse 0.00219
wandb:                                                train_pearson 0.9935
wandb:                                                training_loss 0.36702
wandb:                                                   val_de_mse 0.18048
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.04142
wandb:                                                  val_pearson 0.86316
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_arrayed_RNA_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/vyvhnzom
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_095343-vyvhnzom/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/25 [00:00<?, ?it/s]  4%|â–         | 1/25 [00:39<15:36, 39.01s/it]  8%|â–Š         | 2/25 [01:30<17:52, 46.63s/it] 12%|â–ˆâ–        | 3/25 [02:00<14:15, 38.89s/it] 16%|â–ˆâ–Œ        | 4/25 [02:37<13:23, 38.27s/it] 20%|â–ˆâ–ˆ        | 5/25 [02:38<08:11, 24.59s/it] 24%|â–ˆâ–ˆâ–       | 6/25 [03:46<12:27, 39.34s/it] 28%|â–ˆâ–ˆâ–Š       | 7/25 [04:15<10:51, 36.18s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [05:17<12:34, 44.36s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [06:06<12:09, 45.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [06:47<11:02, 44.18s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [07:26<09:57, 42.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [08:14<09:35, 44.30s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [09:33<10:57, 54.83s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [10:04<08:43, 47.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [11:15<09:06, 54.68s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [11:51<07:22, 49.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [12:08<05:13, 39.24s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [12:26<03:51, 33.02s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [13:30<04:13, 42.18s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [14:11<03:29, 41.96s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [15:03<03:00, 45.01s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [16:28<02:51, 57.02s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [17:29<01:56, 58.08s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [17:35<00:42, 42.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [17:38<00:00, 30.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [17:38<00:00, 42.35s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_rna/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_101510-u5z5sypr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u5z5sypr
  0%|          | 0/4752 [00:00<?, ?it/s]  0%|          | 4/4752 [00:00<02:03, 38.42it/s]  0%|          | 10/4752 [00:00<01:34, 50.06it/s]  0%|          | 16/4752 [00:00<01:42, 46.10it/s]  0%|          | 22/4752 [00:00<01:33, 50.35it/s]  1%|          | 28/4752 [00:00<01:34, 49.91it/s]  1%|          | 34/4752 [00:00<01:35, 49.48it/s]  1%|          | 39/4752 [00:00<01:36, 49.06it/s]  1%|          | 44/4752 [00:00<01:35, 49.15it/s]  1%|          | 49/4752 [00:01<01:36, 48.97it/s]  1%|          | 54/4752 [00:01<01:35, 49.06it/s]  1%|          | 59/4752 [00:01<01:35, 49.30it/s]  1%|â–         | 64/4752 [00:01<01:40, 46.53it/s]  1%|â–         | 70/4752 [00:01<01:34, 49.77it/s]  2%|â–         | 76/4752 [00:01<01:33, 49.77it/s]  2%|â–         | 82/4752 [00:01<01:34, 49.18it/s]  2%|â–         | 87/4752 [00:01<01:35, 49.09it/s]  2%|â–         | 92/4752 [00:01<01:34, 49.13it/s]  2%|â–         | 97/4752 [00:01<01:34, 49.08it/s]  2%|â–         | 102/4752 [00:02<01:35, 48.92it/s]  2%|â–         | 107/4752 [00:02<01:40, 46.20it/s]  2%|â–         | 113/4752 [00:02<01:33, 49.57it/s]  2%|â–         | 118/4752 [00:02<01:34, 49.10it/s]  3%|â–Ž         | 123/4752 [00:02<01:34, 48.98it/s]  3%|â–Ž         | 128/4752 [00:02<01:34, 48.79it/s]  3%|â–Ž         | 133/4752 [00:02<01:35, 48.39it/s]  3%|â–Ž         | 138/4752 [00:02<01:34, 48.65it/s]  3%|â–Ž         | 144/4752 [00:02<01:33, 49.10it/s]  3%|â–Ž         | 149/4752 [00:03<01:33, 49.23it/s]  3%|â–Ž         | 154/4752 [00:03<01:33, 49.14it/s]  3%|â–Ž         | 159/4752 [00:03<01:33, 49.24it/s]  3%|â–Ž         | 164/4752 [00:03<01:33, 49.00it/s]  4%|â–Ž         | 169/4752 [00:03<01:33, 49.05it/s]  4%|â–Ž         | 174/4752 [00:03<01:32, 49.25it/s]  4%|â–         | 179/4752 [00:03<01:33, 48.91it/s]  4%|â–         | 184/4752 [00:03<01:33, 48.71it/s]  4%|â–         | 189/4752 [00:03<01:33, 49.05it/s]  4%|â–         | 194/4752 [00:03<01:33, 49.01it/s]  4%|â–         | 199/4752 [00:04<01:32, 48.96it/s]  4%|â–         | 204/4752 [00:04<01:33, 48.46it/s]  4%|â–         | 209/4752 [00:04<01:34, 48.02it/s]  5%|â–         | 214/4752 [00:04<01:34, 48.02it/s]  5%|â–         | 219/4752 [00:04<01:33, 48.42it/s]  5%|â–         | 224/4752 [00:04<01:32, 48.74it/s]  5%|â–         | 229/4752 [00:04<01:32, 48.81it/s]  5%|â–         | 234/4752 [00:04<01:32, 48.61it/s]  5%|â–Œ         | 239/4752 [00:04<01:32, 48.89it/s]  5%|â–Œ         | 244/4752 [00:05<01:32, 48.73it/s]  5%|â–Œ         | 249/4752 [00:05<01:32, 48.71it/s]  5%|â–Œ         | 254/4752 [00:05<01:31, 48.92it/s]  5%|â–Œ         | 259/4752 [00:05<01:31, 48.97it/s]  6%|â–Œ         | 264/4752 [00:05<01:31, 48.79it/s]  6%|â–Œ         | 269/4752 [00:05<01:31, 48.91it/s]  6%|â–Œ         | 274/4752 [00:05<01:31, 49.04it/s]  6%|â–Œ         | 279/4752 [00:05<01:31, 48.78it/s]  6%|â–Œ         | 284/4752 [00:05<01:31, 48.93it/s]  6%|â–Œ         | 289/4752 [00:05<01:30, 49.23it/s]  6%|â–Œ         | 294/4752 [00:06<01:31, 48.51it/s]  6%|â–‹         | 299/4752 [00:06<01:31, 48.67it/s]  6%|â–‹         | 304/4752 [00:06<01:31, 48.67it/s]  7%|â–‹         | 309/4752 [00:06<01:31, 48.38it/s]  7%|â–‹         | 314/4752 [00:06<01:31, 48.25it/s]  7%|â–‹         | 319/4752 [00:06<01:31, 48.65it/s]  7%|â–‹         | 324/4752 [00:06<01:35, 46.40it/s]  7%|â–‹         | 329/4752 [00:06<01:34, 46.71it/s]  7%|â–‹         | 334/4752 [00:06<01:33, 47.47it/s]  7%|â–‹         | 339/4752 [00:06<01:32, 47.87it/s]  7%|â–‹         | 344/4752 [00:07<01:31, 48.08it/s]  7%|â–‹         | 350/4752 [00:07<01:29, 48.93it/s]  7%|â–‹         | 355/4752 [00:07<01:30, 48.74it/s]  8%|â–Š         | 360/4752 [00:07<01:30, 48.41it/s]  8%|â–Š         | 365/4752 [00:07<01:31, 47.84it/s]  8%|â–Š         | 370/4752 [00:07<01:31, 47.66it/s]  8%|â–Š         | 376/4752 [00:07<01:30, 48.29it/s]  8%|â–Š         | 381/4752 [00:07<01:42, 42.47it/s]  8%|â–Š         | 386/4752 [00:08<01:51, 39.14it/s]  8%|â–Š         | 391/4752 [00:08<01:58, 36.94it/s]  8%|â–Š         | 395/4752 [00:08<02:01, 35.89it/s]  8%|â–Š         | 399/4752 [00:08<02:04, 34.94it/s]  8%|â–Š         | 403/4752 [00:08<02:06, 34.38it/s]  9%|â–Š         | 407/4752 [00:08<02:08, 33.90it/s]  9%|â–Š         | 412/4752 [00:08<01:58, 36.67it/s]  9%|â–‰         | 417/4752 [00:08<01:51, 38.91it/s]  9%|â–‰         | 422/4752 [00:09<01:46, 40.70it/s]  9%|â–‰         | 427/4752 [00:09<01:43, 41.82it/s]  9%|â–‰         | 432/4752 [00:09<01:41, 42.67it/s]  9%|â–‰         | 437/4752 [00:09<01:40, 43.14it/s]  9%|â–‰         | 442/4752 [00:09<01:39, 43.23it/s]  9%|â–‰         | 447/4752 [00:09<01:37, 44.30it/s] 10%|â–‰         | 453/4752 [00:09<01:33, 46.17it/s] 10%|â–‰         | 458/4752 [00:09<01:31, 46.75it/s] 10%|â–‰         | 463/4752 [00:09<01:30, 47.64it/s] 10%|â–‰         | 468/4752 [00:10<01:29, 47.93it/s] 10%|â–‰         | 473/4752 [00:10<01:28, 48.47it/s] 10%|â–ˆ         | 479/4752 [00:10<01:26, 49.20it/s] 10%|â–ˆ         | 484/4752 [00:10<01:26, 49.32it/s] 10%|â–ˆ         | 490/4752 [00:10<01:25, 49.66it/s] 10%|â–ˆ         | 496/4752 [00:10<01:24, 50.15it/s] 11%|â–ˆ         | 502/4752 [00:10<01:24, 50.36it/s] 11%|â–ˆ         | 508/4752 [00:10<01:33, 45.16it/s] 11%|â–ˆ         | 513/4752 [00:10<01:36, 43.90it/s] 11%|â–ˆ         | 518/4752 [00:11<01:38, 42.91it/s] 11%|â–ˆ         | 523/4752 [00:11<01:34, 44.56it/s] 11%|â–ˆ         | 528/4752 [00:11<01:31, 45.93it/s] 11%|â–ˆ         | 533/4752 [00:11<01:32, 45.64it/s] 11%|â–ˆâ–        | 539/4752 [00:11<01:27, 47.99it/s] 11%|â–ˆâ–        | 545/4752 [00:11<01:28, 47.52it/s] 12%|â–ˆâ–        | 550/4752 [00:11<01:38, 42.76it/s] 12%|â–ˆâ–        | 555/4752 [00:11<01:46, 39.30it/s] 12%|â–ˆâ–        | 560/4752 [00:12<01:55, 36.16it/s] 12%|â–ˆâ–        | 564/4752 [00:12<02:00, 34.73it/s] 12%|â–ˆâ–        | 568/4752 [00:12<02:04, 33.50it/s] 12%|â–ˆâ–        | 572/4752 [00:12<02:07, 32.82it/s] 12%|â–ˆâ–        | 576/4752 [00:12<02:07, 32.64it/s] 12%|â–ˆâ–        | 580/4752 [00:12<02:10, 31.92it/s] 12%|â–ˆâ–        | 584/4752 [00:12<02:10, 31.82it/s] 12%|â–ˆâ–        | 588/4752 [00:13<02:10, 31.82it/s] 12%|â–ˆâ–        | 592/4752 [00:13<02:12, 31.38it/s] 13%|â–ˆâ–Ž        | 596/4752 [00:13<02:09, 32.05it/s] 13%|â–ˆâ–Ž        | 601/4752 [00:13<02:08, 32.28it/s] 13%|â–ˆâ–Ž        | 605/4752 [00:13<02:08, 32.31it/s] 13%|â–ˆâ–Ž        | 609/4752 [00:13<02:08, 32.21it/s] 13%|â–ˆâ–Ž        | 613/4752 [00:13<02:09, 32.00it/s] 13%|â–ˆâ–Ž        | 617/4752 [00:13<02:09, 32.05it/s] 13%|â–ˆâ–Ž        | 621/4752 [00:14<02:08, 32.09it/s] 13%|â–ˆâ–Ž        | 625/4752 [00:14<02:08, 32.19it/s] 13%|â–ˆâ–Ž        | 629/4752 [00:14<02:05, 32.83it/s] 13%|â–ˆâ–Ž        | 633/4752 [00:14<02:06, 32.52it/s] 13%|â–ˆâ–Ž        | 637/4752 [00:14<02:07, 32.27it/s] 13%|â–ˆâ–Ž        | 641/4752 [00:14<02:06, 32.58it/s] 14%|â–ˆâ–Ž        | 645/4752 [00:14<02:08, 32.02it/s] 14%|â–ˆâ–Ž        | 649/4752 [00:14<02:04, 33.07it/s] 14%|â–ˆâ–Ž        | 653/4752 [00:15<02:03, 33.06it/s] 14%|â–ˆâ–        | 658/4752 [00:15<01:59, 34.18it/s] 14%|â–ˆâ–        | 664/4752 [00:15<01:43, 39.45it/s] 14%|â–ˆâ–        | 670/4752 [00:15<01:38, 41.38it/s] 14%|â–ˆâ–        | 676/4752 [00:15<01:31, 44.54it/s] 14%|â–ˆâ–        | 683/4752 [00:15<01:22, 49.22it/s] 14%|â–ˆâ–        | 689/4752 [00:15<01:20, 50.17it/s] 15%|â–ˆâ–        | 695/4752 [00:15<01:19, 50.96it/s] 15%|â–ˆâ–        | 701/4752 [00:15<01:17, 52.05it/s] 15%|â–ˆâ–        | 707/4752 [00:16<01:17, 51.91it/s] 15%|â–ˆâ–Œ        | 713/4752 [00:16<01:17, 52.21it/s] 15%|â–ˆâ–Œ        | 719/4752 [00:16<01:16, 52.42it/s] 15%|â–ˆâ–Œ        | 725/4752 [00:16<01:16, 52.84it/s] 15%|â–ˆâ–Œ        | 731/4752 [00:16<01:15, 53.01it/s] 16%|â–ˆâ–Œ        | 737/4752 [00:16<01:15, 53.02it/s] 16%|â–ˆâ–Œ        | 743/4752 [00:16<01:15, 52.78it/s] 16%|â–ˆâ–Œ        | 749/4752 [00:16<01:15, 52.92it/s] 16%|â–ˆâ–Œ        | 755/4752 [00:16<01:15, 52.92it/s] 16%|â–ˆâ–Œ        | 761/4752 [00:17<01:16, 52.48it/s] 16%|â–ˆâ–Œ        | 767/4752 [00:17<01:15, 52.65it/s] 16%|â–ˆâ–‹        | 773/4752 [00:17<01:15, 52.91it/s] 16%|â–ˆâ–‹        | 779/4752 [00:17<01:15, 52.86it/s] 17%|â–ˆâ–‹        | 785/4752 [00:17<01:16, 51.90it/s] 17%|â–ˆâ–‹        | 791/4752 [00:17<01:16, 51.51it/s] 17%|â–ˆâ–‹        | 797/4752 [00:17<01:22, 47.94it/s] 17%|â–ˆâ–‹        | 802/4752 [00:18<01:40, 39.23it/s] 17%|â–ˆâ–‹        | 807/4752 [00:18<01:40, 39.27it/s] 17%|â–ˆâ–‹        | 812/4752 [00:18<01:35, 41.36it/s] 17%|â–ˆâ–‹        | 817/4752 [00:18<01:31, 43.03it/s] 17%|â–ˆâ–‹        | 822/4752 [00:18<01:28, 44.16it/s] 17%|â–ˆâ–‹        | 827/4752 [00:18<01:27, 45.01it/s] 18%|â–ˆâ–Š        | 832/4752 [00:18<01:25, 45.72it/s] 18%|â–ˆâ–Š        | 838/4752 [00:18<01:22, 47.34it/s] 18%|â–ˆâ–Š        | 844/4752 [00:18<01:20, 48.75it/s] 18%|â–ˆâ–Š        | 850/4752 [00:19<01:18, 49.64it/s] 18%|â–ˆâ–Š        | 856/4752 [00:19<01:21, 48.05it/s] 18%|â–ˆâ–Š        | 863/4752 [00:19<01:15, 51.44it/s] 18%|â–ˆâ–Š        | 869/4752 [00:19<01:15, 51.27it/s] 18%|â–ˆâ–Š        | 875/4752 [00:19<01:14, 51.92it/s] 19%|â–ˆâ–Š        | 881/4752 [00:19<01:14, 52.04it/s] 19%|â–ˆâ–Š        | 887/4752 [00:19<01:17, 49.63it/s] 19%|â–ˆâ–‰        | 894/4752 [00:19<01:13, 52.54it/s] 19%|â–ˆâ–‰        | 900/4752 [00:20<01:21, 47.26it/s] 19%|â–ˆâ–‰        | 905/4752 [00:20<01:23, 46.09it/s] 19%|â–ˆâ–‰        | 910/4752 [00:20<01:24, 45.64it/s] 19%|â–ˆâ–‰        | 915/4752 [00:20<01:24, 45.44it/s] 19%|â–ˆâ–‰        | 920/4752 [00:20<01:25, 44.91it/s] 19%|â–ˆâ–‰        | 925/4752 [00:20<01:23, 45.73it/s] 20%|â–ˆâ–‰        | 930/4752 [00:20<01:24, 45.42it/s] 20%|â–ˆâ–‰        | 936/4752 [00:20<01:20, 47.22it/s] 20%|â–ˆâ–‰        | 941/4752 [00:20<01:19, 47.78it/s] 20%|â–ˆâ–‰        | 946/4752 [00:21<01:18, 48.34it/s] 20%|â–ˆâ–ˆ        | 952/4752 [00:21<01:16, 49.99it/s] 20%|â–ˆâ–ˆ        | 958/4752 [00:21<01:14, 50.73it/s] 20%|â–ˆâ–ˆ        | 964/4752 [00:21<01:14, 50.85it/s] 20%|â–ˆâ–ˆ        | 970/4752 [00:21<01:14, 50.97it/s] 21%|â–ˆâ–ˆ        | 976/4752 [00:21<01:17, 48.69it/s] 21%|â–ˆâ–ˆ        | 981/4752 [00:21<01:29, 42.22it/s] 21%|â–ˆâ–ˆ        | 986/4752 [00:21<01:38, 38.23it/s] 21%|â–ˆâ–ˆ        | 990/4752 [00:22<01:38, 38.23it/s] 21%|â–ˆâ–ˆ        | 995/4752 [00:22<01:34, 39.82it/s] 21%|â–ˆâ–ˆ        | 1000/4752 [00:22<01:35, 39.47it/s] 21%|â–ˆâ–ˆ        | 1005/4752 [00:22<01:31, 40.96it/s] 21%|â–ˆâ–ˆâ–       | 1011/4752 [00:22<01:26, 43.10it/s] 21%|â–ˆâ–ˆâ–       | 1017/4752 [00:22<01:21, 46.08it/s] 22%|â–ˆâ–ˆâ–       | 1022/4752 [00:22<01:20, 46.12it/s] 22%|â–ˆâ–ˆâ–       | 1027/4752 [00:22<01:19, 46.74it/s] 22%|â–ˆâ–ˆâ–       | 1033/4752 [00:22<01:16, 48.60it/s] 22%|â–ˆâ–ˆâ–       | 1038/4752 [00:23<01:15, 48.95it/s] 22%|â–ˆâ–ˆâ–       | 1044/4752 [00:23<01:14, 49.57it/s] 22%|â–ˆâ–ˆâ–       | 1050/4752 [00:23<01:13, 50.41it/s] 22%|â–ˆâ–ˆâ–       | 1056/4752 [00:23<01:13, 50.29it/s] 22%|â–ˆâ–ˆâ–       | 1062/4752 [00:23<01:13, 50.48it/s] 22%|â–ˆâ–ˆâ–       | 1068/4752 [00:23<01:12, 50.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 1074/4752 [00:23<01:12, 50.80it/s] 23%|â–ˆâ–ˆâ–Ž       | 1080/4752 [00:23<01:11, 51.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 1086/4752 [00:23<01:11, 51.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 1092/4752 [00:24<01:11, 51.52it/s] 23%|â–ˆâ–ˆâ–Ž       | 1098/4752 [00:24<01:11, 51.30it/s] 23%|â–ˆâ–ˆâ–Ž       | 1104/4752 [00:24<01:12, 50.50it/s] 23%|â–ˆâ–ˆâ–Ž       | 1110/4752 [00:24<01:11, 50.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 1116/4752 [00:24<01:11, 50.81it/s] 24%|â–ˆâ–ˆâ–Ž       | 1122/4752 [00:24<01:14, 48.81it/s] 24%|â–ˆâ–ˆâ–       | 1129/4752 [00:24<01:09, 52.42it/s] 24%|â–ˆâ–ˆâ–       | 1135/4752 [00:24<01:09, 52.34it/s] 24%|â–ˆâ–ˆâ–       | 1141/4752 [00:25<01:09, 51.83it/s] 24%|â–ˆâ–ˆâ–       | 1147/4752 [00:25<01:10, 51.25it/s] 24%|â–ˆâ–ˆâ–       | 1153/4752 [00:25<01:12, 49.60it/s] 24%|â–ˆâ–ˆâ–       | 1158/4752 [00:25<01:14, 47.96it/s] 24%|â–ˆâ–ˆâ–       | 1163/4752 [00:25<01:16, 46.80it/s] 25%|â–ˆâ–ˆâ–       | 1168/4752 [00:25<01:16, 47.01it/s] 25%|â–ˆâ–ˆâ–       | 1173/4752 [00:25<01:16, 46.63it/s] 25%|â–ˆâ–ˆâ–       | 1178/4752 [00:25<01:17, 46.27it/s] 25%|â–ˆâ–ˆâ–       | 1183/4752 [00:25<01:16, 46.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 1188/4752 [00:26<01:17, 45.84it/s] 25%|â–ˆâ–ˆâ–Œ       | 1193/4752 [00:26<01:18, 45.14it/s] 25%|â–ˆâ–ˆâ–Œ       | 1198/4752 [00:26<01:24, 42.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 1203/4752 [00:26<01:23, 42.55it/s] 25%|â–ˆâ–ˆâ–Œ       | 1209/4752 [00:26<01:18, 44.92it/s] 26%|â–ˆâ–ˆâ–Œ       | 1215/4752 [00:26<01:15, 46.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 1220/4752 [00:26<01:14, 47.32it/s] 26%|â–ˆâ–ˆâ–Œ       | 1226/4752 [00:26<01:16, 46.26it/s] 26%|â–ˆâ–ˆâ–Œ       | 1233/4752 [00:27<01:09, 50.47it/s] 26%|â–ˆâ–ˆâ–Œ       | 1239/4752 [00:27<01:09, 50.61it/s] 26%|â–ˆâ–ˆâ–Œ       | 1245/4752 [00:27<01:08, 50.86it/s] 26%|â–ˆâ–ˆâ–‹       | 1251/4752 [00:27<01:07, 51.81it/s] 26%|â–ˆâ–ˆâ–‹       | 1257/4752 [00:27<01:08, 50.96it/s] 27%|â–ˆâ–ˆâ–‹       | 1263/4752 [00:27<01:10, 49.44it/s] 27%|â–ˆâ–ˆâ–‹       | 1268/4752 [00:27<01:12, 48.14it/s] 27%|â–ˆâ–ˆâ–‹       | 1273/4752 [00:27<01:14, 46.93it/s] 27%|â–ˆâ–ˆâ–‹       | 1278/4752 [00:27<01:14, 46.89it/s] 27%|â–ˆâ–ˆâ–‹       | 1283/4752 [00:28<01:14, 46.37it/s] 27%|â–ˆâ–ˆâ–‹       | 1288/4752 [00:28<01:15, 45.71it/s] 27%|â–ˆâ–ˆâ–‹       | 1293/4752 [00:28<01:16, 45.33it/s] 27%|â–ˆâ–ˆâ–‹       | 1298/4752 [00:28<01:15, 45.71it/s] 27%|â–ˆâ–ˆâ–‹       | 1303/4752 [00:28<01:16, 45.21it/s] 28%|â–ˆâ–ˆâ–Š       | 1308/4752 [00:28<01:24, 40.92it/s] 28%|â–ˆâ–ˆâ–Š       | 1313/4752 [00:28<01:32, 37.19it/s] 28%|â–ˆâ–ˆâ–Š       | 1318/4752 [00:28<01:30, 37.98it/s] 28%|â–ˆâ–ˆâ–Š       | 1322/4752 [00:29<01:39, 34.31it/s] 28%|â–ˆâ–ˆâ–Š       | 1326/4752 [00:29<01:40, 34.08it/s] 28%|â–ˆâ–ˆâ–Š       | 1330/4752 [00:29<01:40, 34.06it/s] 28%|â–ˆâ–ˆâ–Š       | 1334/4752 [00:29<01:39, 34.40it/s] 28%|â–ˆâ–ˆâ–Š       | 1338/4752 [00:29<01:36, 35.56it/s] 28%|â–ˆâ–ˆâ–Š       | 1344/4752 [00:29<01:25, 39.98it/s] 28%|â–ˆâ–ˆâ–Š       | 1349/4752 [00:29<01:21, 41.76it/s] 28%|â–ˆâ–ˆâ–Š       | 1354/4752 [00:29<01:19, 42.56it/s] 29%|â–ˆâ–ˆâ–Š       | 1359/4752 [00:30<01:16, 44.32it/s] 29%|â–ˆâ–ˆâ–Š       | 1365/4752 [00:30<01:13, 46.10it/s] 29%|â–ˆâ–ˆâ–‰       | 1371/4752 [00:30<01:11, 47.32it/s] 29%|â–ˆâ–ˆâ–‰       | 1377/4752 [00:30<01:13, 46.19it/s] 29%|â–ˆâ–ˆâ–‰       | 1384/4752 [00:30<01:07, 50.09it/s] 29%|â–ˆâ–ˆâ–‰       | 1390/4752 [00:30<01:06, 50.54it/s] 29%|â–ˆâ–ˆâ–‰       | 1396/4752 [00:30<01:09, 48.62it/s] 30%|â–ˆâ–ˆâ–‰       | 1403/4752 [00:30<01:04, 52.03it/s] 30%|â–ˆâ–ˆâ–‰       | 1409/4752 [00:30<01:05, 50.68it/s] 30%|â–ˆâ–ˆâ–‰       | 1415/4752 [00:31<01:06, 50.18it/s] 30%|â–ˆâ–ˆâ–‰       | 1421/4752 [00:31<01:05, 51.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1427/4752 [00:31<01:21, 40.66it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1432/4752 [00:31<01:26, 38.47it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1437/4752 [00:31<01:28, 37.45it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1441/4752 [00:31<01:29, 37.01it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1445/4752 [00:31<01:28, 37.40it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1449/4752 [00:32<01:27, 37.74it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1454/4752 [00:32<01:26, 37.91it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1460/4752 [00:32<01:15, 43.53it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1465/4752 [00:32<01:12, 45.22it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1471/4752 [00:32<01:13, 44.61it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1478/4752 [00:32<01:13, 44.51it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1483/4752 [00:32<01:22, 39.40it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1488/4752 [00:33<01:29, 36.46it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1492/4752 [00:33<01:34, 34.42it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1497/4752 [00:33<01:36, 33.73it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1501/4752 [00:33<01:35, 34.19it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1506/4752 [00:33<01:27, 37.14it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1510/4752 [00:33<01:27, 37.13it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1515/4752 [00:33<01:22, 39.35it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1520/4752 [00:33<01:18, 41.06it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1526/4752 [00:33<01:10, 45.60it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1531/4752 [00:34<01:13, 44.04it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1536/4752 [00:34<01:21, 39.25it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1541/4752 [00:34<01:28, 36.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1545/4752 [00:34<01:27, 36.52it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1549/4752 [00:34<01:26, 37.08it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1555/4752 [00:34<01:15, 42.45it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1560/4752 [00:34<01:12, 43.75it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1565/4752 [00:34<01:10, 45.19it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1571/4752 [00:35<01:07, 46.94it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1576/4752 [00:35<01:07, 47.40it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1581/4752 [00:35<01:06, 47.68it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1586/4752 [00:35<01:07, 47.07it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1591/4752 [00:35<01:07, 46.50it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1596/4752 [00:35<01:08, 46.31it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1601/4752 [00:35<01:08, 46.00it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1606/4752 [00:35<01:08, 45.70it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1611/4752 [00:35<01:09, 45.21it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1616/4752 [00:36<01:09, 45.24it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1621/4752 [00:36<01:09, 45.10it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1626/4752 [00:36<01:09, 45.12it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1631/4752 [00:36<01:08, 45.65it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1636/4752 [00:36<01:08, 45.77it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1641/4752 [00:36<01:07, 45.75it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1646/4752 [00:36<01:07, 45.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1651/4752 [00:36<01:08, 45.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1656/4752 [00:36<01:07, 45.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1661/4752 [00:37<01:07, 45.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1667/4752 [00:37<01:05, 47.39it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1672/4752 [00:37<01:04, 48.10it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1678/4752 [00:37<01:02, 49.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1684/4752 [00:37<01:01, 49.94it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1690/4752 [00:37<01:01, 50.08it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1696/4752 [00:37<01:02, 49.26it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1701/4752 [00:37<01:03, 48.06it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1706/4752 [00:37<01:04, 47.25it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1711/4752 [00:38<01:04, 46.84it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1716/4752 [00:38<01:04, 46.88it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1721/4752 [00:38<01:08, 44.30it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1726/4752 [00:38<01:58, 25.47it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1739/4752 [00:38<01:10, 42.69it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1745/4752 [00:38<01:08, 43.81it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1751/4752 [00:39<01:07, 44.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1757/4752 [00:39<01:06, 44.79it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1763/4752 [00:39<01:05, 45.81it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1769/4752 [00:39<01:03, 47.15it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1775/4752 [00:39<01:01, 48.74it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1781/4752 [00:39<01:06, 44.81it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1787/4752 [00:39<01:03, 46.59it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1793/4752 [00:39<01:01, 47.84it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1799/4752 [00:40<01:00, 48.82it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1805/4752 [00:40<01:02, 46.85it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1812/4752 [00:40<00:58, 50.54it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1818/4752 [00:40<00:57, 50.72it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1824/4752 [00:40<00:57, 50.88it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1830/4752 [00:40<00:56, 52.03it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1836/4752 [00:40<01:03, 45.65it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1841/4752 [00:40<01:04, 45.38it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1846/4752 [00:41<01:03, 45.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1851/4752 [00:41<01:03, 45.67it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1856/4752 [00:41<01:03, 45.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1862/4752 [00:41<01:01, 47.29it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1868/4752 [00:41<00:59, 48.23it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1874/4752 [00:41<00:58, 49.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1880/4752 [00:41<00:57, 49.78it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1886/4752 [00:41<00:56, 50.78it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1892/4752 [00:41<00:56, 50.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1898/4752 [00:42<00:55, 51.31it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1904/4752 [00:42<00:57, 49.76it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1909/4752 [00:42<01:04, 44.36it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1914/4752 [00:42<01:08, 41.30it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1919/4752 [00:42<01:12, 39.34it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1923/4752 [00:42<01:13, 38.54it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1927/4752 [00:42<01:14, 37.82it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1931/4752 [00:42<01:18, 36.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1935/4752 [00:43<01:17, 36.27it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1940/4752 [00:43<01:17, 36.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1945/4752 [00:43<01:11, 39.07it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1949/4752 [00:43<01:11, 39.09it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1954/4752 [00:43<01:08, 40.66it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1959/4752 [00:43<01:06, 41.96it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1964/4752 [00:43<01:04, 43.33it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1970/4752 [00:43<01:00, 45.91it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1975/4752 [00:43<01:00, 46.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1980/4752 [00:44<00:59, 46.29it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1985/4752 [00:44<01:00, 46.01it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1990/4752 [00:44<01:00, 45.31it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1995/4752 [00:44<01:01, 45.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2000/4752 [00:44<01:01, 45.04it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2005/4752 [00:45<02:44, 16.68it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2032/4752 [00:45<00:57, 47.72it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2042/4752 [00:45<01:01, 44.42it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2050/4752 [00:46<01:29, 30.23it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2057/4752 [00:46<01:22, 32.80it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2063/4752 [00:46<01:20, 33.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2089/4752 [00:46<00:40, 65.30it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2100/4752 [00:46<00:43, 61.58it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2109/4752 [00:47<00:44, 58.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2117/4752 [00:47<00:46, 57.26it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2124/4752 [00:47<01:20, 32.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2149/4752 [00:48<01:01, 42.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2170/4752 [00:48<00:56, 45.39it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2192/4752 [00:48<00:40, 63.27it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2202/4752 [00:48<00:42, 60.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2211/4752 [00:49<00:43, 59.05it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2219/4752 [00:49<00:43, 57.80it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2226/4752 [00:49<00:44, 56.31it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2233/4752 [00:49<01:07, 37.45it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2248/4752 [00:49<00:46, 53.48it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2256/4752 [00:50<00:54, 46.05it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2263/4752 [00:50<00:57, 43.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2269/4752 [00:50<00:58, 42.22it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2275/4752 [00:50<00:58, 42.46it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2280/4752 [00:50<00:59, 41.82it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2286/4752 [00:50<00:55, 44.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2292/4752 [00:50<00:53, 46.23it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2298/4752 [00:51<00:51, 47.80it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2304/4752 [00:51<00:50, 48.76it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2310/4752 [00:51<00:49, 49.23it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2316/4752 [00:51<00:48, 49.93it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2322/4752 [00:51<00:47, 50.98it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2328/4752 [00:51<00:47, 51.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2334/4752 [00:51<00:46, 51.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2340/4752 [00:51<00:47, 50.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2346/4752 [00:51<00:48, 49.68it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2351/4752 [00:52<01:29, 26.86it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2355/4752 [00:52<01:37, 24.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2381/4752 [00:52<00:36, 64.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2392/4752 [00:52<00:40, 57.94it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2401/4752 [00:53<00:42, 55.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2409/4752 [00:53<00:43, 53.40it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2416/4752 [00:53<01:10, 33.37it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2437/4752 [00:53<00:40, 57.05it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2447/4752 [00:54<00:57, 39.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2455/4752 [00:54<00:59, 38.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2478/4752 [00:54<00:36, 62.94it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2489/4752 [00:54<00:38, 58.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2498/4752 [00:55<01:01, 36.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2517/4752 [00:55<00:41, 53.82it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2527/4752 [00:55<00:42, 52.97it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2536/4752 [00:55<00:42, 52.09it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2544/4752 [00:56<00:48, 45.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2551/4752 [00:56<01:15, 29.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2568/4752 [00:56<00:48, 45.09it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2576/4752 [00:57<00:48, 45.29it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2583/4752 [00:57<00:47, 46.15it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2590/4752 [00:57<00:45, 47.06it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2596/4752 [00:57<00:45, 47.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2602/4752 [00:57<00:45, 46.99it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2608/4752 [00:57<00:45, 47.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2614/4752 [00:58<01:19, 26.83it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2636/4752 [00:58<00:39, 53.66it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2645/4752 [00:58<00:37, 55.52it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2653/4752 [00:58<00:38, 55.22it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2660/4752 [00:58<00:38, 54.82it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2667/4752 [00:58<00:38, 54.55it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2674/4752 [00:58<00:37, 54.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2680/4752 [00:59<00:37, 54.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2686/4752 [00:59<00:37, 54.87it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2692/4752 [00:59<00:37, 54.55it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2698/4752 [00:59<00:37, 54.96it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2704/4752 [00:59<00:37, 54.81it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2710/4752 [00:59<00:37, 54.73it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2716/4752 [00:59<00:37, 54.92it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2722/4752 [00:59<00:37, 54.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2728/4752 [00:59<00:37, 54.49it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2734/4752 [01:00<00:37, 54.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2740/4752 [01:00<00:37, 54.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2746/4752 [01:00<00:36, 54.87it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2752/4752 [01:00<00:36, 55.02it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2758/4752 [01:00<00:36, 54.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2764/4752 [01:00<00:36, 54.33it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2770/4752 [01:00<00:36, 54.77it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2776/4752 [01:00<00:36, 54.81it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2782/4752 [01:00<00:37, 52.53it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2789/4752 [01:01<00:35, 55.03it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2795/4752 [01:01<00:35, 55.54it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2801/4752 [01:01<01:02, 31.05it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2822/4752 [01:01<00:31, 61.80it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2831/4752 [01:01<00:33, 57.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2839/4752 [01:02<00:32, 58.34it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2847/4752 [01:02<00:33, 57.01it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2854/4752 [01:02<00:37, 50.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2860/4752 [01:02<00:41, 45.66it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2866/4752 [01:02<00:44, 42.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2871/4752 [01:02<00:43, 43.49it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2876/4752 [01:02<00:42, 43.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2881/4752 [01:03<00:41, 44.61it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2886/4752 [01:03<00:41, 45.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2891/4752 [01:03<00:40, 46.15it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2896/4752 [01:03<00:40, 45.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2901/4752 [01:03<00:40, 45.60it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2906/4752 [01:03<00:40, 45.25it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2911/4752 [01:03<00:42, 43.14it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2917/4752 [01:03<00:38, 47.60it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2922/4752 [01:03<00:39, 46.64it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2927/4752 [01:04<00:40, 44.55it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2932/4752 [01:04<00:46, 39.56it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2937/4752 [01:04<00:44, 41.20it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2943/4752 [01:04<00:41, 43.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2948/4752 [01:04<00:42, 42.83it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2953/4752 [01:04<00:41, 43.27it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2958/4752 [01:04<00:40, 43.92it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2963/4752 [01:04<00:40, 44.11it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2968/4752 [01:05<00:40, 44.22it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2973/4752 [01:05<01:16, 23.34it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2991/4752 [01:05<00:36, 47.66it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2998/4752 [01:05<00:37, 46.54it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3005/4752 [01:05<00:38, 45.96it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3011/4752 [01:06<00:38, 45.44it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3017/4752 [01:06<00:38, 45.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3023/4752 [01:06<01:03, 27.13it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3042/4752 [01:06<00:33, 50.60it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3050/4752 [01:06<00:34, 49.71it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3057/4752 [01:07<00:35, 48.11it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3064/4752 [01:07<00:36, 46.69it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3070/4752 [01:07<00:36, 46.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3076/4752 [01:07<00:36, 45.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3081/4752 [01:07<00:37, 44.26it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3086/4752 [01:07<00:38, 42.99it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3091/4752 [01:07<00:38, 42.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3096/4752 [01:07<00:38, 43.39it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3101/4752 [01:08<00:37, 44.40it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3106/4752 [01:08<00:38, 42.97it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3111/4752 [01:08<00:38, 42.22it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3116/4752 [01:08<00:37, 43.09it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3121/4752 [01:08<00:37, 43.48it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3126/4752 [01:08<00:37, 43.75it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3131/4752 [01:08<00:36, 44.91it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3136/4752 [01:08<00:41, 38.73it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3142/4752 [01:09<00:37, 43.29it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3147/4752 [01:09<00:36, 44.05it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3152/4752 [01:09<00:36, 44.31it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3157/4752 [01:09<00:35, 44.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3162/4752 [01:09<00:36, 44.05it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3167/4752 [01:09<00:36, 43.80it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3172/4752 [01:09<00:36, 43.25it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3177/4752 [01:09<00:36, 43.57it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3182/4752 [01:09<00:35, 43.66it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3187/4752 [01:10<00:35, 43.85it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3192/4752 [01:10<00:36, 42.26it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3197/4752 [01:10<00:35, 43.91it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3202/4752 [01:10<00:34, 45.08it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3207/4752 [01:10<00:33, 46.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3212/4752 [01:10<00:33, 46.29it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3217/4752 [01:10<00:33, 45.95it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3222/4752 [01:10<00:34, 44.56it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3227/4752 [01:10<00:33, 45.27it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3232/4752 [01:11<00:33, 44.88it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3237/4752 [01:11<00:34, 44.39it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3242/4752 [01:11<00:34, 43.32it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3247/4752 [01:11<00:35, 42.65it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3252/4752 [01:11<00:34, 43.07it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3257/4752 [01:11<00:34, 43.29it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3262/4752 [01:11<00:36, 41.33it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3268/4752 [01:11<00:33, 43.98it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3273/4752 [01:12<00:35, 42.21it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3278/4752 [01:12<00:35, 41.89it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3283/4752 [01:12<00:34, 42.66it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3288/4752 [01:12<00:32, 44.37it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3293/4752 [01:12<00:32, 45.54it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3298/4752 [01:12<00:33, 43.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3303/4752 [01:12<00:33, 43.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3308/4752 [01:12<00:32, 43.76it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3313/4752 [01:12<00:32, 44.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3318/4752 [01:13<00:31, 46.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3323/4752 [01:13<00:31, 45.25it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3328/4752 [01:13<00:32, 43.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3333/4752 [01:13<00:33, 42.83it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3338/4752 [01:13<00:33, 41.87it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3344/4752 [01:13<00:31, 45.20it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3349/4752 [01:13<00:30, 45.36it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3354/4752 [01:13<00:31, 44.69it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3359/4752 [01:13<00:31, 44.46it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3364/4752 [01:14<00:31, 44.41it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3369/4752 [01:14<00:30, 45.11it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3374/4752 [01:14<00:30, 45.84it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3379/4752 [01:14<00:30, 45.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3384/4752 [01:14<00:30, 45.03it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3389/4752 [01:14<00:30, 44.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3394/4752 [01:14<00:30, 45.19it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3399/4752 [01:14<00:29, 45.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3404/4752 [01:14<00:29, 45.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3409/4752 [01:15<00:29, 45.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3414/4752 [01:15<00:29, 44.67it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3419/4752 [01:15<00:29, 44.57it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3424/4752 [01:15<00:29, 45.57it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3429/4752 [01:15<00:29, 45.47it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3434/4752 [01:15<00:29, 45.25it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3439/4752 [01:15<00:29, 45.00it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3444/4752 [01:15<00:29, 45.03it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3449/4752 [01:15<00:28, 45.52it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3454/4752 [01:16<00:28, 45.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3459/4752 [01:16<00:28, 45.61it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3464/4752 [01:16<00:28, 45.90it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3469/4752 [01:16<00:28, 45.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3474/4752 [01:16<00:28, 45.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3479/4752 [01:16<00:28, 45.31it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3484/4752 [01:16<00:28, 45.00it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3489/4752 [01:16<00:28, 44.72it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3494/4752 [01:16<00:28, 44.70it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3499/4752 [01:17<00:27, 45.30it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3504/4752 [01:17<00:27, 44.74it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3509/4752 [01:17<00:28, 44.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3514/4752 [01:17<00:28, 44.19it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3519/4752 [01:17<00:29, 42.26it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3524/4752 [01:17<00:29, 41.08it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3529/4752 [01:17<00:29, 41.84it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3534/4752 [01:17<00:28, 43.05it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3539/4752 [01:18<00:27, 43.55it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3545/4752 [01:18<00:28, 42.15it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3550/4752 [01:18<00:28, 42.93it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3555/4752 [01:18<00:27, 43.21it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3560/4752 [01:18<00:27, 43.67it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3565/4752 [01:18<00:26, 43.98it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3570/4752 [01:18<00:26, 44.59it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3575/4752 [01:18<00:26, 44.23it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3580/4752 [01:18<00:26, 44.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3585/4752 [01:19<00:26, 44.54it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3590/4752 [01:19<00:26, 43.97it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3595/4752 [01:19<00:26, 43.71it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3600/4752 [01:19<00:26, 43.88it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3605/4752 [01:19<00:26, 43.59it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3610/4752 [01:19<00:26, 42.50it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3615/4752 [01:19<00:27, 41.78it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3620/4752 [01:19<00:26, 42.23it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3625/4752 [01:19<00:26, 42.77it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3630/4752 [01:20<00:25, 43.36it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3635/4752 [01:20<00:25, 43.25it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3640/4752 [01:20<00:25, 43.23it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3645/4752 [01:20<00:25, 43.70it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3650/4752 [01:20<00:24, 44.31it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3655/4752 [01:20<00:24, 44.81it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3660/4752 [01:20<00:24, 44.78it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3665/4752 [01:20<00:24, 43.77it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3670/4752 [01:21<00:25, 42.81it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3675/4752 [01:21<00:25, 42.86it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3680/4752 [01:21<00:24, 43.70it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3685/4752 [01:21<00:24, 44.15it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3690/4752 [01:21<00:24, 43.47it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3695/4752 [01:21<00:25, 41.64it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3700/4752 [01:21<00:24, 42.15it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3705/4752 [01:21<00:24, 42.34it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3710/4752 [01:21<00:26, 39.07it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3715/4752 [01:22<00:24, 41.55it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3720/4752 [01:22<00:24, 41.56it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3725/4752 [01:22<00:40, 25.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3740/4752 [01:22<00:21, 46.00it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3746/4752 [01:22<00:22, 45.02it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3752/4752 [01:22<00:22, 44.52it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3758/4752 [01:23<00:22, 44.14it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3763/4752 [01:23<00:22, 43.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3768/4752 [01:23<00:22, 42.89it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3773/4752 [01:23<00:22, 42.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3778/4752 [01:23<00:22, 43.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3783/4752 [01:23<00:22, 44.03it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3788/4752 [01:23<00:22, 43.28it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3793/4752 [01:23<00:22, 43.09it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3798/4752 [01:24<00:21, 43.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3803/4752 [01:24<00:21, 44.12it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3808/4752 [01:24<00:21, 44.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3813/4752 [01:24<00:21, 44.19it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3818/4752 [01:24<00:20, 44.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3823/4752 [01:24<00:20, 45.46it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3828/4752 [01:24<00:20, 45.50it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3833/4752 [01:24<00:20, 44.90it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3838/4752 [01:24<00:20, 44.40it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3843/4752 [01:25<00:20, 44.34it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3848/4752 [01:25<00:20, 44.62it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3853/4752 [01:25<00:20, 44.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3858/4752 [01:25<00:20, 44.60it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3863/4752 [01:25<00:20, 43.96it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3868/4752 [01:25<00:20, 42.42it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3873/4752 [01:25<00:20, 42.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3878/4752 [01:25<00:20, 42.69it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3883/4752 [01:25<00:20, 42.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3888/4752 [01:26<00:21, 39.28it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3894/4752 [01:26<00:19, 43.24it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3900/4752 [01:26<00:18, 45.40it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3905/4752 [01:26<00:18, 46.46it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3910/4752 [01:26<00:18, 46.48it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3915/4752 [01:26<00:18, 45.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3920/4752 [01:26<00:18, 45.93it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3925/4752 [01:26<00:17, 46.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3930/4752 [01:27<00:17, 47.01it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3935/4752 [01:27<00:17, 45.49it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3940/4752 [01:27<00:18, 45.01it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3945/4752 [01:27<00:18, 43.90it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3950/4752 [01:27<00:18, 42.58it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3955/4752 [01:27<00:18, 43.32it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3960/4752 [01:27<00:18, 43.83it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3965/4752 [01:27<00:18, 43.06it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3970/4752 [01:27<00:18, 41.54it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3977/4752 [01:28<00:16, 46.36it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3982/4752 [01:28<00:16, 46.61it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3987/4752 [01:28<00:16, 46.19it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3992/4752 [01:28<00:16, 45.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3997/4752 [01:28<00:16, 46.35it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4002/4752 [01:28<00:16, 46.78it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4007/4752 [01:28<00:16, 46.54it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4012/4752 [01:28<00:15, 46.35it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4017/4752 [01:28<00:15, 46.14it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4022/4752 [01:29<00:15, 46.95it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4027/4752 [01:29<00:15, 46.65it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4032/4752 [01:29<00:15, 46.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 4037/4752 [01:29<00:15, 45.99it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4042/4752 [01:29<00:15, 45.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4047/4752 [01:29<00:15, 46.41it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4052/4752 [01:29<00:15, 46.33it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4057/4752 [01:29<00:14, 46.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4062/4752 [01:29<00:14, 46.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4067/4752 [01:30<00:15, 45.52it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4072/4752 [01:30<00:14, 45.79it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4077/4752 [01:30<00:14, 45.15it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4082/4752 [01:30<00:14, 44.98it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4087/4752 [01:30<00:14, 45.07it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4092/4752 [01:30<00:14, 45.05it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4098/4752 [01:30<00:15, 43.46it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4103/4752 [01:30<00:15, 42.08it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4108/4752 [01:30<00:15, 40.42it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4113/4752 [01:31<00:15, 40.22it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4118/4752 [01:31<00:15, 40.53it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4123/4752 [01:31<00:15, 40.60it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4128/4752 [01:31<00:15, 40.88it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4133/4752 [01:31<00:15, 41.09it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4138/4752 [01:31<00:14, 41.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4143/4752 [01:31<00:14, 40.98it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4148/4752 [01:31<00:14, 41.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 4153/4752 [01:32<00:14, 42.52it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4158/4752 [01:32<00:13, 42.75it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4163/4752 [01:32<00:13, 42.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4168/4752 [01:32<00:13, 42.42it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4173/4752 [01:32<00:13, 43.15it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4178/4752 [01:32<00:13, 43.14it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4183/4752 [01:32<00:13, 42.97it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4188/4752 [01:32<00:13, 43.09it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4193/4752 [01:33<00:12, 43.50it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4198/4752 [01:33<00:12, 44.39it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4203/4752 [01:33<00:12, 43.99it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4208/4752 [01:33<00:12, 44.07it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 4213/4752 [01:33<00:12, 43.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4218/4752 [01:33<00:12, 42.73it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4223/4752 [01:33<00:12, 42.76it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4228/4752 [01:33<00:11, 44.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4233/4752 [01:33<00:12, 40.90it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4238/4752 [01:34<00:12, 39.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4243/4752 [01:34<00:13, 38.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4247/4752 [01:34<00:13, 38.65it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4251/4752 [01:34<00:13, 37.91it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4255/4752 [01:34<00:13, 38.06it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4260/4752 [01:34<00:12, 39.24it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4265/4752 [01:34<00:12, 39.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4270/4752 [01:34<00:12, 39.78it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4275/4752 [01:35<00:11, 40.02it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4280/4752 [01:35<00:11, 39.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4285/4752 [01:35<00:11, 40.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4290/4752 [01:35<00:11, 39.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4294/4752 [01:35<00:11, 39.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4299/4752 [01:35<00:11, 39.99it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4304/4752 [01:35<00:11, 39.49it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4308/4752 [01:35<00:11, 39.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4313/4752 [01:35<00:11, 39.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4317/4752 [01:36<00:11, 39.06it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4322/4752 [01:36<00:10, 40.60it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4327/4752 [01:36<00:10, 41.24it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4332/4752 [01:36<00:10, 41.78it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4337/4752 [01:36<00:09, 42.46it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4342/4752 [01:36<00:09, 42.89it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4347/4752 [01:36<00:09, 43.04it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4352/4752 [01:36<00:09, 43.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4357/4752 [01:37<00:09, 42.66it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4362/4752 [01:37<00:09, 42.74it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4367/4752 [01:37<00:08, 43.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4372/4752 [01:37<00:08, 42.41it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4377/4752 [01:37<00:08, 42.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4382/4752 [01:37<00:08, 41.81it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4387/4752 [01:37<00:08, 40.78it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4392/4752 [01:37<00:08, 41.24it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4397/4752 [01:37<00:08, 41.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4402/4752 [01:38<00:08, 39.35it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4406/4752 [01:38<00:09, 38.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4410/4752 [01:38<00:08, 38.31it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4414/4752 [01:38<00:08, 38.10it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4418/4752 [01:38<00:08, 37.96it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4422/4752 [01:38<00:08, 38.19it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4426/4752 [01:38<00:08, 38.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4431/4752 [01:38<00:08, 39.07it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4435/4752 [01:38<00:08, 38.80it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4439/4752 [01:39<00:08, 38.31it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4443/4752 [01:39<00:07, 38.72it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4448/4752 [01:39<00:07, 39.71it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4452/4752 [01:39<00:07, 39.72it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4457/4752 [01:39<00:07, 40.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4462/4752 [01:39<00:07, 40.70it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4467/4752 [01:39<00:07, 40.29it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4472/4752 [01:39<00:06, 40.83it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4477/4752 [01:40<00:06, 41.32it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4482/4752 [01:40<00:06, 41.36it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4487/4752 [01:40<00:06, 41.89it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4492/4752 [01:40<00:06, 41.46it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4497/4752 [01:40<00:05, 42.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4502/4752 [01:40<00:05, 43.05it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4507/4752 [01:40<00:05, 43.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4512/4752 [01:40<00:05, 41.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4517/4752 [01:40<00:05, 40.89it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4522/4752 [01:41<00:05, 40.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4527/4752 [01:41<00:05, 40.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4532/4752 [01:41<00:05, 40.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4537/4752 [01:41<00:05, 40.41it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4542/4752 [01:41<00:05, 40.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4547/4752 [01:41<00:04, 41.94it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4552/4752 [01:41<00:04, 42.42it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4557/4752 [01:41<00:04, 43.31it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4562/4752 [01:42<00:04, 43.05it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4567/4752 [01:42<00:04, 42.91it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 4572/4752 [01:42<00:04, 43.38it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4577/4752 [01:42<00:04, 43.56it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4582/4752 [01:42<00:03, 43.66it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4587/4752 [01:42<00:03, 43.74it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4592/4752 [01:42<00:03, 42.86it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4597/4752 [01:42<00:03, 42.51it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4602/4752 [01:42<00:03, 42.32it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4607/4752 [01:43<00:03, 41.65it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4612/4752 [01:43<00:03, 41.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4617/4752 [01:43<00:03, 41.27it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4622/4752 [01:43<00:03, 41.97it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4627/4752 [01:43<00:02, 42.21it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 4632/4752 [01:43<00:03, 39.54it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4638/4752 [01:43<00:02, 42.85it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4643/4752 [01:43<00:02, 42.97it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4648/4752 [01:44<00:02, 43.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4653/4752 [01:44<00:02, 38.88it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4658/4752 [01:44<00:02, 39.05it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4662/4752 [01:44<00:02, 38.68it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4666/4752 [01:44<00:02, 38.70it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4670/4752 [01:44<00:02, 38.51it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4674/4752 [01:44<00:02, 37.74it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4679/4752 [01:44<00:01, 38.55it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4683/4752 [01:45<00:01, 38.59it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4687/4752 [01:45<00:01, 37.97it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 4692/4752 [01:45<00:01, 39.02it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4697/4752 [01:45<00:01, 39.97it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4702/4752 [01:45<00:01, 40.68it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4707/4752 [01:45<00:01, 41.80it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4712/4752 [01:45<00:00, 42.46it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4717/4752 [01:45<00:00, 42.22it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4722/4752 [01:45<00:00, 42.27it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4727/4752 [01:46<00:00, 42.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4732/4752 [01:46<00:00, 42.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4737/4752 [01:46<00:00, 40.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4743/4752 [01:46<00:00, 43.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4748/4752 [01:46<00:00, 44.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4752/4752 [01:46<00:00, 44.57it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 2.4146
Epoch 1 Step 51 Train Loss: 0.9942
Epoch 1 Step 101 Train Loss: 0.9575
Epoch 1 Step 151 Train Loss: 0.9337
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0053. 
Train Top 20 DE MSE: 0.0437 Validation Top 20 DE MSE: 0.2533. 
Epoch 2 Step 1 Train Loss: 1.0142
Epoch 2 Step 51 Train Loss: 0.9067
Epoch 2 Step 101 Train Loss: 1.0298
Epoch 2 Step 151 Train Loss: 1.0321
Epoch 2: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0054. 
Train Top 20 DE MSE: 0.0350 Validation Top 20 DE MSE: 0.2650. 
Epoch 3 Step 1 Train Loss: 1.0132
Epoch 3 Step 51 Train Loss: 0.8745
Epoch 3 Step 101 Train Loss: 0.9277
Epoch 3 Step 151 Train Loss: 0.9120
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0176 Validation Top 20 DE MSE: 0.2586. 
Epoch 4 Step 1 Train Loss: 0.9470
Epoch 4 Step 51 Train Loss: 0.9134
Epoch 4 Step 101 Train Loss: 0.9764
Epoch 4 Step 151 Train Loss: 0.9353
Epoch 4: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0128 Validation Top 20 DE MSE: 0.2584. 
Epoch 5 Step 1 Train Loss: 1.0079
Epoch 5 Step 51 Train Loss: 0.8844
Epoch 5 Step 101 Train Loss: 0.9909
Epoch 5 Step 151 Train Loss: 0.9513
Epoch 5: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.2573. 
Epoch 6 Step 1 Train Loss: 0.9036
Epoch 6 Step 51 Train Loss: 0.9751
Epoch 6 Step 101 Train Loss: 0.8547
Epoch 6 Step 151 Train Loss: 0.8229
Epoch 6: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0052. 
Train Top 20 DE MSE: 0.0178 Validation Top 20 DE MSE: 0.2626. 
Epoch 7 Step 1 Train Loss: 0.8187
Epoch 7 Step 51 Train Loss: 0.9121
Epoch 7 Step 101 Train Loss: 0.9707
Epoch 7 Step 151 Train Loss: 0.8825
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0109 Validation Top 20 DE MSE: 0.2579. 
Epoch 8 Step 1 Train Loss: 0.8706
Epoch 8 Step 51 Train Loss: 0.8503
Epoch 8 Step 101 Train Loss: 0.8885
Epoch 8 Step 151 Train Loss: 0.8713
Epoch 8: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.2614. 
Epoch 9 Step 1 Train Loss: 0.9022
Epoch 9 Step 51 Train Loss: 0.9301
Epoch 9 Step 101 Train Loss: 0.9441
Epoch 9 Step 151 Train Loss: 0.8895
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.2584. 
Epoch 10 Step 1 Train Loss: 0.8936
Epoch 10 Step 51 Train Loss: 0.8670
Epoch 10 Step 101 Train Loss: 0.8551
Epoch 10 Step 151 Train Loss: 0.8606
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0129 Validation Top 20 DE MSE: 0.2604. 
Epoch 11 Step 1 Train Loss: 0.9694
Epoch 11 Step 51 Train Loss: 0.8282
Epoch 11 Step 101 Train Loss: 0.8096
Epoch 11 Step 151 Train Loss: 0.9439
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.2600. 
Epoch 12 Step 1 Train Loss: 0.9104
Epoch 12 Step 51 Train Loss: 0.9764
Epoch 12 Step 101 Train Loss: 0.9297
Epoch 12 Step 151 Train Loss: 0.8237
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.2634. 
Epoch 13 Step 1 Train Loss: 0.8389
Epoch 13 Step 51 Train Loss: 0.8259
Epoch 13 Step 101 Train Loss: 0.8810
Epoch 13 Step 151 Train Loss: 0.9927
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0138 Validation Top 20 DE MSE: 0.2615. 
Epoch 14 Step 1 Train Loss: 0.9069
Epoch 14 Step 51 Train Loss: 0.8362
Epoch 14 Step 101 Train Loss: 0.8552
Epoch 14 Step 151 Train Loss: 0.8390
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.2576. 
Epoch 15 Step 1 Train Loss: 0.9373
Epoch 15 Step 51 Train Loss: 0.8252
Epoch 15 Step 101 Train Loss: 0.9295
Epoch 15 Step 151 Train Loss: 0.9098
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0052. 
Train Top 20 DE MSE: 0.0172 Validation Top 20 DE MSE: 0.2649. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.4032
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0069352617
test_unseen_single_pearson: 0.983460616645933
test_unseen_single_mse_de: 0.4031876
test_unseen_single_pearson_de: 0.8214378034640601
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.13192505086672898
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4000000000000001
test_unseen_single_frac_sigma_below_1_non_dropout: 0.7833333333333333
test_unseen_single_mse_top20_de_non_dropout: 0.41367424
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.001 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–†â–ƒâ–‚â–â–ƒâ–â–‚â–â–‚â–â–‚â–‚â–â–ƒ
wandb:                                             train_de_pearson â–â–ƒâ–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–‚â–â–
wandb:                                                train_pearson â–â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–„â–ƒâ–…â–‡â–ƒâ–…â–„â–ƒâ–‚â–„â–‚â–„â–„â–„â–…â–‚â–…â–ƒâ–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–…â–…â–‚â–‚â–ƒâ–…â–…â–ƒâ–‚â–„â–…
wandb:                                                   val_de_mse â–â–ˆâ–„â–„â–ƒâ–‡â–„â–†â–„â–…â–…â–‡â–†â–„â–ˆ
wandb:                                               val_de_pearson â–‡â–â–‡â–‡â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–†
wandb:                                                      val_mse â–†â–ˆâ–‚â–â–â–ƒâ–‚â–‚â–â–‚â–â–ƒâ–‚â–â–„
wandb:                                                  val_pearson â–â–‚â–‡â–ˆâ–ˆâ–†â–ˆâ–‡â–ˆâ–†â–ˆâ–†â–‡â–ˆâ–…
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.40319
wandb:                                              test_de_pearson 0.82144
wandb:               test_frac_opposite_direction_top20_non_dropout 0.4
wandb:                          test_frac_sigma_below_1_non_dropout 0.78333
wandb:                                                     test_mse 0.00694
wandb:                                test_mse_top20_de_non_dropout 0.41367
wandb:                                                 test_pearson 0.98346
wandb:                                           test_pearson_delta 0.13193
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.4
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.78333
wandb:                                       test_unseen_single_mse 0.00694
wandb:                                    test_unseen_single_mse_de 0.40319
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.41367
wandb:                                   test_unseen_single_pearson 0.98346
wandb:                                test_unseen_single_pearson_de 0.82144
wandb:                             test_unseen_single_pearson_delta 0.13193
wandb:                                                 train_de_mse 0.01717
wandb:                                             train_de_pearson 0.93191
wandb:                                                    train_mse 0.00158
wandb:                                                train_pearson 0.99625
wandb:                                                training_loss 0.98777
wandb:                                                   val_de_mse 0.26486
wandb:                                               val_de_pearson 0.95785
wandb:                                                      val_mse 0.00518
wandb:                                                  val_pearson 0.98779
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_RNA_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u5z5sypr
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_101510-u5z5sypr/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_102742-3kwhg76b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/3kwhg76b
Start Training...
Epoch 1 Step 1 Train Loss: 1.7973
Epoch 1 Step 51 Train Loss: 1.0835
Epoch 1 Step 101 Train Loss: 0.9860
Epoch 1 Step 151 Train Loss: 0.9483
Epoch 1: Train Overall MSE: 0.0034 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0462 Validation Top 20 DE MSE: 0.1700. 
Epoch 2 Step 1 Train Loss: 0.9152
Epoch 2 Step 51 Train Loss: 0.9949
Epoch 2 Step 101 Train Loss: 0.9039
Epoch 2 Step 151 Train Loss: 1.0329
Epoch 2: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0377 Validation Top 20 DE MSE: 0.1635. 
Epoch 3 Step 1 Train Loss: 0.9857
Epoch 3 Step 51 Train Loss: 0.9483
Epoch 3 Step 101 Train Loss: 0.9724
Epoch 3 Step 151 Train Loss: 0.8920
Epoch 3: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0205 Validation Top 20 DE MSE: 0.1610. 
Epoch 4 Step 1 Train Loss: 0.8768
Epoch 4 Step 51 Train Loss: 0.8570
Epoch 4 Step 101 Train Loss: 0.9418
Epoch 4 Step 151 Train Loss: 0.8153
Epoch 4: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0168 Validation Top 20 DE MSE: 0.1600. 
Epoch 5 Step 1 Train Loss: 1.0139
Epoch 5 Step 51 Train Loss: 0.9667
Epoch 5 Step 101 Train Loss: 0.8756
Epoch 5 Step 151 Train Loss: 0.9253
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.1591. 
Epoch 6 Step 1 Train Loss: 0.8825
Epoch 6 Step 51 Train Loss: 0.8600
Epoch 6 Step 101 Train Loss: 0.9238
Epoch 6 Step 151 Train Loss: 0.9419
Epoch 6: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0148 Validation Top 20 DE MSE: 0.1605. 
Epoch 7 Step 1 Train Loss: 0.9118
Epoch 7 Step 51 Train Loss: 0.8056
Epoch 7 Step 101 Train Loss: 0.8855
Epoch 7 Step 151 Train Loss: 0.8377
Epoch 7: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.1598. 
Epoch 8 Step 1 Train Loss: 0.8608
Epoch 8 Step 51 Train Loss: 0.8018
Epoch 8 Step 101 Train Loss: 0.8750
Epoch 8 Step 151 Train Loss: 0.9254
Epoch 8: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.1568. 
Epoch 9 Step 1 Train Loss: 0.9104
Epoch 9 Step 51 Train Loss: 0.9225
Epoch 9 Step 101 Train Loss: 0.9424
Epoch 9 Step 151 Train Loss: 0.9545
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.1576. 
Epoch 10 Step 1 Train Loss: 0.8621
Epoch 10 Step 51 Train Loss: 0.8614
Epoch 10 Step 101 Train Loss: 0.9041
Epoch 10 Step 151 Train Loss: 0.9047
Epoch 10: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0112 Validation Top 20 DE MSE: 0.1576. 
Epoch 11 Step 1 Train Loss: 0.8393
Epoch 11 Step 51 Train Loss: 1.0284
Epoch 11 Step 101 Train Loss: 0.8268
Epoch 11 Step 151 Train Loss: 0.9606
Epoch 11: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.1581. 
Epoch 12 Step 1 Train Loss: 0.7923
Epoch 12 Step 51 Train Loss: 0.8802
Epoch 12 Step 101 Train Loss: 0.9019
Epoch 12 Step 151 Train Loss: 0.8875
Epoch 12: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.1574. 
Epoch 13 Step 1 Train Loss: 0.8809
Epoch 13 Step 51 Train Loss: 0.8791
Epoch 13 Step 101 Train Loss: 0.8365
Epoch 13 Step 151 Train Loss: 0.9985
Epoch 13: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.1579. 
Epoch 14 Step 1 Train Loss: 0.8957
Epoch 14 Step 51 Train Loss: 0.9118
Epoch 14 Step 101 Train Loss: 0.9117
Epoch 14 Step 151 Train Loss: 0.8687
Epoch 14: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.1573. 
Epoch 15 Step 1 Train Loss: 0.7797
Epoch 15 Step 51 Train Loss: 1.0627
Epoch 15 Step 101 Train Loss: 0.9263
Epoch 15 Step 151 Train Loss: 0.8592
Epoch 15: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0121 Validation Top 20 DE MSE: 0.1572. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.5671
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.008673742
test_unseen_single_pearson: 0.9789854429404863
test_unseen_single_mse_de: 0.56705564
test_unseen_single_pearson_de: 0.9708470435765632
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.22521216915840825
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2916666666666667
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6333333333333333
test_unseen_single_mse_top20_de_non_dropout: 0.5676473
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.004 MB of 0.019 MB uploadedwandb: / 0.016 MB of 0.019 MB uploadedwandb: - 0.016 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–†â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–â–‚â–â–‚
wandb:                                             train_de_pearson â–ˆâ–ƒâ–…â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                    train_mse â–ˆâ–„â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–†â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–â–„â–ƒâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–„â–„â–â–ƒâ–„â–…â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–…â–„â–ƒâ–ƒâ–‚â–‚â–†â–ƒ
wandb:                                                   val_de_mse â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–â–â–â–‚â–â–‚â–â–
wandb:                                               val_de_pearson â–â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:                                                      val_mse â–…â–‚â–â–…â–†â–‡â–†â–†â–‡â–…â–†â–…â–ˆâ–†â–†
wandb:                                                  val_pearson â–„â–‡â–ˆâ–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–ƒâ–„â–â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.56706
wandb:                                              test_de_pearson 0.97085
wandb:               test_frac_opposite_direction_top20_non_dropout 0.29167
wandb:                          test_frac_sigma_below_1_non_dropout 0.63333
wandb:                                                     test_mse 0.00867
wandb:                                test_mse_top20_de_non_dropout 0.56765
wandb:                                                 test_pearson 0.97899
wandb:                                           test_pearson_delta 0.22521
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.29167
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.63333
wandb:                                       test_unseen_single_mse 0.00867
wandb:                                    test_unseen_single_mse_de 0.56706
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.56765
wandb:                                   test_unseen_single_pearson 0.97899
wandb:                                test_unseen_single_pearson_de 0.97085
wandb:                             test_unseen_single_pearson_delta 0.22521
wandb:                                                 train_de_mse 0.0121
wandb:                                             train_de_pearson 0.86232
wandb:                                                    train_mse 0.002
wandb:                                                train_pearson 0.9953
wandb:                                                training_loss 0.98684
wandb:                                                   val_de_mse 0.15725
wandb:                                               val_de_pearson 0.95534
wandb:                                                      val_mse 0.00326
wandb:                                                  val_pearson 0.99229
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_RNA_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/3kwhg76b
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_102742-3kwhg76b/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_103735-ccpw5ztx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/ccpw5ztx
Start Training...
Epoch 1 Step 1 Train Loss: 2.0775
Epoch 1 Step 51 Train Loss: 1.1305
Epoch 1 Step 101 Train Loss: 1.0002
Epoch 1 Step 151 Train Loss: 0.9692
Epoch 1: Train Overall MSE: 0.0041 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0633 Validation Top 20 DE MSE: 0.0396. 
Epoch 2 Step 1 Train Loss: 0.9913
Epoch 2 Step 51 Train Loss: 0.8834
Epoch 2 Step 101 Train Loss: 0.8434
Epoch 2 Step 151 Train Loss: 0.9394
Epoch 2: Train Overall MSE: 0.0036 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0249 Validation Top 20 DE MSE: 0.0336. 
Epoch 3 Step 1 Train Loss: 0.9564
Epoch 3 Step 51 Train Loss: 0.9078
Epoch 3 Step 101 Train Loss: 0.8884
Epoch 3 Step 151 Train Loss: 0.8689
Epoch 3: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0416 Validation Top 20 DE MSE: 0.0349. 
Epoch 4 Step 1 Train Loss: 0.8678
Epoch 4 Step 51 Train Loss: 0.8219
Epoch 4 Step 101 Train Loss: 0.8850
Epoch 4 Step 151 Train Loss: 0.9101
Epoch 4: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0233 Validation Top 20 DE MSE: 0.0335. 
Epoch 5 Step 1 Train Loss: 0.9140
Epoch 5 Step 51 Train Loss: 0.8546
Epoch 5 Step 101 Train Loss: 0.9182
Epoch 5 Step 151 Train Loss: 0.8007
Epoch 5: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0250 Validation Top 20 DE MSE: 0.0333. 
Epoch 6 Step 1 Train Loss: 0.8865
Epoch 6 Step 51 Train Loss: 0.9350
Epoch 6 Step 101 Train Loss: 1.0233
Epoch 6 Step 151 Train Loss: 0.9374
Epoch 6: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0250 Validation Top 20 DE MSE: 0.0324. 
Epoch 7 Step 1 Train Loss: 0.9727
Epoch 7 Step 51 Train Loss: 0.8337
Epoch 7 Step 101 Train Loss: 0.8639
Epoch 7 Step 151 Train Loss: 0.8751
Epoch 7: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0215 Validation Top 20 DE MSE: 0.0321. 
Epoch 8 Step 1 Train Loss: 0.8008
Epoch 8 Step 51 Train Loss: 0.8952
Epoch 8 Step 101 Train Loss: 0.8993
Epoch 8 Step 151 Train Loss: 0.8948
Epoch 8: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0210 Validation Top 20 DE MSE: 0.0325. 
Epoch 9 Step 1 Train Loss: 0.9190
Epoch 9 Step 51 Train Loss: 0.9383
Epoch 9 Step 101 Train Loss: 0.8399
Epoch 9 Step 151 Train Loss: 0.9233
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0182 Validation Top 20 DE MSE: 0.0325. 
Epoch 10 Step 1 Train Loss: 0.9770
Epoch 10 Step 51 Train Loss: 0.9461
Epoch 10 Step 101 Train Loss: 0.9244
Epoch 10 Step 151 Train Loss: 0.8684
Epoch 10: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0186 Validation Top 20 DE MSE: 0.0323. 
Epoch 11 Step 1 Train Loss: 1.0195
Epoch 11 Step 51 Train Loss: 0.9086
Epoch 11 Step 101 Train Loss: 0.8776
Epoch 11 Step 151 Train Loss: 0.9825
Epoch 11: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0233 Validation Top 20 DE MSE: 0.0323. 
Epoch 12 Step 1 Train Loss: 0.8673
Epoch 12 Step 51 Train Loss: 0.9178
Epoch 12 Step 101 Train Loss: 0.9152
Epoch 12 Step 151 Train Loss: 0.8529
Epoch 12: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0212 Validation Top 20 DE MSE: 0.0314. 
Epoch 13 Step 1 Train Loss: 0.8111
Epoch 13 Step 51 Train Loss: 0.8344
Epoch 13 Step 101 Train Loss: 0.9561
Epoch 13 Step 151 Train Loss: 0.8961
Epoch 13: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0231 Validation Top 20 DE MSE: 0.0321. 
Epoch 14 Step 1 Train Loss: 0.8588
Epoch 14 Step 51 Train Loss: 0.9301
Epoch 14 Step 101 Train Loss: 0.9178
Epoch 14 Step 151 Train Loss: 0.7862
Epoch 14: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.0320. 
Epoch 15 Step 1 Train Loss: 0.9573
Epoch 15 Step 51 Train Loss: 0.9273
Epoch 15 Step 101 Train Loss: 0.9363
Epoch 15 Step 151 Train Loss: 0.8054
Epoch 15: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0236 Validation Top 20 DE MSE: 0.0319. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1397
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002134653
test_unseen_single_pearson: 0.9948082911017168
test_unseen_single_mse_de: 0.1397473
test_unseen_single_pearson_de: 0.9839483000386072
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.26398049174661503
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2833333333333334
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8916666666666666
test_unseen_single_mse_top20_de_non_dropout: 0.13997833
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.001 MB of 0.019 MB uploadedwandb: / 0.016 MB of 0.019 MB uploadedwandb: - 0.016 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–…â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚
wandb:                                             train_de_pearson â–†â–ˆâ–‡â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                    train_mse â–ˆâ–†â–‚â–â–â–‚â–â–‚â–â–‚â–â–â–â–‚â–
wandb:                                                train_pearson â–â–ƒâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:                                                training_loss â–ˆâ–†â–†â–ƒâ–â–‚â–…â–„â–‚â–„â–ˆâ–ƒâ–„â–…â–„â–„â–ƒâ–‚â–„â–â–‚â–‚â–‚â–„â–ƒâ–…â–ƒâ–ƒâ–„â–„â–â–‚â–„â–ƒâ–…â–„â–‚â–„â–‚â–ƒ
wandb:                                                   val_de_mse â–ˆâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–
wandb:                                               val_de_pearson â–â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡
wandb:                                                      val_mse â–ˆâ–ƒâ–„â–â–„â–†â–‚â–…â–‚â–…â–ƒâ–‚â–„â–„â–ƒ
wandb:                                                  val_pearson â–â–†â–…â–ˆâ–…â–ƒâ–†â–…â–‡â–„â–†â–‡â–…â–…â–†
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.13975
wandb:                                              test_de_pearson 0.98395
wandb:               test_frac_opposite_direction_top20_non_dropout 0.28333
wandb:                          test_frac_sigma_below_1_non_dropout 0.89167
wandb:                                                     test_mse 0.00213
wandb:                                test_mse_top20_de_non_dropout 0.13998
wandb:                                                 test_pearson 0.99481
wandb:                                           test_pearson_delta 0.26398
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.28333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.89167
wandb:                                       test_unseen_single_mse 0.00213
wandb:                                    test_unseen_single_mse_de 0.13975
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.13998
wandb:                                   test_unseen_single_pearson 0.99481
wandb:                                test_unseen_single_pearson_de 0.98395
wandb:                             test_unseen_single_pearson_delta 0.26398
wandb:                                                 train_de_mse 0.02364
wandb:                                             train_de_pearson 0.84068
wandb:                                                    train_mse 0.00217
wandb:                                                train_pearson 0.99492
wandb:                                                training_loss 0.82361
wandb:                                                   val_de_mse 0.03191
wandb:                                               val_de_pearson 0.99288
wandb:                                                      val_mse 0.00218
wandb:                                                  val_pearson 0.99501
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_RNA_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/ccpw5ztx
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_103735-ccpw5ztx/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_104740-ulqjfq9g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/ulqjfq9g
Start Training...
Epoch 1 Step 1 Train Loss: 1.6043
Epoch 1 Step 51 Train Loss: 1.0883
Epoch 1 Step 101 Train Loss: 1.0260
Epoch 1 Step 151 Train Loss: 0.9553
Epoch 1: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0095. 
Train Top 20 DE MSE: 0.0385 Validation Top 20 DE MSE: 0.8879. 
Epoch 2 Step 1 Train Loss: 0.9284
Epoch 2 Step 51 Train Loss: 0.9176
Epoch 2 Step 101 Train Loss: 0.9794
Epoch 2 Step 151 Train Loss: 0.8844
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0091. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.8736. 
Epoch 3 Step 1 Train Loss: 0.9479
Epoch 3 Step 51 Train Loss: 0.9505
Epoch 3 Step 101 Train Loss: 0.7680
Epoch 3 Step 151 Train Loss: 1.0164
Epoch 3: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0093. 
Train Top 20 DE MSE: 0.0196 Validation Top 20 DE MSE: 0.9076. 
Epoch 4 Step 1 Train Loss: 0.9089
Epoch 4 Step 51 Train Loss: 0.9174
Epoch 4 Step 101 Train Loss: 0.9311
Epoch 4 Step 151 Train Loss: 0.8880
Epoch 4: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0092. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.9064. 
Epoch 5 Step 1 Train Loss: 0.8913
Epoch 5 Step 51 Train Loss: 0.8714
Epoch 5 Step 101 Train Loss: 0.9152
Epoch 5 Step 151 Train Loss: 0.8919
Epoch 5: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0090. 
Train Top 20 DE MSE: 0.0120 Validation Top 20 DE MSE: 0.8846. 
Epoch 6 Step 1 Train Loss: 1.0935
Epoch 6 Step 51 Train Loss: 0.8984
Epoch 6 Step 101 Train Loss: 0.9849
Epoch 6 Step 151 Train Loss: 0.8718
Epoch 6: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0093. 
Train Top 20 DE MSE: 0.0131 Validation Top 20 DE MSE: 0.9036. 
Epoch 7 Step 1 Train Loss: 0.9292
Epoch 7 Step 51 Train Loss: 0.8529
Epoch 7 Step 101 Train Loss: 0.8306
Epoch 7 Step 151 Train Loss: 0.8951
Epoch 7: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0092. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.8912. 
Epoch 8 Step 1 Train Loss: 0.9175
Epoch 8 Step 51 Train Loss: 0.8928
Epoch 8 Step 101 Train Loss: 0.8665
Epoch 8 Step 151 Train Loss: 0.9514
Epoch 8: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0089. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.8710. 
Epoch 9 Step 1 Train Loss: 0.9063
Epoch 9 Step 51 Train Loss: 0.9125
Epoch 9 Step 101 Train Loss: 0.9818
Epoch 9 Step 151 Train Loss: 0.8977
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0091. 
Train Top 20 DE MSE: 0.0111 Validation Top 20 DE MSE: 0.8939. 
Epoch 10 Step 1 Train Loss: 0.8510
Epoch 10 Step 51 Train Loss: 0.8049
Epoch 10 Step 101 Train Loss: 0.8780
Epoch 10 Step 151 Train Loss: 0.9076
Epoch 10: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0093. 
Train Top 20 DE MSE: 0.0112 Validation Top 20 DE MSE: 0.9001. 
Epoch 11 Step 1 Train Loss: 0.8963
Epoch 11 Step 51 Train Loss: 0.9965
Epoch 11 Step 101 Train Loss: 0.8429
Epoch 11 Step 151 Train Loss: 0.9010
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0092. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.8960. 
Epoch 12 Step 1 Train Loss: 0.9181
Epoch 12 Step 51 Train Loss: 0.8360
Epoch 12 Step 101 Train Loss: 0.9215
Epoch 12 Step 151 Train Loss: 0.8472
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0092. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.8964. 
Epoch 13 Step 1 Train Loss: 0.9521
Epoch 13 Step 51 Train Loss: 0.8836
Epoch 13 Step 101 Train Loss: 0.9165
Epoch 13 Step 151 Train Loss: 0.8891
Epoch 13: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0091. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.8875. 
Epoch 14 Step 1 Train Loss: 0.8364
Epoch 14 Step 51 Train Loss: 0.7884
Epoch 14 Step 101 Train Loss: 0.9407
Epoch 14 Step 151 Train Loss: 0.8268
Epoch 14: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0089. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.8624. 
Epoch 15 Step 1 Train Loss: 0.8674
Epoch 15 Step 51 Train Loss: 0.8327
Epoch 15 Step 101 Train Loss: 0.8989
Epoch 15 Step 151 Train Loss: 0.8473
Epoch 15: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0087. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.8536. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.5756
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.008118165
test_unseen_single_pearson: 0.9809081012517531
test_unseen_single_mse_de: 0.57559365
test_unseen_single_pearson_de: 0.9736632110128598
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: -0.022962506480245896
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4749999999999999
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6666666666666666
test_unseen_single_mse_top20_de_non_dropout: 0.57884306
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.016 MB uploadedwandb: | 0.001 MB of 0.019 MB uploadedwandb: / 0.004 MB of 0.019 MB uploadedwandb: - 0.008 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb: | 0.019 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–
wandb:                                             train_de_pearson â–ˆâ–„â–†â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:                                                    train_mse â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–…â–†â–…â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–…â–ƒâ–„â–ƒâ–†â–â–‚â–ƒâ–…â–„â–ƒâ–„â–„â–„â–‚â–ƒâ–„â–ƒâ–„â–ƒâ–„
wandb:                                                   val_de_mse â–…â–„â–ˆâ–ˆâ–…â–‡â–†â–ƒâ–†â–‡â–†â–‡â–…â–‚â–
wandb:                                               val_de_pearson â–ƒâ–ˆâ–â–„â–„â–„â–„â–…â–„â–ƒâ–„â–„â–„â–†â–‡
wandb:                                                      val_mse â–ˆâ–„â–†â–…â–„â–†â–…â–ƒâ–„â–†â–…â–…â–„â–‚â–
wandb:                                                  val_pearson â–â–„â–„â–„â–†â–„â–…â–†â–…â–„â–…â–…â–…â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.57559
wandb:                                              test_de_pearson 0.97366
wandb:               test_frac_opposite_direction_top20_non_dropout 0.475
wandb:                          test_frac_sigma_below_1_non_dropout 0.66667
wandb:                                                     test_mse 0.00812
wandb:                                test_mse_top20_de_non_dropout 0.57884
wandb:                                                 test_pearson 0.98091
wandb:                                           test_pearson_delta -0.02296
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.475
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.66667
wandb:                                       test_unseen_single_mse 0.00812
wandb:                                    test_unseen_single_mse_de 0.57559
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.57884
wandb:                                   test_unseen_single_pearson 0.98091
wandb:                                test_unseen_single_pearson_de 0.97366
wandb:                             test_unseen_single_pearson_delta -0.02296
wandb:                                                 train_de_mse 0.00801
wandb:                                             train_de_pearson 0.85588
wandb:                                                    train_mse 0.0018
wandb:                                                train_pearson 0.99579
wandb:                                                training_loss 0.94442
wandb:                                                   val_de_mse 0.85358
wandb:                                               val_de_pearson 0.93829
wandb:                                                      val_mse 0.00871
wandb:                                                  val_pearson 0.97951
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_RNA_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/ulqjfq9g
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_104740-ulqjfq9g/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_105851-m1na3ftm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/m1na3ftm
Start Training...
Epoch 1 Step 1 Train Loss: 1.9841
Epoch 1 Step 51 Train Loss: 1.0073
Epoch 1 Step 101 Train Loss: 1.0768
Epoch 1 Step 151 Train Loss: 1.0453
Epoch 1: Train Overall MSE: 0.0041 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0561 Validation Top 20 DE MSE: 0.0068. 
Epoch 2 Step 1 Train Loss: 0.9766
Epoch 2 Step 51 Train Loss: 0.8754
Epoch 2 Step 101 Train Loss: 0.9361
Epoch 2 Step 151 Train Loss: 0.9628
Epoch 2: Train Overall MSE: 0.0035 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0717 Validation Top 20 DE MSE: 0.0058. 
Epoch 3 Step 1 Train Loss: 0.8597
Epoch 3 Step 51 Train Loss: 1.0418
Epoch 3 Step 101 Train Loss: 0.9274
Epoch 3 Step 151 Train Loss: 0.7817
Epoch 3: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0461 Validation Top 20 DE MSE: 0.0061. 
Epoch 4 Step 1 Train Loss: 0.8663
Epoch 4 Step 51 Train Loss: 0.9130
Epoch 4 Step 101 Train Loss: 0.9568
Epoch 4 Step 151 Train Loss: 0.7799
Epoch 4: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0324 Validation Top 20 DE MSE: 0.0064. 
Epoch 5 Step 1 Train Loss: 1.0404
Epoch 5 Step 51 Train Loss: 0.9528
Epoch 5 Step 101 Train Loss: 0.9867
Epoch 5 Step 151 Train Loss: 0.8833
Epoch 5: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0233 Validation Top 20 DE MSE: 0.0066. 
Epoch 6 Step 1 Train Loss: 0.8680
Epoch 6 Step 51 Train Loss: 0.8818
Epoch 6 Step 101 Train Loss: 0.8781
Epoch 6 Step 151 Train Loss: 0.9599
Epoch 6: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0214 Validation Top 20 DE MSE: 0.0068. 
Epoch 7 Step 1 Train Loss: 0.9413
Epoch 7 Step 51 Train Loss: 0.9120
Epoch 7 Step 101 Train Loss: 0.9867
Epoch 7 Step 151 Train Loss: 0.9271
Epoch 7: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0180 Validation Top 20 DE MSE: 0.0065. 
Epoch 8 Step 1 Train Loss: 0.9716
Epoch 8 Step 51 Train Loss: 0.9171
Epoch 8 Step 101 Train Loss: 0.8468
Epoch 8 Step 151 Train Loss: 0.8901
Epoch 8: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0271 Validation Top 20 DE MSE: 0.0069. 
Epoch 9 Step 1 Train Loss: 0.8853
Epoch 9 Step 51 Train Loss: 0.9356
Epoch 9 Step 101 Train Loss: 0.9177
Epoch 9 Step 151 Train Loss: 0.8605
Epoch 9: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0195 Validation Top 20 DE MSE: 0.0066. 
Epoch 10 Step 1 Train Loss: 0.8968
Epoch 10 Step 51 Train Loss: 0.9571
Epoch 10 Step 101 Train Loss: 0.9065
Epoch 10 Step 151 Train Loss: 0.8709
Epoch 10: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0180 Validation Top 20 DE MSE: 0.0068. 
Epoch 11 Step 1 Train Loss: 0.8816
Epoch 11 Step 51 Train Loss: 0.9253
Epoch 11 Step 101 Train Loss: 1.0045
Epoch 11 Step 151 Train Loss: 0.9047
Epoch 11: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0215 Validation Top 20 DE MSE: 0.0068. 
Epoch 12 Step 1 Train Loss: 1.0139
Epoch 12 Step 51 Train Loss: 0.8386
Epoch 12 Step 101 Train Loss: 0.8410
Epoch 12 Step 151 Train Loss: 0.9875
Epoch 12: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.0066. 
Epoch 13 Step 1 Train Loss: 0.8836
Epoch 13 Step 51 Train Loss: 0.9118
Epoch 13 Step 101 Train Loss: 0.9343
Epoch 13 Step 151 Train Loss: 0.9093
Epoch 13: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0197 Validation Top 20 DE MSE: 0.0068. 
Epoch 14 Step 1 Train Loss: 1.0211
Epoch 14 Step 51 Train Loss: 0.7908
Epoch 14 Step 101 Train Loss: 0.9668
Epoch 14 Step 151 Train Loss: 0.8923
Epoch 14: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0183 Validation Top 20 DE MSE: 0.0066. 
Epoch 15 Step 1 Train Loss: 0.8201
Epoch 15 Step 51 Train Loss: 0.9338
Epoch 15 Step 101 Train Loss: 0.8808
Epoch 15 Step 151 Train Loss: 0.9003
Epoch 15: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0203 Validation Top 20 DE MSE: 0.0066. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1427
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0029560074
test_unseen_single_pearson: 0.9930090557710702
test_unseen_single_mse_de: 0.14271717
test_unseen_single_pearson_de: 0.9682738592416021
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.15827125953574414
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.43333333333333335
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9083333333333333
test_unseen_single_mse_top20_de_non_dropout: 0.14307362
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.003 MB of 0.019 MB uploadedwandb: / 0.013 MB of 0.019 MB uploadedwandb: - 0.013 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–†â–ˆâ–…â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–ˆâ–„â–„â–â–‚â–â–‚â–â–â–‚â–â–â–â–‚â–
wandb:                                                    train_mse â–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–‡â–ƒâ–„â–…â–„â–†â–ƒâ–„â–‚â–‚â–‚â–ƒâ–…â–…â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–‚â–ƒâ–‚â–„â–â–ƒâ–„â–ƒâ–‚â–„
wandb:                                                   val_de_mse â–ˆâ–â–ƒâ–…â–†â–‡â–…â–ˆâ–†â–‡â–‡â–†â–ˆâ–†â–†
wandb:                                               val_de_pearson â–†â–ˆâ–…â–ƒâ–ƒâ–â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–
wandb:                                                      val_mse â–‡â–â–„â–„â–…â–†â–„â–‡â–†â–‡â–‡â–…â–ˆâ–†â–‡
wandb:                                                  val_pearson â–‚â–ˆâ–…â–…â–„â–ƒâ–…â–‚â–ƒâ–‚â–‚â–„â–â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.14272
wandb:                                              test_de_pearson 0.96827
wandb:               test_frac_opposite_direction_top20_non_dropout 0.43333
wandb:                          test_frac_sigma_below_1_non_dropout 0.90833
wandb:                                                     test_mse 0.00296
wandb:                                test_mse_top20_de_non_dropout 0.14307
wandb:                                                 test_pearson 0.99301
wandb:                                           test_pearson_delta 0.15827
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.43333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.90833
wandb:                                       test_unseen_single_mse 0.00296
wandb:                                    test_unseen_single_mse_de 0.14272
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.14307
wandb:                                   test_unseen_single_pearson 0.99301
wandb:                                test_unseen_single_pearson_de 0.96827
wandb:                             test_unseen_single_pearson_delta 0.15827
wandb:                                                 train_de_mse 0.02028
wandb:                                             train_de_pearson 0.86077
wandb:                                                    train_mse 0.0026
wandb:                                                train_pearson 0.99387
wandb:                                                training_loss 0.97501
wandb:                                                   val_de_mse 0.00661
wandb:                                               val_de_pearson 0.99389
wandb:                                                      val_mse 0.00146
wandb:                                                  val_pearson 0.99657
wandb: 
wandb: ðŸš€ View run PapalexiSatija2021_eccite_RNA_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/m1na3ftm
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_105851-m1na3ftm/logs
