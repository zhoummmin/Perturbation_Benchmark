{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb16a0d-ff04-4479-933c-314298eaeeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/07grn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4927f97-390e-4608-ae75-e68e8643cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import pickle \n",
    "import sys\n",
    "sys.path.append('/home/share/huadjyin/home/zhoumin3/zhoumin/gears/GEARS_misc/legacy')\n",
    "from model import linear_model\n",
    "from utils import parse_any_pert\n",
    "from tqdm import tqdm\n",
    "#from data import PertDataloader\n",
    "#sys.path.append('/home/share/huadjyin/home/fengtiannan/zhoumin/gears/GEARS_misc/gears')\n",
    "from gears import PertData, GEARS\n",
    "import torch\n",
    "from gears.inference import evaluate, compute_metrics, deeper_analysis, non_dropout_analysis, non_zero_analysis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "#seed = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271b9b6-a893-4f92-a5fd-66447421463b",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750a715-606b-4980-9cfd-e10641123144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "[]\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:6\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "Edges: 493742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 150/150 [04:05<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/grn/split3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:6\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "Edges: 493742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 143/143 [03:59<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/grn/split4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:6\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "Edges: 493742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 147/147 [03:38<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/grn/split5\n",
      "Dixit_combined finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "graph_type = 'grnboost'\n",
    "names = ['Dixit_combined']\n",
    "\n",
    "for name in names:\n",
    "    name_lower = name.lower()\n",
    "    data_path = f'./{name_lower}/perturb_processed.h5ad'\n",
    "    adata = sc.read_h5ad(data_path)\n",
    "    adata.var['gene_symbols'] = adata.var['gene_name']\n",
    "    pert_data = PertData('./')\n",
    "    data_name = f'./{name_lower}'\n",
    "    data_path = os.path.join('./', data_name)\n",
    "    pert_data.load(data_name=data_name, data_path=data_path)\n",
    "    for split_num in range(3,6):\n",
    "        pert_data.prepare_split(split='simulation', seed=split_num)\n",
    "        pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "    \n",
    "        split_path = f\"./{data_name}/splits/{data_name}_simulation_{split_num}_0.75.pkl\"\n",
    "        import pickle \n",
    "        split = pickle.load(open(split_path, 'rb'))\n",
    "        \n",
    "        condition2set = {}\n",
    "            \n",
    "        for i,j in split.items():\n",
    "            for k in j:\n",
    "                condition2set[k] = i\n",
    "                \n",
    "        graph_path = f'./01scenic_adj/{data_name}_adjacencies_{split_num}.csv'\n",
    "        weights_path = f'./03learn_weights/{data_name}/{data_name}_split{split_num}_top50_linear_learntweights.csv'\n",
    "        \n",
    "        gene_list = adata.var.gene_name.values\n",
    "        \n",
    "        model = linear_model(graph_path=graph_path, \n",
    "                     weights_path=weights_path, \n",
    "                     gene_list = gene_list,\n",
    "                     binary=False, \n",
    "                     pos_edges=False, \n",
    "                     hops=1,\n",
    "                     species='human')\n",
    "        pred_delta = {pert: model.simulate_pert(parse_any_pert(pert)) for pert in split['test']}\n",
    "        adata_ctrl = adata[adata.obs.condition == 'ctrl']\n",
    "        pert_cat = []\n",
    "        pred = []\n",
    "        truth = []\n",
    "        pred_de = []\n",
    "        truth_de = []\n",
    "        results = {}\n",
    "        gene_ids_de = [] # zhoumin\n",
    "        \n",
    "        for batch in tqdm(pert_data.dataloader['test_loader']):\n",
    "            \n",
    "            pert_cat.extend(batch.pert)\n",
    "            p = np.array([pred_delta[i]+adata_ctrl.X[np.random.randint(0, adata_ctrl.shape[0])].toarray().reshape(-1,) for i in batch.pert])\n",
    "            t = batch.y\n",
    "        \n",
    "            pred.extend(p)\n",
    "            truth.extend(t.cpu())\n",
    "    \n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "                gene_names_de = [pert_data.adata.var_names[i] for i in de_idx]\n",
    "                gene_ids_de.append(gene_names_de)\n",
    "    \n",
    "            \n",
    "        # all genes\n",
    "        results['pert_cat'] = np.array(pert_cat)\n",
    "        \n",
    "        pred = np.stack(pred)\n",
    "        truth = torch.stack(truth)\n",
    "        results['pred']= pred\n",
    "        results['truth']= truth.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        pred_de = np.stack(pred_de)\n",
    "        truth_de = torch.stack(truth_de)\n",
    "        results['pred_de']= pred_de\n",
    "        results['truth_de']= truth_de.detach().cpu().numpy()\n",
    "        results[\"gene_ids_de\"] = gene_ids_de\n",
    "        from pathlib import Path\n",
    "        save_dir = Path(f\"/home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/{name}/grn/split{split_num}/\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"saving to {save_dir}\")\n",
    "        import pickle\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_res.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        test_res = results\n",
    "        test_metrics, test_pert_res = compute_metrics(test_res)\n",
    "        deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
    "        non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
    "        non_zero_res = non_zero_analysis(pert_data.adata, test_res)\n",
    "        \n",
    "        import pickle\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_metrics.pkl', 'wb') as f:\n",
    "            pickle.dump(test_metrics, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_pert_res.pkl', 'wb') as f:\n",
    "            pickle.dump(test_pert_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_deeper_res.pkl', 'wb') as f:\n",
    "            pickle.dump(deeper_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_non_dropout_res.pkl', 'wb') as f:\n",
    "            pickle.dump(non_dropout_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_non_zero_res.pkl', 'wb') as f:\n",
    "            pickle.dump(non_zero_res, f)\n",
    "        \n",
    "        \n",
    "    print(f'{name} finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322694e6-bb36-4f3f-928a-f36cb2261e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfb98cba-1158-4d1a-ad82-177655967d28",
   "metadata": {},
   "source": [
    "## replogle_k562_essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf9926-d0f4-4909-8ec2-f4851f6aed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "[]\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:266\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "Edges: 493742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 561/561 [22:31<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_k562_essential/grn/split1\n",
      "Replogle_k562_essential finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "graph_type = 'grnboost'\n",
    "names = ['Replogle_k562_essential']\n",
    "\n",
    "for name in names:\n",
    "    name_lower = name.lower()\n",
    "    data_path = f'./{name_lower}/perturb_processed.h5ad'\n",
    "    adata = sc.read_h5ad(data_path)\n",
    "    adata.var['gene_symbols'] = adata.var['gene_name']\n",
    "    pert_data = PertData('./')\n",
    "    data_name = f'./{name_lower}'\n",
    "    data_path = os.path.join('./', data_name)\n",
    "    pert_data.load(data_name=data_name, data_path=data_path)\n",
    "    for split_num in range(1,2):\n",
    "        pert_data.prepare_split(split='simulation', seed=split_num)\n",
    "        pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "    \n",
    "        split_path = f\"./{data_name}/splits/{data_name}_simulation_{split_num}_0.75.pkl\"\n",
    "        import pickle \n",
    "        split = pickle.load(open(split_path, 'rb'))\n",
    "        \n",
    "        condition2set = {}\n",
    "            \n",
    "        for i,j in split.items():\n",
    "            for k in j:\n",
    "                condition2set[k] = i\n",
    "                \n",
    "        graph_path = f'./01scenic_adj/{data_name}_adjacencies_{split_num}.csv'\n",
    "        weights_path = f'./03learn_weights/{data_name}/{data_name}_split{split_num}_top50_linear_learntweights.csv'\n",
    "        \n",
    "        gene_list = adata.var.gene_name.values\n",
    "        \n",
    "        model = linear_model(graph_path=graph_path, \n",
    "                     weights_path=weights_path, \n",
    "                     gene_list = gene_list,\n",
    "                     binary=False, \n",
    "                     pos_edges=False, \n",
    "                     hops=1,\n",
    "                     species='human')\n",
    "        pred_delta = {pert: model.simulate_pert(parse_any_pert(pert)) for pert in split['test']}\n",
    "        adata_ctrl = adata[adata.obs.condition == 'ctrl']\n",
    "        pert_cat = []\n",
    "        pred = []\n",
    "        truth = []\n",
    "        pred_de = []\n",
    "        truth_de = []\n",
    "        results = {}\n",
    "        gene_ids_de = [] # zhoumin\n",
    "        \n",
    "        for batch in tqdm(pert_data.dataloader['test_loader']):\n",
    "            \n",
    "            pert_cat.extend(batch.pert)\n",
    "            p = np.array([pred_delta[i]+adata_ctrl.X[np.random.randint(0, adata_ctrl.shape[0])].toarray().reshape(-1,) for i in batch.pert])\n",
    "            t = batch.y\n",
    "        \n",
    "            pred.extend(p)\n",
    "            truth.extend(t.cpu())\n",
    "    \n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "                gene_names_de = [pert_data.adata.var_names[i] for i in de_idx]\n",
    "                gene_ids_de.append(gene_names_de)\n",
    "    \n",
    "            \n",
    "        # all genes\n",
    "        results['pert_cat'] = np.array(pert_cat)\n",
    "        \n",
    "        pred = np.stack(pred)\n",
    "        truth = torch.stack(truth)\n",
    "        results['pred']= pred\n",
    "        results['truth']= truth.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        pred_de = np.stack(pred_de)\n",
    "        truth_de = torch.stack(truth_de)\n",
    "        results['pred_de']= pred_de\n",
    "        results['truth_de']= truth_de.detach().cpu().numpy()\n",
    "        results[\"gene_ids_de\"] = gene_ids_de\n",
    "        from pathlib import Path\n",
    "        save_dir = Path(f\"/home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/{name}/grn/split{split_num}/\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"saving to {save_dir}\")\n",
    "        import pickle\n",
    "        # 保存为.pkl文件\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_res.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        test_res = results\n",
    "        test_metrics, test_pert_res = compute_metrics(test_res)\n",
    "        deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
    "        non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
    "        non_zero_res = non_zero_analysis(pert_data.adata, test_res)\n",
    "        \n",
    "        import pickle\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_metrics.pkl', 'wb') as f:\n",
    "            pickle.dump(test_metrics, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_pert_res.pkl', 'wb') as f:\n",
    "            pickle.dump(test_pert_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_deeper_res.pkl', 'wb') as f:\n",
    "            pickle.dump(deeper_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_non_dropout_res.pkl', 'wb') as f:\n",
    "            pickle.dump(non_dropout_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_non_zero_res.pkl', 'wb') as f:\n",
    "            pickle.dump(non_zero_res, f)\n",
    "        \n",
    "        \n",
    "    print(f'{name} finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2d180-047c-4476-ad35-8df0c533655e",
   "metadata": {},
   "source": [
    "## replogle_rpe1_essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a8aca-592e-4e2c-918a-d3b259bc3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "[]\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:360\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "Edges: 493742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 516/516 [19:20<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Replogle_rpe1_essential/grn/split2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'AC118549.1+ctrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m test_res \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m     91\u001b[0m test_metrics, test_pert_res \u001b[38;5;241m=\u001b[39m compute_metrics(test_res)\n\u001b[0;32m---> 92\u001b[0m deeper_res \u001b[38;5;241m=\u001b[39m \u001b[43mdeeper_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpert_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m non_dropout_res \u001b[38;5;241m=\u001b[39m non_dropout_analysis(pert_data\u001b[38;5;241m.\u001b[39madata, test_res)\n\u001b[1;32m     94\u001b[0m non_zero_res \u001b[38;5;241m=\u001b[39m non_zero_analysis(pert_data\u001b[38;5;241m.\u001b[39madata, test_res)\n",
      "File \u001b[0;32m~/.conda/envs/gears/lib/python3.9/site-packages/gears/inference.py:392\u001b[0m, in \u001b[0;36mdeeper_analysis\u001b[0;34m(adata, test_res, de_column_prefix, most_variable_genes)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pert \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(test_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpert_cat\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    391\u001b[0m     pert_metric[pert] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 392\u001b[0m     de_idx \u001b[38;5;241m=\u001b[39m [geneid2idx[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m adata\u001b[38;5;241m.\u001b[39muns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_genes_groups_cov_all\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[43mpert2pert_full_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpert\u001b[49m\u001b[43m]\u001b[49m][:\u001b[38;5;241m20\u001b[39m]]\n\u001b[1;32m    393\u001b[0m     de_idx_200 \u001b[38;5;241m=\u001b[39m [geneid2idx[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m adata\u001b[38;5;241m.\u001b[39muns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_genes_groups_cov_all\u001b[39m\u001b[38;5;124m'\u001b[39m][pert2pert_full_id[pert]][:\u001b[38;5;241m200\u001b[39m]]\n\u001b[1;32m    394\u001b[0m     de_idx_100 \u001b[38;5;241m=\u001b[39m [geneid2idx[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m adata\u001b[38;5;241m.\u001b[39muns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_genes_groups_cov_all\u001b[39m\u001b[38;5;124m'\u001b[39m][pert2pert_full_id[pert]][:\u001b[38;5;241m100\u001b[39m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AC118549.1+ctrl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "graph_type = 'grnboost'\n",
    "names = ['Replogle_rpe1_essential']\n",
    "\n",
    "for name in names:\n",
    "    name_lower = name.lower()\n",
    "    data_path = f'./{name_lower}/perturb_processed.h5ad'\n",
    "    adata = sc.read_h5ad(data_path)\n",
    "    adata.var['gene_symbols'] = adata.var['gene_name']\n",
    "    pert_data = PertData('./')\n",
    "    data_name = f'./{name_lower}'\n",
    "    data_path = os.path.join('./', data_name)\n",
    "    pert_data.load(data_name=data_name, data_path=data_path)\n",
    "    for split_num in range(2,3):\n",
    "        pert_data.prepare_split(split='simulation', seed=split_num)\n",
    "        pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "    \n",
    "        split_path = f\"./{data_name}/splits/{data_name}_simulation_{split_num}_0.75.pkl\"\n",
    "        import pickle \n",
    "        split = pickle.load(open(split_path, 'rb'))\n",
    "        \n",
    "        condition2set = {}\n",
    "            \n",
    "        for i,j in split.items():\n",
    "            for k in j:\n",
    "                condition2set[k] = i\n",
    "                \n",
    "        graph_path = f'./01scenic_adj/{data_name}_adjacencies_{split_num}.csv'\n",
    "        weights_path = f'./03learn_weights/{data_name}/{data_name}_split{split_num}_top50_linear_learntweights.csv'\n",
    "        \n",
    "        gene_list = adata.var.gene_name.values\n",
    "        \n",
    "        model = linear_model(graph_path=graph_path, \n",
    "                     weights_path=weights_path, \n",
    "                     gene_list = gene_list,\n",
    "                     binary=False, \n",
    "                     pos_edges=False, \n",
    "                     hops=1,\n",
    "                     species='human')\n",
    "        pred_delta = {pert: model.simulate_pert(parse_any_pert(pert)) for pert in split['test']}\n",
    "        adata_ctrl = adata[adata.obs.condition == 'ctrl']\n",
    "        pert_cat = []\n",
    "        pred = []\n",
    "        truth = []\n",
    "        pred_de = []\n",
    "        truth_de = []\n",
    "        results = {}\n",
    "        gene_ids_de = [] # zhoumin\n",
    "        \n",
    "        for batch in tqdm(pert_data.dataloader['test_loader']):\n",
    "            \n",
    "            pert_cat.extend(batch.pert)\n",
    "            p = np.array([pred_delta[i]+adata_ctrl.X[np.random.randint(0, adata_ctrl.shape[0])].toarray().reshape(-1,) for i in batch.pert])\n",
    "            t = batch.y\n",
    "        \n",
    "            pred.extend(p)\n",
    "            truth.extend(t.cpu())\n",
    "    \n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "                gene_names_de = [pert_data.adata.var_names[i] for i in de_idx]\n",
    "                gene_ids_de.append(gene_names_de)\n",
    "    \n",
    "            \n",
    "        # all genes\n",
    "        results['pert_cat'] = np.array(pert_cat)\n",
    "        \n",
    "        pred = np.stack(pred)\n",
    "        truth = torch.stack(truth)\n",
    "        results['pred']= pred\n",
    "        results['truth']= truth.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        pred_de = np.stack(pred_de)\n",
    "        truth_de = torch.stack(truth_de)\n",
    "        results['pred_de']= pred_de\n",
    "        results['truth_de']= truth_de.detach().cpu().numpy()\n",
    "        results[\"gene_ids_de\"] = gene_ids_de\n",
    "        from pathlib import Path\n",
    "        save_dir = Path(f\"/home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/{name}/grn/split{split_num}/\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"saving to {save_dir}\")\n",
    "        import pickle\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_res.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        test_res = results\n",
    "        test_metrics, test_pert_res = compute_metrics(test_res)\n",
    "        deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
    "        non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
    "        non_zero_res = non_zero_analysis(pert_data.adata, test_res)\n",
    "        \n",
    "        import pickle\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_metrics.pkl', 'wb') as f:\n",
    "            pickle.dump(test_metrics, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_test_pert_res.pkl', 'wb') as f:\n",
    "            pickle.dump(test_pert_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_deeper_res.pkl', 'wb') as f:\n",
    "            pickle.dump(deeper_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_non_dropout_res.pkl', 'wb') as f:\n",
    "            pickle.dump(non_dropout_res, f)\n",
    "        with open(f'{save_dir}/{name}_split{split_num}_non_zero_res.pkl', 'wb') as f:\n",
    "            pickle.dump(non_zero_res, f)\n",
    "        \n",
    "        \n",
    "    print(f'{name} finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88301693-e7d4-4168-acc0-fa553f85e1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gears",
   "language": "python",
   "name": "gears"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
