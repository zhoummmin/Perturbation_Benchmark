Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:24
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_001517-qaqtjulv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRa_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/qaqtjulv
wandb: WARNING Serializing object of type ndarray that is 20619392 bytes
  0%|                                                  | 0/3536 [00:00<?, ?it/s]  0%|                                          | 7/3536 [00:00<00:54, 65.03it/s]  0%|‚ñè                                        | 15/3536 [00:00<00:50, 70.06it/s]  1%|‚ñé                                        | 23/3536 [00:00<00:48, 72.60it/s]  1%|‚ñç                                        | 33/3536 [00:00<00:43, 80.29it/s]  1%|‚ñç                                        | 42/3536 [00:00<00:43, 81.16it/s]  1%|‚ñå                                        | 51/3536 [00:00<00:41, 83.07it/s]  2%|‚ñã                                        | 60/3536 [00:00<00:43, 79.65it/s]  2%|‚ñä                                        | 70/3536 [00:00<00:42, 80.99it/s]  2%|‚ñâ                                        | 80/3536 [00:00<00:40, 84.90it/s]  3%|‚ñà                                        | 89/3536 [00:01<00:40, 84.27it/s]  3%|‚ñà‚ñè                                       | 98/3536 [00:01<00:40, 84.97it/s]  3%|‚ñà‚ñè                                      | 107/3536 [00:01<00:40, 84.85it/s]  3%|‚ñà‚ñé                                      | 116/3536 [00:01<00:40, 84.29it/s]  4%|‚ñà‚ñç                                      | 125/3536 [00:01<00:40, 84.86it/s]  4%|‚ñà‚ñå                                      | 134/3536 [00:01<00:40, 84.15it/s]  4%|‚ñà‚ñå                                      | 143/3536 [00:01<00:40, 83.72it/s]  4%|‚ñà‚ñã                                      | 152/3536 [00:01<00:40, 84.04it/s]  5%|‚ñà‚ñä                                      | 161/3536 [00:01<00:41, 81.18it/s]  5%|‚ñà‚ñâ                                      | 170/3536 [00:02<00:41, 81.73it/s]  5%|‚ñà‚ñà                                      | 180/3536 [00:02<00:39, 85.27it/s]  5%|‚ñà‚ñà‚ñè                                     | 189/3536 [00:02<00:39, 83.85it/s]  6%|‚ñà‚ñà‚ñè                                     | 198/3536 [00:02<00:43, 76.68it/s]  6%|‚ñà‚ñà‚ñé                                     | 209/3536 [00:02<00:41, 80.92it/s]  6%|‚ñà‚ñà‚ñç                                     | 219/3536 [00:02<00:40, 82.12it/s]  6%|‚ñà‚ñà‚ñå                                     | 229/3536 [00:02<00:38, 85.24it/s]  7%|‚ñà‚ñà‚ñã                                     | 238/3536 [00:02<00:39, 84.35it/s]  7%|‚ñà‚ñà‚ñä                                     | 247/3536 [00:02<00:38, 84.63it/s]  7%|‚ñà‚ñà‚ñâ                                     | 256/3536 [00:03<00:38, 84.37it/s]  7%|‚ñà‚ñà‚ñâ                                     | 265/3536 [00:03<00:40, 81.10it/s]  8%|‚ñà‚ñà‚ñà                                     | 274/3536 [00:03<00:39, 81.73it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 284/3536 [00:03<00:38, 84.89it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 293/3536 [00:03<00:38, 84.70it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 302/3536 [00:03<00:38, 84.84it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 311/3536 [00:03<00:38, 83.68it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 320/3536 [00:03<00:38, 83.16it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 329/3536 [00:03<00:38, 82.96it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 338/3536 [00:04<00:38, 82.89it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 347/3536 [00:04<00:39, 80.99it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 357/3536 [00:04<00:37, 84.25it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 366/3536 [00:04<00:37, 83.59it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 375/3536 [00:04<00:37, 83.83it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 384/3536 [00:04<00:37, 83.90it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 393/3536 [00:04<00:37, 83.53it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 402/3536 [00:04<00:37, 83.71it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 411/3536 [00:04<00:38, 80.80it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 421/3536 [00:05<00:38, 81.39it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 431/3536 [00:05<00:36, 84.29it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 440/3536 [00:05<00:37, 83.53it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 449/3536 [00:05<00:37, 83.31it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 458/3536 [00:05<00:37, 82.80it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 467/3536 [00:05<00:37, 81.56it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 476/3536 [00:05<00:37, 82.57it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 485/3536 [00:05<00:37, 81.66it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 494/3536 [00:05<00:37, 81.14it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 503/3536 [00:06<00:39, 77.44it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 513/3536 [00:06<00:37, 80.72it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 522/3536 [00:06<00:36, 81.50it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 531/3536 [00:06<00:36, 81.68it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 540/3536 [00:06<00:36, 81.33it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 549/3536 [00:06<00:37, 78.76it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 558/3536 [00:06<00:36, 81.77it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 567/3536 [00:06<00:36, 81.66it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 576/3536 [00:06<00:36, 81.62it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 585/3536 [00:07<00:36, 80.93it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 594/3536 [00:07<00:36, 80.25it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 603/3536 [00:07<00:36, 80.24it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 612/3536 [00:07<00:37, 77.44it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 622/3536 [00:07<00:36, 80.72it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 631/3536 [00:07<00:35, 80.91it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 640/3536 [00:07<00:35, 81.04it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 649/3536 [00:07<00:35, 81.53it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 658/3536 [00:08<00:35, 80.81it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 667/3536 [00:08<00:35, 81.61it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 676/3536 [00:08<00:46, 61.79it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 683/3536 [00:08<00:47, 60.32it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 690/3536 [00:08<00:49, 57.52it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 701/3536 [00:08<00:49, 57.06it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 719/3536 [00:08<00:34, 82.06it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 729/3536 [00:09<00:34, 80.49it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 738/3536 [00:09<00:34, 80.98it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 747/3536 [00:09<00:34, 81.53it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 756/3536 [00:09<00:33, 82.44it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 766/3536 [00:09<00:32, 84.71it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 775/3536 [00:09<00:33, 81.52it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 785/3536 [00:09<00:33, 81.13it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 795/3536 [00:09<00:33, 81.08it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 805/3536 [00:09<00:32, 83.80it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 814/3536 [00:10<00:32, 83.39it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 823/3536 [00:10<00:32, 82.81it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 832/3536 [00:10<00:32, 82.10it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 841/3536 [00:10<00:32, 82.17it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 850/3536 [00:10<00:32, 82.66it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 859/3536 [00:10<00:32, 81.94it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 868/3536 [00:10<00:32, 81.78it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 877/3536 [00:10<00:33, 78.81it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 886/3536 [00:10<00:32, 81.80it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 895/3536 [00:11<00:32, 80.79it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 904/3536 [00:11<00:33, 78.32it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 914/3536 [00:11<00:32, 81.92it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 923/3536 [00:11<00:31, 81.88it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 932/3536 [00:11<00:31, 82.21it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 941/3536 [00:11<00:31, 82.40it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 950/3536 [00:11<00:31, 82.19it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 959/3536 [00:11<00:31, 82.29it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 968/3536 [00:11<00:31, 82.22it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 977/3536 [00:12<00:31, 79.98it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 987/3536 [00:12<00:30, 83.68it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 996/3536 [00:12<00:30, 82.56it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1005/3536 [00:12<00:31, 81.60it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1014/3536 [00:12<00:31, 79.45it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1022/3536 [00:12<00:32, 77.65it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1030/3536 [00:12<00:46, 54.23it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1040/3536 [00:13<00:39, 63.82it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1048/3536 [00:13<00:43, 57.50it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1055/3536 [00:13<00:45, 54.95it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1062/3536 [00:13<00:44, 55.72it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1068/3536 [00:13<00:49, 49.47it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1074/3536 [00:13<00:56, 43.82it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1086/3536 [00:13<00:41, 59.46it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1093/3536 [00:14<00:47, 51.36it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1101/3536 [00:14<00:42, 56.82it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1108/3536 [00:14<00:42, 56.81it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1115/3536 [00:14<00:41, 58.26it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1122/3536 [00:14<00:41, 57.83it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1130/3536 [00:14<00:39, 61.60it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1137/3536 [00:14<00:38, 61.85it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1144/3536 [00:14<00:40, 59.36it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1151/3536 [00:15<00:43, 55.41it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1159/3536 [00:15<00:40, 59.30it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1166/3536 [00:15<00:40, 58.95it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1173/3536 [00:15<00:38, 61.31it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1180/3536 [00:15<00:38, 60.71it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1187/3536 [00:15<00:37, 63.02it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1194/3536 [00:15<00:40, 57.87it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1201/3536 [00:15<00:38, 60.60it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1208/3536 [00:15<00:39, 58.60it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1215/3536 [00:16<00:40, 57.09it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1221/3536 [00:16<00:40, 56.81it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1228/3536 [00:16<00:39, 58.70it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1234/3536 [00:16<00:40, 57.24it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1241/3536 [00:16<00:39, 58.56it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1247/3536 [00:16<00:40, 57.16it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1253/3536 [00:16<00:40, 56.08it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1259/3536 [00:16<00:41, 55.35it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1265/3536 [00:17<00:41, 54.85it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1271/3536 [00:17<00:41, 54.54it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1277/3536 [00:17<00:42, 53.64it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1284/3536 [00:17<00:40, 56.08it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1292/3536 [00:17<00:36, 61.02it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1302/3536 [00:17<00:31, 70.32it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1311/3536 [00:17<00:30, 74.07it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1320/3536 [00:17<00:28, 76.41it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1328/3536 [00:17<00:29, 76.07it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1337/3536 [00:17<00:28, 78.23it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1347/3536 [00:18<00:26, 82.74it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1356/3536 [00:18<00:26, 80.75it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1366/3536 [00:18<00:26, 81.86it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1375/3536 [00:18<00:26, 82.31it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1384/3536 [00:18<00:26, 82.65it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1393/3536 [00:18<00:25, 82.84it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1403/3536 [00:18<00:24, 85.91it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1412/3536 [00:18<00:25, 83.04it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1421/3536 [00:18<00:25, 82.92it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1431/3536 [00:19<00:24, 86.37it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1440/3536 [00:19<00:24, 86.03it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1449/3536 [00:19<00:24, 85.76it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1458/3536 [00:19<00:24, 85.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1467/3536 [00:19<00:24, 85.71it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1476/3536 [00:19<00:24, 82.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1486/3536 [00:19<00:24, 82.28it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1496/3536 [00:19<00:23, 85.55it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1505/3536 [00:19<00:23, 85.33it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1514/3536 [00:20<00:24, 82.27it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1524/3536 [00:20<00:23, 85.51it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1533/3536 [00:20<00:23, 85.33it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1542/3536 [00:20<00:24, 82.17it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1552/3536 [00:20<00:23, 82.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1561/3536 [00:20<00:23, 83.45it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1570/3536 [00:20<00:23, 83.71it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1580/3536 [00:20<00:23, 83.96it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1589/3536 [00:20<00:23, 84.27it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1598/3536 [00:21<00:23, 81.54it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1609/3536 [00:21<00:21, 87.82it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1618/3536 [00:21<00:22, 86.73it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1627/3536 [00:21<00:22, 83.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1637/3536 [00:21<00:22, 86.18it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1646/3536 [00:21<00:22, 85.46it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1655/3536 [00:21<00:22, 82.41it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1665/3536 [00:21<00:21, 85.69it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1674/3536 [00:21<00:21, 85.23it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1683/3536 [00:22<00:21, 84.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1692/3536 [00:22<00:21, 84.07it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1701/3536 [00:22<00:21, 83.58it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1710/3536 [00:22<00:21, 83.83it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1719/3536 [00:22<00:22, 81.13it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1728/3536 [00:22<00:22, 82.13it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1738/3536 [00:22<00:20, 85.68it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1747/3536 [00:22<00:21, 84.49it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1756/3536 [00:22<00:21, 81.81it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1766/3536 [00:23<00:20, 85.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1775/3536 [00:23<00:21, 81.65it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1784/3536 [00:23<00:21, 82.34it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1793/3536 [00:23<00:21, 79.37it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1802/3536 [00:23<00:21, 81.25it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1811/3536 [00:23<00:22, 76.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1819/3536 [00:23<00:22, 74.77it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1827/3536 [00:23<00:23, 72.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1838/3536 [00:24<00:20, 81.91it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1847/3536 [00:24<00:20, 82.89it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1856/3536 [00:24<00:20, 81.15it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1866/3536 [00:24<00:19, 85.63it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1875/3536 [00:24<00:19, 85.56it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1884/3536 [00:24<00:19, 83.25it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1894/3536 [00:24<00:18, 86.97it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1903/3536 [00:24<00:18, 87.28it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1912/3536 [00:24<00:18, 87.24it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1921/3536 [00:24<00:18, 86.99it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1930/3536 [00:25<00:18, 86.83it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1939/3536 [00:25<00:19, 83.76it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1949/3536 [00:25<00:18, 84.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1958/3536 [00:25<00:18, 84.97it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1967/3536 [00:25<00:18, 85.50it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 1977/3536 [00:25<00:18, 85.85it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1986/3536 [00:25<00:18, 85.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1995/3536 [00:25<00:18, 85.13it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2005/3536 [00:25<00:17, 85.74it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2015/3536 [00:26<00:16, 89.49it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2024/3536 [00:26<00:17, 88.65it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2033/3536 [00:26<00:16, 88.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2042/3536 [00:26<00:17, 85.53it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2051/3536 [00:26<00:18, 82.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2060/3536 [00:26<00:17, 82.64it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2069/3536 [00:26<00:17, 84.00it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2078/3536 [00:26<00:17, 84.68it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2087/3536 [00:26<00:17, 85.10it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2096/3536 [00:27<00:16, 85.57it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2105/3536 [00:27<00:17, 82.50it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2114/3536 [00:27<00:17, 83.56it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2123/3536 [00:27<00:17, 81.44it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2133/3536 [00:27<00:16, 85.35it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2142/3536 [00:27<00:17, 80.99it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2152/3536 [00:27<00:16, 81.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2162/3536 [00:27<00:16, 85.66it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2171/3536 [00:27<00:16, 83.08it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2180/3536 [00:28<00:18, 71.71it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2194/3536 [00:28<00:15, 87.17it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2204/3536 [00:28<00:16, 81.34it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2213/3536 [00:28<00:17, 77.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2221/3536 [00:28<00:24, 54.11it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2235/3536 [00:28<00:18, 70.35it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2244/3536 [00:28<00:17, 73.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2253/3536 [00:29<00:17, 74.72it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2262/3536 [00:29<00:17, 73.31it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2271/3536 [00:29<00:16, 76.40it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2280/3536 [00:29<00:16, 76.53it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2288/3536 [00:29<00:16, 76.83it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2296/3536 [00:29<00:16, 76.86it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2304/3536 [00:29<00:16, 76.93it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2312/3536 [00:29<00:15, 77.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2320/3536 [00:29<00:15, 77.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2328/3536 [00:30<00:15, 77.00it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2336/3536 [00:30<00:15, 75.69it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2344/3536 [00:30<00:16, 74.28it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2354/3536 [00:30<00:15, 76.44it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2362/3536 [00:30<00:16, 69.32it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2375/3536 [00:30<00:14, 80.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2384/3536 [00:30<00:14, 80.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2393/3536 [00:30<00:13, 81.81it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2402/3536 [00:31<00:14, 78.71it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2410/3536 [00:31<00:14, 78.92it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2418/3536 [00:31<00:14, 77.11it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2428/3536 [00:31<00:13, 80.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2437/3536 [00:31<00:13, 81.34it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2446/3536 [00:31<00:13, 81.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2455/3536 [00:31<00:13, 82.08it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2464/3536 [00:31<00:13, 82.41it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2473/3536 [00:31<00:12, 81.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2482/3536 [00:32<00:15, 69.82it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2494/3536 [00:32<00:13, 79.94it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2503/3536 [00:32<00:13, 78.41it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2513/3536 [00:32<00:12, 82.10it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2522/3536 [00:32<00:12, 82.04it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2531/3536 [00:32<00:13, 75.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2542/3536 [00:32<00:12, 81.88it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2551/3536 [00:32<00:11, 82.23it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2560/3536 [00:32<00:12, 79.80it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2569/3536 [00:33<00:12, 80.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2579/3536 [00:33<00:11, 83.47it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2588/3536 [00:33<00:11, 82.36it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2597/3536 [00:33<00:11, 80.25it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2606/3536 [00:33<00:12, 73.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2614/3536 [00:33<00:14, 64.34it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2623/3536 [00:33<00:13, 67.55it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2631/3536 [00:33<00:13, 69.01it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2639/3536 [00:34<00:13, 67.22it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2646/3536 [00:34<00:13, 66.36it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2654/3536 [00:34<00:13, 66.32it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2663/3536 [00:34<00:12, 68.70it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2671/3536 [00:34<00:12, 67.82it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2678/3536 [00:34<00:13, 64.36it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2689/3536 [00:34<00:11, 71.73it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2698/3536 [00:34<00:11, 72.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2707/3536 [00:35<00:10, 75.55it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2715/3536 [00:35<00:10, 75.11it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2723/3536 [00:35<00:11, 72.47it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2732/3536 [00:35<00:10, 76.31it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2740/3536 [00:35<00:10, 76.69it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2748/3536 [00:35<00:10, 74.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2757/3536 [00:35<00:09, 78.53it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2765/3536 [00:35<00:09, 78.53it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2773/3536 [00:35<00:10, 76.07it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2781/3536 [00:36<00:11, 65.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2794/3536 [00:36<00:09, 76.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2804/3536 [00:36<00:09, 80.62it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2813/3536 [00:36<00:08, 81.51it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2822/3536 [00:36<00:08, 81.65it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2831/3536 [00:36<00:08, 82.49it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2840/3536 [00:36<00:08, 82.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2849/3536 [00:36<00:08, 79.18it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2859/3536 [00:37<00:08, 79.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2868/3536 [00:37<00:08, 80.71it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2877/3536 [00:37<00:08, 80.96it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2886/3536 [00:37<00:07, 81.38it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2895/3536 [00:37<00:07, 81.53it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2904/3536 [00:37<00:07, 81.14it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2913/3536 [00:37<00:07, 81.87it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2922/3536 [00:37<00:07, 81.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2931/3536 [00:37<00:07, 82.55it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2940/3536 [00:37<00:07, 79.91it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2950/3536 [00:38<00:07, 83.27it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2959/3536 [00:38<00:06, 83.28it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2968/3536 [00:38<00:07, 79.96it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2977/3536 [00:38<00:07, 72.77it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2985/3536 [00:38<00:07, 72.68it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2994/3536 [00:38<00:07, 75.51it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3004/3536 [00:38<00:06, 79.47it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3013/3536 [00:38<00:06, 79.41it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3021/3536 [00:39<00:06, 77.02it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3031/3536 [00:39<00:06, 81.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3040/3536 [00:39<00:06, 81.36it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3049/3536 [00:39<00:06, 79.23it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3059/3536 [00:39<00:05, 83.36it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3068/3536 [00:39<00:05, 83.04it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3077/3536 [00:39<00:05, 82.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3086/3536 [00:39<00:06, 74.36it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3094/3536 [00:39<00:06, 70.20it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3102/3536 [00:40<00:06, 70.25it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3110/3536 [00:40<00:06, 70.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3118/3536 [00:40<00:05, 70.43it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3126/3536 [00:40<00:06, 67.01it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3133/3536 [00:40<00:06, 61.22it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3143/3536 [00:40<00:05, 68.85it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3151/3536 [00:40<00:05, 65.90it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3158/3536 [00:40<00:05, 65.09it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3168/3536 [00:41<00:05, 72.03it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3176/3536 [00:41<00:05, 69.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3183/3536 [00:41<00:05, 65.69it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3190/3536 [00:41<00:05, 66.05it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3197/3536 [00:41<00:05, 65.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3204/3536 [00:41<00:05, 62.84it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3214/3536 [00:41<00:04, 71.81it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3223/3536 [00:41<00:04, 75.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3232/3536 [00:41<00:03, 76.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3242/3536 [00:42<00:03, 82.26it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3251/3536 [00:42<00:03, 80.96it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3261/3536 [00:42<00:03, 85.18it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3270/3536 [00:42<00:03, 82.76it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3280/3536 [00:42<00:03, 83.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3290/3536 [00:42<00:02, 84.23it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3300/3536 [00:42<00:02, 86.98it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3309/3536 [00:42<00:02, 86.43it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3318/3536 [00:42<00:02, 82.98it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3327/3536 [00:43<00:03, 67.93it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3343/3536 [00:43<00:02, 88.48it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3353/3536 [00:43<00:02, 87.44it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3363/3536 [00:43<00:01, 86.76it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3373/3536 [00:43<00:01, 86.44it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3382/3536 [00:43<00:01, 86.47it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3391/3536 [00:43<00:01, 86.31it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3400/3536 [00:43<00:01, 85.88it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3409/3536 [00:44<00:01, 85.95it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3418/3536 [00:44<00:01, 85.83it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3427/3536 [00:44<00:01, 83.77it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3436/3536 [00:44<00:01, 84.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3445/3536 [00:44<00:01, 84.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3454/3536 [00:44<00:00, 84.54it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3463/3536 [00:44<00:01, 64.95it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3475/3536 [00:44<00:00, 76.74it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3484/3536 [00:45<00:00, 79.25it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3493/3536 [00:45<00:00, 79.90it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3502/3536 [00:45<00:00, 78.64it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3512/3536 [00:45<00:00, 82.86it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3521/3536 [00:45<00:00, 83.12it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3530/3536 [00:45<00:00, 82.96it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3536/3536 [00:45<00:00, 77.47it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.4950
Epoch 1 Step 51 Train Loss: 0.5007
Epoch 1 Step 101 Train Loss: 0.4417
Epoch 1 Step 151 Train Loss: 0.4959
Epoch 1 Step 201 Train Loss: 0.5799
Epoch 1 Step 251 Train Loss: 0.4613
Epoch 1 Step 301 Train Loss: 0.4702
Epoch 1 Step 351 Train Loss: 0.4914
Epoch 1: Train Overall MSE: 0.0039 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0310 Validation Top 20 DE MSE: 0.0314. 
Epoch 2 Step 1 Train Loss: 0.5028
Epoch 2 Step 51 Train Loss: 0.5098
Epoch 2 Step 101 Train Loss: 0.5204
Epoch 2 Step 151 Train Loss: 0.5029
Epoch 2 Step 201 Train Loss: 0.4764
Epoch 2 Step 251 Train Loss: 0.4767
Epoch 2 Step 301 Train Loss: 0.4914
Epoch 2 Step 351 Train Loss: 0.4760
Epoch 2: Train Overall MSE: 0.0032 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0297 Validation Top 20 DE MSE: 0.0291. 
Epoch 3 Step 1 Train Loss: 0.5056
Epoch 3 Step 51 Train Loss: 0.5216
Epoch 3 Step 101 Train Loss: 0.4671
Epoch 3 Step 151 Train Loss: 0.5206
Epoch 3 Step 201 Train Loss: 0.4522
Epoch 3 Step 251 Train Loss: 0.5269
Epoch 3 Step 301 Train Loss: 0.4568
Epoch 3 Step 351 Train Loss: 0.4320
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0251 Validation Top 20 DE MSE: 0.0354. 
Epoch 4 Step 1 Train Loss: 0.4849
Epoch 4 Step 51 Train Loss: 0.4969
Epoch 4 Step 101 Train Loss: 0.5030
Epoch 4 Step 151 Train Loss: 0.5328
Epoch 4 Step 201 Train Loss: 0.5363
Epoch 4 Step 251 Train Loss: 0.5320
Epoch 4 Step 301 Train Loss: 0.4742
Epoch 4 Step 351 Train Loss: 0.5014
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0195 Validation Top 20 DE MSE: 0.0321. 
Epoch 5 Step 1 Train Loss: 0.4845
Epoch 5 Step 51 Train Loss: 0.4393
Epoch 5 Step 101 Train Loss: 0.5289
Epoch 5 Step 151 Train Loss: 0.4735
Epoch 5 Step 201 Train Loss: 0.4279
Epoch 5 Step 251 Train Loss: 0.4514
Epoch 5 Step 301 Train Loss: 0.4201
Epoch 5 Step 351 Train Loss: 0.4408
Epoch 5: Train Overall MSE: 0.0034 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0858 Validation Top 20 DE MSE: 0.0278. 
Epoch 6 Step 1 Train Loss: 0.4067
Epoch 6 Step 51 Train Loss: 0.4283
Epoch 6 Step 101 Train Loss: 0.4452
Epoch 6 Step 151 Train Loss: 0.4550
Epoch 6 Step 201 Train Loss: 0.5143
Epoch 6 Step 251 Train Loss: 0.4567
Epoch 6 Step 301 Train Loss: 0.4576
Epoch 6 Step 351 Train Loss: 0.4967
Epoch 6: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0276 Validation Top 20 DE MSE: 0.0305. 
Epoch 7 Step 1 Train Loss: 0.4897
Epoch 7 Step 51 Train Loss: 0.4912
Epoch 7 Step 101 Train Loss: 0.4790
Epoch 7 Step 151 Train Loss: 0.4714
Epoch 7 Step 201 Train Loss: 0.4731
Epoch 7 Step 251 Train Loss: 0.4468
Epoch 7 Step 301 Train Loss: 0.5010
Epoch 7 Step 351 Train Loss: 0.5579
Epoch 7: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0427 Validation Top 20 DE MSE: 0.0311. 
Epoch 8 Step 1 Train Loss: 0.4894
Epoch 8 Step 51 Train Loss: 0.4990
Epoch 8 Step 101 Train Loss: 0.4923
Epoch 8 Step 151 Train Loss: 0.5393
Epoch 8 Step 201 Train Loss: 0.4645
Epoch 8 Step 251 Train Loss: 0.4492
Epoch 8 Step 301 Train Loss: 0.4985
Epoch 8 Step 351 Train Loss: 0.5427
Epoch 8: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0371 Validation Top 20 DE MSE: 0.0302. 
Epoch 9 Step 1 Train Loss: 0.5018
Epoch 9 Step 51 Train Loss: 0.4489
Epoch 9 Step 101 Train Loss: 0.5459
Epoch 9 Step 151 Train Loss: 0.4649
Epoch 9 Step 201 Train Loss: 0.4824
Epoch 9 Step 251 Train Loss: 0.4553
Epoch 9 Step 301 Train Loss: 0.4239
Epoch 9 Step 351 Train Loss: 0.4849
Epoch 9: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0461 Validation Top 20 DE MSE: 0.0325. 
Epoch 10 Step 1 Train Loss: 0.4756
Epoch 10 Step 51 Train Loss: 0.5042
Epoch 10 Step 101 Train Loss: 0.4835
Epoch 10 Step 151 Train Loss: 0.4564
Epoch 10 Step 201 Train Loss: 0.5180
Epoch 10 Step 251 Train Loss: 0.5014
Epoch 10 Step 301 Train Loss: 0.5075
Epoch 10 Step 351 Train Loss: 0.5133
Epoch 10: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0598 Validation Top 20 DE MSE: 0.0326. 
Epoch 11 Step 1 Train Loss: 0.4700
Epoch 11 Step 51 Train Loss: 0.4229
Epoch 11 Step 101 Train Loss: 0.4852
Epoch 11 Step 151 Train Loss: 0.5006
Epoch 11 Step 201 Train Loss: 0.4229
Epoch 11 Step 251 Train Loss: 0.4751
Epoch 11 Step 301 Train Loss: 0.4189
Epoch 11 Step 351 Train Loss: 0.4776
Epoch 11: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0400 Validation Top 20 DE MSE: 0.0309. 
Epoch 12 Step 1 Train Loss: 0.4659
Epoch 12 Step 51 Train Loss: 0.4952
Epoch 12 Step 101 Train Loss: 0.4549
Epoch 12 Step 151 Train Loss: 0.4583
Epoch 12 Step 201 Train Loss: 0.4972
Epoch 12 Step 251 Train Loss: 0.4867
Epoch 12 Step 301 Train Loss: 0.4495
Epoch 12 Step 351 Train Loss: 0.4617
Epoch 12: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0377 Validation Top 20 DE MSE: 0.0302. 
Epoch 13 Step 1 Train Loss: 0.4749
Epoch 13 Step 51 Train Loss: 0.5562
Epoch 13 Step 101 Train Loss: 0.4533
Epoch 13 Step 151 Train Loss: 0.5499
Epoch 13 Step 201 Train Loss: 0.4904
Epoch 13 Step 251 Train Loss: 0.4663
Epoch 13 Step 301 Train Loss: 0.4710
Epoch 13 Step 351 Train Loss: 0.4670
Epoch 13: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0482 Validation Top 20 DE MSE: 0.0301. 
Epoch 14 Step 1 Train Loss: 0.5349
Epoch 14 Step 51 Train Loss: 0.4551
Epoch 14 Step 101 Train Loss: 0.4888
Epoch 14 Step 151 Train Loss: 0.4981
Epoch 14 Step 201 Train Loss: 0.4612
Epoch 14 Step 251 Train Loss: 0.4602
Epoch 14 Step 301 Train Loss: 0.5352
Epoch 14 Step 351 Train Loss: 0.4603
Epoch 14: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0481 Validation Top 20 DE MSE: 0.0327. 
Epoch 15 Step 1 Train Loss: 0.4627
Epoch 15 Step 51 Train Loss: 0.4538
Epoch 15 Step 101 Train Loss: 0.5284
Epoch 15 Step 151 Train Loss: 0.4970
Epoch 15 Step 201 Train Loss: 0.5142
Epoch 15 Step 251 Train Loss: 0.5136
Epoch 15 Step 301 Train Loss: 0.5365
Epoch 15 Step 351 Train Loss: 0.5451
Epoch 15: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0398 Validation Top 20 DE MSE: 0.0320. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0288
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002172897
test_unseen_single_pearson: 0.9944469538051911
test_unseen_single_mse_de: 0.028791962
test_unseen_single_pearson_de: 0.9785639133590506
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.29281845450917615
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.28125
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9395833333333332
test_unseen_single_mse_top20_de_non_dropout: 0.030747516
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.001 MB of 0.023 MB uploadedwandb: / 0.001 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñà‚ñà‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá
wandb:                                                training_loss ‚ñÉ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÅ
wandb:                                                   val_de_mse ‚ñÑ‚ñÇ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÖ
wandb:                                               val_de_pearson ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb:                                                      val_mse ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÑ‚ñÅ‚ñà‚ñà‚ñÉ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.02879
wandb:                                              test_de_pearson 0.97856
wandb:               test_frac_opposite_direction_top20_non_dropout 0.28125
wandb:                          test_frac_sigma_below_1_non_dropout 0.93958
wandb:                                                     test_mse 0.00217
wandb:                                test_mse_top20_de_non_dropout 0.03075
wandb:                                                 test_pearson 0.99445
wandb:                                           test_pearson_delta 0.29282
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.28125
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93958
wandb:                                       test_unseen_single_mse 0.00217
wandb:                                    test_unseen_single_mse_de 0.02879
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.03075
wandb:                                   test_unseen_single_pearson 0.99445
wandb:                                test_unseen_single_pearson_de 0.97856
wandb:                             test_unseen_single_pearson_delta 0.29282
wandb:                                                 train_de_mse 0.03976
wandb:                                             train_de_pearson 0.9447
wandb:                                                    train_mse 0.0024
wandb:                                                train_pearson 0.99389
wandb:                                                training_loss 0.47415
wandb:                                                   val_de_mse 0.03197
wandb:                                               val_de_pearson 0.89563
wandb:                                                      val_mse 0.00213
wandb:                                                  val_pearson 0.9944
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRa_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/qaqtjulv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_001517-qaqtjulv/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:24
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_002757-cbdn1h3r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRa_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/cbdn1h3r
wandb: WARNING Serializing object of type ndarray that is 20619392 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4716
Epoch 1 Step 51 Train Loss: 0.4331
Epoch 1 Step 101 Train Loss: 0.4629
Epoch 1 Step 151 Train Loss: 0.4746
Epoch 1 Step 201 Train Loss: 0.5118
Epoch 1 Step 251 Train Loss: 0.5423
Epoch 1 Step 301 Train Loss: 0.5380
Epoch 1: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0228 Validation Top 20 DE MSE: 0.0236. 
Epoch 2 Step 1 Train Loss: 0.4462
Epoch 2 Step 51 Train Loss: 0.4644
Epoch 2 Step 101 Train Loss: 0.4386
Epoch 2 Step 151 Train Loss: 0.5027
Epoch 2 Step 201 Train Loss: 0.4760
Epoch 2 Step 251 Train Loss: 0.4569
Epoch 2 Step 301 Train Loss: 0.4803
Epoch 2: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0243 Validation Top 20 DE MSE: 0.0242. 
Epoch 3 Step 1 Train Loss: 0.4353
Epoch 3 Step 51 Train Loss: 0.4513
Epoch 3 Step 101 Train Loss: 0.4951
Epoch 3 Step 151 Train Loss: 0.4588
Epoch 3 Step 201 Train Loss: 0.5092
Epoch 3 Step 251 Train Loss: 0.5134
Epoch 3 Step 301 Train Loss: 0.4602
Epoch 3: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0229 Validation Top 20 DE MSE: 0.0260. 
Epoch 4 Step 1 Train Loss: 0.4640
Epoch 4 Step 51 Train Loss: 0.4556
Epoch 4 Step 101 Train Loss: 0.5122
Epoch 4 Step 151 Train Loss: 0.5143
Epoch 4 Step 201 Train Loss: 0.5016
Epoch 4 Step 251 Train Loss: 0.4845
Epoch 4 Step 301 Train Loss: 0.4499
Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0179 Validation Top 20 DE MSE: 0.0263. 
Epoch 5 Step 1 Train Loss: 0.5229
Epoch 5 Step 51 Train Loss: 0.5014
Epoch 5 Step 101 Train Loss: 0.5562
Epoch 5 Step 151 Train Loss: 0.4909
Epoch 5 Step 201 Train Loss: 0.4709
Epoch 5 Step 251 Train Loss: 0.4783
Epoch 5 Step 301 Train Loss: 0.5061
Epoch 5: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0184 Validation Top 20 DE MSE: 0.0242. 
Epoch 6 Step 1 Train Loss: 0.4555
Epoch 6 Step 51 Train Loss: 0.5192
Epoch 6 Step 101 Train Loss: 0.4714
Epoch 6 Step 151 Train Loss: 0.5059
Epoch 6 Step 201 Train Loss: 0.4513
Epoch 6 Step 251 Train Loss: 0.4674
Epoch 6 Step 301 Train Loss: 0.4498
Epoch 6: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0178 Validation Top 20 DE MSE: 0.0215. 
Epoch 7 Step 1 Train Loss: 0.5273
Epoch 7 Step 51 Train Loss: 0.4747
Epoch 7 Step 101 Train Loss: 0.4927
Epoch 7 Step 151 Train Loss: 0.4703
Epoch 7 Step 201 Train Loss: 0.4819
Epoch 7 Step 251 Train Loss: 0.4944
Epoch 7 Step 301 Train Loss: 0.4266
Epoch 7: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0165 Validation Top 20 DE MSE: 0.0217. 
Epoch 8 Step 1 Train Loss: 0.4521
Epoch 8 Step 51 Train Loss: 0.5041
Epoch 8 Step 101 Train Loss: 0.5333
Epoch 8 Step 151 Train Loss: 0.4793
Epoch 8 Step 201 Train Loss: 0.5343
Epoch 8 Step 251 Train Loss: 0.4984
Epoch 8 Step 301 Train Loss: 0.4470
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0176 Validation Top 20 DE MSE: 0.0232. 
Epoch 9 Step 1 Train Loss: 0.4363
Epoch 9 Step 51 Train Loss: 0.4607
Epoch 9 Step 101 Train Loss: 0.4514
Epoch 9 Step 151 Train Loss: 0.4519
Epoch 9 Step 201 Train Loss: 0.4300
Epoch 9 Step 251 Train Loss: 0.4455
Epoch 9 Step 301 Train Loss: 0.4678
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0175 Validation Top 20 DE MSE: 0.0225. 
Epoch 10 Step 1 Train Loss: 0.5080
Epoch 10 Step 51 Train Loss: 0.5280
Epoch 10 Step 101 Train Loss: 0.5215
Epoch 10 Step 151 Train Loss: 0.4977
Epoch 10 Step 201 Train Loss: 0.4608
Epoch 10 Step 251 Train Loss: 0.5280
Epoch 10 Step 301 Train Loss: 0.4600
Epoch 10: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0170 Validation Top 20 DE MSE: 0.0224. 
Epoch 11 Step 1 Train Loss: 0.4436
Epoch 11 Step 51 Train Loss: 0.4541
Epoch 11 Step 101 Train Loss: 0.4646
Epoch 11 Step 151 Train Loss: 0.4356
Epoch 11 Step 201 Train Loss: 0.4924
Epoch 11 Step 251 Train Loss: 0.5210
Epoch 11 Step 301 Train Loss: 0.4835
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0170 Validation Top 20 DE MSE: 0.0234. 
Epoch 12 Step 1 Train Loss: 0.4556
Epoch 12 Step 51 Train Loss: 0.5642
Epoch 12 Step 101 Train Loss: 0.4832
Epoch 12 Step 151 Train Loss: 0.4502
Epoch 12 Step 201 Train Loss: 0.4776
Epoch 12 Step 251 Train Loss: 0.4275
Epoch 12 Step 301 Train Loss: 0.5302
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0168 Validation Top 20 DE MSE: 0.0227. 
Epoch 13 Step 1 Train Loss: 0.4788
Epoch 13 Step 51 Train Loss: 0.4540
Epoch 13 Step 101 Train Loss: 0.4344
Epoch 13 Step 151 Train Loss: 0.5043
Epoch 13 Step 201 Train Loss: 0.4731
Epoch 13 Step 251 Train Loss: 0.4788
Epoch 13 Step 301 Train Loss: 0.4850
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0171 Validation Top 20 DE MSE: 0.0222. 
Epoch 14 Step 1 Train Loss: 0.4948
Epoch 14 Step 51 Train Loss: 0.4448
Epoch 14 Step 101 Train Loss: 0.4692
Epoch 14 Step 151 Train Loss: 0.4651
Epoch 14 Step 201 Train Loss: 0.4792
Epoch 14 Step 251 Train Loss: 0.4967
Epoch 14 Step 301 Train Loss: 0.4833
Epoch 14: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0174 Validation Top 20 DE MSE: 0.0233. 
Epoch 15 Step 1 Train Loss: 0.5097
Epoch 15 Step 51 Train Loss: 0.4621
Epoch 15 Step 101 Train Loss: 0.4910
Epoch 15 Step 151 Train Loss: 0.4781
Epoch 15 Step 201 Train Loss: 0.4981
Epoch 15 Step 251 Train Loss: 0.4073
Epoch 15 Step 301 Train Loss: 0.4481
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0169 Validation Top 20 DE MSE: 0.0222. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1456
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.003553835
test_unseen_single_pearson: 0.9905152429119507
test_unseen_single_mse_de: 0.14559002
test_unseen_single_pearson_de: 0.9436859759472314
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.22429069463168302
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2520833333333334
test_unseen_single_frac_sigma_below_1_non_dropout: 0.975
test_unseen_single_mse_top20_de_non_dropout: 0.14647241
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá
wandb:                                                training_loss ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÅ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñá
wandb:                                                   val_de_mse ‚ñÑ‚ñÖ‚ñà‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà
wandb:                                                      val_mse ‚ñá‚ñà‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ
wandb:                                                  val_pearson ‚ñÉ‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.14559
wandb:                                              test_de_pearson 0.94369
wandb:               test_frac_opposite_direction_top20_non_dropout 0.25208
wandb:                          test_frac_sigma_below_1_non_dropout 0.975
wandb:                                                     test_mse 0.00355
wandb:                                test_mse_top20_de_non_dropout 0.14647
wandb:                                                 test_pearson 0.99052
wandb:                                           test_pearson_delta 0.22429
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25208
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.975
wandb:                                       test_unseen_single_mse 0.00355
wandb:                                    test_unseen_single_mse_de 0.14559
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.14647
wandb:                                   test_unseen_single_pearson 0.99052
wandb:                                test_unseen_single_pearson_de 0.94369
wandb:                             test_unseen_single_pearson_delta 0.22429
wandb:                                                 train_de_mse 0.01691
wandb:                                             train_de_pearson 0.94696
wandb:                                                    train_mse 0.00174
wandb:                                                train_pearson 0.99553
wandb:                                                training_loss 0.4608
wandb:                                                   val_de_mse 0.02225
wandb:                                               val_de_pearson 0.9836
wandb:                                                      val_mse 0.00168
wandb:                                                  val_pearson 0.99554
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRa_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/cbdn1h3r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_002757-cbdn1h3r/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:24
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_003822-v853wz64
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRa_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/v853wz64
wandb: WARNING Serializing object of type ndarray that is 20619392 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4662
Epoch 1 Step 51 Train Loss: 0.4846
Epoch 1 Step 101 Train Loss: 0.5044
Epoch 1 Step 151 Train Loss: 0.5052
Epoch 1 Step 201 Train Loss: 0.5807
Epoch 1 Step 251 Train Loss: 0.5521
Epoch 1 Step 301 Train Loss: 0.4724
Epoch 1 Step 351 Train Loss: 0.4361
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0523 Validation Top 20 DE MSE: 0.0515. 
Epoch 2 Step 1 Train Loss: 0.4539
Epoch 2 Step 51 Train Loss: 0.5097
Epoch 2 Step 101 Train Loss: 0.5351
Epoch 2 Step 151 Train Loss: 0.4470
Epoch 2 Step 201 Train Loss: 0.4768
Epoch 2 Step 251 Train Loss: 0.4954
Epoch 2 Step 301 Train Loss: 0.5067
Epoch 2 Step 351 Train Loss: 0.5862
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0341 Validation Top 20 DE MSE: 0.0420. 
Epoch 3 Step 1 Train Loss: 0.4501
Epoch 3 Step 51 Train Loss: 0.4593
Epoch 3 Step 101 Train Loss: 0.5423
Epoch 3 Step 151 Train Loss: 0.6059
Epoch 3 Step 201 Train Loss: 0.4584
Epoch 3 Step 251 Train Loss: 0.5631
Epoch 3 Step 301 Train Loss: 0.4556
Epoch 3 Step 351 Train Loss: 0.5041
Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0189 Validation Top 20 DE MSE: 0.0380. 
Epoch 4 Step 1 Train Loss: 0.4378
Epoch 4 Step 51 Train Loss: 0.4618
Epoch 4 Step 101 Train Loss: 0.4694
Epoch 4 Step 151 Train Loss: 0.5541
Epoch 4 Step 201 Train Loss: 0.4898
Epoch 4 Step 251 Train Loss: 0.4925
Epoch 4 Step 301 Train Loss: 0.4937
Epoch 4 Step 351 Train Loss: 0.5117
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0220 Validation Top 20 DE MSE: 0.0433. 
Epoch 5 Step 1 Train Loss: 0.4593
Epoch 5 Step 51 Train Loss: 0.4509
Epoch 5 Step 101 Train Loss: 0.5595
Epoch 5 Step 151 Train Loss: 0.4850
Epoch 5 Step 201 Train Loss: 0.4750
Epoch 5 Step 251 Train Loss: 0.4882
Epoch 5 Step 301 Train Loss: 0.5218
Epoch 5 Step 351 Train Loss: 0.4728
Epoch 5: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0184 Validation Top 20 DE MSE: 0.0375. 
Epoch 6 Step 1 Train Loss: 0.5182
Epoch 6 Step 51 Train Loss: 0.4915
Epoch 6 Step 101 Train Loss: 0.5334
Epoch 6 Step 151 Train Loss: 0.4727
Epoch 6 Step 201 Train Loss: 0.5233
Epoch 6 Step 251 Train Loss: 0.4675
Epoch 6 Step 301 Train Loss: 0.5098
Epoch 6 Step 351 Train Loss: 0.4766
Epoch 6: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0243 Validation Top 20 DE MSE: 0.0435. 
Epoch 7 Step 1 Train Loss: 0.4753
Epoch 7 Step 51 Train Loss: 0.4903
Epoch 7 Step 101 Train Loss: 0.4615
Epoch 7 Step 151 Train Loss: 0.4956
Epoch 7 Step 201 Train Loss: 0.4502
Epoch 7 Step 251 Train Loss: 0.5432
Epoch 7 Step 301 Train Loss: 0.4897
Epoch 7 Step 351 Train Loss: 0.4669
Epoch 7: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0384. 
Epoch 8 Step 1 Train Loss: 0.5004
Epoch 8 Step 51 Train Loss: 0.5068
Epoch 8 Step 101 Train Loss: 0.4808
Epoch 8 Step 151 Train Loss: 0.5424
Epoch 8 Step 201 Train Loss: 0.5009
Epoch 8 Step 251 Train Loss: 0.4881
Epoch 8 Step 301 Train Loss: 0.4949
Epoch 8 Step 351 Train Loss: 0.5197
Epoch 8: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0246 Validation Top 20 DE MSE: 0.0409. 
Epoch 9 Step 1 Train Loss: 0.5254
Epoch 9 Step 51 Train Loss: 0.5031
Epoch 9 Step 101 Train Loss: 0.5363
Epoch 9 Step 151 Train Loss: 0.5271
Epoch 9 Step 201 Train Loss: 0.5320
Epoch 9 Step 251 Train Loss: 0.5467
Epoch 9 Step 301 Train Loss: 0.4929
Epoch 9 Step 351 Train Loss: 0.4653
Epoch 9: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0301 Validation Top 20 DE MSE: 0.0408. 
Epoch 10 Step 1 Train Loss: 0.5027
Epoch 10 Step 51 Train Loss: 0.4921
Epoch 10 Step 101 Train Loss: 0.4871
Epoch 10 Step 151 Train Loss: 0.5136
Epoch 10 Step 201 Train Loss: 0.4568
Epoch 10 Step 251 Train Loss: 0.4548
Epoch 10 Step 301 Train Loss: 0.4634
Epoch 10 Step 351 Train Loss: 0.5311
Epoch 10: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0325 Validation Top 20 DE MSE: 0.0410. 
Epoch 11 Step 1 Train Loss: 0.5238
Epoch 11 Step 51 Train Loss: 0.4850
Epoch 11 Step 101 Train Loss: 0.4454
Epoch 11 Step 151 Train Loss: 0.4624
Epoch 11 Step 201 Train Loss: 0.4723
Epoch 11 Step 251 Train Loss: 0.4781
Epoch 11 Step 301 Train Loss: 0.4888
Epoch 11 Step 351 Train Loss: 0.5229
Epoch 11: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0350 Validation Top 20 DE MSE: 0.0392. 
Epoch 12 Step 1 Train Loss: 0.4773
Epoch 12 Step 51 Train Loss: 0.4813
Epoch 12 Step 101 Train Loss: 0.5294
Epoch 12 Step 151 Train Loss: 0.4621
Epoch 12 Step 201 Train Loss: 0.5301
Epoch 12 Step 251 Train Loss: 0.5009
Epoch 12 Step 301 Train Loss: 0.4818
Epoch 12 Step 351 Train Loss: 0.4970
Epoch 12: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0407 Validation Top 20 DE MSE: 0.0368. 
Epoch 13 Step 1 Train Loss: 0.4848
Epoch 13 Step 51 Train Loss: 0.4930
Epoch 13 Step 101 Train Loss: 0.4853
Epoch 13 Step 151 Train Loss: 0.5600
Epoch 13 Step 201 Train Loss: 0.4913
Epoch 13 Step 251 Train Loss: 0.4889
Epoch 13 Step 301 Train Loss: 0.4202
Epoch 13 Step 351 Train Loss: 0.4686
Epoch 13: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0317 Validation Top 20 DE MSE: 0.0392. 
Epoch 14 Step 1 Train Loss: 0.5300
Epoch 14 Step 51 Train Loss: 0.5149
Epoch 14 Step 101 Train Loss: 0.5130
Epoch 14 Step 151 Train Loss: 0.5918
Epoch 14 Step 201 Train Loss: 0.5328
Epoch 14 Step 251 Train Loss: 0.5393
Epoch 14 Step 301 Train Loss: 0.4758
Epoch 14 Step 351 Train Loss: 0.4849
Epoch 14: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0268 Validation Top 20 DE MSE: 0.0369. 
Epoch 15 Step 1 Train Loss: 0.5500
Epoch 15 Step 51 Train Loss: 0.5190
Epoch 15 Step 101 Train Loss: 0.4837
Epoch 15 Step 151 Train Loss: 0.4575
Epoch 15 Step 201 Train Loss: 0.4541
Epoch 15 Step 251 Train Loss: 0.5063
Epoch 15 Step 301 Train Loss: 0.4862
Epoch 15 Step 351 Train Loss: 0.4722
Epoch 15: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0408 Validation Top 20 DE MSE: 0.0377. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0270
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0020299305
test_unseen_single_pearson: 0.9948375717880401
test_unseen_single_mse_de: 0.027039701
test_unseen_single_pearson_de: 0.9472711528912269
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.28152659241235206
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.26458333333333334
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9520833333333334
test_unseen_single_mse_top20_de_non_dropout: 0.028733991
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.007 MB of 0.022 MB uploadedwandb: - 0.015 MB of 0.022 MB uploadedwandb: \ 0.015 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÜ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ
wandb:                                                    train_mse ‚ñá‚ñà‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÑ‚ñÜ
wandb:                                                train_pearson ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÑ
wandb:                                                training_loss ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ
wandb:                                                   val_de_mse ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñà‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb:                                                      val_mse ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñá‚ñÜ
wandb:                                                  val_pearson ‚ñá‚ñÖ‚ñÅ‚ñà‚ñÖ‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.02704
wandb:                                              test_de_pearson 0.94727
wandb:               test_frac_opposite_direction_top20_non_dropout 0.26458
wandb:                          test_frac_sigma_below_1_non_dropout 0.95208
wandb:                                                     test_mse 0.00203
wandb:                                test_mse_top20_de_non_dropout 0.02873
wandb:                                                 test_pearson 0.99484
wandb:                                           test_pearson_delta 0.28153
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.26458
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.95208
wandb:                                       test_unseen_single_mse 0.00203
wandb:                                    test_unseen_single_mse_de 0.02704
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.02873
wandb:                                   test_unseen_single_pearson 0.99484
wandb:                                test_unseen_single_pearson_de 0.94727
wandb:                             test_unseen_single_pearson_delta 0.28153
wandb:                                                 train_de_mse 0.04076
wandb:                                             train_de_pearson 0.94643
wandb:                                                    train_mse 0.00253
wandb:                                                train_pearson 0.99363
wandb:                                                training_loss 0.45583
wandb:                                                   val_de_mse 0.03769
wandb:                                               val_de_pearson 0.97909
wandb:                                                      val_mse 0.00268
wandb:                                                  val_pearson 0.99328
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRa_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/v853wz64
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_003822-v853wz64/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:24
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_004905-xj7m0c0k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRa_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/xj7m0c0k
wandb: WARNING Serializing object of type ndarray that is 20619392 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4873
Epoch 1 Step 51 Train Loss: 0.5264
Epoch 1 Step 101 Train Loss: 0.5469
Epoch 1 Step 151 Train Loss: 0.4959
Epoch 1 Step 201 Train Loss: 0.4665
Epoch 1 Step 251 Train Loss: 0.6052
Epoch 1 Step 301 Train Loss: 0.4816
Epoch 1: Train Overall MSE: 0.0039 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0239 Validation Top 20 DE MSE: 0.0149. 
Epoch 2 Step 1 Train Loss: 0.4743
Epoch 2 Step 51 Train Loss: 0.4788
Epoch 2 Step 101 Train Loss: 0.4845
Epoch 2 Step 151 Train Loss: 0.5201
Epoch 2 Step 201 Train Loss: 0.5196
Epoch 2 Step 251 Train Loss: 0.5520
Epoch 2 Step 301 Train Loss: 0.5427
Epoch 2: Train Overall MSE: 0.0034 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0241 Validation Top 20 DE MSE: 0.0162. 
Epoch 3 Step 1 Train Loss: 0.4789
Epoch 3 Step 51 Train Loss: 0.5181
Epoch 3 Step 101 Train Loss: 0.5056
Epoch 3 Step 151 Train Loss: 0.4857
Epoch 3 Step 201 Train Loss: 0.5571
Epoch 3 Step 251 Train Loss: 0.4928
Epoch 3 Step 301 Train Loss: 0.4811
Epoch 3: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0230 Validation Top 20 DE MSE: 0.0169. 
Epoch 4 Step 1 Train Loss: 0.5081
Epoch 4 Step 51 Train Loss: 0.5194
Epoch 4 Step 101 Train Loss: 0.5589
Epoch 4 Step 151 Train Loss: 0.4749
Epoch 4 Step 201 Train Loss: 0.5695
Epoch 4 Step 251 Train Loss: 0.4356
Epoch 4 Step 301 Train Loss: 0.4963
Epoch 4: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0270 Validation Top 20 DE MSE: 0.0182. 
Epoch 5 Step 1 Train Loss: 0.5523
Epoch 5 Step 51 Train Loss: 0.4997
Epoch 5 Step 101 Train Loss: 0.5562
Epoch 5 Step 151 Train Loss: 0.5040
Epoch 5 Step 201 Train Loss: 0.4968
Epoch 5 Step 251 Train Loss: 0.5401
Epoch 5 Step 301 Train Loss: 0.5057
Epoch 5: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0217 Validation Top 20 DE MSE: 0.0189. 
Epoch 6 Step 1 Train Loss: 0.5085
Epoch 6 Step 51 Train Loss: 0.4960
Epoch 6 Step 101 Train Loss: 0.4760
Epoch 6 Step 151 Train Loss: 0.4826
Epoch 6 Step 201 Train Loss: 0.4851
Epoch 6 Step 251 Train Loss: 0.4699
Epoch 6 Step 301 Train Loss: 0.5083
Epoch 6: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0231 Validation Top 20 DE MSE: 0.0193. 
Epoch 7 Step 1 Train Loss: 0.4805
Epoch 7 Step 51 Train Loss: 0.4877
Epoch 7 Step 101 Train Loss: 0.4949
Epoch 7 Step 151 Train Loss: 0.4747
Epoch 7 Step 201 Train Loss: 0.5405
Epoch 7 Step 251 Train Loss: 0.5830
Epoch 7 Step 301 Train Loss: 0.4886
Epoch 7: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0237 Validation Top 20 DE MSE: 0.0195. 
Epoch 8 Step 1 Train Loss: 0.4644
Epoch 8 Step 51 Train Loss: 0.4717
Epoch 8 Step 101 Train Loss: 0.4651
Epoch 8 Step 151 Train Loss: 0.5084
Epoch 8 Step 201 Train Loss: 0.5424
Epoch 8 Step 251 Train Loss: 0.4810
Epoch 8 Step 301 Train Loss: 0.5209
Epoch 8: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0226 Validation Top 20 DE MSE: 0.0192. 
Epoch 9 Step 1 Train Loss: 0.4526
Epoch 9 Step 51 Train Loss: 0.5154
Epoch 9 Step 101 Train Loss: 0.5442
Epoch 9 Step 151 Train Loss: 0.5465
Epoch 9 Step 201 Train Loss: 0.4920
Epoch 9 Step 251 Train Loss: 0.4951
Epoch 9 Step 301 Train Loss: 0.5000
Epoch 9: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0226 Validation Top 20 DE MSE: 0.0188. 
Epoch 10 Step 1 Train Loss: 0.5199
Epoch 10 Step 51 Train Loss: 0.4872
Epoch 10 Step 101 Train Loss: 0.5246
Epoch 10 Step 151 Train Loss: 0.5043
Epoch 10 Step 201 Train Loss: 0.4821
Epoch 10 Step 251 Train Loss: 0.5530
Epoch 10 Step 301 Train Loss: 0.5541
Epoch 10: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0217 Validation Top 20 DE MSE: 0.0187. 
Epoch 11 Step 1 Train Loss: 0.4820
Epoch 11 Step 51 Train Loss: 0.5082
Epoch 11 Step 101 Train Loss: 0.5721
Epoch 11 Step 151 Train Loss: 0.4707
Epoch 11 Step 201 Train Loss: 0.4662
Epoch 11 Step 251 Train Loss: 0.4820
Epoch 11 Step 301 Train Loss: 0.5398
Epoch 11: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0223 Validation Top 20 DE MSE: 0.0196. 
Epoch 12 Step 1 Train Loss: 0.5119
Epoch 12 Step 51 Train Loss: 0.4665
Epoch 12 Step 101 Train Loss: 0.4755
Epoch 12 Step 151 Train Loss: 0.5333
Epoch 12 Step 201 Train Loss: 0.5202
Epoch 12 Step 251 Train Loss: 0.5442
Epoch 12 Step 301 Train Loss: 0.5520
Epoch 12: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0216 Validation Top 20 DE MSE: 0.0192. 
Epoch 13 Step 1 Train Loss: 0.4986
Epoch 13 Step 51 Train Loss: 0.4994
Epoch 13 Step 101 Train Loss: 0.4762
Epoch 13 Step 151 Train Loss: 0.4755
Epoch 13 Step 201 Train Loss: 0.5550
Epoch 13 Step 251 Train Loss: 0.5466
Epoch 13 Step 301 Train Loss: 0.5508
Epoch 13: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0226 Validation Top 20 DE MSE: 0.0189. 
Epoch 14 Step 1 Train Loss: 0.5154
Epoch 14 Step 51 Train Loss: 0.5711
Epoch 14 Step 101 Train Loss: 0.4724
Epoch 14 Step 151 Train Loss: 0.5470
Epoch 14 Step 201 Train Loss: 0.5150
Epoch 14 Step 251 Train Loss: 0.5119
Epoch 14 Step 301 Train Loss: 0.4659
Epoch 14: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0222 Validation Top 20 DE MSE: 0.0190. 
Epoch 15 Step 1 Train Loss: 0.5132
Epoch 15 Step 51 Train Loss: 0.4799
Epoch 15 Step 101 Train Loss: 0.5209
Epoch 15 Step 151 Train Loss: 0.5160
Epoch 15 Step 201 Train Loss: 0.4684
Epoch 15 Step 251 Train Loss: 0.5422
Epoch 15 Step 301 Train Loss: 0.4763
Epoch 15: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0216 Validation Top 20 DE MSE: 0.0191. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0542
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002971069
test_unseen_single_pearson: 0.9922214006577444
test_unseen_single_mse_de: 0.054192375
test_unseen_single_pearson_de: 0.9563508122684171
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.24069803509150786
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2708333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9395833333333333
test_unseen_single_mse_top20_de_non_dropout: 0.05810647
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.001 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà
wandb:                                                    train_mse ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá
wandb:                                               val_de_pearson ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                      val_mse ‚ñà‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.05419
wandb:                                              test_de_pearson 0.95635
wandb:               test_frac_opposite_direction_top20_non_dropout 0.27083
wandb:                          test_frac_sigma_below_1_non_dropout 0.93958
wandb:                                                     test_mse 0.00297
wandb:                                test_mse_top20_de_non_dropout 0.05811
wandb:                                                 test_pearson 0.99222
wandb:                                           test_pearson_delta 0.2407
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.27083
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93958
wandb:                                       test_unseen_single_mse 0.00297
wandb:                                    test_unseen_single_mse_de 0.05419
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.05811
wandb:                                   test_unseen_single_pearson 0.99222
wandb:                                test_unseen_single_pearson_de 0.95635
wandb:                             test_unseen_single_pearson_delta 0.2407
wandb:                                                 train_de_mse 0.02164
wandb:                                             train_de_pearson 0.95005
wandb:                                                    train_mse 0.0025
wandb:                                                train_pearson 0.99375
wandb:                                                training_loss 0.51538
wandb:                                                   val_de_mse 0.0191
wandb:                                               val_de_pearson 0.99082
wandb:                                                      val_mse 0.00129
wandb:                                                  val_pearson 0.9967
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRa_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/xj7m0c0k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_004905-xj7m0c0k/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:24
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_005928-zxjqi41a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRa_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zxjqi41a
wandb: WARNING Serializing object of type ndarray that is 20619392 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4562
Epoch 1 Step 51 Train Loss: 0.5648
Epoch 1 Step 101 Train Loss: 0.4951
Epoch 1 Step 151 Train Loss: 0.5174
Epoch 1 Step 201 Train Loss: 0.4908
Epoch 1 Step 251 Train Loss: 0.5214
Epoch 1 Step 301 Train Loss: 0.4874
Epoch 1: Train Overall MSE: 0.0050 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0383 Validation Top 20 DE MSE: 0.0484. 
Epoch 2 Step 1 Train Loss: 0.4991
Epoch 2 Step 51 Train Loss: 0.4940
Epoch 2 Step 101 Train Loss: 0.4598
Epoch 2 Step 151 Train Loss: 0.5149
Epoch 2 Step 201 Train Loss: 0.5366
Epoch 2 Step 251 Train Loss: 0.5064
Epoch 2 Step 301 Train Loss: 0.4662
Epoch 2: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0263 Validation Top 20 DE MSE: 0.0392. 
Epoch 3 Step 1 Train Loss: 0.5142
Epoch 3 Step 51 Train Loss: 0.4244
Epoch 3 Step 101 Train Loss: 0.5331
Epoch 3 Step 151 Train Loss: 0.5857
Epoch 3 Step 201 Train Loss: 0.5083
Epoch 3 Step 251 Train Loss: 0.4903
Epoch 3 Step 301 Train Loss: 0.6246
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0216 Validation Top 20 DE MSE: 0.0338. 
Epoch 4 Step 1 Train Loss: 0.4779
Epoch 4 Step 51 Train Loss: 0.4374
Epoch 4 Step 101 Train Loss: 0.5056
Epoch 4 Step 151 Train Loss: 0.4797
Epoch 4 Step 201 Train Loss: 0.4625
Epoch 4 Step 251 Train Loss: 0.4933
Epoch 4 Step 301 Train Loss: 0.5166
Epoch 4: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0210 Validation Top 20 DE MSE: 0.0406. 
Epoch 5 Step 1 Train Loss: 0.4920
Epoch 5 Step 51 Train Loss: 0.4860
Epoch 5 Step 101 Train Loss: 0.5880
Epoch 5 Step 151 Train Loss: 0.4669
Epoch 5 Step 201 Train Loss: 0.4943
Epoch 5 Step 251 Train Loss: 0.4704
Epoch 5 Step 301 Train Loss: 0.4714
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0212 Validation Top 20 DE MSE: 0.0370. 
Epoch 6 Step 1 Train Loss: 0.5407
Epoch 6 Step 51 Train Loss: 0.5230
Epoch 6 Step 101 Train Loss: 0.4516
Epoch 6 Step 151 Train Loss: 0.4932
Epoch 6 Step 201 Train Loss: 0.4583
Epoch 6 Step 251 Train Loss: 0.5526
Epoch 6 Step 301 Train Loss: 0.4776
Epoch 6: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0224 Validation Top 20 DE MSE: 0.0350. 
Epoch 7 Step 1 Train Loss: 0.4707
Epoch 7 Step 51 Train Loss: 0.5453
Epoch 7 Step 101 Train Loss: 0.5072
Epoch 7 Step 151 Train Loss: 0.5098
Epoch 7 Step 201 Train Loss: 0.4365
Epoch 7 Step 251 Train Loss: 0.4500
Epoch 7 Step 301 Train Loss: 0.4905
Epoch 7: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0263 Validation Top 20 DE MSE: 0.0347. 
Epoch 8 Step 1 Train Loss: 0.4393
Epoch 8 Step 51 Train Loss: 0.4695
Epoch 8 Step 101 Train Loss: 0.4530
Epoch 8 Step 151 Train Loss: 0.4596
Epoch 8 Step 201 Train Loss: 0.4588
Epoch 8 Step 251 Train Loss: 0.4895
Epoch 8 Step 301 Train Loss: 0.4851
Epoch 8: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0234 Validation Top 20 DE MSE: 0.0362. 
Epoch 9 Step 1 Train Loss: 0.4965
Epoch 9 Step 51 Train Loss: 0.5359
Epoch 9 Step 101 Train Loss: 0.5126
Epoch 9 Step 151 Train Loss: 0.4819
Epoch 9 Step 201 Train Loss: 0.5078
Epoch 9 Step 251 Train Loss: 0.4217
Epoch 9 Step 301 Train Loss: 0.4939
Epoch 9: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0266 Validation Top 20 DE MSE: 0.0353. 
Epoch 10 Step 1 Train Loss: 0.4552
Epoch 10 Step 51 Train Loss: 0.5030
Epoch 10 Step 101 Train Loss: 0.5089
Epoch 10 Step 151 Train Loss: 0.4850
Epoch 10 Step 201 Train Loss: 0.4832
Epoch 10 Step 251 Train Loss: 0.5308
Epoch 10 Step 301 Train Loss: 0.4294
Epoch 10: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0284 Validation Top 20 DE MSE: 0.0389. 
Epoch 11 Step 1 Train Loss: 0.4713
Epoch 11 Step 51 Train Loss: 0.5119
Epoch 11 Step 101 Train Loss: 0.5147
Epoch 11 Step 151 Train Loss: 0.4674
Epoch 11 Step 201 Train Loss: 0.5011
Epoch 11 Step 251 Train Loss: 0.5729
Epoch 11 Step 301 Train Loss: 0.4860
Epoch 11: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0248 Validation Top 20 DE MSE: 0.0361. 
Epoch 12 Step 1 Train Loss: 0.5225
Epoch 12 Step 51 Train Loss: 0.4632
Epoch 12 Step 101 Train Loss: 0.4728
Epoch 12 Step 151 Train Loss: 0.4533
Epoch 12 Step 201 Train Loss: 0.5202
Epoch 12 Step 251 Train Loss: 0.5001
Epoch 12 Step 301 Train Loss: 0.5108
Epoch 12: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0273 Validation Top 20 DE MSE: 0.0403. 
Epoch 13 Step 1 Train Loss: 0.4861
Epoch 13 Step 51 Train Loss: 0.4692
Epoch 13 Step 101 Train Loss: 0.4799
Epoch 13 Step 151 Train Loss: 0.4972
Epoch 13 Step 201 Train Loss: 0.4609
Epoch 13 Step 251 Train Loss: 0.4534
Epoch 13 Step 301 Train Loss: 0.4926
Epoch 13: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0412 Validation Top 20 DE MSE: 0.0339. 
Epoch 14 Step 1 Train Loss: 0.5619
Epoch 14 Step 51 Train Loss: 0.4647
Epoch 14 Step 101 Train Loss: 0.4623
Epoch 14 Step 151 Train Loss: 0.4737
Epoch 14 Step 201 Train Loss: 0.4904
Epoch 14 Step 251 Train Loss: 0.4574
Epoch 14 Step 301 Train Loss: 0.5278
Epoch 14: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0293 Validation Top 20 DE MSE: 0.0376. 
Epoch 15 Step 1 Train Loss: 0.5686
Epoch 15 Step 51 Train Loss: 0.4959
Epoch 15 Step 101 Train Loss: 0.4683
Epoch 15 Step 151 Train Loss: 0.4793
Epoch 15 Step 201 Train Loss: 0.4739
Epoch 15 Step 251 Train Loss: 0.4780
Epoch 15 Step 301 Train Loss: 0.4865
Epoch 15: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0284 Validation Top 20 DE MSE: 0.0387. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0341
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0020478787
test_unseen_single_pearson: 0.9946710477241649
test_unseen_single_mse_de: 0.034146037
test_unseen_single_pearson_de: 0.9585060178183196
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.2705805453238053
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.27708333333333335
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9375
test_unseen_single_mse_top20_de_non_dropout: 0.037739757
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÑ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá
wandb:                                                training_loss ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ
wandb:                                                   val_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ
wandb:                                                      val_mse ‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.03415
wandb:                                              test_de_pearson 0.95851
wandb:               test_frac_opposite_direction_top20_non_dropout 0.27708
wandb:                          test_frac_sigma_below_1_non_dropout 0.9375
wandb:                                                     test_mse 0.00205
wandb:                                test_mse_top20_de_non_dropout 0.03774
wandb:                                                 test_pearson 0.99467
wandb:                                           test_pearson_delta 0.27058
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.27708
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9375
wandb:                                       test_unseen_single_mse 0.00205
wandb:                                    test_unseen_single_mse_de 0.03415
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.03774
wandb:                                   test_unseen_single_pearson 0.99467
wandb:                                test_unseen_single_pearson_de 0.95851
wandb:                             test_unseen_single_pearson_delta 0.27058
wandb:                                                 train_de_mse 0.02835
wandb:                                             train_de_pearson 0.95264
wandb:                                                    train_mse 0.00235
wandb:                                                train_pearson 0.99405
wandb:                                                training_loss 0.57004
wandb:                                                   val_de_mse 0.03873
wandb:                                               val_de_pearson 0.95129
wandb:                                                      val_mse 0.0019
wandb:                                                  val_pearson 0.995
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRa_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/zxjqi41a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_005928-zxjqi41a/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:46
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_011028-vsm5rc37
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRi_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/vsm5rc37
wandb: WARNING Serializing object of type ndarray that is 21106816 bytes
  0%|                                                  | 0/3277 [00:00<?, ?it/s]  0%|                                          | 8/3277 [00:00<00:41, 78.77it/s]  1%|‚ñè                                        | 17/3277 [00:00<00:40, 79.72it/s]  1%|‚ñé                                        | 27/3277 [00:00<00:38, 84.80it/s]  1%|‚ñç                                        | 37/3277 [00:00<00:37, 87.49it/s]  1%|‚ñå                                        | 47/3277 [00:00<00:36, 88.61it/s]  2%|‚ñã                                        | 56/3277 [00:00<00:36, 88.94it/s]  2%|‚ñä                                        | 65/3277 [00:00<00:36, 88.65it/s]  2%|‚ñâ                                        | 74/3277 [00:00<00:36, 88.37it/s]  3%|‚ñà                                        | 84/3277 [00:00<00:35, 89.44it/s]  3%|‚ñà‚ñè                                       | 93/3277 [00:01<00:35, 89.05it/s]  3%|‚ñà‚ñé                                      | 103/3277 [00:01<00:35, 89.74it/s]  3%|‚ñà‚ñç                                      | 113/3277 [00:01<00:35, 90.33it/s]  4%|‚ñà‚ñå                                      | 123/3277 [00:01<00:34, 90.46it/s]  4%|‚ñà‚ñå                                      | 133/3277 [00:01<00:34, 89.99it/s]  4%|‚ñà‚ñã                                      | 142/3277 [00:01<00:35, 89.46it/s]  5%|‚ñà‚ñä                                      | 152/3277 [00:01<00:34, 89.74it/s]  5%|‚ñà‚ñâ                                      | 162/3277 [00:01<00:34, 90.15it/s]  5%|‚ñà‚ñà                                      | 172/3277 [00:01<00:34, 90.50it/s]  6%|‚ñà‚ñà‚ñè                                     | 182/3277 [00:02<00:34, 90.44it/s]  6%|‚ñà‚ñà‚ñé                                     | 192/3277 [00:02<00:34, 90.61it/s]  6%|‚ñà‚ñà‚ñç                                     | 202/3277 [00:02<00:34, 87.94it/s]  6%|‚ñà‚ñà‚ñå                                     | 212/3277 [00:02<00:33, 91.13it/s]  7%|‚ñà‚ñà‚ñã                                     | 222/3277 [00:02<00:33, 90.40it/s]  7%|‚ñà‚ñà‚ñä                                     | 232/3277 [00:02<00:33, 90.13it/s]  7%|‚ñà‚ñà‚ñâ                                     | 242/3277 [00:02<00:33, 90.21it/s]  8%|‚ñà‚ñà‚ñà                                     | 252/3277 [00:02<00:33, 90.20it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 262/3277 [00:02<00:33, 90.17it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 272/3277 [00:03<00:33, 89.66it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 281/3277 [00:03<00:33, 88.57it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 290/3277 [00:03<00:34, 87.85it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 299/3277 [00:03<00:34, 87.00it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 308/3277 [00:03<00:33, 87.52it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 317/3277 [00:03<00:33, 87.72it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 326/3277 [00:03<00:33, 88.12it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 335/3277 [00:03<00:33, 88.59it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 344/3277 [00:03<00:33, 87.95it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 353/3277 [00:03<00:33, 88.05it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 362/3277 [00:04<00:33, 87.48it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 371/3277 [00:04<00:33, 87.33it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 380/3277 [00:04<00:33, 87.59it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 390/3277 [00:04<00:32, 88.62it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 399/3277 [00:04<00:33, 85.80it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 410/3277 [00:04<00:31, 90.00it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 420/3277 [00:04<00:32, 88.12it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 430/3277 [00:04<00:31, 91.32it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 440/3277 [00:04<00:31, 90.91it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 450/3277 [00:05<00:31, 90.56it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 460/3277 [00:05<00:31, 90.58it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 470/3277 [00:05<00:30, 91.01it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 480/3277 [00:05<00:30, 90.93it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 490/3277 [00:05<00:30, 90.40it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 500/3277 [00:05<00:30, 89.70it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 509/3277 [00:05<00:30, 89.42it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 518/3277 [00:05<00:31, 88.49it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 527/3277 [00:05<00:31, 88.21it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 536/3277 [00:06<00:31, 86.60it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 546/3277 [00:06<00:30, 89.75it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 556/3277 [00:06<00:29, 90.77it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 566/3277 [00:06<00:30, 90.21it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 576/3277 [00:06<00:29, 90.07it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 586/3277 [00:06<00:30, 89.68it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 595/3277 [00:06<00:30, 88.64it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 605/3277 [00:06<00:30, 89.05it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 614/3277 [00:06<00:29, 89.22it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 623/3277 [00:07<00:30, 86.02it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 633/3277 [00:07<00:29, 89.33it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 642/3277 [00:07<00:29, 88.65it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 651/3277 [00:07<00:34, 75.57it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 665/3277 [00:07<00:28, 91.54it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 675/3277 [00:07<00:29, 87.48it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 686/3277 [00:07<00:28, 91.60it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 696/3277 [00:07<00:28, 90.02it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 706/3277 [00:07<00:28, 89.87it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 716/3277 [00:08<00:28, 89.55it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 726/3277 [00:08<00:28, 88.88it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 736/3277 [00:08<00:28, 89.55it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 746/3277 [00:08<00:28, 88.27it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 755/3277 [00:08<00:35, 71.16it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 771/3277 [00:08<00:27, 91.38it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 781/3277 [00:08<00:27, 91.37it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 791/3277 [00:08<00:27, 90.79it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 801/3277 [00:09<00:28, 87.90it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 811/3277 [00:09<00:27, 90.25it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 821/3277 [00:09<00:27, 88.51it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 831/3277 [00:09<00:27, 88.82it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 840/3277 [00:09<00:27, 88.52it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 849/3277 [00:09<00:27, 88.10it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 858/3277 [00:09<00:27, 88.37it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 867/3277 [00:09<00:27, 87.72it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 876/3277 [00:09<00:28, 85.06it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 886/3277 [00:09<00:27, 88.26it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 895/3277 [00:10<00:27, 87.21it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 904/3277 [00:10<00:27, 87.08it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 913/3277 [00:10<00:27, 85.39it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 923/3277 [00:10<00:26, 88.15it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 932/3277 [00:10<00:26, 87.19it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 941/3277 [00:10<00:28, 82.91it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 950/3277 [00:10<00:29, 79.93it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 959/3277 [00:10<00:29, 78.66it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 967/3277 [00:10<00:29, 77.50it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 975/3277 [00:11<00:38, 59.49it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 989/3277 [00:11<00:29, 76.84it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 998/3277 [00:11<00:29, 76.16it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1007/3277 [00:11<00:29, 75.81it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1015/3277 [00:11<00:29, 75.64it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1023/3277 [00:11<00:29, 75.20it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1031/3277 [00:11<00:30, 74.70it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1039/3277 [00:11<00:30, 73.55it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1047/3277 [00:12<00:34, 65.33it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1055/3277 [00:12<00:32, 68.27it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1063/3277 [00:12<00:31, 70.79it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1071/3277 [00:12<00:30, 72.33it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1079/3277 [00:12<00:29, 73.66it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1087/3277 [00:12<00:29, 74.21it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1095/3277 [00:12<00:30, 71.83it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1103/3277 [00:12<00:29, 73.13it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1112/3277 [00:13<00:28, 76.69it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1120/3277 [00:13<00:28, 76.50it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1128/3277 [00:13<00:28, 76.54it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1136/3277 [00:13<00:28, 76.38it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1144/3277 [00:13<00:27, 76.49it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1153/3277 [00:13<00:26, 79.04it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1162/3277 [00:13<00:26, 80.93it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1172/3277 [00:13<00:24, 85.88it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1181/3277 [00:13<00:24, 86.96it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1190/3277 [00:13<00:24, 86.53it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1200/3277 [00:14<00:23, 87.83it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1209/3277 [00:14<00:23, 87.06it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1218/3277 [00:14<00:23, 87.03it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1227/3277 [00:14<00:23, 85.84it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1236/3277 [00:14<00:24, 82.83it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1247/3277 [00:14<00:22, 89.73it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1257/3277 [00:14<00:22, 89.65it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1266/3277 [00:14<00:27, 71.96it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1274/3277 [00:14<00:27, 72.31it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1283/3277 [00:15<00:25, 76.69it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1293/3277 [00:15<00:23, 82.77it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1303/3277 [00:15<00:23, 85.62it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1312/3277 [00:15<00:26, 75.23it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1325/3277 [00:15<00:21, 89.15it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1335/3277 [00:15<00:21, 89.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1345/3277 [00:15<00:21, 89.43it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1355/3277 [00:15<00:21, 89.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1365/3277 [00:16<00:22, 85.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1376/3277 [00:16<00:21, 88.98it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1387/3277 [00:16<00:20, 92.21it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1397/3277 [00:16<00:21, 89.05it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1406/3277 [00:16<00:24, 75.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1418/3277 [00:16<00:21, 85.08it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1427/3277 [00:16<00:21, 85.59it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1436/3277 [00:16<00:22, 80.76it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1445/3277 [00:17<00:24, 75.13it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1453/3277 [00:17<00:26, 69.61it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1461/3277 [00:17<00:25, 70.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1469/3277 [00:17<00:26, 69.23it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1477/3277 [00:17<00:25, 71.66it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1485/3277 [00:17<00:24, 73.49it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1493/3277 [00:17<00:23, 75.18it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1501/3277 [00:17<00:23, 76.45it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1509/3277 [00:17<00:22, 76.89it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1517/3277 [00:17<00:22, 77.50it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1525/3277 [00:18<00:22, 78.18it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1533/3277 [00:18<00:22, 78.39it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1541/3277 [00:18<00:22, 78.61it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1550/3277 [00:18<00:21, 79.07it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1558/3277 [00:18<00:21, 79.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1566/3277 [00:18<00:22, 76.26it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1576/3277 [00:18<00:21, 79.99it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1584/3277 [00:18<00:21, 79.81it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1592/3277 [00:18<00:21, 79.65it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1601/3277 [00:19<00:21, 79.77it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1609/3277 [00:19<00:20, 79.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1617/3277 [00:19<00:20, 79.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1626/3277 [00:19<00:20, 79.49it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1634/3277 [00:19<00:20, 79.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1642/3277 [00:19<00:20, 79.06it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1650/3277 [00:19<00:20, 78.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1658/3277 [00:19<00:21, 76.10it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1668/3277 [00:19<00:20, 80.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1677/3277 [00:20<00:20, 79.64it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1686/3277 [00:20<00:19, 80.24it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1695/3277 [00:20<00:19, 82.21it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1704/3277 [00:20<00:18, 84.12it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1713/3277 [00:20<00:18, 85.42it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1722/3277 [00:20<00:18, 84.86it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1733/3277 [00:20<00:17, 89.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1743/3277 [00:20<00:17, 87.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1753/3277 [00:20<00:17, 88.54it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1763/3277 [00:20<00:16, 91.03it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1773/3277 [00:21<00:16, 91.19it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1783/3277 [00:21<00:16, 91.00it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1793/3277 [00:21<00:16, 89.42it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1804/3277 [00:21<00:15, 92.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1814/3277 [00:21<00:15, 91.94it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1824/3277 [00:21<00:15, 92.03it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 1834/3277 [00:21<00:15, 90.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1844/3277 [00:21<00:17, 83.78it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1853/3277 [00:22<00:17, 83.55it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 1862/3277 [00:22<00:17, 80.59it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1871/3277 [00:22<00:17, 78.92it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1879/3277 [00:22<00:17, 77.94it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 1887/3277 [00:22<00:18, 77.12it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1895/3277 [00:22<00:18, 76.37it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 1903/3277 [00:22<00:18, 75.94it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 1911/3277 [00:22<00:17, 76.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 1920/3277 [00:22<00:17, 77.92it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 1930/3277 [00:22<00:16, 82.25it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1940/3277 [00:23<00:15, 85.02it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 1950/3277 [00:23<00:15, 85.20it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 1961/3277 [00:23<00:14, 89.92it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 1971/3277 [00:23<00:14, 90.85it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 1981/3277 [00:23<00:14, 91.14it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 1991/3277 [00:23<00:14, 88.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2002/3277 [00:23<00:13, 92.67it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2012/3277 [00:23<00:13, 92.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2022/3277 [00:24<00:14, 88.94it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2033/3277 [00:24<00:13, 91.95it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2043/3277 [00:24<00:13, 89.72it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2054/3277 [00:24<00:13, 93.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2064/3277 [00:24<00:13, 92.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2074/3277 [00:24<00:12, 93.06it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2084/3277 [00:24<00:12, 92.70it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2094/3277 [00:24<00:13, 90.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2105/3277 [00:24<00:12, 93.82it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2115/3277 [00:25<00:12, 93.46it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2125/3277 [00:25<00:12, 90.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2135/3277 [00:25<00:12, 93.25it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2145/3277 [00:25<00:12, 90.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2156/3277 [00:25<00:11, 93.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2166/3277 [00:25<00:11, 93.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2176/3277 [00:25<00:11, 93.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2186/3277 [00:25<00:12, 87.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2198/3277 [00:25<00:11, 94.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2208/3277 [00:26<00:11, 93.60it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2218/3277 [00:26<00:11, 93.29it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2228/3277 [00:26<00:11, 92.79it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2238/3277 [00:26<00:11, 88.15it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2247/3277 [00:26<00:11, 87.81it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2257/3277 [00:26<00:11, 88.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2266/3277 [00:26<00:11, 87.32it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2277/3277 [00:26<00:10, 91.50it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2287/3277 [00:26<00:10, 91.37it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2297/3277 [00:27<00:10, 91.99it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2307/3277 [00:27<00:10, 91.39it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2317/3277 [00:27<00:10, 91.72it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2327/3277 [00:27<00:10, 91.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2337/3277 [00:27<00:10, 88.38it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2346/3277 [00:27<00:10, 88.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2356/3277 [00:27<00:10, 91.54it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2366/3277 [00:27<00:10, 85.99it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2375/3277 [00:27<00:10, 85.33it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2384/3277 [00:28<00:10, 81.59it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2393/3277 [00:28<00:11, 77.01it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2401/3277 [00:28<00:11, 76.35it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2410/3277 [00:28<00:11, 78.64it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2418/3277 [00:28<00:11, 77.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2426/3277 [00:28<00:11, 76.90it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2434/3277 [00:28<00:11, 73.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2442/3277 [00:28<00:12, 67.92it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2449/3277 [00:28<00:12, 65.84it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2457/3277 [00:29<00:11, 69.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2467/3277 [00:29<00:10, 76.18it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2477/3277 [00:29<00:09, 82.33it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2487/3277 [00:29<00:09, 86.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2496/3277 [00:29<00:11, 70.47it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2509/3277 [00:29<00:09, 82.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2518/3277 [00:29<00:09, 80.15it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2527/3277 [00:29<00:09, 76.33it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2536/3277 [00:30<00:09, 78.31it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2545/3277 [00:30<00:09, 75.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2554/3277 [00:30<00:09, 78.31it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2562/3277 [00:30<00:09, 77.42it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2570/3277 [00:30<00:09, 77.33it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2578/3277 [00:30<00:10, 69.68it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2588/3277 [00:30<00:08, 76.75it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2596/3277 [00:30<00:08, 76.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2604/3277 [00:30<00:08, 76.99it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2612/3277 [00:31<00:08, 73.91it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2620/3277 [00:31<00:08, 73.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2628/3277 [00:31<00:08, 74.56it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2636/3277 [00:31<00:08, 75.36it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2644/3277 [00:31<00:08, 75.69it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2652/3277 [00:31<00:08, 75.81it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2660/3277 [00:31<00:08, 75.35it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2668/3277 [00:31<00:08, 75.63it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2676/3277 [00:31<00:07, 76.05it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2684/3277 [00:31<00:07, 76.53it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2692/3277 [00:32<00:07, 76.54it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2700/3277 [00:32<00:07, 76.21it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2708/3277 [00:32<00:07, 76.62it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2716/3277 [00:32<00:07, 76.85it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2724/3277 [00:32<00:07, 76.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2732/3277 [00:32<00:07, 76.52it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2740/3277 [00:32<00:07, 76.25it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2748/3277 [00:32<00:06, 76.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2756/3277 [00:32<00:06, 76.31it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2764/3277 [00:33<00:06, 76.08it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2772/3277 [00:33<00:06, 76.05it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2780/3277 [00:33<00:06, 76.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 2788/3277 [00:33<00:06, 71.25it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 2796/3277 [00:33<00:06, 72.83it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2806/3277 [00:33<00:05, 79.06it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 2815/3277 [00:33<00:05, 80.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 2825/3277 [00:33<00:05, 83.96it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 2834/3277 [00:33<00:05, 85.39it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2844/3277 [00:34<00:04, 86.95it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2853/3277 [00:34<00:05, 81.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 2864/3277 [00:34<00:04, 87.32it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2873/3277 [00:34<00:05, 77.28it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2881/3277 [00:34<00:05, 74.20it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2891/3277 [00:34<00:05, 76.01it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2899/3277 [00:34<00:05, 72.71it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2907/3277 [00:34<00:05, 73.85it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2917/3277 [00:34<00:04, 78.26it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2926/3277 [00:35<00:04, 78.61it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2934/3277 [00:35<00:04, 78.51it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2943/3277 [00:35<00:04, 79.67it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2951/3277 [00:35<00:04, 75.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2961/3277 [00:35<00:03, 81.68it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2970/3277 [00:35<00:03, 82.31it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2979/3277 [00:35<00:03, 77.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2988/3277 [00:35<00:03, 77.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2998/3277 [00:36<00:03, 80.93it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3007/3277 [00:36<00:03, 78.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3016/3277 [00:36<00:03, 80.11it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3025/3277 [00:36<00:03, 79.81it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3034/3277 [00:36<00:03, 80.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3043/3277 [00:36<00:02, 82.64it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3052/3277 [00:36<00:03, 69.35it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3066/3277 [00:36<00:02, 84.75it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3075/3277 [00:36<00:02, 83.36it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3084/3277 [00:37<00:02, 83.90it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3093/3277 [00:37<00:02, 79.97it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3102/3277 [00:37<00:02, 78.54it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3112/3277 [00:37<00:01, 83.86it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3122/3277 [00:37<00:01, 86.30it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3131/3277 [00:37<00:01, 85.48it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3141/3277 [00:37<00:01, 85.33it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3152/3277 [00:37<00:01, 89.67it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3163/3277 [00:37<00:01, 93.56it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3173/3277 [00:38<00:01, 93.81it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3183/3277 [00:38<00:01, 88.80it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3195/3277 [00:38<00:00, 93.08it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3206/3277 [00:38<00:00, 92.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3216/3277 [00:38<00:00, 93.07it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3226/3277 [00:38<00:00, 94.26it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3236/3277 [00:38<00:00, 93.86it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3246/3277 [00:38<00:00, 94.08it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3256/3277 [00:38<00:00, 90.61it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3266/3277 [00:39<00:00, 91.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3277/3277 [00:39<00:00, 94.28it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3277/3277 [00:39<00:00, 83.62it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.5570
Epoch 1 Step 51 Train Loss: 0.6067
Epoch 1 Step 101 Train Loss: 0.5747
Epoch 1 Step 151 Train Loss: 0.5634
Epoch 1 Step 201 Train Loss: 0.5422
Epoch 1 Step 251 Train Loss: 0.5851
Epoch 1 Step 301 Train Loss: 0.5857
Epoch 1 Step 351 Train Loss: 0.5628
Epoch 1 Step 401 Train Loss: 0.5340
Epoch 1 Step 451 Train Loss: 0.5679
Epoch 1 Step 501 Train Loss: 0.5424
Epoch 1 Step 551 Train Loss: 0.5923
Epoch 1 Step 601 Train Loss: 0.5361
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0123 Validation Top 20 DE MSE: 0.0160. 
Epoch 2 Step 1 Train Loss: 0.5286
Epoch 2 Step 51 Train Loss: 0.5046
Epoch 2 Step 101 Train Loss: 0.5786
Epoch 2 Step 151 Train Loss: 0.5186
Epoch 2 Step 201 Train Loss: 0.4904
Epoch 2 Step 251 Train Loss: 0.5979
Epoch 2 Step 301 Train Loss: 0.5391
Epoch 2 Step 351 Train Loss: 0.5387
Epoch 2 Step 401 Train Loss: 0.5166
Epoch 2 Step 451 Train Loss: 0.5654
Epoch 2 Step 501 Train Loss: 0.5370
Epoch 2 Step 551 Train Loss: 0.6447
Epoch 2 Step 601 Train Loss: 0.5163
Epoch 2: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0139 Validation Top 20 DE MSE: 0.0156. 
Epoch 3 Step 1 Train Loss: 0.5293
Epoch 3 Step 51 Train Loss: 0.6170
Epoch 3 Step 101 Train Loss: 0.5638
Epoch 3 Step 151 Train Loss: 0.5788
Epoch 3 Step 201 Train Loss: 0.5152
Epoch 3 Step 251 Train Loss: 0.5446
Epoch 3 Step 301 Train Loss: 0.5483
Epoch 3 Step 351 Train Loss: 0.5325
Epoch 3 Step 401 Train Loss: 0.5096
Epoch 3 Step 451 Train Loss: 0.4742
Epoch 3 Step 501 Train Loss: 0.5317
Epoch 3 Step 551 Train Loss: 0.5529
Epoch 3 Step 601 Train Loss: 0.5705
Epoch 3: Train Overall MSE: 0.0042 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0157 Validation Top 20 DE MSE: 0.0171. 
Epoch 4 Step 1 Train Loss: 0.5456
Epoch 4 Step 51 Train Loss: 0.5581
Epoch 4 Step 101 Train Loss: 0.5010
Epoch 4 Step 151 Train Loss: 0.5955
Epoch 4 Step 201 Train Loss: 0.6012
Epoch 4 Step 251 Train Loss: 0.5429
Epoch 4 Step 301 Train Loss: 0.5694
Epoch 4 Step 351 Train Loss: 0.5422
Epoch 4 Step 401 Train Loss: 0.5993
Epoch 4 Step 451 Train Loss: 0.5637
Epoch 4 Step 501 Train Loss: 0.5000
Epoch 4 Step 551 Train Loss: 0.5762
Epoch 4 Step 601 Train Loss: 0.5335
Epoch 4: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0127 Validation Top 20 DE MSE: 0.0157. 
Epoch 5 Step 1 Train Loss: 0.5077
Epoch 5 Step 51 Train Loss: 0.5465
Epoch 5 Step 101 Train Loss: 0.5138
Epoch 5 Step 151 Train Loss: 0.5228
Epoch 5 Step 201 Train Loss: 0.5619
Epoch 5 Step 251 Train Loss: 0.5144
Epoch 5 Step 301 Train Loss: 0.5142
Epoch 5 Step 351 Train Loss: 0.5791
Epoch 5 Step 401 Train Loss: 0.5878
Epoch 5 Step 451 Train Loss: 0.6649
Epoch 5 Step 501 Train Loss: 0.5202
Epoch 5 Step 551 Train Loss: 0.5549
Epoch 5 Step 601 Train Loss: 0.5646
Epoch 5: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0206 Validation Top 20 DE MSE: 0.0154. 
Epoch 6 Step 1 Train Loss: 0.6069
Epoch 6 Step 51 Train Loss: 0.5240
Epoch 6 Step 101 Train Loss: 0.5270
Epoch 6 Step 151 Train Loss: 0.5458
Epoch 6 Step 201 Train Loss: 0.5559
Epoch 6 Step 251 Train Loss: 0.5317
Epoch 6 Step 301 Train Loss: 0.5438
Epoch 6 Step 351 Train Loss: 0.5883
Epoch 6 Step 401 Train Loss: 0.5607
Epoch 6 Step 451 Train Loss: 0.5018
Epoch 6 Step 501 Train Loss: 0.5159
Epoch 6 Step 551 Train Loss: 0.5059
Epoch 6 Step 601 Train Loss: 0.5712
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0152 Validation Top 20 DE MSE: 0.0163. 
Epoch 7 Step 1 Train Loss: 0.5603
Epoch 7 Step 51 Train Loss: 0.5343
Epoch 7 Step 101 Train Loss: 0.5075
Epoch 7 Step 151 Train Loss: 0.5906
Epoch 7 Step 201 Train Loss: 0.5304
Epoch 7 Step 251 Train Loss: 0.5107
Epoch 7 Step 301 Train Loss: 0.5021
Epoch 7 Step 351 Train Loss: 0.5568
Epoch 7 Step 401 Train Loss: 0.5999
Epoch 7 Step 451 Train Loss: 0.5680
Epoch 7 Step 501 Train Loss: 0.6091
Epoch 7 Step 551 Train Loss: 0.5003
Epoch 7 Step 601 Train Loss: 0.5666
Epoch 7: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0195 Validation Top 20 DE MSE: 0.0156. 
Epoch 8 Step 1 Train Loss: 0.5264
Epoch 8 Step 51 Train Loss: 0.5702
Epoch 8 Step 101 Train Loss: 0.5417
Epoch 8 Step 151 Train Loss: 0.5463
Epoch 8 Step 201 Train Loss: 0.5257
Epoch 8 Step 251 Train Loss: 0.5384
Epoch 8 Step 301 Train Loss: 0.5332
Epoch 8 Step 351 Train Loss: 0.5782
Epoch 8 Step 401 Train Loss: 0.5387
Epoch 8 Step 451 Train Loss: 0.5586
Epoch 8 Step 501 Train Loss: 0.5345
Epoch 8 Step 551 Train Loss: 0.5950
Epoch 8 Step 601 Train Loss: 0.5750
Epoch 8: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0157 Validation Top 20 DE MSE: 0.0157. 
Epoch 9 Step 1 Train Loss: 0.5974
Epoch 9 Step 51 Train Loss: 0.5252
Epoch 9 Step 101 Train Loss: 0.5865
Epoch 9 Step 151 Train Loss: 0.5771
Epoch 9 Step 201 Train Loss: 0.5801
Epoch 9 Step 251 Train Loss: 0.5232
Epoch 9 Step 301 Train Loss: 0.5482
Epoch 9 Step 351 Train Loss: 0.5208
Epoch 9 Step 401 Train Loss: 0.5805
Epoch 9 Step 451 Train Loss: 0.5686
Epoch 9 Step 501 Train Loss: 0.5441
Epoch 9 Step 551 Train Loss: 0.4955
Epoch 9 Step 601 Train Loss: 0.5546
Epoch 9: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0190 Validation Top 20 DE MSE: 0.0157. 
Epoch 10 Step 1 Train Loss: 0.5997
Epoch 10 Step 51 Train Loss: 0.4764
Epoch 10 Step 101 Train Loss: 0.5369
Epoch 10 Step 151 Train Loss: 0.5600
Epoch 10 Step 201 Train Loss: 0.6032
Epoch 10 Step 251 Train Loss: 0.5024
Epoch 10 Step 301 Train Loss: 0.6046
Epoch 10 Step 351 Train Loss: 0.4788
Epoch 10 Step 401 Train Loss: 0.5109
Epoch 10 Step 451 Train Loss: 0.5335
Epoch 10 Step 501 Train Loss: 0.5531
Epoch 10 Step 551 Train Loss: 0.6006
Epoch 10 Step 601 Train Loss: 0.5481
Epoch 10: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0193 Validation Top 20 DE MSE: 0.0156. 
Epoch 11 Step 1 Train Loss: 0.5726
Epoch 11 Step 51 Train Loss: 0.4969
Epoch 11 Step 101 Train Loss: 0.5703
Epoch 11 Step 151 Train Loss: 0.5558
Epoch 11 Step 201 Train Loss: 0.4742
Epoch 11 Step 251 Train Loss: 0.5683
Epoch 11 Step 301 Train Loss: 0.5671
Epoch 11 Step 351 Train Loss: 0.5867
Epoch 11 Step 401 Train Loss: 0.5036
Epoch 11 Step 451 Train Loss: 0.5462
Epoch 11 Step 501 Train Loss: 0.5295
Epoch 11 Step 551 Train Loss: 0.5211
Epoch 11 Step 601 Train Loss: 0.5725
Epoch 11: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0209 Validation Top 20 DE MSE: 0.0156. 
Epoch 12 Step 1 Train Loss: 0.5716
Epoch 12 Step 51 Train Loss: 0.5178
Epoch 12 Step 101 Train Loss: 0.5770
Epoch 12 Step 151 Train Loss: 0.5441
Epoch 12 Step 201 Train Loss: 0.5678
Epoch 12 Step 251 Train Loss: 0.6207
Epoch 12 Step 301 Train Loss: 0.6114
Epoch 12 Step 351 Train Loss: 0.5012
Epoch 12 Step 401 Train Loss: 0.5366
Epoch 12 Step 451 Train Loss: 0.5976
Epoch 12 Step 501 Train Loss: 0.5014
Epoch 12 Step 551 Train Loss: 0.5057
Epoch 12 Step 601 Train Loss: 0.5260
Epoch 12: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0158 Validation Top 20 DE MSE: 0.0157. 
Epoch 13 Step 1 Train Loss: 0.4937
Epoch 13 Step 51 Train Loss: 0.5557
Epoch 13 Step 101 Train Loss: 0.5687
Epoch 13 Step 151 Train Loss: 0.5341
Epoch 13 Step 201 Train Loss: 0.5469
Epoch 13 Step 251 Train Loss: 0.5292
Epoch 13 Step 301 Train Loss: 0.5366
Epoch 13 Step 351 Train Loss: 0.5128
Epoch 13 Step 401 Train Loss: 0.5788
Epoch 13 Step 451 Train Loss: 0.5330
Epoch 13 Step 501 Train Loss: 0.5141
Epoch 13 Step 551 Train Loss: 0.5691
Epoch 13 Step 601 Train Loss: 0.5392
Epoch 13: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0343 Validation Top 20 DE MSE: 0.0157. 
Epoch 14 Step 1 Train Loss: 0.5142
Epoch 14 Step 51 Train Loss: 0.6103
Epoch 14 Step 101 Train Loss: 0.5401
Epoch 14 Step 151 Train Loss: 0.5352
Epoch 14 Step 201 Train Loss: 0.4714
Epoch 14 Step 251 Train Loss: 0.4783
Epoch 14 Step 301 Train Loss: 0.6036
Epoch 14 Step 351 Train Loss: 0.5673
Epoch 14 Step 401 Train Loss: 0.5474
Epoch 14 Step 451 Train Loss: 0.5477
Epoch 14 Step 501 Train Loss: 0.6379
Epoch 14 Step 551 Train Loss: 0.6076
Epoch 14 Step 601 Train Loss: 0.6017
Epoch 14: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0293 Validation Top 20 DE MSE: 0.0157. 
Epoch 15 Step 1 Train Loss: 0.5094
Epoch 15 Step 51 Train Loss: 0.5647
Epoch 15 Step 101 Train Loss: 0.5413
Epoch 15 Step 151 Train Loss: 0.5378
Epoch 15 Step 201 Train Loss: 0.5479
Epoch 15 Step 251 Train Loss: 0.5490
Epoch 15 Step 301 Train Loss: 0.5739
Epoch 15 Step 351 Train Loss: 0.5345
Epoch 15 Step 401 Train Loss: 0.5315
Epoch 15 Step 451 Train Loss: 0.5935
Epoch 15 Step 501 Train Loss: 0.5844
Epoch 15 Step 551 Train Loss: 0.4631
Epoch 15 Step 601 Train Loss: 0.5637
Epoch 15: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0191 Validation Top 20 DE MSE: 0.0157. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0098
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014608462
test_unseen_single_pearson: 0.9959629593665135
test_unseen_single_mse_de: 0.009805177
test_unseen_single_pearson_de: 0.9720771441711413
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.20655562166193708
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.325
test_unseen_single_frac_sigma_below_1_non_dropout: 0.948913043478261
test_unseen_single_mse_top20_de_non_dropout: 0.011919978
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.003 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñÜ‚ñÉ
wandb:                                             train_de_pearson ‚ñà‚ñà‚ñÅ‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:                                                    train_mse ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                train_pearson ‚ñà‚ñá‚ñÅ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà
wandb:                                                training_loss ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñÑ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñá‚ñà‚ñÅ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00981
wandb:                                              test_de_pearson 0.97208
wandb:               test_frac_opposite_direction_top20_non_dropout 0.325
wandb:                          test_frac_sigma_below_1_non_dropout 0.94891
wandb:                                                     test_mse 0.00146
wandb:                                test_mse_top20_de_non_dropout 0.01192
wandb:                                                 test_pearson 0.99596
wandb:                                           test_pearson_delta 0.20656
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.325
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.94891
wandb:                                       test_unseen_single_mse 0.00146
wandb:                                    test_unseen_single_mse_de 0.00981
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01192
wandb:                                   test_unseen_single_pearson 0.99596
wandb:                                test_unseen_single_pearson_de 0.97208
wandb:                             test_unseen_single_pearson_delta 0.20656
wandb:                                                 train_de_mse 0.01906
wandb:                                             train_de_pearson 0.96862
wandb:                                                    train_mse 0.00126
wandb:                                                train_pearson 0.9965
wandb:                                                training_loss 0.52793
wandb:                                                   val_de_mse 0.01566
wandb:                                               val_de_pearson 0.96561
wandb:                                                      val_mse 0.0012
wandb:                                                  val_pearson 0.99664
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRi_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/vsm5rc37
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_011028-vsm5rc37/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:46
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_013325-5uqs2zbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRi_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/5uqs2zbc
wandb: WARNING Serializing object of type ndarray that is 21106816 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5215
Epoch 1 Step 51 Train Loss: 0.6061
Epoch 1 Step 101 Train Loss: 0.5284
Epoch 1 Step 151 Train Loss: 0.5278
Epoch 1 Step 201 Train Loss: 0.5368
Epoch 1 Step 251 Train Loss: 0.5940
Epoch 1 Step 301 Train Loss: 0.5611
Epoch 1 Step 351 Train Loss: 0.5201
Epoch 1 Step 401 Train Loss: 0.6081
Epoch 1 Step 451 Train Loss: 0.5705
Epoch 1 Step 501 Train Loss: 0.5852
Epoch 1 Step 551 Train Loss: 0.4632
Epoch 1 Step 601 Train Loss: 0.5524
Epoch 1: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0121 Validation Top 20 DE MSE: 0.0383. 
Epoch 2 Step 1 Train Loss: 0.5759
Epoch 2 Step 51 Train Loss: 0.5779
Epoch 2 Step 101 Train Loss: 0.5412
Epoch 2 Step 151 Train Loss: 0.5633
Epoch 2 Step 201 Train Loss: 0.5330
Epoch 2 Step 251 Train Loss: 0.5487
Epoch 2 Step 301 Train Loss: 0.5484
Epoch 2 Step 351 Train Loss: 0.5363
Epoch 2 Step 401 Train Loss: 0.5302
Epoch 2 Step 451 Train Loss: 0.5900
Epoch 2 Step 501 Train Loss: 0.5354
Epoch 2 Step 551 Train Loss: 0.5629
Epoch 2 Step 601 Train Loss: 0.5692
Epoch 2: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0412. 
Epoch 3 Step 1 Train Loss: 0.5474
Epoch 3 Step 51 Train Loss: 0.5355
Epoch 3 Step 101 Train Loss: 0.5696
Epoch 3 Step 151 Train Loss: 0.5309
Epoch 3 Step 201 Train Loss: 0.5830
Epoch 3 Step 251 Train Loss: 0.4872
Epoch 3 Step 301 Train Loss: 0.5867
Epoch 3 Step 351 Train Loss: 0.5449
Epoch 3 Step 401 Train Loss: 0.5899
Epoch 3 Step 451 Train Loss: 0.5837
Epoch 3 Step 501 Train Loss: 0.5594
Epoch 3 Step 551 Train Loss: 0.5473
Epoch 3 Step 601 Train Loss: 0.5319
Epoch 3: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.0401. 
Epoch 4 Step 1 Train Loss: 0.5613
Epoch 4 Step 51 Train Loss: 0.5588
Epoch 4 Step 101 Train Loss: 0.5233
Epoch 4 Step 151 Train Loss: 0.6038
Epoch 4 Step 201 Train Loss: 0.6319
Epoch 4 Step 251 Train Loss: 0.5350
Epoch 4 Step 301 Train Loss: 0.5605
Epoch 4 Step 351 Train Loss: 0.5028
Epoch 4 Step 401 Train Loss: 0.4770
Epoch 4 Step 451 Train Loss: 0.5346
Epoch 4 Step 501 Train Loss: 0.5875
Epoch 4 Step 551 Train Loss: 0.6570
Epoch 4 Step 601 Train Loss: 0.6318
Epoch 4: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0115 Validation Top 20 DE MSE: 0.0377. 
Epoch 5 Step 1 Train Loss: 0.4902
Epoch 5 Step 51 Train Loss: 0.5437
Epoch 5 Step 101 Train Loss: 0.5285
Epoch 5 Step 151 Train Loss: 0.5354
Epoch 5 Step 201 Train Loss: 0.5092
Epoch 5 Step 251 Train Loss: 0.4885
Epoch 5 Step 301 Train Loss: 0.5614
Epoch 5 Step 351 Train Loss: 0.6329
Epoch 5 Step 401 Train Loss: 0.5215
Epoch 5 Step 451 Train Loss: 0.4903
Epoch 5 Step 501 Train Loss: 0.5458
Epoch 5 Step 551 Train Loss: 0.5774
Epoch 5 Step 601 Train Loss: 0.5513
Epoch 5: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0389. 
Epoch 6 Step 1 Train Loss: 0.4810
Epoch 6 Step 51 Train Loss: 0.4792
Epoch 6 Step 101 Train Loss: 0.5438
Epoch 6 Step 151 Train Loss: 0.5592
Epoch 6 Step 201 Train Loss: 0.5230
Epoch 6 Step 251 Train Loss: 0.5197
Epoch 6 Step 301 Train Loss: 0.5254
Epoch 6 Step 351 Train Loss: 0.5494
Epoch 6 Step 401 Train Loss: 0.6252
Epoch 6 Step 451 Train Loss: 0.4766
Epoch 6 Step 501 Train Loss: 0.5629
Epoch 6 Step 551 Train Loss: 0.5528
Epoch 6 Step 601 Train Loss: 0.5777
Epoch 6: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0398. 
Epoch 7 Step 1 Train Loss: 0.5213
Epoch 7 Step 51 Train Loss: 0.4883
Epoch 7 Step 101 Train Loss: 0.5341
Epoch 7 Step 151 Train Loss: 0.5024
Epoch 7 Step 201 Train Loss: 0.6219
Epoch 7 Step 251 Train Loss: 0.5530
Epoch 7 Step 301 Train Loss: 0.5440
Epoch 7 Step 351 Train Loss: 0.5550
Epoch 7 Step 401 Train Loss: 0.5079
Epoch 7 Step 451 Train Loss: 0.5239
Epoch 7 Step 501 Train Loss: 0.5493
Epoch 7 Step 551 Train Loss: 0.5906
Epoch 7 Step 601 Train Loss: 0.6141
Epoch 7: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0400. 
Epoch 8 Step 1 Train Loss: 0.5430
Epoch 8 Step 51 Train Loss: 0.5227
Epoch 8 Step 101 Train Loss: 0.6106
Epoch 8 Step 151 Train Loss: 0.5659
Epoch 8 Step 201 Train Loss: 0.5734
Epoch 8 Step 251 Train Loss: 0.5291
Epoch 8 Step 301 Train Loss: 0.5529
Epoch 8 Step 351 Train Loss: 0.5733
Epoch 8 Step 401 Train Loss: 0.5410
Epoch 8 Step 451 Train Loss: 0.6025
Epoch 8 Step 501 Train Loss: 0.5524
Epoch 8 Step 551 Train Loss: 0.6221
Epoch 8 Step 601 Train Loss: 0.5612
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0400. 
Epoch 9 Step 1 Train Loss: 0.5129
Epoch 9 Step 51 Train Loss: 0.6253
Epoch 9 Step 101 Train Loss: 0.5593
Epoch 9 Step 151 Train Loss: 0.5200
Epoch 9 Step 201 Train Loss: 0.5321
Epoch 9 Step 251 Train Loss: 0.5644
Epoch 9 Step 301 Train Loss: 0.5537
Epoch 9 Step 351 Train Loss: 0.5071
Epoch 9 Step 401 Train Loss: 0.5193
Epoch 9 Step 451 Train Loss: 0.5527
Epoch 9 Step 501 Train Loss: 0.6083
Epoch 9 Step 551 Train Loss: 0.5798
Epoch 9 Step 601 Train Loss: 0.4901
Epoch 9: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0402. 
Epoch 10 Step 1 Train Loss: 0.5611
Epoch 10 Step 51 Train Loss: 0.5978
Epoch 10 Step 101 Train Loss: 0.5739
Epoch 10 Step 151 Train Loss: 0.5493
Epoch 10 Step 201 Train Loss: 0.5168
Epoch 10 Step 251 Train Loss: 0.5184
Epoch 10 Step 301 Train Loss: 0.5341
Epoch 10 Step 351 Train Loss: 0.5243
Epoch 10 Step 401 Train Loss: 0.5956
Epoch 10 Step 451 Train Loss: 0.5498
Epoch 10 Step 501 Train Loss: 0.5467
Epoch 10 Step 551 Train Loss: 0.5520
Epoch 10 Step 601 Train Loss: 0.5206
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0403. 
Epoch 11 Step 1 Train Loss: 0.5244
Epoch 11 Step 51 Train Loss: 0.5713
Epoch 11 Step 101 Train Loss: 0.5203
Epoch 11 Step 151 Train Loss: 0.5809
Epoch 11 Step 201 Train Loss: 0.5465
Epoch 11 Step 251 Train Loss: 0.4983
Epoch 11 Step 301 Train Loss: 0.5610
Epoch 11 Step 351 Train Loss: 0.5338
Epoch 11 Step 401 Train Loss: 0.6016
Epoch 11 Step 451 Train Loss: 0.5017
Epoch 11 Step 501 Train Loss: 0.5177
Epoch 11 Step 551 Train Loss: 0.5971
Epoch 11 Step 601 Train Loss: 0.5293
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0402. 
Epoch 12 Step 1 Train Loss: 0.4971
Epoch 12 Step 51 Train Loss: 0.5717
Epoch 12 Step 101 Train Loss: 0.5104
Epoch 12 Step 151 Train Loss: 0.5226
Epoch 12 Step 201 Train Loss: 0.5927
Epoch 12 Step 251 Train Loss: 0.5246
Epoch 12 Step 301 Train Loss: 0.5636
Epoch 12 Step 351 Train Loss: 0.5526
Epoch 12 Step 401 Train Loss: 0.5725
Epoch 12 Step 451 Train Loss: 0.4679
Epoch 12 Step 501 Train Loss: 0.5731
Epoch 12 Step 551 Train Loss: 0.5760
Epoch 12 Step 601 Train Loss: 0.5032
Epoch 12: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0402. 
Epoch 13 Step 1 Train Loss: 0.5627
Epoch 13 Step 51 Train Loss: 0.5111
Epoch 13 Step 101 Train Loss: 0.5142
Epoch 13 Step 151 Train Loss: 0.5038
Epoch 13 Step 201 Train Loss: 0.5990
Epoch 13 Step 251 Train Loss: 0.5875
Epoch 13 Step 301 Train Loss: 0.5746
Epoch 13 Step 351 Train Loss: 0.5771
Epoch 13 Step 401 Train Loss: 0.5884
Epoch 13 Step 451 Train Loss: 0.5344
Epoch 13 Step 501 Train Loss: 0.5578
Epoch 13 Step 551 Train Loss: 0.5276
Epoch 13 Step 601 Train Loss: 0.5334
Epoch 13: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0402. 
Epoch 14 Step 1 Train Loss: 0.5387
Epoch 14 Step 51 Train Loss: 0.5177
Epoch 14 Step 101 Train Loss: 0.5701
Epoch 14 Step 151 Train Loss: 0.5214
Epoch 14 Step 201 Train Loss: 0.5469
Epoch 14 Step 251 Train Loss: 0.5585
Epoch 14 Step 301 Train Loss: 0.5649
Epoch 14 Step 351 Train Loss: 0.5419
Epoch 14 Step 401 Train Loss: 0.5497
Epoch 14 Step 451 Train Loss: 0.5161
Epoch 14 Step 501 Train Loss: 0.6099
Epoch 14 Step 551 Train Loss: 0.4990
Epoch 14 Step 601 Train Loss: 0.5181
Epoch 14: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0402. 
Epoch 15 Step 1 Train Loss: 0.6191
Epoch 15 Step 51 Train Loss: 0.5353
Epoch 15 Step 101 Train Loss: 0.5550
Epoch 15 Step 151 Train Loss: 0.6144
Epoch 15 Step 201 Train Loss: 0.5692
Epoch 15 Step 251 Train Loss: 0.6775
Epoch 15 Step 301 Train Loss: 0.5197
Epoch 15 Step 351 Train Loss: 0.5290
Epoch 15 Step 401 Train Loss: 0.5206
Epoch 15 Step 451 Train Loss: 0.5217
Epoch 15 Step 501 Train Loss: 0.5069
Epoch 15 Step 551 Train Loss: 0.5693
Epoch 15 Step 601 Train Loss: 0.5040
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0402. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0095
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014591921
test_unseen_single_pearson: 0.9959320639610777
test_unseen_single_mse_de: 0.009499795
test_unseen_single_pearson_de: 0.9716032760222726
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.17498138598211338
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3043478260869565
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9706521739130436
test_unseen_single_mse_top20_de_non_dropout: 0.011710711
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.003 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ
wandb:                                                   val_de_mse ‚ñÇ‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                               val_de_pearson ‚ñÑ‚ñà‚ñá‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                      val_mse ‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÉ‚ñá‚ñá‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.0095
wandb:                                              test_de_pearson 0.9716
wandb:               test_frac_opposite_direction_top20_non_dropout 0.30435
wandb:                          test_frac_sigma_below_1_non_dropout 0.97065
wandb:                                                     test_mse 0.00146
wandb:                                test_mse_top20_de_non_dropout 0.01171
wandb:                                                 test_pearson 0.99593
wandb:                                           test_pearson_delta 0.17498
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.30435
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.97065
wandb:                                       test_unseen_single_mse 0.00146
wandb:                                    test_unseen_single_mse_de 0.0095
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01171
wandb:                                   test_unseen_single_pearson 0.99593
wandb:                                test_unseen_single_pearson_de 0.9716
wandb:                             test_unseen_single_pearson_delta 0.17498
wandb:                                                 train_de_mse 0.0099
wandb:                                             train_de_pearson 0.97041
wandb:                                                    train_mse 0.00108
wandb:                                                train_pearson 0.997
wandb:                                                training_loss 0.56027
wandb:                                                   val_de_mse 0.04016
wandb:                                               val_de_pearson 0.97859
wandb:                                                      val_mse 0.00173
wandb:                                                  val_pearson 0.99514
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRi_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/5uqs2zbc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_013325-5uqs2zbc/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:46
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_015447-j6jx56lo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRi_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/j6jx56lo
wandb: WARNING Serializing object of type ndarray that is 21106816 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5165
Epoch 1 Step 51 Train Loss: 0.5858
Epoch 1 Step 101 Train Loss: 0.5947
Epoch 1 Step 151 Train Loss: 0.6250
Epoch 1 Step 201 Train Loss: 0.5249
Epoch 1 Step 251 Train Loss: 0.5442
Epoch 1 Step 301 Train Loss: 0.5217
Epoch 1 Step 351 Train Loss: 0.5608
Epoch 1 Step 401 Train Loss: 0.5389
Epoch 1 Step 451 Train Loss: 0.5611
Epoch 1 Step 501 Train Loss: 0.5358
Epoch 1 Step 551 Train Loss: 0.6005
Epoch 1 Step 601 Train Loss: 0.5423
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0052. 
Epoch 2 Step 1 Train Loss: 0.5917
Epoch 2 Step 51 Train Loss: 0.5662
Epoch 2 Step 101 Train Loss: 0.5580
Epoch 2 Step 151 Train Loss: 0.4939
Epoch 2 Step 201 Train Loss: 0.5579
Epoch 2 Step 251 Train Loss: 0.5913
Epoch 2 Step 301 Train Loss: 0.5394
Epoch 2 Step 351 Train Loss: 0.4526
Epoch 2 Step 401 Train Loss: 0.5300
Epoch 2 Step 451 Train Loss: 0.5799
Epoch 2 Step 501 Train Loss: 0.5277
Epoch 2 Step 551 Train Loss: 0.6187
Epoch 2 Step 601 Train Loss: 0.5618
Epoch 2: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.0056. 
Epoch 3 Step 1 Train Loss: 0.5510
Epoch 3 Step 51 Train Loss: 0.5520
Epoch 3 Step 101 Train Loss: 0.5613
Epoch 3 Step 151 Train Loss: 0.6186
Epoch 3 Step 201 Train Loss: 0.5049
Epoch 3 Step 251 Train Loss: 0.5785
Epoch 3 Step 301 Train Loss: 0.5475
Epoch 3 Step 351 Train Loss: 0.5194
Epoch 3 Step 401 Train Loss: 0.5174
Epoch 3 Step 451 Train Loss: 0.5181
Epoch 3 Step 501 Train Loss: 0.5450
Epoch 3 Step 551 Train Loss: 0.5295
Epoch 3 Step 601 Train Loss: 0.5393
Epoch 3: Train Overall MSE: 0.0043 Validation Overall MSE: 0.0038. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0103. 
Epoch 4 Step 1 Train Loss: 0.5011
Epoch 4 Step 51 Train Loss: 0.5633
Epoch 4 Step 101 Train Loss: 0.5483
Epoch 4 Step 151 Train Loss: 0.5821
Epoch 4 Step 201 Train Loss: 0.5045
Epoch 4 Step 251 Train Loss: 0.5083
Epoch 4 Step 301 Train Loss: 0.5018
Epoch 4 Step 351 Train Loss: 0.5275
Epoch 4 Step 401 Train Loss: 0.5745
Epoch 4 Step 451 Train Loss: 0.6196
Epoch 4 Step 501 Train Loss: 0.5492
Epoch 4 Step 551 Train Loss: 0.5991
Epoch 4 Step 601 Train Loss: 0.5262
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0127 Validation Top 20 DE MSE: 0.0054. 
Epoch 5 Step 1 Train Loss: 0.4689
Epoch 5 Step 51 Train Loss: 0.5334
Epoch 5 Step 101 Train Loss: 0.5568
Epoch 5 Step 151 Train Loss: 0.5486
Epoch 5 Step 201 Train Loss: 0.5468
Epoch 5 Step 251 Train Loss: 0.5075
Epoch 5 Step 301 Train Loss: 0.5560
Epoch 5 Step 351 Train Loss: 0.5473
Epoch 5 Step 401 Train Loss: 0.5539
Epoch 5 Step 451 Train Loss: 0.5124
Epoch 5 Step 501 Train Loss: 0.5469
Epoch 5 Step 551 Train Loss: 0.4930
Epoch 5 Step 601 Train Loss: 0.5530
Epoch 5: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.0054. 
Epoch 6 Step 1 Train Loss: 0.5225
Epoch 6 Step 51 Train Loss: 0.5664
Epoch 6 Step 101 Train Loss: 0.4861
Epoch 6 Step 151 Train Loss: 0.5888
Epoch 6 Step 201 Train Loss: 0.5713
Epoch 6 Step 251 Train Loss: 0.6018
Epoch 6 Step 301 Train Loss: 0.5685
Epoch 6 Step 351 Train Loss: 0.5809
Epoch 6 Step 401 Train Loss: 0.5632
Epoch 6 Step 451 Train Loss: 0.5446
Epoch 6 Step 501 Train Loss: 0.5937
Epoch 6 Step 551 Train Loss: 0.4853
Epoch 6 Step 601 Train Loss: 0.5175
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0389 Validation Top 20 DE MSE: 0.0054. 
Epoch 7 Step 1 Train Loss: 0.5773
Epoch 7 Step 51 Train Loss: 0.5061
Epoch 7 Step 101 Train Loss: 0.5579
Epoch 7 Step 151 Train Loss: 0.5272
Epoch 7 Step 201 Train Loss: 0.5470
Epoch 7 Step 251 Train Loss: 0.5895
Epoch 7 Step 301 Train Loss: 0.4881
Epoch 7 Step 351 Train Loss: 0.5344
Epoch 7 Step 401 Train Loss: 0.5768
Epoch 7 Step 451 Train Loss: 0.5331
Epoch 7 Step 501 Train Loss: 0.5312
Epoch 7 Step 551 Train Loss: 0.5736
Epoch 7 Step 601 Train Loss: 0.4821
Epoch 7: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0228 Validation Top 20 DE MSE: 0.0054. 
Epoch 8 Step 1 Train Loss: 0.5468
Epoch 8 Step 51 Train Loss: 0.5713
Epoch 8 Step 101 Train Loss: 0.6363
Epoch 8 Step 151 Train Loss: 0.5096
Epoch 8 Step 201 Train Loss: 0.5222
Epoch 8 Step 251 Train Loss: 0.5310
Epoch 8 Step 301 Train Loss: 0.5676
Epoch 8 Step 351 Train Loss: 0.5481
Epoch 8 Step 401 Train Loss: 0.4959
Epoch 8 Step 451 Train Loss: 0.5551
Epoch 8 Step 501 Train Loss: 0.5089
Epoch 8 Step 551 Train Loss: 0.5731
Epoch 8 Step 601 Train Loss: 0.5038
Epoch 8: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0193 Validation Top 20 DE MSE: 0.0054. 
Epoch 9 Step 1 Train Loss: 0.5948
Epoch 9 Step 51 Train Loss: 0.5824
Epoch 9 Step 101 Train Loss: 0.5564
Epoch 9 Step 151 Train Loss: 0.6268
Epoch 9 Step 201 Train Loss: 0.5365
Epoch 9 Step 251 Train Loss: 0.5198
Epoch 9 Step 301 Train Loss: 0.5703
Epoch 9 Step 351 Train Loss: 0.5398
Epoch 9 Step 401 Train Loss: 0.4620
Epoch 9 Step 451 Train Loss: 0.5473
Epoch 9 Step 501 Train Loss: 0.5046
Epoch 9 Step 551 Train Loss: 0.4872
Epoch 9 Step 601 Train Loss: 0.5262
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0348 Validation Top 20 DE MSE: 0.0054. 
Epoch 10 Step 1 Train Loss: 0.5830
Epoch 10 Step 51 Train Loss: 0.5682
Epoch 10 Step 101 Train Loss: 0.4958
Epoch 10 Step 151 Train Loss: 0.5466
Epoch 10 Step 201 Train Loss: 0.5207
Epoch 10 Step 251 Train Loss: 0.5369
Epoch 10 Step 301 Train Loss: 0.5186
Epoch 10 Step 351 Train Loss: 0.5583
Epoch 10 Step 401 Train Loss: 0.5469
Epoch 10 Step 451 Train Loss: 0.5552
Epoch 10 Step 501 Train Loss: 0.5601
Epoch 10 Step 551 Train Loss: 0.5655
Epoch 10 Step 601 Train Loss: 0.5422
Epoch 10: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0947 Validation Top 20 DE MSE: 0.0054. 
Epoch 11 Step 1 Train Loss: 0.5071
Epoch 11 Step 51 Train Loss: 0.5188
Epoch 11 Step 101 Train Loss: 0.5235
Epoch 11 Step 151 Train Loss: 0.5694
Epoch 11 Step 201 Train Loss: 0.5322
Epoch 11 Step 251 Train Loss: 0.5744
Epoch 11 Step 301 Train Loss: 0.5424
Epoch 11 Step 351 Train Loss: 0.5187
Epoch 11 Step 401 Train Loss: 0.5892
Epoch 11 Step 451 Train Loss: 0.5349
Epoch 11 Step 501 Train Loss: 0.5796
Epoch 11 Step 551 Train Loss: 0.5450
Epoch 11 Step 601 Train Loss: 0.5578
Epoch 11: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0580 Validation Top 20 DE MSE: 0.0054. 
Epoch 12 Step 1 Train Loss: 0.5337
Epoch 12 Step 51 Train Loss: 0.5556
Epoch 12 Step 101 Train Loss: 0.4944
Epoch 12 Step 151 Train Loss: 0.6013
Epoch 12 Step 201 Train Loss: 0.5410
Epoch 12 Step 251 Train Loss: 0.5319
Epoch 12 Step 301 Train Loss: 0.5188
Epoch 12 Step 351 Train Loss: 0.5592
Epoch 12 Step 401 Train Loss: 0.5045
Epoch 12 Step 451 Train Loss: 0.5267
Epoch 12 Step 501 Train Loss: 0.5341
Epoch 12 Step 551 Train Loss: 0.5808
Epoch 12 Step 601 Train Loss: 0.5814
Epoch 12: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0407 Validation Top 20 DE MSE: 0.0054. 
Epoch 13 Step 1 Train Loss: 0.5925
Epoch 13 Step 51 Train Loss: 0.5351
Epoch 13 Step 101 Train Loss: 0.5008
Epoch 13 Step 151 Train Loss: 0.5071
Epoch 13 Step 201 Train Loss: 0.5281
Epoch 13 Step 251 Train Loss: 0.6071
Epoch 13 Step 301 Train Loss: 0.5344
Epoch 13 Step 351 Train Loss: 0.5568
Epoch 13 Step 401 Train Loss: 0.5551
Epoch 13 Step 451 Train Loss: 0.5569
Epoch 13 Step 501 Train Loss: 0.5652
Epoch 13 Step 551 Train Loss: 0.6134
Epoch 13 Step 601 Train Loss: 0.5489
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0382 Validation Top 20 DE MSE: 0.0054. 
Epoch 14 Step 1 Train Loss: 0.5251
Epoch 14 Step 51 Train Loss: 0.5314
Epoch 14 Step 101 Train Loss: 0.5328
Epoch 14 Step 151 Train Loss: 0.5727
Epoch 14 Step 201 Train Loss: 0.4732
Epoch 14 Step 251 Train Loss: 0.5822
Epoch 14 Step 301 Train Loss: 0.5495
Epoch 14 Step 351 Train Loss: 0.5331
Epoch 14 Step 401 Train Loss: 0.5642
Epoch 14 Step 451 Train Loss: 0.5855
Epoch 14 Step 501 Train Loss: 0.5337
Epoch 14 Step 551 Train Loss: 0.6500
Epoch 14 Step 601 Train Loss: 0.5415
Epoch 14: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0210 Validation Top 20 DE MSE: 0.0054. 
Epoch 15 Step 1 Train Loss: 0.4838
Epoch 15 Step 51 Train Loss: 0.5042
Epoch 15 Step 101 Train Loss: 0.5545
Epoch 15 Step 151 Train Loss: 0.5680
Epoch 15 Step 201 Train Loss: 0.5032
Epoch 15 Step 251 Train Loss: 0.6088
Epoch 15 Step 301 Train Loss: 0.5966
Epoch 15 Step 351 Train Loss: 0.5291
Epoch 15 Step 401 Train Loss: 0.5080
Epoch 15 Step 451 Train Loss: 0.5277
Epoch 15 Step 501 Train Loss: 0.5541
Epoch 15 Step 551 Train Loss: 0.5606
Epoch 15 Step 601 Train Loss: 0.5431
Epoch 15: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0167 Validation Top 20 DE MSE: 0.0054. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0124
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0013438876
test_unseen_single_pearson: 0.9962593546459696
test_unseen_single_mse_de: 0.012409716
test_unseen_single_pearson_de: 0.9692557706091397
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.22720701141644897
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.24891304347826085
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9739130434782608
test_unseen_single_mse_top20_de_non_dropout: 0.014500306
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.001 MB of 0.025 MB uploadedwandb: / 0.003 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb: | 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:                                                    train_mse ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñá‚ñà‚ñÅ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ
wandb:                                                   val_de_mse ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñá‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01241
wandb:                                              test_de_pearson 0.96926
wandb:               test_frac_opposite_direction_top20_non_dropout 0.24891
wandb:                          test_frac_sigma_below_1_non_dropout 0.97391
wandb:                                                     test_mse 0.00134
wandb:                                test_mse_top20_de_non_dropout 0.0145
wandb:                                                 test_pearson 0.99626
wandb:                                           test_pearson_delta 0.22721
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.24891
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.97391
wandb:                                       test_unseen_single_mse 0.00134
wandb:                                    test_unseen_single_mse_de 0.01241
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0145
wandb:                                   test_unseen_single_pearson 0.99626
wandb:                                test_unseen_single_pearson_de 0.96926
wandb:                             test_unseen_single_pearson_delta 0.22721
wandb:                                                 train_de_mse 0.01666
wandb:                                             train_de_pearson 0.97141
wandb:                                                    train_mse 0.0012
wandb:                                                train_pearson 0.99664
wandb:                                                training_loss 0.57867
wandb:                                                   val_de_mse 0.00539
wandb:                                               val_de_pearson 0.95902
wandb:                                                      val_mse 0.00113
wandb:                                                  val_pearson 0.99684
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRi_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/j6jx56lo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_015447-j6jx56lo/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:46
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_021810-ae2gfi9v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRi_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/ae2gfi9v
wandb: WARNING Serializing object of type ndarray that is 21106816 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5456
Epoch 1 Step 51 Train Loss: 0.5105
Epoch 1 Step 101 Train Loss: 0.5889
Epoch 1 Step 151 Train Loss: 0.6028
Epoch 1 Step 201 Train Loss: 0.6362
Epoch 1 Step 251 Train Loss: 0.6016
Epoch 1 Step 301 Train Loss: 0.6203
Epoch 1 Step 351 Train Loss: 0.5804
Epoch 1 Step 401 Train Loss: 0.6166
Epoch 1 Step 451 Train Loss: 0.5140
Epoch 1 Step 501 Train Loss: 0.5133
Epoch 1 Step 551 Train Loss: 0.5659
Epoch 1 Step 601 Train Loss: 0.5498
Epoch 1: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0124 Validation Top 20 DE MSE: 0.0101. 
Epoch 2 Step 1 Train Loss: 0.5220
Epoch 2 Step 51 Train Loss: 0.5416
Epoch 2 Step 101 Train Loss: 0.4996
Epoch 2 Step 151 Train Loss: 0.5849
Epoch 2 Step 201 Train Loss: 0.5310
Epoch 2 Step 251 Train Loss: 0.5799
Epoch 2 Step 301 Train Loss: 0.5631
Epoch 2 Step 351 Train Loss: 0.5458
Epoch 2 Step 401 Train Loss: 0.5114
Epoch 2 Step 451 Train Loss: 0.5176
Epoch 2 Step 501 Train Loss: 0.5033
Epoch 2 Step 551 Train Loss: 0.5581
Epoch 2 Step 601 Train Loss: 0.5303
Epoch 2: Train Overall MSE: 0.0066 Validation Overall MSE: 0.0049. 
Train Top 20 DE MSE: 0.0364 Validation Top 20 DE MSE: 0.0163. 
Epoch 3 Step 1 Train Loss: 0.4755
Epoch 3 Step 51 Train Loss: 0.5444
Epoch 3 Step 101 Train Loss: 0.5745
Epoch 3 Step 151 Train Loss: 0.5145
Epoch 3 Step 201 Train Loss: 0.5993
Epoch 3 Step 251 Train Loss: 0.5711
Epoch 3 Step 301 Train Loss: 0.5725
Epoch 3 Step 351 Train Loss: 0.5560
Epoch 3 Step 401 Train Loss: 0.5684
Epoch 3 Step 451 Train Loss: 0.5345
Epoch 3 Step 501 Train Loss: 0.5156
Epoch 3 Step 551 Train Loss: 0.5458
Epoch 3 Step 601 Train Loss: 0.5595
Epoch 3: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0144 Validation Top 20 DE MSE: 0.0098. 
Epoch 4 Step 1 Train Loss: 0.4869
Epoch 4 Step 51 Train Loss: 0.5537
Epoch 4 Step 101 Train Loss: 0.5198
Epoch 4 Step 151 Train Loss: 0.4805
Epoch 4 Step 201 Train Loss: 0.5573
Epoch 4 Step 251 Train Loss: 0.5535
Epoch 4 Step 301 Train Loss: 0.6158
Epoch 4 Step 351 Train Loss: 0.6121
Epoch 4 Step 401 Train Loss: 0.5589
Epoch 4 Step 451 Train Loss: 0.5460
Epoch 4 Step 501 Train Loss: 0.5833
Epoch 4 Step 551 Train Loss: 0.5298
Epoch 4 Step 601 Train Loss: 0.5884
Epoch 4: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0140 Validation Top 20 DE MSE: 0.0099. 
Epoch 5 Step 1 Train Loss: 0.5780
Epoch 5 Step 51 Train Loss: 0.5962
Epoch 5 Step 101 Train Loss: 0.5279
Epoch 5 Step 151 Train Loss: 0.4968
Epoch 5 Step 201 Train Loss: 0.5509
Epoch 5 Step 251 Train Loss: 0.5005
Epoch 5 Step 301 Train Loss: 0.5040
Epoch 5 Step 351 Train Loss: 0.5871
Epoch 5 Step 401 Train Loss: 0.5761
Epoch 5 Step 451 Train Loss: 0.5092
Epoch 5 Step 501 Train Loss: 0.5488
Epoch 5 Step 551 Train Loss: 0.5849
Epoch 5 Step 601 Train Loss: 0.5082
Epoch 5: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0211 Validation Top 20 DE MSE: 0.0098. 
Epoch 6 Step 1 Train Loss: 0.5446
Epoch 6 Step 51 Train Loss: 0.5546
Epoch 6 Step 101 Train Loss: 0.5645
Epoch 6 Step 151 Train Loss: 0.5332
Epoch 6 Step 201 Train Loss: 0.5421
Epoch 6 Step 251 Train Loss: 0.5547
Epoch 6 Step 301 Train Loss: 0.5488
Epoch 6 Step 351 Train Loss: 0.4664
Epoch 6 Step 401 Train Loss: 0.5192
Epoch 6 Step 451 Train Loss: 0.5443
Epoch 6 Step 501 Train Loss: 0.4683
Epoch 6 Step 551 Train Loss: 0.4791
Epoch 6 Step 601 Train Loss: 0.5537
Epoch 6: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0843 Validation Top 20 DE MSE: 0.0098. 
Epoch 7 Step 1 Train Loss: 0.5512
Epoch 7 Step 51 Train Loss: 0.6000
Epoch 7 Step 101 Train Loss: 0.5779
Epoch 7 Step 151 Train Loss: 0.6076
Epoch 7 Step 201 Train Loss: 0.6310
Epoch 7 Step 251 Train Loss: 0.5117
Epoch 7 Step 301 Train Loss: 0.5773
Epoch 7 Step 351 Train Loss: 0.5143
Epoch 7 Step 401 Train Loss: 0.5429
Epoch 7 Step 451 Train Loss: 0.5034
Epoch 7 Step 501 Train Loss: 0.5910
Epoch 7 Step 551 Train Loss: 0.5530
Epoch 7 Step 601 Train Loss: 0.5342
Epoch 7: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0229 Validation Top 20 DE MSE: 0.0098. 
Epoch 8 Step 1 Train Loss: 0.5382
Epoch 8 Step 51 Train Loss: 0.5551
Epoch 8 Step 101 Train Loss: 0.5278
Epoch 8 Step 151 Train Loss: 0.5307
Epoch 8 Step 201 Train Loss: 0.5657
Epoch 8 Step 251 Train Loss: 0.5120
Epoch 8 Step 301 Train Loss: 0.5105
Epoch 8 Step 351 Train Loss: 0.6215
Epoch 8 Step 401 Train Loss: 0.5547
Epoch 8 Step 451 Train Loss: 0.5345
Epoch 8 Step 501 Train Loss: 0.5774
Epoch 8 Step 551 Train Loss: 0.5539
Epoch 8 Step 601 Train Loss: 0.5447
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0286 Validation Top 20 DE MSE: 0.0098. 
Epoch 9 Step 1 Train Loss: 0.4848
Epoch 9 Step 51 Train Loss: 0.5989
Epoch 9 Step 101 Train Loss: 0.5407
Epoch 9 Step 151 Train Loss: 0.5365
Epoch 9 Step 201 Train Loss: 0.5944
Epoch 9 Step 251 Train Loss: 0.5280
Epoch 9 Step 301 Train Loss: 0.5497
Epoch 9 Step 351 Train Loss: 0.5024
Epoch 9 Step 401 Train Loss: 0.5456
Epoch 9 Step 451 Train Loss: 0.5081
Epoch 9 Step 501 Train Loss: 0.5370
Epoch 9 Step 551 Train Loss: 0.6360
Epoch 9 Step 601 Train Loss: 0.5299
Epoch 9: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.1157 Validation Top 20 DE MSE: 0.0098. 
Epoch 10 Step 1 Train Loss: 0.5230
Epoch 10 Step 51 Train Loss: 0.5395
Epoch 10 Step 101 Train Loss: 0.5571
Epoch 10 Step 151 Train Loss: 0.5411
Epoch 10 Step 201 Train Loss: 0.6031
Epoch 10 Step 251 Train Loss: 0.5487
Epoch 10 Step 301 Train Loss: 0.5300
Epoch 10 Step 351 Train Loss: 0.6509
Epoch 10 Step 401 Train Loss: 0.6016
Epoch 10 Step 451 Train Loss: 0.5319
Epoch 10 Step 501 Train Loss: 0.5434
Epoch 10 Step 551 Train Loss: 0.5282
Epoch 10 Step 601 Train Loss: 0.5730
Epoch 10: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0196 Validation Top 20 DE MSE: 0.0098. 
Epoch 11 Step 1 Train Loss: 0.5057
Epoch 11 Step 51 Train Loss: 0.5701
Epoch 11 Step 101 Train Loss: 0.5418
Epoch 11 Step 151 Train Loss: 0.4850
Epoch 11 Step 201 Train Loss: 0.5394
Epoch 11 Step 251 Train Loss: 0.6039
Epoch 11 Step 301 Train Loss: 0.5293
Epoch 11 Step 351 Train Loss: 0.5197
Epoch 11 Step 401 Train Loss: 0.4996
Epoch 11 Step 451 Train Loss: 0.5411
Epoch 11 Step 501 Train Loss: 0.6022
Epoch 11 Step 551 Train Loss: 0.5248
Epoch 11 Step 601 Train Loss: 0.5542
Epoch 11: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0528 Validation Top 20 DE MSE: 0.0098. 
Epoch 12 Step 1 Train Loss: 0.5598
Epoch 12 Step 51 Train Loss: 0.5223
Epoch 12 Step 101 Train Loss: 0.5691
Epoch 12 Step 151 Train Loss: 0.4931
Epoch 12 Step 201 Train Loss: 0.5721
Epoch 12 Step 251 Train Loss: 0.5074
Epoch 12 Step 301 Train Loss: 0.5925
Epoch 12 Step 351 Train Loss: 0.5626
Epoch 12 Step 401 Train Loss: 0.5022
Epoch 12 Step 451 Train Loss: 0.5212
Epoch 12 Step 501 Train Loss: 0.5342
Epoch 12 Step 551 Train Loss: 0.5332
Epoch 12 Step 601 Train Loss: 0.5167
Epoch 12: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0896 Validation Top 20 DE MSE: 0.0098. 
Epoch 13 Step 1 Train Loss: 0.5796
Epoch 13 Step 51 Train Loss: 0.5337
Epoch 13 Step 101 Train Loss: 0.5580
Epoch 13 Step 151 Train Loss: 0.5081
Epoch 13 Step 201 Train Loss: 0.5239
Epoch 13 Step 251 Train Loss: 0.5060
Epoch 13 Step 301 Train Loss: 0.5321
Epoch 13 Step 351 Train Loss: 0.5651
Epoch 13 Step 401 Train Loss: 0.5148
Epoch 13 Step 451 Train Loss: 0.5418
Epoch 13 Step 501 Train Loss: 0.5455
Epoch 13 Step 551 Train Loss: 0.5234
Epoch 13 Step 601 Train Loss: 0.5432
Epoch 13: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0184 Validation Top 20 DE MSE: 0.0098. 
Epoch 14 Step 1 Train Loss: 0.5662
Epoch 14 Step 51 Train Loss: 0.5284
Epoch 14 Step 101 Train Loss: 0.5171
Epoch 14 Step 151 Train Loss: 0.5465
Epoch 14 Step 201 Train Loss: 0.5297
Epoch 14 Step 251 Train Loss: 0.5635
Epoch 14 Step 301 Train Loss: 0.5641
Epoch 14 Step 351 Train Loss: 0.5834
Epoch 14 Step 401 Train Loss: 0.5556
Epoch 14 Step 451 Train Loss: 0.5748
Epoch 14 Step 501 Train Loss: 0.5293
Epoch 14 Step 551 Train Loss: 0.5832
Epoch 14 Step 601 Train Loss: 0.5703
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0352 Validation Top 20 DE MSE: 0.0098. 
Epoch 15 Step 1 Train Loss: 0.6239
Epoch 15 Step 51 Train Loss: 0.5641
Epoch 15 Step 101 Train Loss: 0.5277
Epoch 15 Step 151 Train Loss: 0.5826
Epoch 15 Step 201 Train Loss: 0.5290
Epoch 15 Step 251 Train Loss: 0.5442
Epoch 15 Step 301 Train Loss: 0.5547
Epoch 15 Step 351 Train Loss: 0.5207
Epoch 15 Step 401 Train Loss: 0.5514
Epoch 15 Step 451 Train Loss: 0.5018
Epoch 15 Step 501 Train Loss: 0.5155
Epoch 15 Step 551 Train Loss: 0.5114
Epoch 15 Step 601 Train Loss: 0.5468
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0391 Validation Top 20 DE MSE: 0.0098. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0091
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0010312321
test_unseen_single_pearson: 0.9971225183258194
test_unseen_single_mse_de: 0.00914477
test_unseen_single_pearson_de: 0.9722032100685333
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.2644963803519115
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2608695652173913
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9826086956521741
test_unseen_single_mse_top20_de_non_dropout: 0.010849901
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.021 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.003 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÑ‚ñÜ‚ñÅ‚ñÉ‚ñÉ
wandb:                                             train_de_pearson ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá
wandb:                                                    train_mse ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                train_pearson ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00914
wandb:                                              test_de_pearson 0.9722
wandb:               test_frac_opposite_direction_top20_non_dropout 0.26087
wandb:                          test_frac_sigma_below_1_non_dropout 0.98261
wandb:                                                     test_mse 0.00103
wandb:                                test_mse_top20_de_non_dropout 0.01085
wandb:                                                 test_pearson 0.99712
wandb:                                           test_pearson_delta 0.2645
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.26087
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98261
wandb:                                       test_unseen_single_mse 0.00103
wandb:                                    test_unseen_single_mse_de 0.00914
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01085
wandb:                                   test_unseen_single_pearson 0.99712
wandb:                                test_unseen_single_pearson_de 0.9722
wandb:                             test_unseen_single_pearson_delta 0.2645
wandb:                                                 train_de_mse 0.03908
wandb:                                             train_de_pearson 0.9654
wandb:                                                    train_mse 0.00165
wandb:                                                train_pearson 0.9956
wandb:                                                training_loss 0.55001
wandb:                                                   val_de_mse 0.00981
wandb:                                               val_de_pearson 0.96886
wandb:                                                      val_mse 0.00129
wandb:                                                  val_pearson 0.99641
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRi_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/ae2gfi9v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_021810-ae2gfi9v/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:46
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_024010-wp5acm67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2021_CRISPRi_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/wp5acm67
wandb: WARNING Serializing object of type ndarray that is 21106816 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5302
Epoch 1 Step 51 Train Loss: 0.5618
Epoch 1 Step 101 Train Loss: 0.5161
Epoch 1 Step 151 Train Loss: 0.5733
Epoch 1 Step 201 Train Loss: 0.5342
Epoch 1 Step 251 Train Loss: 0.5957
Epoch 1 Step 301 Train Loss: 0.6094
Epoch 1 Step 351 Train Loss: 0.5734
Epoch 1 Step 401 Train Loss: 0.5344
Epoch 1 Step 451 Train Loss: 0.5612
Epoch 1 Step 501 Train Loss: 0.5712
Epoch 1 Step 551 Train Loss: 0.5503
Epoch 1 Step 601 Train Loss: 0.5290
Epoch 1: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0170. 
Epoch 2 Step 1 Train Loss: 0.5060
Epoch 2 Step 51 Train Loss: 0.5092
Epoch 2 Step 101 Train Loss: 0.5430
Epoch 2 Step 151 Train Loss: 0.5627
Epoch 2 Step 201 Train Loss: 0.5788
Epoch 2 Step 251 Train Loss: 0.5656
Epoch 2 Step 301 Train Loss: 0.5706
Epoch 2 Step 351 Train Loss: 0.5866
Epoch 2 Step 401 Train Loss: 0.5180
Epoch 2 Step 451 Train Loss: 0.5643
Epoch 2 Step 501 Train Loss: 0.5941
Epoch 2 Step 551 Train Loss: 0.4904
Epoch 2 Step 601 Train Loss: 0.5094
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0172. 
Epoch 3 Step 1 Train Loss: 0.5437
Epoch 3 Step 51 Train Loss: 0.5074
Epoch 3 Step 101 Train Loss: 0.5408
Epoch 3 Step 151 Train Loss: 0.5539
Epoch 3 Step 201 Train Loss: 0.5686
Epoch 3 Step 251 Train Loss: 0.5786
Epoch 3 Step 301 Train Loss: 0.5621
Epoch 3 Step 351 Train Loss: 0.5691
Epoch 3 Step 401 Train Loss: 0.5749
Epoch 3 Step 451 Train Loss: 0.5170
Epoch 3 Step 501 Train Loss: 0.5439
Epoch 3 Step 551 Train Loss: 0.6246
Epoch 3 Step 601 Train Loss: 0.5916
Epoch 3: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0166. 
Epoch 4 Step 1 Train Loss: 0.5741
Epoch 4 Step 51 Train Loss: 0.5335
Epoch 4 Step 101 Train Loss: 0.5424
Epoch 4 Step 151 Train Loss: 0.5553
Epoch 4 Step 201 Train Loss: 0.5465
Epoch 4 Step 251 Train Loss: 0.5309
Epoch 4 Step 301 Train Loss: 0.5265
Epoch 4 Step 351 Train Loss: 0.5775
Epoch 4 Step 401 Train Loss: 0.5211
Epoch 4 Step 451 Train Loss: 0.6109
Epoch 4 Step 501 Train Loss: 0.5717
Epoch 4 Step 551 Train Loss: 0.5160
Epoch 4 Step 601 Train Loss: 0.5439
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0172. 
Epoch 5 Step 1 Train Loss: 0.5853
Epoch 5 Step 51 Train Loss: 0.5607
Epoch 5 Step 101 Train Loss: 0.5090
Epoch 5 Step 151 Train Loss: 0.5482
Epoch 5 Step 201 Train Loss: 0.5461
Epoch 5 Step 251 Train Loss: 0.5327
Epoch 5 Step 301 Train Loss: 0.5150
Epoch 5 Step 351 Train Loss: 0.5498
Epoch 5 Step 401 Train Loss: 0.5151
Epoch 5 Step 451 Train Loss: 0.5587
Epoch 5 Step 501 Train Loss: 0.5691
Epoch 5 Step 551 Train Loss: 0.4765
Epoch 5 Step 601 Train Loss: 0.5358
Epoch 5: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0170. 
Epoch 6 Step 1 Train Loss: 0.5679
Epoch 6 Step 51 Train Loss: 0.5430
Epoch 6 Step 101 Train Loss: 0.5923
Epoch 6 Step 151 Train Loss: 0.5613
Epoch 6 Step 201 Train Loss: 0.5695
Epoch 6 Step 251 Train Loss: 0.5153
Epoch 6 Step 301 Train Loss: 0.6147
Epoch 6 Step 351 Train Loss: 0.5279
Epoch 6 Step 401 Train Loss: 0.4957
Epoch 6 Step 451 Train Loss: 0.4743
Epoch 6 Step 501 Train Loss: 0.5672
Epoch 6 Step 551 Train Loss: 0.6196
Epoch 6 Step 601 Train Loss: 0.5451
Epoch 6: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0170. 
Epoch 7 Step 1 Train Loss: 0.5381
Epoch 7 Step 51 Train Loss: 0.6010
Epoch 7 Step 101 Train Loss: 0.5335
Epoch 7 Step 151 Train Loss: 0.5601
Epoch 7 Step 201 Train Loss: 0.4917
Epoch 7 Step 251 Train Loss: 0.5009
Epoch 7 Step 301 Train Loss: 0.5280
Epoch 7 Step 351 Train Loss: 0.4993
Epoch 7 Step 401 Train Loss: 0.5365
Epoch 7 Step 451 Train Loss: 0.5236
Epoch 7 Step 501 Train Loss: 0.5574
Epoch 7 Step 551 Train Loss: 0.5897
Epoch 7 Step 601 Train Loss: 0.5127
Epoch 7: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0169. 
Epoch 8 Step 1 Train Loss: 0.5571
Epoch 8 Step 51 Train Loss: 0.5628
Epoch 8 Step 101 Train Loss: 0.6159
Epoch 8 Step 151 Train Loss: 0.5414
Epoch 8 Step 201 Train Loss: 0.5424
Epoch 8 Step 251 Train Loss: 0.6237
Epoch 8 Step 301 Train Loss: 0.5268
Epoch 8 Step 351 Train Loss: 0.5171
Epoch 8 Step 401 Train Loss: 0.5810
Epoch 8 Step 451 Train Loss: 0.5609
Epoch 8 Step 501 Train Loss: 0.5783
Epoch 8 Step 551 Train Loss: 0.5349
Epoch 8 Step 601 Train Loss: 0.5959
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0169. 
Epoch 9 Step 1 Train Loss: 0.4997
Epoch 9 Step 51 Train Loss: 0.5545
Epoch 9 Step 101 Train Loss: 0.5349
Epoch 9 Step 151 Train Loss: 0.5232
Epoch 9 Step 201 Train Loss: 0.5555
Epoch 9 Step 251 Train Loss: 0.5461
Epoch 9 Step 301 Train Loss: 0.5501
Epoch 9 Step 351 Train Loss: 0.5647
Epoch 9 Step 401 Train Loss: 0.5670
Epoch 9 Step 451 Train Loss: 0.4979
Epoch 9 Step 501 Train Loss: 0.5737
Epoch 9 Step 551 Train Loss: 0.4760
Epoch 9 Step 601 Train Loss: 0.5092
Epoch 9: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Epoch 10 Step 1 Train Loss: 0.5354
Epoch 10 Step 51 Train Loss: 0.5854
Epoch 10 Step 101 Train Loss: 0.5419
Epoch 10 Step 151 Train Loss: 0.5540
Epoch 10 Step 201 Train Loss: 0.5343
Epoch 10 Step 251 Train Loss: 0.5733
Epoch 10 Step 301 Train Loss: 0.5319
Epoch 10 Step 351 Train Loss: 0.5592
Epoch 10 Step 401 Train Loss: 0.5132
Epoch 10 Step 451 Train Loss: 0.5415
Epoch 10 Step 501 Train Loss: 0.4753
Epoch 10 Step 551 Train Loss: 0.5641
Epoch 10 Step 601 Train Loss: 0.6136
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Epoch 11 Step 1 Train Loss: 0.5745
Epoch 11 Step 51 Train Loss: 0.5538
Epoch 11 Step 101 Train Loss: 0.5451
Epoch 11 Step 151 Train Loss: 0.5099
Epoch 11 Step 201 Train Loss: 0.5716
Epoch 11 Step 251 Train Loss: 0.5687
Epoch 11 Step 301 Train Loss: 0.5604
Epoch 11 Step 351 Train Loss: 0.5545
Epoch 11 Step 401 Train Loss: 0.5840
Epoch 11 Step 451 Train Loss: 0.5396
Epoch 11 Step 501 Train Loss: 0.5033
Epoch 11 Step 551 Train Loss: 0.5858
Epoch 11 Step 601 Train Loss: 0.5400
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Epoch 12 Step 1 Train Loss: 0.5447
Epoch 12 Step 51 Train Loss: 0.5133
Epoch 12 Step 101 Train Loss: 0.5420
Epoch 12 Step 151 Train Loss: 0.5513
Epoch 12 Step 201 Train Loss: 0.5027
Epoch 12 Step 251 Train Loss: 0.5466
Epoch 12 Step 301 Train Loss: 0.5724
Epoch 12 Step 351 Train Loss: 0.5427
Epoch 12 Step 401 Train Loss: 0.5309
Epoch 12 Step 451 Train Loss: 0.5025
Epoch 12 Step 501 Train Loss: 0.4974
Epoch 12 Step 551 Train Loss: 0.5520
Epoch 12 Step 601 Train Loss: 0.5870
Epoch 12: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Epoch 13 Step 1 Train Loss: 0.5670
Epoch 13 Step 51 Train Loss: 0.5684
Epoch 13 Step 101 Train Loss: 0.4901
Epoch 13 Step 151 Train Loss: 0.5411
Epoch 13 Step 201 Train Loss: 0.5810
Epoch 13 Step 251 Train Loss: 0.5151
Epoch 13 Step 301 Train Loss: 0.5721
Epoch 13 Step 351 Train Loss: 0.5895
Epoch 13 Step 401 Train Loss: 0.5533
Epoch 13 Step 451 Train Loss: 0.6028
Epoch 13 Step 501 Train Loss: 0.5370
Epoch 13 Step 551 Train Loss: 0.5637
Epoch 13 Step 601 Train Loss: 0.4803
Epoch 13: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Epoch 14 Step 1 Train Loss: 0.5605
Epoch 14 Step 51 Train Loss: 0.5850
Epoch 14 Step 101 Train Loss: 0.4961
Epoch 14 Step 151 Train Loss: 0.5507
Epoch 14 Step 201 Train Loss: 0.5050
Epoch 14 Step 251 Train Loss: 0.5535
Epoch 14 Step 301 Train Loss: 0.4987
Epoch 14 Step 351 Train Loss: 0.5181
Epoch 14 Step 401 Train Loss: 0.5593
Epoch 14 Step 451 Train Loss: 0.5819
Epoch 14 Step 501 Train Loss: 0.4867
Epoch 14 Step 551 Train Loss: 0.5475
Epoch 14 Step 601 Train Loss: 0.5213
Epoch 14: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Epoch 15 Step 1 Train Loss: 0.5300
Epoch 15 Step 51 Train Loss: 0.5298
Epoch 15 Step 101 Train Loss: 0.5277
Epoch 15 Step 151 Train Loss: 0.6142
Epoch 15 Step 201 Train Loss: 0.5068
Epoch 15 Step 251 Train Loss: 0.5373
Epoch 15 Step 301 Train Loss: 0.5308
Epoch 15 Step 351 Train Loss: 0.5159
Epoch 15 Step 401 Train Loss: 0.6088
Epoch 15 Step 451 Train Loss: 0.5588
Epoch 15 Step 501 Train Loss: 0.5522
Epoch 15 Step 551 Train Loss: 0.5623
Epoch 15 Step 601 Train Loss: 0.5442
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0169. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0198
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014913805
test_unseen_single_pearson: 0.9958142924381636
test_unseen_single_mse_de: 0.019803371
test_unseen_single_pearson_de: 0.9610008189349954
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.24168343823882477
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.292391304347826
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9565217391304351
test_unseen_single_mse_top20_de_non_dropout: 0.02161581
Done!
wandb: - 0.001 MB of 0.025 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.025 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñà‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñÜ‚ñà‚ñÅ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                                               val_de_pearson ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÑ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÖ‚ñá‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.0198
wandb:                                              test_de_pearson 0.961
wandb:               test_frac_opposite_direction_top20_non_dropout 0.29239
wandb:                          test_frac_sigma_below_1_non_dropout 0.95652
wandb:                                                     test_mse 0.00149
wandb:                                test_mse_top20_de_non_dropout 0.02162
wandb:                                                 test_pearson 0.99581
wandb:                                           test_pearson_delta 0.24168
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.29239
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.95652
wandb:                                       test_unseen_single_mse 0.00149
wandb:                                    test_unseen_single_mse_de 0.0198
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.02162
wandb:                                   test_unseen_single_pearson 0.99581
wandb:                                test_unseen_single_pearson_de 0.961
wandb:                             test_unseen_single_pearson_delta 0.24168
wandb:                                                 train_de_mse 0.00882
wandb:                                             train_de_pearson 0.97423
wandb:                                                    train_mse 0.00107
wandb:                                                train_pearson 0.99703
wandb:                                                training_loss 0.52374
wandb:                                                   val_de_mse 0.01691
wandb:                                               val_de_pearson 0.98193
wandb:                                                      val_mse 0.00126
wandb:                                                  val_pearson 0.99649
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2021_CRISPRi_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/wp5acm67
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_024010-wp5acm67/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:16
combo_seen1:76
combo_seen2:22
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_030257-61zq3kb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_day7neuron_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/61zq3kb6
wandb: WARNING Serializing object of type ndarray that is 20545664 bytes
  0%|                                                  | 0/3459 [00:00<?, ?it/s]  0%|                                          | 9/3459 [00:00<00:39, 87.09it/s]  1%|‚ñè                                        | 18/3459 [00:00<00:41, 83.45it/s]  1%|‚ñé                                        | 30/3459 [00:00<00:35, 96.82it/s]  1%|‚ñç                                        | 41/3459 [00:00<00:35, 96.60it/s]  2%|‚ñå                                       | 53/3459 [00:00<00:33, 102.04it/s]  2%|‚ñã                                       | 64/3459 [00:00<00:33, 102.72it/s]  2%|‚ñä                                       | 75/3459 [00:00<00:32, 103.14it/s]  2%|‚ñâ                                       | 86/3459 [00:00<00:33, 102.07it/s]  3%|‚ñà                                       | 97/3459 [00:00<00:33, 100.91it/s]  3%|‚ñà‚ñè                                     | 108/3459 [00:01<00:32, 101.57it/s]  3%|‚ñà‚ñé                                     | 119/3459 [00:01<00:32, 102.39it/s]  4%|‚ñà‚ñç                                     | 130/3459 [00:01<00:32, 102.50it/s]  4%|‚ñà‚ñå                                     | 141/3459 [00:01<00:32, 102.19it/s]  4%|‚ñà‚ñä                                      | 152/3459 [00:01<00:33, 99.53it/s]  5%|‚ñà‚ñâ                                      | 163/3459 [00:01<00:33, 99.80it/s]  5%|‚ñà‚ñâ                                     | 174/3459 [00:01<00:32, 102.57it/s]  5%|‚ñà‚ñà                                     | 185/3459 [00:01<00:32, 102.19it/s]  6%|‚ñà‚ñà‚ñè                                    | 196/3459 [00:01<00:31, 103.01it/s]  6%|‚ñà‚ñà‚ñé                                    | 207/3459 [00:02<00:32, 100.21it/s]  6%|‚ñà‚ñà‚ñç                                    | 219/3459 [00:02<00:31, 104.03it/s]  7%|‚ñà‚ñà‚ñå                                    | 230/3459 [00:02<00:31, 101.71it/s]  7%|‚ñà‚ñà‚ñä                                     | 241/3459 [00:02<00:32, 97.71it/s]  7%|‚ñà‚ñà‚ñâ                                     | 251/3459 [00:02<00:32, 97.95it/s]  8%|‚ñà‚ñà‚ñà                                     | 262/3459 [00:02<00:33, 96.26it/s]  8%|‚ñà‚ñà‚ñà                                    | 274/3459 [00:02<00:31, 100.85it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                   | 285/3459 [00:02<00:31, 101.40it/s]  9%|‚ñà‚ñà‚ñà‚ñé                                   | 296/3459 [00:02<00:31, 101.75it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 307/3459 [00:03<00:32, 97.74it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                   | 318/3459 [00:03<00:31, 100.79it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 329/3459 [00:03<00:31, 97.92it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                   | 340/3459 [00:03<00:30, 101.02it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                   | 351/3459 [00:03<00:30, 101.35it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                   | 362/3459 [00:03<00:30, 101.97it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 373/3459 [00:03<00:30, 101.97it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 384/3459 [00:03<00:30, 101.40it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 395/3459 [00:03<00:30, 101.12it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 406/3459 [00:04<00:30, 100.75it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 417/3459 [00:04<00:30, 99.34it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 428/3459 [00:04<00:30, 100.66it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 439/3459 [00:04<00:29, 101.68it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 450/3459 [00:04<00:29, 100.57it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 461/3459 [00:04<00:31, 93.82it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 471/3459 [00:04<00:31, 95.36it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 481/3459 [00:04<00:31, 95.29it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 492/3459 [00:04<00:30, 96.81it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 503/3459 [00:05<00:30, 98.34it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 514/3459 [00:05<00:29, 99.70it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 524/3459 [00:05<00:29, 99.29it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 534/3459 [00:05<00:30, 97.16it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 545/3459 [00:05<00:29, 99.86it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 556/3459 [00:05<00:29, 97.56it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 567/3459 [00:05<00:29, 98.72it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 579/3459 [00:05<00:28, 102.29it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 590/3459 [00:05<00:27, 102.66it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 601/3459 [00:06<00:28, 101.15it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 612/3459 [00:06<00:28, 98.94it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 623/3459 [00:06<00:28, 99.14it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 634/3459 [00:06<00:28, 100.51it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 645/3459 [00:06<00:27, 100.68it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 656/3459 [00:06<00:27, 101.02it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 668/3459 [00:06<00:26, 103.76it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 679/3459 [00:06<00:26, 103.11it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 690/3459 [00:06<00:26, 102.71it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 701/3459 [00:06<00:27, 101.40it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 712/3459 [00:07<00:27, 101.28it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 723/3459 [00:07<00:27, 99.13it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 735/3459 [00:07<00:26, 102.48it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 746/3459 [00:07<00:29, 91.75it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 760/3459 [00:07<00:26, 101.83it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 771/3459 [00:07<00:26, 102.57it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 782/3459 [00:07<00:26, 101.84it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 793/3459 [00:07<00:25, 102.71it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 804/3459 [00:08<00:26, 99.72it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 816/3459 [00:08<00:26, 98.59it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 828/3459 [00:08<00:25, 104.33it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 839/3459 [00:08<00:25, 101.56it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 850/3459 [00:08<00:25, 102.54it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 861/3459 [00:08<00:26, 99.23it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 871/3459 [00:08<00:30, 85.53it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 887/3459 [00:08<00:25, 102.06it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 898/3459 [00:08<00:24, 104.07it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 909/3459 [00:09<00:25, 100.26it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 920/3459 [00:09<00:24, 102.83it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 931/3459 [00:09<00:24, 101.96it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 942/3459 [00:09<00:25, 99.49it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 954/3459 [00:09<00:24, 102.39it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 965/3459 [00:09<00:25, 99.28it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 976/3459 [00:09<00:24, 100.33it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 987/3459 [00:09<00:24, 100.14it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 999/3459 [00:09<00:23, 102.56it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1010/3459 [00:10<00:24, 101.67it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1021/3459 [00:10<00:24, 99.65it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1033/3459 [00:10<00:23, 102.81it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1044/3459 [00:10<00:23, 103.25it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1055/3459 [00:10<00:24, 100.09it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1066/3459 [00:10<00:24, 97.86it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1078/3459 [00:10<00:23, 100.94it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1089/3459 [00:10<00:27, 86.48it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1105/3459 [00:11<00:23, 101.98it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1116/3459 [00:11<00:22, 102.30it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1127/3459 [00:12<01:11, 32.83it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1141/3459 [00:12<01:15, 30.55it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1161/3459 [00:12<00:55, 41.06it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1168/3459 [00:12<00:52, 43.38it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1175/3459 [00:13<00:52, 43.44it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1260/3459 [00:13<00:13, 159.80it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1288/3459 [00:13<00:17, 124.66it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1310/3459 [00:13<00:19, 112.10it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1328/3459 [00:14<00:20, 105.23it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1343/3459 [00:14<00:20, 103.63it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1357/3459 [00:14<00:20, 100.76it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1370/3459 [00:14<00:20, 102.55it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1382/3459 [00:14<00:20, 101.33it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1394/3459 [00:14<00:20, 99.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1405/3459 [00:14<00:20, 99.33it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1417/3459 [00:14<00:19, 102.25it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1428/3459 [00:15<00:20, 100.95it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1439/3459 [00:15<00:20, 98.12it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1449/3459 [00:15<00:20, 98.33it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1460/3459 [00:15<00:20, 98.55it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1471/3459 [00:15<00:19, 100.73it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1482/3459 [00:15<00:19, 99.92it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1493/3459 [00:15<00:19, 100.16it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1504/3459 [00:15<00:19, 99.83it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1515/3459 [00:15<00:19, 100.07it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1526/3459 [00:16<00:19, 96.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1537/3459 [00:16<00:19, 97.54it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1547/3459 [00:16<00:19, 97.90it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1557/3459 [00:16<00:19, 98.16it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1567/3459 [00:16<00:19, 98.55it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1577/3459 [00:16<00:19, 95.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1589/3459 [00:16<00:18, 99.96it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1601/3459 [00:16<00:18, 102.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1612/3459 [00:16<00:18, 98.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1623/3459 [00:17<00:18, 99.75it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1634/3459 [00:17<00:17, 102.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1645/3459 [00:17<00:17, 101.86it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1656/3459 [00:17<00:17, 101.43it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1667/3459 [00:17<00:18, 97.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1678/3459 [00:17<00:18, 97.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1689/3459 [00:17<00:18, 97.67it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1701/3459 [00:17<00:17, 98.88it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1711/3459 [00:17<00:17, 98.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1722/3459 [00:18<00:17, 101.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1733/3459 [00:18<00:17, 98.19it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1745/3459 [00:18<00:16, 101.47it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1756/3459 [00:18<00:16, 100.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1767/3459 [00:18<00:17, 97.29it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1777/3459 [00:18<00:17, 97.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1787/3459 [00:18<00:17, 97.31it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1798/3459 [00:18<00:16, 97.99it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1810/3459 [00:18<00:16, 98.97it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1821/3459 [00:19<00:16, 101.26it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1832/3459 [00:19<00:16, 100.39it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1843/3459 [00:19<00:16, 97.30it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1853/3459 [00:19<00:17, 94.02it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1865/3459 [00:19<00:16, 96.29it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1877/3459 [00:19<00:15, 102.33it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1888/3459 [00:19<00:15, 98.68it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 1899/3459 [00:19<00:15, 100.94it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1910/3459 [00:19<00:15, 100.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1921/3459 [00:20<00:15, 99.87it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 1932/3459 [00:20<00:15, 100.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1943/3459 [00:20<00:15, 100.77it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1954/3459 [00:20<00:15, 97.49it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 1966/3459 [00:20<00:15, 98.56it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1976/3459 [00:20<00:17, 83.68it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 1992/3459 [00:20<00:14, 100.05it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2003/3459 [00:20<00:14, 100.64it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2014/3459 [00:20<00:14, 102.98it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2025/3459 [00:21<00:14, 99.77it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2037/3459 [00:21<00:13, 102.73it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2048/3459 [00:21<00:13, 101.74it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2059/3459 [00:21<00:13, 101.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2070/3459 [00:21<00:14, 97.36it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2080/3459 [00:21<00:14, 95.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2092/3459 [00:21<00:14, 97.24it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2104/3459 [00:21<00:13, 98.01it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2116/3459 [00:22<00:13, 101.45it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2127/3459 [00:22<00:13, 100.43it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2138/3459 [00:22<00:13, 100.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2149/3459 [00:22<00:12, 102.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2160/3459 [00:22<00:12, 102.45it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2171/3459 [00:22<00:13, 98.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2183/3459 [00:22<00:12, 99.24it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2194/3459 [00:22<00:12, 99.62it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2204/3459 [00:22<00:12, 99.26it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2215/3459 [00:22<00:12, 102.07it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2226/3459 [00:23<00:12, 101.61it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2237/3459 [00:23<00:12, 101.32it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2248/3459 [00:23<00:12, 100.55it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2259/3459 [00:23<00:11, 100.38it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2270/3459 [00:23<00:12, 97.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2281/3459 [00:23<00:11, 100.27it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2292/3459 [00:23<00:11, 98.27it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2302/3459 [00:23<00:11, 98.45it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2313/3459 [00:23<00:11, 101.32it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2324/3459 [00:24<00:11, 100.35it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2335/3459 [00:24<00:11, 97.22it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2346/3459 [00:24<00:11, 97.34it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2357/3459 [00:24<00:11, 95.38it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2368/3459 [00:24<00:11, 99.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2378/3459 [00:24<00:10, 99.17it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2390/3459 [00:24<00:10, 102.34it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2401/3459 [00:24<00:10, 101.88it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2412/3459 [00:24<00:10, 101.80it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2423/3459 [00:25<00:10, 100.82it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2434/3459 [00:25<00:10, 100.75it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2445/3459 [00:25<00:10, 100.51it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2456/3459 [00:25<00:10, 99.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2467/3459 [00:25<00:10, 97.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2477/3459 [00:25<00:10, 97.53it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2489/3459 [00:25<00:09, 102.00it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2500/3459 [00:25<00:09, 99.42it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2510/3459 [00:26<00:11, 81.03it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2526/3459 [00:26<00:09, 100.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2538/3459 [00:26<00:08, 103.43it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2549/3459 [00:26<00:08, 102.25it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2560/3459 [00:26<00:09, 96.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2571/3459 [00:26<00:09, 96.46it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2582/3459 [00:26<00:08, 98.19it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2593/3459 [00:26<00:08, 96.48it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2606/3459 [00:26<00:08, 100.75it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2617/3459 [00:27<00:08, 98.32it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2629/3459 [00:27<00:08, 101.71it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2641/3459 [00:27<00:07, 104.49it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2652/3459 [00:27<00:07, 103.58it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2663/3459 [00:27<00:07, 103.14it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2674/3459 [00:27<00:07, 102.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2685/3459 [00:27<00:07, 102.35it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2696/3459 [00:27<00:07, 101.57it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2707/3459 [00:27<00:07, 101.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2718/3459 [00:28<00:07, 98.71it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2730/3459 [00:28<00:07, 99.71it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2741/3459 [00:28<00:07, 101.22it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2753/3459 [00:28<00:06, 104.27it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2764/3459 [00:28<00:06, 103.54it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2775/3459 [00:28<00:06, 102.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2786/3459 [00:28<00:06, 103.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2797/3459 [00:28<00:06, 102.97it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2808/3459 [00:28<00:06, 102.57it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2819/3459 [00:29<00:06, 99.13it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2830/3459 [00:29<00:06, 99.99it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2842/3459 [00:29<00:05, 103.60it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2853/3459 [00:29<00:06, 100.10it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2865/3459 [00:29<00:05, 103.27it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2876/3459 [00:29<00:05, 103.27it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2887/3459 [00:29<00:05, 102.94it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2898/3459 [00:29<00:05, 100.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2909/3459 [00:29<00:05, 100.99it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2920/3459 [00:30<00:05, 101.59it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 2931/3459 [00:30<00:05, 101.60it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 2942/3459 [00:30<00:05, 102.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2954/3459 [00:30<00:04, 105.14it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 2965/3459 [00:30<00:04, 101.83it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 2977/3459 [00:30<00:04, 104.84it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2988/3459 [00:30<00:04, 104.42it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2999/3459 [00:30<00:04, 104.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3010/3459 [00:30<00:04, 103.84it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3021/3459 [00:30<00:04, 103.57it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3032/3459 [00:31<00:04, 103.35it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3043/3459 [00:31<00:04, 100.85it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3055/3459 [00:31<00:03, 101.46it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3067/3459 [00:31<00:03, 101.88it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3078/3459 [00:31<00:03, 103.39it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3089/3459 [00:31<00:03, 103.63it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3100/3459 [00:31<00:03, 98.37it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3113/3459 [00:31<00:03, 105.05it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3124/3459 [00:31<00:03, 104.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3135/3459 [00:32<00:03, 104.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3146/3459 [00:32<00:03, 103.47it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3157/3459 [00:32<00:02, 103.07it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3168/3459 [00:32<00:02, 102.65it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3179/3459 [00:32<00:02, 99.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3191/3459 [00:32<00:02, 102.96it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3202/3459 [00:32<00:02, 97.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3215/3459 [00:32<00:02, 100.93it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3227/3459 [00:33<00:02, 100.63it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3239/3459 [00:33<00:02, 101.29it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3251/3459 [00:33<00:02, 103.73it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3262/3459 [00:33<00:01, 103.37it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3273/3459 [00:33<00:01, 103.10it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3284/3459 [00:33<00:01, 100.07it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3296/3459 [00:33<00:01, 103.15it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3307/3459 [00:33<00:01, 100.16it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3319/3459 [00:33<00:01, 100.84it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3331/3459 [00:34<00:01, 101.10it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3343/3459 [00:34<00:01, 101.32it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3354/3459 [00:34<00:01, 102.05it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3366/3459 [00:34<00:00, 104.23it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3377/3459 [00:34<00:00, 103.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3388/3459 [00:34<00:00, 99.81it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3400/3459 [00:34<00:00, 102.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3411/3459 [00:34<00:00, 99.47it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3422/3459 [00:34<00:00, 99.28it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3432/3459 [00:35<00:00, 97.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3444/3459 [00:35<00:00, 102.54it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3455/3459 [00:35<00:00, 98.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3459/3459 [00:35<00:00, 97.98it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.1809
Epoch 1 Step 51 Train Loss: 0.1703
Epoch 1 Step 101 Train Loss: 0.1565
Epoch 1 Step 151 Train Loss: 0.1475
Epoch 1 Step 201 Train Loss: 0.1411
Epoch 1 Step 251 Train Loss: 0.1656
Epoch 1 Step 301 Train Loss: 0.1505
Epoch 1 Step 351 Train Loss: 0.1473
Epoch 1 Step 401 Train Loss: 0.1579
Epoch 1 Step 451 Train Loss: 0.1214
Epoch 1 Step 501 Train Loss: 0.1489
Epoch 1 Step 551 Train Loss: 0.1210
Epoch 1: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0054. 
Epoch 2 Step 1 Train Loss: 0.1440
Epoch 2 Step 51 Train Loss: 0.1118
Epoch 2 Step 101 Train Loss: 0.1494
Epoch 2 Step 151 Train Loss: 0.1472
Epoch 2 Step 201 Train Loss: 0.1446
Epoch 2 Step 251 Train Loss: 0.1143
Epoch 2 Step 301 Train Loss: 0.1251
Epoch 2 Step 351 Train Loss: 0.1664
Epoch 2 Step 401 Train Loss: 0.1467
Epoch 2 Step 451 Train Loss: 0.1212
Epoch 2 Step 501 Train Loss: 0.1244
Epoch 2 Step 551 Train Loss: 0.1098
Epoch 2: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0051. 
Epoch 3 Step 1 Train Loss: 0.1065
Epoch 3 Step 51 Train Loss: 0.1171
Epoch 3 Step 101 Train Loss: 0.1039
Epoch 3 Step 151 Train Loss: 0.1181
Epoch 3 Step 201 Train Loss: 0.1053
Epoch 3 Step 251 Train Loss: 0.1285
Epoch 3 Step 301 Train Loss: 0.0780
Epoch 3 Step 351 Train Loss: 0.0965
Epoch 3 Step 401 Train Loss: 0.1007
Epoch 3 Step 451 Train Loss: 0.1033
Epoch 3 Step 501 Train Loss: 0.0795
Epoch 3 Step 551 Train Loss: 0.0789
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0049. 
Epoch 4 Step 1 Train Loss: 0.0807
Epoch 4 Step 51 Train Loss: 0.0780
Epoch 4 Step 101 Train Loss: 0.0884
Epoch 4 Step 151 Train Loss: 0.1026
Epoch 4 Step 201 Train Loss: 0.0916
Epoch 4 Step 251 Train Loss: 0.0887
Epoch 4 Step 301 Train Loss: 0.0887
Epoch 4 Step 351 Train Loss: 0.0930
Epoch 4 Step 401 Train Loss: 0.0938
Epoch 4 Step 451 Train Loss: 0.0881
Epoch 4 Step 501 Train Loss: 0.0959
Epoch 4 Step 551 Train Loss: 0.0870
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 5 Step 1 Train Loss: 0.0814
Epoch 5 Step 51 Train Loss: 0.1110
Epoch 5 Step 101 Train Loss: 0.0885
Epoch 5 Step 151 Train Loss: 0.1015
Epoch 5 Step 201 Train Loss: 0.0937
Epoch 5 Step 251 Train Loss: 0.0887
Epoch 5 Step 301 Train Loss: 0.1026
Epoch 5 Step 351 Train Loss: 0.0939
Epoch 5 Step 401 Train Loss: 0.0901
Epoch 5 Step 451 Train Loss: 0.0972
Epoch 5 Step 501 Train Loss: 0.0834
Epoch 5 Step 551 Train Loss: 0.1086
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0049. 
Epoch 6 Step 1 Train Loss: 0.1007
Epoch 6 Step 51 Train Loss: 0.0954
Epoch 6 Step 101 Train Loss: 0.1173
Epoch 6 Step 151 Train Loss: 0.0783
Epoch 6 Step 201 Train Loss: 0.0897
Epoch 6 Step 251 Train Loss: 0.0889
Epoch 6 Step 301 Train Loss: 0.0855
Epoch 6 Step 351 Train Loss: 0.0996
Epoch 6 Step 401 Train Loss: 0.1010
Epoch 6 Step 451 Train Loss: 0.0922
Epoch 6 Step 501 Train Loss: 0.0878
Epoch 6 Step 551 Train Loss: 0.1019
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 7 Step 1 Train Loss: 0.1010
Epoch 7 Step 51 Train Loss: 0.0930
Epoch 7 Step 101 Train Loss: 0.0986
Epoch 7 Step 151 Train Loss: 0.0836
Epoch 7 Step 201 Train Loss: 0.1042
Epoch 7 Step 251 Train Loss: 0.0920
Epoch 7 Step 301 Train Loss: 0.1096
Epoch 7 Step 351 Train Loss: 0.0999
Epoch 7 Step 401 Train Loss: 0.1039
Epoch 7 Step 451 Train Loss: 0.0913
Epoch 7 Step 501 Train Loss: 0.0913
Epoch 7 Step 551 Train Loss: 0.0981
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0049. 
Epoch 8 Step 1 Train Loss: 0.0917
Epoch 8 Step 51 Train Loss: 0.1206
Epoch 8 Step 101 Train Loss: 0.1201
Epoch 8 Step 151 Train Loss: 0.0949
Epoch 8 Step 201 Train Loss: 0.1125
Epoch 8 Step 251 Train Loss: 0.1214
Epoch 8 Step 301 Train Loss: 0.1083
Epoch 8 Step 351 Train Loss: 0.1050
Epoch 8 Step 401 Train Loss: 0.0926
Epoch 8 Step 451 Train Loss: 0.0916
Epoch 8 Step 501 Train Loss: 0.0904
Epoch 8 Step 551 Train Loss: 0.0877
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 9 Step 1 Train Loss: 0.0967
Epoch 9 Step 51 Train Loss: 0.1011
Epoch 9 Step 101 Train Loss: 0.1225
Epoch 9 Step 151 Train Loss: 0.0898
Epoch 9 Step 201 Train Loss: 0.0985
Epoch 9 Step 251 Train Loss: 0.1031
Epoch 9 Step 301 Train Loss: 0.0904
Epoch 9 Step 351 Train Loss: 0.0974
Epoch 9 Step 401 Train Loss: 0.0862
Epoch 9 Step 451 Train Loss: 0.0982
Epoch 9 Step 501 Train Loss: 0.1054
Epoch 9 Step 551 Train Loss: 0.0920
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 10 Step 1 Train Loss: 0.1017
Epoch 10 Step 51 Train Loss: 0.0843
Epoch 10 Step 101 Train Loss: 0.0953
Epoch 10 Step 151 Train Loss: 0.1049
Epoch 10 Step 201 Train Loss: 0.1012
Epoch 10 Step 251 Train Loss: 0.1057
Epoch 10 Step 301 Train Loss: 0.0824
Epoch 10 Step 351 Train Loss: 0.0884
Epoch 10 Step 401 Train Loss: 0.0849
Epoch 10 Step 451 Train Loss: 0.0967
Epoch 10 Step 501 Train Loss: 0.1030
Epoch 10 Step 551 Train Loss: 0.1154
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 11 Step 1 Train Loss: 0.0938
Epoch 11 Step 51 Train Loss: 0.0935
Epoch 11 Step 101 Train Loss: 0.0884
Epoch 11 Step 151 Train Loss: 0.1050
Epoch 11 Step 201 Train Loss: 0.1141
Epoch 11 Step 251 Train Loss: 0.0998
Epoch 11 Step 301 Train Loss: 0.0830
Epoch 11 Step 351 Train Loss: 0.1176
Epoch 11 Step 401 Train Loss: 0.0927
Epoch 11 Step 451 Train Loss: 0.0929
Epoch 11 Step 501 Train Loss: 0.0921
Epoch 11 Step 551 Train Loss: 0.1009
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 12 Step 1 Train Loss: 0.1144
Epoch 12 Step 51 Train Loss: 0.0992
Epoch 12 Step 101 Train Loss: 0.1057
Epoch 12 Step 151 Train Loss: 0.0992
Epoch 12 Step 201 Train Loss: 0.1147
Epoch 12 Step 251 Train Loss: 0.1051
Epoch 12 Step 301 Train Loss: 0.0962
Epoch 12 Step 351 Train Loss: 0.0975
Epoch 12 Step 401 Train Loss: 0.0957
Epoch 12 Step 451 Train Loss: 0.0954
Epoch 12 Step 501 Train Loss: 0.0967
Epoch 12 Step 551 Train Loss: 0.1179
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0049. 
Epoch 13 Step 1 Train Loss: 0.1007
Epoch 13 Step 51 Train Loss: 0.1041
Epoch 13 Step 101 Train Loss: 0.1161
Epoch 13 Step 151 Train Loss: 0.0979
Epoch 13 Step 201 Train Loss: 0.1036
Epoch 13 Step 251 Train Loss: 0.1115
Epoch 13 Step 301 Train Loss: 0.0975
Epoch 13 Step 351 Train Loss: 0.0947
Epoch 13 Step 401 Train Loss: 0.1076
Epoch 13 Step 451 Train Loss: 0.0921
Epoch 13 Step 501 Train Loss: 0.1046
Epoch 13 Step 551 Train Loss: 0.0971
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 14 Step 1 Train Loss: 0.1133
Epoch 14 Step 51 Train Loss: 0.0923
Epoch 14 Step 101 Train Loss: 0.0910
Epoch 14 Step 151 Train Loss: 0.1010
Epoch 14 Step 201 Train Loss: 0.0929
Epoch 14 Step 251 Train Loss: 0.0930
Epoch 14 Step 301 Train Loss: 0.0858
Epoch 14 Step 351 Train Loss: 0.0935
Epoch 14 Step 401 Train Loss: 0.1004
Epoch 14 Step 451 Train Loss: 0.0954
Epoch 14 Step 501 Train Loss: 0.1133
Epoch 14 Step 551 Train Loss: 0.1077
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Epoch 15 Step 1 Train Loss: 0.1201
Epoch 15 Step 51 Train Loss: 0.0974
Epoch 15 Step 101 Train Loss: 0.0928
Epoch 15 Step 151 Train Loss: 0.1066
Epoch 15 Step 201 Train Loss: 0.0921
Epoch 15 Step 251 Train Loss: 0.1248
Epoch 15 Step 301 Train Loss: 0.1014
Epoch 15 Step 351 Train Loss: 0.1143
Epoch 15 Step 401 Train Loss: 0.0896
Epoch 15 Step 451 Train Loss: 0.1020
Epoch 15 Step 501 Train Loss: 0.1133
Epoch 15 Step 551 Train Loss: 0.1112
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0049. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0033
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0010996431
test_combo_seen0_pearson: 0.8879333995896017
test_combo_seen0_mse_de: 0.004989407
test_combo_seen0_pearson_de: 0.39528546664975606
test_combo_seen1_mse: 0.0009871079
test_combo_seen1_pearson: 0.8953442498823497
test_combo_seen1_mse_de: 0.0024620662
test_combo_seen1_pearson_de: 0.4444746829367172
test_combo_seen2_mse: 0.0014880039
test_combo_seen2_pearson: 0.8494956172149408
test_combo_seen2_mse_de: 0.0057788077
test_combo_seen2_pearson_de: 0.34606683501363034
test_unseen_single_mse: 5.935635e-05
test_unseen_single_pearson: 0.9925156625007308
test_unseen_single_mse_de: 0.0001232666
test_unseen_single_pearson_de: 0.6411375127381698
test_combo_seen0_pearson_delta: -0.017932021127063318
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.40312499999999996
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.475
test_combo_seen0_mse_top20_de_non_dropout: 0.009903101
test_combo_seen1_pearson_delta: 0.019148166422594398
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.40263157894736845
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.4157894736842106
test_combo_seen1_mse_top20_de_non_dropout: 0.004122564
test_combo_seen2_pearson_delta: -0.014818034845807675
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.38409090909090904
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.4318181818181818
test_combo_seen2_mse_top20_de_non_dropout: 0.010997721
test_unseen_single_pearson_delta: 0.14142766131275167
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.37142857142857144
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5857142857142856
test_unseen_single_mse_top20_de_non_dropout: 0.00011737051
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.023 MB of 0.025 MB uploadedwandb: / 0.023 MB of 0.025 MB uploadedwandb: - 0.023 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÑ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.40312
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.475
wandb:                                         test_combo_seen0_mse 0.0011
wandb:                                      test_combo_seen0_mse_de 0.00499
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.0099
wandb:                                     test_combo_seen0_pearson 0.88793
wandb:                                  test_combo_seen0_pearson_de 0.39529
wandb:                               test_combo_seen0_pearson_delta -0.01793
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.40263
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.41579
wandb:                                         test_combo_seen1_mse 0.00099
wandb:                                      test_combo_seen1_mse_de 0.00246
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.00412
wandb:                                     test_combo_seen1_pearson 0.89534
wandb:                                  test_combo_seen1_pearson_de 0.44447
wandb:                               test_combo_seen1_pearson_delta 0.01915
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.38409
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.43182
wandb:                                         test_combo_seen2_mse 0.00149
wandb:                                      test_combo_seen2_mse_de 0.00578
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.011
wandb:                                     test_combo_seen2_pearson 0.8495
wandb:                                  test_combo_seen2_pearson_de 0.34607
wandb:                               test_combo_seen2_pearson_delta -0.01482
wandb:                                                  test_de_mse 0.00326
wandb:                                              test_de_pearson 0.43146
wandb:               test_frac_opposite_direction_top20_non_dropout 0.39752
wandb:                          test_frac_sigma_below_1_non_dropout 0.43636
wandb:                                                     test_mse 0.00104
wandb:                                test_mse_top20_de_non_dropout 0.00591
wandb:                                                 test_pearson 0.89165
wandb:                                           test_pearson_delta 0.01514
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.37143
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.58571
wandb:                                       test_unseen_single_mse 6e-05
wandb:                                    test_unseen_single_mse_de 0.00012
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00012
wandb:                                   test_unseen_single_pearson 0.99252
wandb:                                test_unseen_single_pearson_de 0.64114
wandb:                             test_unseen_single_pearson_delta 0.14143
wandb:                                                 train_de_mse 0.00334
wandb:                                             train_de_pearson 0.48247
wandb:                                                    train_mse 0.00093
wandb:                                                train_pearson 0.90286
wandb:                                                training_loss 0.0912
wandb:                                                   val_de_mse 0.00491
wandb:                                               val_de_pearson 0.27732
wandb:                                                      val_mse 0.00146
wandb:                                                  val_pearson 0.84875
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_day7neuron_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/61zq3kb6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_030257-61zq3kb6/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:18
combo_seen1:77
combo_seen2:22
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_031755-ctubybsb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_day7neuron_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/ctubybsb
wandb: WARNING Serializing object of type ndarray that is 20545664 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1743
Epoch 1 Step 51 Train Loss: 0.1875
Epoch 1 Step 101 Train Loss: 0.1645
Epoch 1 Step 151 Train Loss: 0.1398
Epoch 1 Step 201 Train Loss: 0.1491
Epoch 1 Step 251 Train Loss: 0.1546
Epoch 1 Step 301 Train Loss: 0.1292
Epoch 1 Step 351 Train Loss: 0.1492
Epoch 1 Step 401 Train Loss: 0.1418
Epoch 1 Step 451 Train Loss: 0.1396
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0026. 
Epoch 2 Step 1 Train Loss: 0.1388
Epoch 2 Step 51 Train Loss: 0.1377
Epoch 2 Step 101 Train Loss: 0.1232
Epoch 2 Step 151 Train Loss: 0.1450
Epoch 2 Step 201 Train Loss: 0.1458
Epoch 2 Step 251 Train Loss: 0.1411
Epoch 2 Step 301 Train Loss: 0.1346
Epoch 2 Step 351 Train Loss: 0.1348
Epoch 2 Step 401 Train Loss: 0.1299
Epoch 2 Step 451 Train Loss: 0.1277
Epoch 2: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0021. 
Epoch 3 Step 1 Train Loss: 0.1233
Epoch 3 Step 51 Train Loss: 0.1196
Epoch 3 Step 101 Train Loss: 0.1488
Epoch 3 Step 151 Train Loss: 0.1025
Epoch 3 Step 201 Train Loss: 0.1117
Epoch 3 Step 251 Train Loss: 0.1211
Epoch 3 Step 301 Train Loss: 0.1080
Epoch 3 Step 351 Train Loss: 0.1008
Epoch 3 Step 401 Train Loss: 0.1034
Epoch 3 Step 451 Train Loss: 0.1197
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0056 Validation Top 20 DE MSE: 0.0044. 
Epoch 4 Step 1 Train Loss: 0.0932
Epoch 4 Step 51 Train Loss: 0.1250
Epoch 4 Step 101 Train Loss: 0.1233
Epoch 4 Step 151 Train Loss: 0.1175
Epoch 4 Step 201 Train Loss: 0.1331
Epoch 4 Step 251 Train Loss: 0.1177
Epoch 4 Step 301 Train Loss: 0.1133
Epoch 4 Step 351 Train Loss: 0.0796
Epoch 4 Step 401 Train Loss: 0.0854
Epoch 4 Step 451 Train Loss: 0.0914
Epoch 4: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 5 Step 1 Train Loss: 0.1007
Epoch 5 Step 51 Train Loss: 0.0922
Epoch 5 Step 101 Train Loss: 0.0940
Epoch 5 Step 151 Train Loss: 0.0865
Epoch 5 Step 201 Train Loss: 0.1046
Epoch 5 Step 251 Train Loss: 0.0946
Epoch 5 Step 301 Train Loss: 0.0775
Epoch 5 Step 351 Train Loss: 0.1112
Epoch 5 Step 401 Train Loss: 0.0904
Epoch 5 Step 451 Train Loss: 0.0887
Epoch 5: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 6 Step 1 Train Loss: 0.1022
Epoch 6 Step 51 Train Loss: 0.0811
Epoch 6 Step 101 Train Loss: 0.0796
Epoch 6 Step 151 Train Loss: 0.0792
Epoch 6 Step 201 Train Loss: 0.0864
Epoch 6 Step 251 Train Loss: 0.0876
Epoch 6 Step 301 Train Loss: 0.0816
Epoch 6 Step 351 Train Loss: 0.0914
Epoch 6 Step 401 Train Loss: 0.1000
Epoch 6 Step 451 Train Loss: 0.0989
Epoch 6: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 7 Step 1 Train Loss: 0.0841
Epoch 7 Step 51 Train Loss: 0.1017
Epoch 7 Step 101 Train Loss: 0.0896
Epoch 7 Step 151 Train Loss: 0.1296
Epoch 7 Step 201 Train Loss: 0.1018
Epoch 7 Step 251 Train Loss: 0.1043
Epoch 7 Step 301 Train Loss: 0.0980
Epoch 7 Step 351 Train Loss: 0.0917
Epoch 7 Step 401 Train Loss: 0.0901
Epoch 7 Step 451 Train Loss: 0.0819
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 8 Step 1 Train Loss: 0.1020
Epoch 8 Step 51 Train Loss: 0.0987
Epoch 8 Step 101 Train Loss: 0.0888
Epoch 8 Step 151 Train Loss: 0.0895
Epoch 8 Step 201 Train Loss: 0.1001
Epoch 8 Step 251 Train Loss: 0.0928
Epoch 8 Step 301 Train Loss: 0.0904
Epoch 8 Step 351 Train Loss: 0.1135
Epoch 8 Step 401 Train Loss: 0.0978
Epoch 8 Step 451 Train Loss: 0.0994
Epoch 8: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 9 Step 1 Train Loss: 0.0964
Epoch 9 Step 51 Train Loss: 0.1082
Epoch 9 Step 101 Train Loss: 0.1028
Epoch 9 Step 151 Train Loss: 0.0933
Epoch 9 Step 201 Train Loss: 0.0885
Epoch 9 Step 251 Train Loss: 0.1003
Epoch 9 Step 301 Train Loss: 0.0947
Epoch 9 Step 351 Train Loss: 0.1036
Epoch 9 Step 401 Train Loss: 0.1036
Epoch 9 Step 451 Train Loss: 0.0948
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 10 Step 1 Train Loss: 0.1011
Epoch 10 Step 51 Train Loss: 0.0915
Epoch 10 Step 101 Train Loss: 0.0949
Epoch 10 Step 151 Train Loss: 0.0949
Epoch 10 Step 201 Train Loss: 0.1001
Epoch 10 Step 251 Train Loss: 0.0922
Epoch 10 Step 301 Train Loss: 0.0905
Epoch 10 Step 351 Train Loss: 0.0846
Epoch 10 Step 401 Train Loss: 0.1064
Epoch 10 Step 451 Train Loss: 0.0940
Epoch 10: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 11 Step 1 Train Loss: 0.1101
Epoch 11 Step 51 Train Loss: 0.0879
Epoch 11 Step 101 Train Loss: 0.0939
Epoch 11 Step 151 Train Loss: 0.0939
Epoch 11 Step 201 Train Loss: 0.0943
Epoch 11 Step 251 Train Loss: 0.0883
Epoch 11 Step 301 Train Loss: 0.0848
Epoch 11 Step 351 Train Loss: 0.1058
Epoch 11 Step 401 Train Loss: 0.0978
Epoch 11 Step 451 Train Loss: 0.0942
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 12 Step 1 Train Loss: 0.0826
Epoch 12 Step 51 Train Loss: 0.0892
Epoch 12 Step 101 Train Loss: 0.0994
Epoch 12 Step 151 Train Loss: 0.1138
Epoch 12 Step 201 Train Loss: 0.0929
Epoch 12 Step 251 Train Loss: 0.0894
Epoch 12 Step 301 Train Loss: 0.1090
Epoch 12 Step 351 Train Loss: 0.1088
Epoch 12 Step 401 Train Loss: 0.1099
Epoch 12 Step 451 Train Loss: 0.1089
Epoch 12: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 13 Step 1 Train Loss: 0.1060
Epoch 13 Step 51 Train Loss: 0.1055
Epoch 13 Step 101 Train Loss: 0.0847
Epoch 13 Step 151 Train Loss: 0.0920
Epoch 13 Step 201 Train Loss: 0.1126
Epoch 13 Step 251 Train Loss: 0.0890
Epoch 13 Step 301 Train Loss: 0.0988
Epoch 13 Step 351 Train Loss: 0.0947
Epoch 13 Step 401 Train Loss: 0.0911
Epoch 13 Step 451 Train Loss: 0.0926
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 14 Step 1 Train Loss: 0.0940
Epoch 14 Step 51 Train Loss: 0.1111
Epoch 14 Step 101 Train Loss: 0.0870
Epoch 14 Step 151 Train Loss: 0.0923
Epoch 14 Step 201 Train Loss: 0.1001
Epoch 14 Step 251 Train Loss: 0.1015
Epoch 14 Step 301 Train Loss: 0.1060
Epoch 14 Step 351 Train Loss: 0.0866
Epoch 14 Step 401 Train Loss: 0.0884
Epoch 14 Step 451 Train Loss: 0.0977
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Epoch 15 Step 1 Train Loss: 0.0869
Epoch 15 Step 51 Train Loss: 0.0920
Epoch 15 Step 101 Train Loss: 0.0909
Epoch 15 Step 151 Train Loss: 0.1042
Epoch 15 Step 201 Train Loss: 0.0923
Epoch 15 Step 251 Train Loss: 0.0835
Epoch 15 Step 301 Train Loss: 0.0931
Epoch 15 Step 351 Train Loss: 0.0930
Epoch 15 Step 401 Train Loss: 0.0851
Epoch 15 Step 451 Train Loss: 0.0924
Epoch 15: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0020. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0036
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0010722245
test_combo_seen0_pearson: 0.8840699720744337
test_combo_seen0_mse_de: 0.0033167256
test_combo_seen0_pearson_de: 0.410450988359395
test_combo_seen1_mse: 0.0010820398
test_combo_seen1_pearson: 0.886452853324601
test_combo_seen1_mse_de: 0.0038014716
test_combo_seen1_pearson_de: 0.45116775387261
test_combo_seen2_mse: 0.0015439149
test_combo_seen2_pearson: 0.8427816754314282
test_combo_seen2_mse_de: 0.0042158314
test_combo_seen2_pearson_de: 0.4012171131291287
test_unseen_single_mse: 3.698285e-05
test_unseen_single_pearson: 0.9952223921792844
test_unseen_single_mse_de: 3.5069155e-05
test_unseen_single_pearson_de: 0.6871128029978174
test_combo_seen0_pearson_delta: 0.00896127313142638
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.36666666666666664
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.4083333333333334
test_combo_seen0_mse_top20_de_non_dropout: 0.00380491
test_combo_seen1_pearson_delta: 0.014589718108372363
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.42207792207792216
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.40779220779220776
test_combo_seen1_mse_top20_de_non_dropout: 0.0075428933
test_combo_seen2_pearson_delta: 0.010098037899539562
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.31136363636363634
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.4545454545454545
test_combo_seen2_mse_top20_de_non_dropout: 0.008407843
test_unseen_single_pearson_delta: 0.118836754297945
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.32857142857142857
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6714285714285715
test_unseen_single_mse_top20_de_non_dropout: 3.3649663e-05
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.004 MB of 0.024 MB uploadedwandb: / 0.004 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÉ‚ñá‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÖ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÖ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÑ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.36667
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.40833
wandb:                                         test_combo_seen0_mse 0.00107
wandb:                                      test_combo_seen0_mse_de 0.00332
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.0038
wandb:                                     test_combo_seen0_pearson 0.88407
wandb:                                  test_combo_seen0_pearson_de 0.41045
wandb:                               test_combo_seen0_pearson_delta 0.00896
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.42208
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.40779
wandb:                                         test_combo_seen1_mse 0.00108
wandb:                                      test_combo_seen1_mse_de 0.0038
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.00754
wandb:                                     test_combo_seen1_pearson 0.88645
wandb:                                  test_combo_seen1_pearson_de 0.45117
wandb:                               test_combo_seen1_pearson_delta 0.01459
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.31136
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.45455
wandb:                                         test_combo_seen2_mse 0.00154
wandb:                                      test_combo_seen2_mse_de 0.00422
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.00841
wandb:                                     test_combo_seen2_pearson 0.84278
wandb:                                  test_combo_seen2_pearson_de 0.40122
wandb:                               test_combo_seen2_pearson_delta 0.0101
wandb:                                                  test_de_mse 0.00359
wandb:                                              test_de_pearson 0.44971
wandb:               test_frac_opposite_direction_top20_non_dropout 0.38911
wandb:                          test_frac_sigma_below_1_non_dropout 0.43105
wandb:                                                     test_mse 0.0011
wandb:                                test_mse_top20_de_non_dropout 0.00673
wandb:                                                 test_pearson 0.8845
wandb:                                           test_pearson_delta 0.01886
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.32857
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.67143
wandb:                                       test_unseen_single_mse 4e-05
wandb:                                    test_unseen_single_mse_de 4e-05
wandb:                  test_unseen_single_mse_top20_de_non_dropout 3e-05
wandb:                                   test_unseen_single_pearson 0.99522
wandb:                                test_unseen_single_pearson_de 0.68711
wandb:                             test_unseen_single_pearson_delta 0.11884
wandb:                                                 train_de_mse 0.00363
wandb:                                             train_de_pearson 0.39472
wandb:                                                    train_mse 0.00097
wandb:                                                train_pearson 0.89816
wandb:                                                training_loss 0.10126
wandb:                                                   val_de_mse 0.002
wandb:                                               val_de_pearson 0.41291
wandb:                                                      val_mse 0.00086
wandb:                                                  val_pearson 0.91225
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_day7neuron_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/ctubybsb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_031755-ctubybsb/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:18
combo_seen1:79
combo_seen2:21
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_033037-wvinwj6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_day7neuron_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/wvinwj6b
wandb: WARNING Serializing object of type ndarray that is 20545664 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1813
Epoch 1 Step 51 Train Loss: 0.1793
Epoch 1 Step 101 Train Loss: 0.1761
Epoch 1 Step 151 Train Loss: 0.1661
Epoch 1 Step 201 Train Loss: 0.1386
Epoch 1 Step 251 Train Loss: 0.1557
Epoch 1 Step 301 Train Loss: 0.1433
Epoch 1 Step 351 Train Loss: 0.1564
Epoch 1 Step 401 Train Loss: 0.1242
Epoch 1 Step 451 Train Loss: 0.1447
Epoch 1 Step 501 Train Loss: 0.1266
Epoch 1 Step 551 Train Loss: 0.1539
Epoch 1 Step 601 Train Loss: 0.1308
Epoch 1 Step 651 Train Loss: 0.1450
Epoch 1 Step 701 Train Loss: 0.1442
Epoch 1 Step 751 Train Loss: 0.1199
Epoch 1 Step 801 Train Loss: 0.1203
Epoch 1: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0042. 
Epoch 2 Step 1 Train Loss: 0.1068
Epoch 2 Step 51 Train Loss: 0.1124
Epoch 2 Step 101 Train Loss: 0.1409
Epoch 2 Step 151 Train Loss: 0.1187
Epoch 2 Step 201 Train Loss: 0.1135
Epoch 2 Step 251 Train Loss: 0.1007
Epoch 2 Step 301 Train Loss: 0.1103
Epoch 2 Step 351 Train Loss: 0.0875
Epoch 2 Step 401 Train Loss: 0.0936
Epoch 2 Step 451 Train Loss: 0.0935
Epoch 2 Step 501 Train Loss: 0.1024
Epoch 2 Step 551 Train Loss: 0.0945
Epoch 2 Step 601 Train Loss: 0.0829
Epoch 2 Step 651 Train Loss: 0.0794
Epoch 2 Step 701 Train Loss: 0.1017
Epoch 2 Step 751 Train Loss: 0.0908
Epoch 2 Step 801 Train Loss: 0.0798
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 3 Step 1 Train Loss: 0.0951
Epoch 3 Step 51 Train Loss: 0.0808
Epoch 3 Step 101 Train Loss: 0.1006
Epoch 3 Step 151 Train Loss: 0.0723
Epoch 3 Step 201 Train Loss: 0.0804
Epoch 3 Step 251 Train Loss: 0.0874
Epoch 3 Step 301 Train Loss: 0.0884
Epoch 3 Step 351 Train Loss: 0.0921
Epoch 3 Step 401 Train Loss: 0.0866
Epoch 3 Step 451 Train Loss: 0.0802
Epoch 3 Step 501 Train Loss: 0.0893
Epoch 3 Step 551 Train Loss: 0.1024
Epoch 3 Step 601 Train Loss: 0.1151
Epoch 3 Step 651 Train Loss: 0.0856
Epoch 3 Step 701 Train Loss: 0.0943
Epoch 3 Step 751 Train Loss: 0.0874
Epoch 3 Step 801 Train Loss: 0.0843
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 4 Step 1 Train Loss: 0.0743
Epoch 4 Step 51 Train Loss: 0.0835
Epoch 4 Step 101 Train Loss: 0.0799
Epoch 4 Step 151 Train Loss: 0.0938
Epoch 4 Step 201 Train Loss: 0.0818
Epoch 4 Step 251 Train Loss: 0.0964
Epoch 4 Step 301 Train Loss: 0.0991
Epoch 4 Step 351 Train Loss: 0.0949
Epoch 4 Step 401 Train Loss: 0.0808
Epoch 4 Step 451 Train Loss: 0.0933
Epoch 4 Step 501 Train Loss: 0.0807
Epoch 4 Step 551 Train Loss: 0.1038
Epoch 4 Step 601 Train Loss: 0.0816
Epoch 4 Step 651 Train Loss: 0.0858
Epoch 4 Step 701 Train Loss: 0.0888
Epoch 4 Step 751 Train Loss: 0.0932
Epoch 4 Step 801 Train Loss: 0.1028
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 5 Step 1 Train Loss: 0.0940
Epoch 5 Step 51 Train Loss: 0.0842
Epoch 5 Step 101 Train Loss: 0.0987
Epoch 5 Step 151 Train Loss: 0.0893
Epoch 5 Step 201 Train Loss: 0.0914
Epoch 5 Step 251 Train Loss: 0.0837
Epoch 5 Step 301 Train Loss: 0.0892
Epoch 5 Step 351 Train Loss: 0.1051
Epoch 5 Step 401 Train Loss: 0.0870
Epoch 5 Step 451 Train Loss: 0.0870
Epoch 5 Step 501 Train Loss: 0.0874
Epoch 5 Step 551 Train Loss: 0.1001
Epoch 5 Step 601 Train Loss: 0.0971
Epoch 5 Step 651 Train Loss: 0.1031
Epoch 5 Step 701 Train Loss: 0.1015
Epoch 5 Step 751 Train Loss: 0.0952
Epoch 5 Step 801 Train Loss: 0.0881
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 6 Step 1 Train Loss: 0.0769
Epoch 6 Step 51 Train Loss: 0.0773
Epoch 6 Step 101 Train Loss: 0.0860
Epoch 6 Step 151 Train Loss: 0.0985
Epoch 6 Step 201 Train Loss: 0.0888
Epoch 6 Step 251 Train Loss: 0.0907
Epoch 6 Step 301 Train Loss: 0.0875
Epoch 6 Step 351 Train Loss: 0.0938
Epoch 6 Step 401 Train Loss: 0.0802
Epoch 6 Step 451 Train Loss: 0.0901
Epoch 6 Step 501 Train Loss: 0.0876
Epoch 6 Step 551 Train Loss: 0.0931
Epoch 6 Step 601 Train Loss: 0.0794
Epoch 6 Step 651 Train Loss: 0.0807
Epoch 6 Step 701 Train Loss: 0.0894
Epoch 6 Step 751 Train Loss: 0.1029
Epoch 6 Step 801 Train Loss: 0.0984
Epoch 6: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 7 Step 1 Train Loss: 0.0916
Epoch 7 Step 51 Train Loss: 0.0940
Epoch 7 Step 101 Train Loss: 0.0841
Epoch 7 Step 151 Train Loss: 0.0765
Epoch 7 Step 201 Train Loss: 0.0860
Epoch 7 Step 251 Train Loss: 0.0995
Epoch 7 Step 301 Train Loss: 0.0891
Epoch 7 Step 351 Train Loss: 0.0986
Epoch 7 Step 401 Train Loss: 0.0892
Epoch 7 Step 451 Train Loss: 0.0878
Epoch 7 Step 501 Train Loss: 0.0804
Epoch 7 Step 551 Train Loss: 0.1083
Epoch 7 Step 601 Train Loss: 0.0923
Epoch 7 Step 651 Train Loss: 0.0831
Epoch 7 Step 701 Train Loss: 0.0907
Epoch 7 Step 751 Train Loss: 0.0967
Epoch 7 Step 801 Train Loss: 0.0922
Epoch 7: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 8 Step 1 Train Loss: 0.0942
Epoch 8 Step 51 Train Loss: 0.0918
Epoch 8 Step 101 Train Loss: 0.0865
Epoch 8 Step 151 Train Loss: 0.0825
Epoch 8 Step 201 Train Loss: 0.0959
Epoch 8 Step 251 Train Loss: 0.0941
Epoch 8 Step 301 Train Loss: 0.0870
Epoch 8 Step 351 Train Loss: 0.0882
Epoch 8 Step 401 Train Loss: 0.0953
Epoch 8 Step 451 Train Loss: 0.0913
Epoch 8 Step 501 Train Loss: 0.1139
Epoch 8 Step 551 Train Loss: 0.0828
Epoch 8 Step 601 Train Loss: 0.1048
Epoch 8 Step 651 Train Loss: 0.0928
Epoch 8 Step 701 Train Loss: 0.0921
Epoch 8 Step 751 Train Loss: 0.0857
Epoch 8 Step 801 Train Loss: 0.0877
Epoch 8: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 9 Step 1 Train Loss: 0.0925
Epoch 9 Step 51 Train Loss: 0.0898
Epoch 9 Step 101 Train Loss: 0.1080
Epoch 9 Step 151 Train Loss: 0.1085
Epoch 9 Step 201 Train Loss: 0.0999
Epoch 9 Step 251 Train Loss: 0.0848
Epoch 9 Step 301 Train Loss: 0.1003
Epoch 9 Step 351 Train Loss: 0.1016
Epoch 9 Step 401 Train Loss: 0.1029
Epoch 9 Step 451 Train Loss: 0.0943
Epoch 9 Step 501 Train Loss: 0.0912
Epoch 9 Step 551 Train Loss: 0.1017
Epoch 9 Step 601 Train Loss: 0.0936
Epoch 9 Step 651 Train Loss: 0.1130
Epoch 9 Step 701 Train Loss: 0.0811
Epoch 9 Step 751 Train Loss: 0.0985
Epoch 9 Step 801 Train Loss: 0.0941
Epoch 9: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 10 Step 1 Train Loss: 0.1072
Epoch 10 Step 51 Train Loss: 0.0904
Epoch 10 Step 101 Train Loss: 0.1007
Epoch 10 Step 151 Train Loss: 0.0985
Epoch 10 Step 201 Train Loss: 0.0975
Epoch 10 Step 251 Train Loss: 0.0988
Epoch 10 Step 301 Train Loss: 0.0989
Epoch 10 Step 351 Train Loss: 0.0897
Epoch 10 Step 401 Train Loss: 0.0833
Epoch 10 Step 451 Train Loss: 0.0939
Epoch 10 Step 501 Train Loss: 0.0947
Epoch 10 Step 551 Train Loss: 0.0980
Epoch 10 Step 601 Train Loss: 0.0951
Epoch 10 Step 651 Train Loss: 0.0950
Epoch 10 Step 701 Train Loss: 0.1087
Epoch 10 Step 751 Train Loss: 0.0980
Epoch 10 Step 801 Train Loss: 0.0912
Epoch 10: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 11 Step 1 Train Loss: 0.0842
Epoch 11 Step 51 Train Loss: 0.1016
Epoch 11 Step 101 Train Loss: 0.0880
Epoch 11 Step 151 Train Loss: 0.1130
Epoch 11 Step 201 Train Loss: 0.0923
Epoch 11 Step 251 Train Loss: 0.0813
Epoch 11 Step 301 Train Loss: 0.1037
Epoch 11 Step 351 Train Loss: 0.0893
Epoch 11 Step 401 Train Loss: 0.1059
Epoch 11 Step 451 Train Loss: 0.0979
Epoch 11 Step 501 Train Loss: 0.0949
Epoch 11 Step 551 Train Loss: 0.0997
Epoch 11 Step 601 Train Loss: 0.0985
Epoch 11 Step 651 Train Loss: 0.0825
Epoch 11 Step 701 Train Loss: 0.1015
Epoch 11 Step 751 Train Loss: 0.0892
Epoch 11 Step 801 Train Loss: 0.0947
Epoch 11: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 12 Step 1 Train Loss: 0.1037
Epoch 12 Step 51 Train Loss: 0.0957
Epoch 12 Step 101 Train Loss: 0.0917
Epoch 12 Step 151 Train Loss: 0.0998
Epoch 12 Step 201 Train Loss: 0.0860
Epoch 12 Step 251 Train Loss: 0.0899
Epoch 12 Step 301 Train Loss: 0.1044
Epoch 12 Step 351 Train Loss: 0.1053
Epoch 12 Step 401 Train Loss: 0.0980
Epoch 12 Step 451 Train Loss: 0.1031
Epoch 12 Step 501 Train Loss: 0.0969
Epoch 12 Step 551 Train Loss: 0.0869
Epoch 12 Step 601 Train Loss: 0.0957
Epoch 12 Step 651 Train Loss: 0.1108
Epoch 12 Step 701 Train Loss: 0.0961
Epoch 12 Step 751 Train Loss: 0.0988
Epoch 12 Step 801 Train Loss: 0.1002
Epoch 12: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 13 Step 1 Train Loss: 0.0963
Epoch 13 Step 51 Train Loss: 0.0942
Epoch 13 Step 101 Train Loss: 0.1153
Epoch 13 Step 151 Train Loss: 0.0943
Epoch 13 Step 201 Train Loss: 0.1064
Epoch 13 Step 251 Train Loss: 0.0899
Epoch 13 Step 301 Train Loss: 0.0960
Epoch 13 Step 351 Train Loss: 0.1121
Epoch 13 Step 401 Train Loss: 0.1037
Epoch 13 Step 451 Train Loss: 0.0940
Epoch 13 Step 501 Train Loss: 0.0882
Epoch 13 Step 551 Train Loss: 0.1123
Epoch 13 Step 601 Train Loss: 0.1041
Epoch 13 Step 651 Train Loss: 0.0935
Epoch 13 Step 701 Train Loss: 0.0959
Epoch 13 Step 751 Train Loss: 0.0981
Epoch 13 Step 801 Train Loss: 0.1002
Epoch 13: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 14 Step 1 Train Loss: 0.1044
Epoch 14 Step 51 Train Loss: 0.0899
Epoch 14 Step 101 Train Loss: 0.0943
Epoch 14 Step 151 Train Loss: 0.0803
Epoch 14 Step 201 Train Loss: 0.1018
Epoch 14 Step 251 Train Loss: 0.0931
Epoch 14 Step 301 Train Loss: 0.0857
Epoch 14 Step 351 Train Loss: 0.0998
Epoch 14 Step 401 Train Loss: 0.0840
Epoch 14 Step 451 Train Loss: 0.0877
Epoch 14 Step 501 Train Loss: 0.1037
Epoch 14 Step 551 Train Loss: 0.1077
Epoch 14 Step 601 Train Loss: 0.0939
Epoch 14 Step 651 Train Loss: 0.0861
Epoch 14 Step 701 Train Loss: 0.1070
Epoch 14 Step 751 Train Loss: 0.1046
Epoch 14 Step 801 Train Loss: 0.0974
Epoch 14: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Epoch 15 Step 1 Train Loss: 0.0859
Epoch 15 Step 51 Train Loss: 0.0879
Epoch 15 Step 101 Train Loss: 0.0993
Epoch 15 Step 151 Train Loss: 0.0948
Epoch 15 Step 201 Train Loss: 0.0868
Epoch 15 Step 251 Train Loss: 0.1011
Epoch 15 Step 301 Train Loss: 0.0903
Epoch 15 Step 351 Train Loss: 0.1072
Epoch 15 Step 401 Train Loss: 0.1133
Epoch 15 Step 451 Train Loss: 0.1082
Epoch 15 Step 501 Train Loss: 0.1050
Epoch 15 Step 551 Train Loss: 0.0950
Epoch 15 Step 601 Train Loss: 0.1049
Epoch 15 Step 651 Train Loss: 0.0957
Epoch 15 Step 701 Train Loss: 0.1014
Epoch 15 Step 751 Train Loss: 0.1025
Epoch 15 Step 801 Train Loss: 0.1083
Epoch 15: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0039. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0035
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0014226296
test_combo_seen0_pearson: 0.8579640039002561
test_combo_seen0_mse_de: 0.0061226147
test_combo_seen0_pearson_de: 0.31863208626989237
test_combo_seen1_mse: 0.0011619603
test_combo_seen1_pearson: 0.8781589778438593
test_combo_seen1_mse_de: 0.0030984743
test_combo_seen1_pearson_de: 0.41240964349757414
test_combo_seen2_mse: 0.0011922175
test_combo_seen2_pearson: 0.8766642900346132
test_combo_seen2_mse_de: 0.0038835034
test_combo_seen2_pearson_de: 0.347445786554896
test_unseen_single_mse: 6.3076666e-05
test_unseen_single_pearson: 0.9918714146679619
test_unseen_single_mse_de: 0.00013282857
test_unseen_single_pearson_de: 0.6693761559483752
test_combo_seen0_pearson_delta: 0.0277472071300622
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.39722222222222225
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.39166666666666666
test_combo_seen0_mse_top20_de_non_dropout: 0.0128021985
test_combo_seen1_pearson_delta: 0.014684390351712899
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.38734177215189874
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.4
test_combo_seen1_mse_top20_de_non_dropout: 0.0056415517
test_combo_seen2_pearson_delta: 0.00424204676049106
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.38095238095238093
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.4142857142857143
test_combo_seen2_mse_top20_de_non_dropout: 0.0077705095
test_unseen_single_pearson_delta: 0.11791212708035356
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.34285714285714286
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6714285714285715
test_unseen_single_mse_top20_de_non_dropout: 0.00013228612
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.004 MB of 0.028 MB uploadedwandb: / 0.004 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñá‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.39722
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.39167
wandb:                                         test_combo_seen0_mse 0.00142
wandb:                                      test_combo_seen0_mse_de 0.00612
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.0128
wandb:                                     test_combo_seen0_pearson 0.85796
wandb:                                  test_combo_seen0_pearson_de 0.31863
wandb:                               test_combo_seen0_pearson_delta 0.02775
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.38734
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.4
wandb:                                         test_combo_seen1_mse 0.00116
wandb:                                      test_combo_seen1_mse_de 0.0031
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.00564
wandb:                                     test_combo_seen1_pearson 0.87816
wandb:                                  test_combo_seen1_pearson_de 0.41241
wandb:                               test_combo_seen1_pearson_delta 0.01468
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.38095
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.41429
wandb:                                         test_combo_seen2_mse 0.00119
wandb:                                      test_combo_seen2_mse_de 0.00388
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.00777
wandb:                                     test_combo_seen2_pearson 0.87666
wandb:                                  test_combo_seen2_pearson_de 0.34745
wandb:                               test_combo_seen2_pearson_delta 0.00424
wandb:                                                  test_de_mse 0.0035
wandb:                                              test_de_pearson 0.40238
wandb:               test_frac_opposite_direction_top20_non_dropout 0.3852
wandb:                          test_frac_sigma_below_1_non_dropout 0.4164
wandb:                                                     test_mse 0.00114
wandb:                                test_mse_top20_de_non_dropout 0.00672
wandb:                                                 test_pearson 0.88137
wandb:                                           test_pearson_delta 0.02059
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.34286
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.67143
wandb:                                       test_unseen_single_mse 6e-05
wandb:                                    test_unseen_single_mse_de 0.00013
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00013
wandb:                                   test_unseen_single_pearson 0.99187
wandb:                                test_unseen_single_pearson_de 0.66938
wandb:                             test_unseen_single_pearson_delta 0.11791
wandb:                                                 train_de_mse 0.00303
wandb:                                             train_de_pearson 0.4883
wandb:                                                    train_mse 0.00075
wandb:                                                train_pearson 0.91929
wandb:                                                training_loss 0.10631
wandb:                                                   val_de_mse 0.00393
wandb:                                               val_de_pearson 0.31994
wandb:                                                      val_mse 0.00142
wandb:                                                  val_pearson 0.85734
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_day7neuron_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/wvinwj6b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_033037-wvinwj6b/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:15
combo_seen1:81
combo_seen2:21
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_034804-2qbra5z6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_day7neuron_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/2qbra5z6
wandb: WARNING Serializing object of type ndarray that is 20545664 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1802
Epoch 1 Step 51 Train Loss: 0.1812
Epoch 1 Step 101 Train Loss: 0.1632
Epoch 1 Step 151 Train Loss: 0.1751
Epoch 1 Step 201 Train Loss: 0.1486
Epoch 1 Step 251 Train Loss: 0.1585
Epoch 1 Step 301 Train Loss: 0.1590
Epoch 1 Step 351 Train Loss: 0.1620
Epoch 1 Step 401 Train Loss: 0.1611
Epoch 1 Step 451 Train Loss: 0.1484
Epoch 1 Step 501 Train Loss: 0.1638
Epoch 1 Step 551 Train Loss: 0.1473
Epoch 1 Step 601 Train Loss: 0.1612
Epoch 1: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0051. 
Epoch 2 Step 1 Train Loss: 0.1664
Epoch 2 Step 51 Train Loss: 0.1503
Epoch 2 Step 101 Train Loss: 0.1300
Epoch 2 Step 151 Train Loss: 0.1245
Epoch 2 Step 201 Train Loss: 0.1351
Epoch 2 Step 251 Train Loss: 0.1362
Epoch 2 Step 301 Train Loss: 0.1239
Epoch 2 Step 351 Train Loss: 0.1158
Epoch 2 Step 401 Train Loss: 0.1142
Epoch 2 Step 451 Train Loss: 0.1330
Epoch 2 Step 501 Train Loss: 0.1188
Epoch 2 Step 551 Train Loss: 0.0999
Epoch 2 Step 601 Train Loss: 0.1353
Epoch 2: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0053. 
Epoch 3 Step 1 Train Loss: 0.1059
Epoch 3 Step 51 Train Loss: 0.1074
Epoch 3 Step 101 Train Loss: 0.0969
Epoch 3 Step 151 Train Loss: 0.1055
Epoch 3 Step 201 Train Loss: 0.0935
Epoch 3 Step 251 Train Loss: 0.0821
Epoch 3 Step 301 Train Loss: 0.0972
Epoch 3 Step 351 Train Loss: 0.0925
Epoch 3 Step 401 Train Loss: 0.0967
Epoch 3 Step 451 Train Loss: 0.0983
Epoch 3 Step 501 Train Loss: 0.0823
Epoch 3 Step 551 Train Loss: 0.0844
Epoch 3 Step 601 Train Loss: 0.0837
Epoch 3: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 4 Step 1 Train Loss: 0.0855
Epoch 4 Step 51 Train Loss: 0.0886
Epoch 4 Step 101 Train Loss: 0.0917
Epoch 4 Step 151 Train Loss: 0.1011
Epoch 4 Step 201 Train Loss: 0.1071
Epoch 4 Step 251 Train Loss: 0.0977
Epoch 4 Step 301 Train Loss: 0.0933
Epoch 4 Step 351 Train Loss: 0.0816
Epoch 4 Step 401 Train Loss: 0.0748
Epoch 4 Step 451 Train Loss: 0.0817
Epoch 4 Step 501 Train Loss: 0.1021
Epoch 4 Step 551 Train Loss: 0.0843
Epoch 4 Step 601 Train Loss: 0.0898
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 5 Step 1 Train Loss: 0.0940
Epoch 5 Step 51 Train Loss: 0.0899
Epoch 5 Step 101 Train Loss: 0.1015
Epoch 5 Step 151 Train Loss: 0.0821
Epoch 5 Step 201 Train Loss: 0.0832
Epoch 5 Step 251 Train Loss: 0.0959
Epoch 5 Step 301 Train Loss: 0.1027
Epoch 5 Step 351 Train Loss: 0.0835
Epoch 5 Step 401 Train Loss: 0.0886
Epoch 5 Step 451 Train Loss: 0.0863
Epoch 5 Step 501 Train Loss: 0.0824
Epoch 5 Step 551 Train Loss: 0.0990
Epoch 5 Step 601 Train Loss: 0.0931
Epoch 5: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 6 Step 1 Train Loss: 0.0914
Epoch 6 Step 51 Train Loss: 0.0920
Epoch 6 Step 101 Train Loss: 0.0976
Epoch 6 Step 151 Train Loss: 0.0835
Epoch 6 Step 201 Train Loss: 0.0769
Epoch 6 Step 251 Train Loss: 0.0884
Epoch 6 Step 301 Train Loss: 0.1044
Epoch 6 Step 351 Train Loss: 0.0889
Epoch 6 Step 401 Train Loss: 0.1079
Epoch 6 Step 451 Train Loss: 0.0914
Epoch 6 Step 501 Train Loss: 0.0888
Epoch 6 Step 551 Train Loss: 0.0838
Epoch 6 Step 601 Train Loss: 0.1223
Epoch 6: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 7 Step 1 Train Loss: 0.0853
Epoch 7 Step 51 Train Loss: 0.0871
Epoch 7 Step 101 Train Loss: 0.0944
Epoch 7 Step 151 Train Loss: 0.0977
Epoch 7 Step 201 Train Loss: 0.0759
Epoch 7 Step 251 Train Loss: 0.0976
Epoch 7 Step 301 Train Loss: 0.0878
Epoch 7 Step 351 Train Loss: 0.0883
Epoch 7 Step 401 Train Loss: 0.0891
Epoch 7 Step 451 Train Loss: 0.1112
Epoch 7 Step 501 Train Loss: 0.1079
Epoch 7 Step 551 Train Loss: 0.0901
Epoch 7 Step 601 Train Loss: 0.0997
Epoch 7: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 8 Step 1 Train Loss: 0.1062
Epoch 8 Step 51 Train Loss: 0.1069
Epoch 8 Step 101 Train Loss: 0.1049
Epoch 8 Step 151 Train Loss: 0.1050
Epoch 8 Step 201 Train Loss: 0.0894
Epoch 8 Step 251 Train Loss: 0.1005
Epoch 8 Step 301 Train Loss: 0.0906
Epoch 8 Step 351 Train Loss: 0.0888
Epoch 8 Step 401 Train Loss: 0.0963
Epoch 8 Step 451 Train Loss: 0.1094
Epoch 8 Step 501 Train Loss: 0.1063
Epoch 8 Step 551 Train Loss: 0.0973
Epoch 8 Step 601 Train Loss: 0.0902
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 9 Step 1 Train Loss: 0.1044
Epoch 9 Step 51 Train Loss: 0.0913
Epoch 9 Step 101 Train Loss: 0.1060
Epoch 9 Step 151 Train Loss: 0.0857
Epoch 9 Step 201 Train Loss: 0.0871
Epoch 9 Step 251 Train Loss: 0.1021
Epoch 9 Step 301 Train Loss: 0.0926
Epoch 9 Step 351 Train Loss: 0.1228
Epoch 9 Step 401 Train Loss: 0.1113
Epoch 9 Step 451 Train Loss: 0.1043
Epoch 9 Step 501 Train Loss: 0.0942
Epoch 9 Step 551 Train Loss: 0.0984
Epoch 9 Step 601 Train Loss: 0.0911
Epoch 9: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0047. 
Epoch 10 Step 1 Train Loss: 0.0900
Epoch 10 Step 51 Train Loss: 0.1000
Epoch 10 Step 101 Train Loss: 0.0919
Epoch 10 Step 151 Train Loss: 0.0940
Epoch 10 Step 201 Train Loss: 0.1044
Epoch 10 Step 251 Train Loss: 0.0887
Epoch 10 Step 301 Train Loss: 0.0909
Epoch 10 Step 351 Train Loss: 0.0916
Epoch 10 Step 401 Train Loss: 0.0816
Epoch 10 Step 451 Train Loss: 0.0886
Epoch 10 Step 501 Train Loss: 0.0848
Epoch 10 Step 551 Train Loss: 0.0961
Epoch 10 Step 601 Train Loss: 0.0874
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 11 Step 1 Train Loss: 0.1025
Epoch 11 Step 51 Train Loss: 0.0887
Epoch 11 Step 101 Train Loss: 0.0927
Epoch 11 Step 151 Train Loss: 0.0914
Epoch 11 Step 201 Train Loss: 0.0861
Epoch 11 Step 251 Train Loss: 0.0902
Epoch 11 Step 301 Train Loss: 0.0811
Epoch 11 Step 351 Train Loss: 0.1038
Epoch 11 Step 401 Train Loss: 0.1201
Epoch 11 Step 451 Train Loss: 0.0937
Epoch 11 Step 501 Train Loss: 0.0966
Epoch 11 Step 551 Train Loss: 0.0826
Epoch 11 Step 601 Train Loss: 0.0921
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 12 Step 1 Train Loss: 0.0922
Epoch 12 Step 51 Train Loss: 0.1131
Epoch 12 Step 101 Train Loss: 0.0955
Epoch 12 Step 151 Train Loss: 0.0833
Epoch 12 Step 201 Train Loss: 0.0935
Epoch 12 Step 251 Train Loss: 0.0935
Epoch 12 Step 301 Train Loss: 0.0990
Epoch 12 Step 351 Train Loss: 0.0926
Epoch 12 Step 401 Train Loss: 0.0952
Epoch 12 Step 451 Train Loss: 0.0927
Epoch 12 Step 501 Train Loss: 0.1021
Epoch 12 Step 551 Train Loss: 0.0959
Epoch 12 Step 601 Train Loss: 0.0961
Epoch 12: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 13 Step 1 Train Loss: 0.1018
Epoch 13 Step 51 Train Loss: 0.0826
Epoch 13 Step 101 Train Loss: 0.1014
Epoch 13 Step 151 Train Loss: 0.0948
Epoch 13 Step 201 Train Loss: 0.0875
Epoch 13 Step 251 Train Loss: 0.0857
Epoch 13 Step 301 Train Loss: 0.0808
Epoch 13 Step 351 Train Loss: 0.0973
Epoch 13 Step 401 Train Loss: 0.1172
Epoch 13 Step 451 Train Loss: 0.0895
Epoch 13 Step 501 Train Loss: 0.1131
Epoch 13 Step 551 Train Loss: 0.0895
Epoch 13 Step 601 Train Loss: 0.0968
Epoch 13: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 14 Step 1 Train Loss: 0.0922
Epoch 14 Step 51 Train Loss: 0.1103
Epoch 14 Step 101 Train Loss: 0.0871
Epoch 14 Step 151 Train Loss: 0.1129
Epoch 14 Step 201 Train Loss: 0.0795
Epoch 14 Step 251 Train Loss: 0.1053
Epoch 14 Step 301 Train Loss: 0.0964
Epoch 14 Step 351 Train Loss: 0.0853
Epoch 14 Step 401 Train Loss: 0.0858
Epoch 14 Step 451 Train Loss: 0.0928
Epoch 14 Step 501 Train Loss: 0.1036
Epoch 14 Step 551 Train Loss: 0.0899
Epoch 14 Step 601 Train Loss: 0.1105
Epoch 14: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Epoch 15 Step 1 Train Loss: 0.0898
Epoch 15 Step 51 Train Loss: 0.0983
Epoch 15 Step 101 Train Loss: 0.0912
Epoch 15 Step 151 Train Loss: 0.1160
Epoch 15 Step 201 Train Loss: 0.0899
Epoch 15 Step 251 Train Loss: 0.1001
Epoch 15 Step 301 Train Loss: 0.1069
Epoch 15 Step 351 Train Loss: 0.1079
Epoch 15 Step 401 Train Loss: 0.0978
Epoch 15 Step 451 Train Loss: 0.0900
Epoch 15 Step 501 Train Loss: 0.0957
Epoch 15 Step 551 Train Loss: 0.0852
Epoch 15 Step 601 Train Loss: 0.0983
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0047. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0031
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0010298907
test_combo_seen0_pearson: 0.8905601874385688
test_combo_seen0_mse_de: 0.003905076
test_combo_seen0_pearson_de: 0.3761624986339693
test_combo_seen1_mse: 0.0010254614
test_combo_seen1_pearson: 0.8913284263260545
test_combo_seen1_mse_de: 0.003352418
test_combo_seen1_pearson_de: 0.41217495488787614
test_combo_seen2_mse: 0.0011296807
test_combo_seen2_pearson: 0.8799927550697553
test_combo_seen2_mse_de: 0.002676791
test_combo_seen2_pearson_de: 0.5868273121866603
test_unseen_single_mse: 8.6127664e-05
test_unseen_single_pearson: 0.9892966482915707
test_unseen_single_mse_de: 0.00021594086
test_unseen_single_pearson_de: 0.6810249871235603
test_combo_seen0_pearson_delta: -0.005588621834586625
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.3966666666666666
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.4633333333333333
test_combo_seen0_mse_top20_de_non_dropout: 0.009440311
test_combo_seen1_pearson_delta: 0.014902961904329392
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.3962962962962963
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.4018518518518518
test_combo_seen1_mse_top20_de_non_dropout: 0.0057938383
test_combo_seen2_pearson_delta: 0.011798502462086186
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.3452380952380952
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.47380952380952385
test_combo_seen2_mse_top20_de_non_dropout: 0.004506577
test_unseen_single_pearson_delta: 0.09200384184209819
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4357142857142858
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5285714285714286
test_unseen_single_mse_top20_de_non_dropout: 0.00022587576
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.026 MB uploadedwandb: | 0.009 MB of 0.026 MB uploadedwandb: / 0.009 MB of 0.026 MB uploadedwandb: - 0.026 MB of 0.026 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:                                                   val_de_mse ‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÜ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÖ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.39667
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.46333
wandb:                                         test_combo_seen0_mse 0.00103
wandb:                                      test_combo_seen0_mse_de 0.00391
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.00944
wandb:                                     test_combo_seen0_pearson 0.89056
wandb:                                  test_combo_seen0_pearson_de 0.37616
wandb:                               test_combo_seen0_pearson_delta -0.00559
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.3963
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.40185
wandb:                                         test_combo_seen1_mse 0.00103
wandb:                                      test_combo_seen1_mse_de 0.00335
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.00579
wandb:                                     test_combo_seen1_pearson 0.89133
wandb:                                  test_combo_seen1_pearson_de 0.41217
wandb:                               test_combo_seen1_pearson_delta 0.0149
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.34524
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.47381
wandb:                                         test_combo_seen2_mse 0.00113
wandb:                                      test_combo_seen2_mse_de 0.00268
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.00451
wandb:                                     test_combo_seen2_pearson 0.87999
wandb:                                  test_combo_seen2_pearson_de 0.58683
wandb:                               test_combo_seen2_pearson_delta 0.0118
wandb:                                                  test_de_mse 0.00313
wandb:                                              test_de_pearson 0.45257
wandb:               test_frac_opposite_direction_top20_non_dropout 0.38992
wandb:                          test_frac_sigma_below_1_non_dropout 0.42863
wandb:                                                     test_mse 0.00099
wandb:                                test_mse_top20_de_non_dropout 0.0057
wandb:                                                 test_pearson 0.89485
wandb:                                           test_pearson_delta 0.01625
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.43571
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.52857
wandb:                                       test_unseen_single_mse 9e-05
wandb:                                    test_unseen_single_mse_de 0.00022
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00023
wandb:                                   test_unseen_single_pearson 0.9893
wandb:                                test_unseen_single_pearson_de 0.68102
wandb:                             test_unseen_single_pearson_delta 0.092
wandb:                                                 train_de_mse 0.00353
wandb:                                             train_de_pearson 0.45056
wandb:                                                    train_mse 0.00107
wandb:                                                train_pearson 0.89182
wandb:                                                training_loss 0.08428
wandb:                                                   val_de_mse 0.00472
wandb:                                               val_de_pearson 0.27848
wandb:                                                      val_mse 0.00122
wandb:                                                  val_pearson 0.87184
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_day7neuron_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/2qbra5z6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_034804-2qbra5z6/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:13
combo_seen1:75
combo_seen2:23
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_040231-fqkiy8xl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_day7neuron_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/fqkiy8xl
wandb: WARNING Serializing object of type ndarray that is 20545664 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1765
Epoch 1 Step 51 Train Loss: 0.1890
Epoch 1 Step 101 Train Loss: 0.1486
Epoch 1 Step 151 Train Loss: 0.1453
Epoch 1 Step 201 Train Loss: 0.1475
Epoch 1 Step 251 Train Loss: 0.1534
Epoch 1 Step 301 Train Loss: 0.1493
Epoch 1 Step 351 Train Loss: 0.1353
Epoch 1 Step 401 Train Loss: 0.1336
Epoch 1 Step 451 Train Loss: 0.1184
Epoch 1 Step 501 Train Loss: 0.1333
Epoch 1 Step 551 Train Loss: 0.1417
Epoch 1: Train Overall MSE: 0.0957 Validation Overall MSE: 0.1219. 
Train Top 20 DE MSE: 0.1855 Validation Top 20 DE MSE: 0.2216. 
Epoch 2 Step 1 Train Loss: 0.1329
Epoch 2 Step 51 Train Loss: 0.1274
Epoch 2 Step 101 Train Loss: 0.1315
Epoch 2 Step 151 Train Loss: 0.1179
Epoch 2 Step 201 Train Loss: 0.1294
Epoch 2 Step 251 Train Loss: 0.1347
Epoch 2 Step 301 Train Loss: 0.1348
Epoch 2 Step 351 Train Loss: 0.1188
Epoch 2 Step 401 Train Loss: 0.1331
Epoch 2 Step 451 Train Loss: 0.1176
Epoch 2 Step 501 Train Loss: 0.1075
Epoch 2 Step 551 Train Loss: 0.1154
Epoch 2: Train Overall MSE: 0.0060 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0152 Validation Top 20 DE MSE: 0.0178. 
Epoch 3 Step 1 Train Loss: 0.1249
Epoch 3 Step 51 Train Loss: 0.1143
Epoch 3 Step 101 Train Loss: 0.1246
Epoch 3 Step 151 Train Loss: 0.1283
Epoch 3 Step 201 Train Loss: 0.1014
Epoch 3 Step 251 Train Loss: 0.1420
Epoch 3 Step 301 Train Loss: 0.0935
Epoch 3 Step 351 Train Loss: 0.1101
Epoch 3 Step 401 Train Loss: 0.1000
Epoch 3 Step 451 Train Loss: 0.0983
Epoch 3 Step 501 Train Loss: 0.0906
Epoch 3 Step 551 Train Loss: 0.0810
Epoch 3: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0047 Validation Top 20 DE MSE: 0.0061. 
Epoch 4 Step 1 Train Loss: 0.0954
Epoch 4 Step 51 Train Loss: 0.0797
Epoch 4 Step 101 Train Loss: 0.1031
Epoch 4 Step 151 Train Loss: 0.0929
Epoch 4 Step 201 Train Loss: 0.0895
Epoch 4 Step 251 Train Loss: 0.0951
Epoch 4 Step 301 Train Loss: 0.0950
Epoch 4 Step 351 Train Loss: 0.0905
Epoch 4 Step 401 Train Loss: 0.0911
Epoch 4 Step 451 Train Loss: 0.0829
Epoch 4 Step 501 Train Loss: 0.0924
Epoch 4 Step 551 Train Loss: 0.0996
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0038. 
Epoch 5 Step 1 Train Loss: 0.0959
Epoch 5 Step 51 Train Loss: 0.0989
Epoch 5 Step 101 Train Loss: 0.0910
Epoch 5 Step 151 Train Loss: 0.0964
Epoch 5 Step 201 Train Loss: 0.0997
Epoch 5 Step 251 Train Loss: 0.0880
Epoch 5 Step 301 Train Loss: 0.1075
Epoch 5 Step 351 Train Loss: 0.0768
Epoch 5 Step 401 Train Loss: 0.0821
Epoch 5 Step 451 Train Loss: 0.0927
Epoch 5 Step 501 Train Loss: 0.0776
Epoch 5 Step 551 Train Loss: 0.0974
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0036. 
Epoch 6 Step 1 Train Loss: 0.0869
Epoch 6 Step 51 Train Loss: 0.0916
Epoch 6 Step 101 Train Loss: 0.0848
Epoch 6 Step 151 Train Loss: 0.1127
Epoch 6 Step 201 Train Loss: 0.1007
Epoch 6 Step 251 Train Loss: 0.0969
Epoch 6 Step 301 Train Loss: 0.0965
Epoch 6 Step 351 Train Loss: 0.0959
Epoch 6 Step 401 Train Loss: 0.0954
Epoch 6 Step 451 Train Loss: 0.1098
Epoch 6 Step 501 Train Loss: 0.1005
Epoch 6 Step 551 Train Loss: 0.0990
Epoch 6: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 7 Step 1 Train Loss: 0.0822
Epoch 7 Step 51 Train Loss: 0.0988
Epoch 7 Step 101 Train Loss: 0.0995
Epoch 7 Step 151 Train Loss: 0.0990
Epoch 7 Step 201 Train Loss: 0.1012
Epoch 7 Step 251 Train Loss: 0.0970
Epoch 7 Step 301 Train Loss: 0.0913
Epoch 7 Step 351 Train Loss: 0.0889
Epoch 7 Step 401 Train Loss: 0.0949
Epoch 7 Step 451 Train Loss: 0.0909
Epoch 7 Step 501 Train Loss: 0.0976
Epoch 7 Step 551 Train Loss: 0.0862
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 8 Step 1 Train Loss: 0.1057
Epoch 8 Step 51 Train Loss: 0.1039
Epoch 8 Step 101 Train Loss: 0.0969
Epoch 8 Step 151 Train Loss: 0.0916
Epoch 8 Step 201 Train Loss: 0.0999
Epoch 8 Step 251 Train Loss: 0.0876
Epoch 8 Step 301 Train Loss: 0.0911
Epoch 8 Step 351 Train Loss: 0.0984
Epoch 8 Step 401 Train Loss: 0.0986
Epoch 8 Step 451 Train Loss: 0.1173
Epoch 8 Step 501 Train Loss: 0.0946
Epoch 8 Step 551 Train Loss: 0.0956
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 9 Step 1 Train Loss: 0.1019
Epoch 9 Step 51 Train Loss: 0.0876
Epoch 9 Step 101 Train Loss: 0.1131
Epoch 9 Step 151 Train Loss: 0.1004
Epoch 9 Step 201 Train Loss: 0.1107
Epoch 9 Step 251 Train Loss: 0.0974
Epoch 9 Step 301 Train Loss: 0.0868
Epoch 9 Step 351 Train Loss: 0.1012
Epoch 9 Step 401 Train Loss: 0.0918
Epoch 9 Step 451 Train Loss: 0.1012
Epoch 9 Step 501 Train Loss: 0.1151
Epoch 9 Step 551 Train Loss: 0.0978
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 10 Step 1 Train Loss: 0.0961
Epoch 10 Step 51 Train Loss: 0.1011
Epoch 10 Step 101 Train Loss: 0.0930
Epoch 10 Step 151 Train Loss: 0.0961
Epoch 10 Step 201 Train Loss: 0.0988
Epoch 10 Step 251 Train Loss: 0.0904
Epoch 10 Step 301 Train Loss: 0.0891
Epoch 10 Step 351 Train Loss: 0.1006
Epoch 10 Step 401 Train Loss: 0.0890
Epoch 10 Step 451 Train Loss: 0.0885
Epoch 10 Step 501 Train Loss: 0.1087
Epoch 10 Step 551 Train Loss: 0.0871
Epoch 10: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0037. 
Epoch 11 Step 1 Train Loss: 0.0907
Epoch 11 Step 51 Train Loss: 0.0883
Epoch 11 Step 101 Train Loss: 0.0928
Epoch 11 Step 151 Train Loss: 0.1014
Epoch 11 Step 201 Train Loss: 0.0950
Epoch 11 Step 251 Train Loss: 0.0981
Epoch 11 Step 301 Train Loss: 0.1041
Epoch 11 Step 351 Train Loss: 0.0906
Epoch 11 Step 401 Train Loss: 0.0979
Epoch 11 Step 451 Train Loss: 0.0996
Epoch 11 Step 501 Train Loss: 0.1015
Epoch 11 Step 551 Train Loss: 0.0963
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 12 Step 1 Train Loss: 0.1002
Epoch 12 Step 51 Train Loss: 0.1013
Epoch 12 Step 101 Train Loss: 0.0925
Epoch 12 Step 151 Train Loss: 0.0915
Epoch 12 Step 201 Train Loss: 0.0858
Epoch 12 Step 251 Train Loss: 0.0924
Epoch 12 Step 301 Train Loss: 0.0908
Epoch 12 Step 351 Train Loss: 0.0969
Epoch 12 Step 401 Train Loss: 0.0984
Epoch 12 Step 451 Train Loss: 0.1146
Epoch 12 Step 501 Train Loss: 0.1016
Epoch 12 Step 551 Train Loss: 0.1057
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 13 Step 1 Train Loss: 0.1014
Epoch 13 Step 51 Train Loss: 0.0989
Epoch 13 Step 101 Train Loss: 0.0992
Epoch 13 Step 151 Train Loss: 0.1011
Epoch 13 Step 201 Train Loss: 0.0977
Epoch 13 Step 251 Train Loss: 0.0964
Epoch 13 Step 301 Train Loss: 0.0960
Epoch 13 Step 351 Train Loss: 0.1050
Epoch 13 Step 401 Train Loss: 0.0935
Epoch 13 Step 451 Train Loss: 0.1012
Epoch 13 Step 501 Train Loss: 0.0921
Epoch 13 Step 551 Train Loss: 0.0920
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 14 Step 1 Train Loss: 0.1242
Epoch 14 Step 51 Train Loss: 0.1043
Epoch 14 Step 101 Train Loss: 0.1049
Epoch 14 Step 151 Train Loss: 0.0862
Epoch 14 Step 201 Train Loss: 0.0882
Epoch 14 Step 251 Train Loss: 0.1011
Epoch 14 Step 301 Train Loss: 0.0909
Epoch 14 Step 351 Train Loss: 0.1094
Epoch 14 Step 401 Train Loss: 0.0929
Epoch 14 Step 451 Train Loss: 0.1158
Epoch 14 Step 501 Train Loss: 0.0950
Epoch 14 Step 551 Train Loss: 0.0906
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Epoch 15 Step 1 Train Loss: 0.1114
Epoch 15 Step 51 Train Loss: 0.1028
Epoch 15 Step 101 Train Loss: 0.0880
Epoch 15 Step 151 Train Loss: 0.0997
Epoch 15 Step 201 Train Loss: 0.1148
Epoch 15 Step 251 Train Loss: 0.0861
Epoch 15 Step 301 Train Loss: 0.0876
Epoch 15 Step 351 Train Loss: 0.1050
Epoch 15 Step 401 Train Loss: 0.0948
Epoch 15 Step 451 Train Loss: 0.0985
Epoch 15 Step 501 Train Loss: 0.0905
Epoch 15 Step 551 Train Loss: 0.1130
Epoch 15: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0036. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0035
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.001004421
test_combo_seen0_pearson: 0.8913123538932284
test_combo_seen0_mse_de: 0.0048231715
test_combo_seen0_pearson_de: 0.29828007884906266
test_combo_seen1_mse: 0.0012210826
test_combo_seen1_pearson: 0.8725951866709253
test_combo_seen1_mse_de: 0.0035262143
test_combo_seen1_pearson_de: 0.2609068582043585
test_combo_seen2_mse: 0.001189834
test_combo_seen2_pearson: 0.8769122554429124
test_combo_seen2_mse_de: 0.0036380272
test_combo_seen2_pearson_de: 0.23457306611363907
test_unseen_single_mse: 7.132714e-05
test_unseen_single_pearson: 0.9907929191532443
test_unseen_single_mse_de: 0.00017506653
test_unseen_single_pearson_de: 0.586123358180486
test_combo_seen0_pearson_delta: 0.0720959945206994
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.41538461538461535
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.3230769230769231
test_combo_seen0_mse_top20_de_non_dropout: 0.010849981
test_combo_seen1_pearson_delta: 0.038534273592302576
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.3426666666666667
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.3186666666666667
test_combo_seen1_mse_top20_de_non_dropout: 0.005697387
test_combo_seen2_pearson_delta: 0.03490186800461397
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.33478260869565224
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.28478260869565214
test_combo_seen2_mse_top20_de_non_dropout: 0.008390107
test_unseen_single_pearson_delta: 0.12392456731308239
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3785714285714286
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5071428571428571
test_unseen_single_mse_top20_de_non_dropout: 0.00017266863
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.004 MB of 0.025 MB uploadedwandb: | 0.025 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.41538
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.32308
wandb:                                         test_combo_seen0_mse 0.001
wandb:                                      test_combo_seen0_mse_de 0.00482
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.01085
wandb:                                     test_combo_seen0_pearson 0.89131
wandb:                                  test_combo_seen0_pearson_de 0.29828
wandb:                               test_combo_seen0_pearson_delta 0.0721
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.34267
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.31867
wandb:                                         test_combo_seen1_mse 0.00122
wandb:                                      test_combo_seen1_mse_de 0.00353
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.0057
wandb:                                     test_combo_seen1_pearson 0.8726
wandb:                                  test_combo_seen1_pearson_de 0.26091
wandb:                               test_combo_seen1_pearson_delta 0.03853
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.33478
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.28478
wandb:                                         test_combo_seen2_mse 0.00119
wandb:                                      test_combo_seen2_mse_de 0.00364
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.00839
wandb:                                     test_combo_seen2_pearson 0.87691
wandb:                                  test_combo_seen2_pearson_de 0.23457
wandb:                               test_combo_seen2_pearson_delta 0.0349
wandb:                                                  test_de_mse 0.00349
wandb:                                              test_de_pearson 0.27918
wandb:               test_frac_opposite_direction_top20_non_dropout 0.35127
wandb:                          test_frac_sigma_below_1_non_dropout 0.32373
wandb:                                                     test_mse 0.00112
wandb:                                test_mse_top20_de_non_dropout 0.00646
wandb:                                                 test_pearson 0.88251
wandb:                                           test_pearson_delta 0.04659
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.37857
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.50714
wandb:                                       test_unseen_single_mse 7e-05
wandb:                                    test_unseen_single_mse_de 0.00018
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00017
wandb:                                   test_unseen_single_pearson 0.99079
wandb:                                test_unseen_single_pearson_de 0.58612
wandb:                             test_unseen_single_pearson_delta 0.12392
wandb:                                                 train_de_mse 0.00257
wandb:                                             train_de_pearson 0.35536
wandb:                                                    train_mse 0.00095
wandb:                                                train_pearson 0.8995
wandb:                                                training_loss 0.09388
wandb:                                                   val_de_mse 0.00361
wandb:                                               val_de_pearson 0.32408
wandb:                                                      val_mse 0.00142
wandb:                                                  val_pearson 0.84966
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_day7neuron_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/fqkiy8xl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_040231-fqkiy8xl/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:21
combo_seen1:100
combo_seen2:26
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_041646-a0n8th7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_iPSC_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/a0n8th7l
wandb: WARNING Serializing object of type ndarray that is 20562048 bytes
  0%|                                                  | 0/3982 [00:00<?, ?it/s]  0%|                                          | 7/3982 [00:00<00:59, 66.95it/s]  0%|‚ñè                                        | 15/3982 [00:00<00:56, 70.06it/s]  1%|‚ñè                                        | 22/3982 [00:00<00:58, 68.03it/s]  1%|‚ñé                                        | 33/3982 [00:00<00:48, 80.98it/s]  1%|‚ñç                                        | 42/3982 [00:00<00:50, 78.30it/s]  1%|‚ñå                                        | 52/3982 [00:00<00:47, 82.62it/s]  2%|‚ñã                                        | 61/3982 [00:00<00:47, 82.94it/s]  2%|‚ñã                                        | 70/3982 [00:00<00:46, 83.26it/s]  2%|‚ñä                                        | 79/3982 [00:00<00:46, 83.28it/s]  2%|‚ñâ                                        | 88/3982 [00:01<00:47, 81.50it/s]  2%|‚ñâ                                        | 97/3982 [00:01<00:48, 80.21it/s]  3%|‚ñà                                       | 106/3982 [00:01<00:48, 80.17it/s]  3%|‚ñà‚ñè                                      | 115/3982 [00:01<00:48, 79.92it/s]  3%|‚ñà‚ñè                                      | 123/3982 [00:01<00:48, 79.67it/s]  3%|‚ñà‚ñé                                      | 131/3982 [00:01<00:48, 79.32it/s]  4%|‚ñà‚ñç                                      | 140/3982 [00:01<00:48, 79.84it/s]  4%|‚ñà‚ñç                                      | 148/3982 [00:01<00:48, 79.66it/s]  4%|‚ñà‚ñå                                      | 157/3982 [00:01<00:47, 80.08it/s]  4%|‚ñà‚ñã                                      | 166/3982 [00:02<00:54, 70.43it/s]  4%|‚ñà‚ñä                                      | 176/3982 [00:02<00:49, 77.21it/s]  5%|‚ñà‚ñä                                      | 186/3982 [00:02<00:46, 81.99it/s]  5%|‚ñà‚ñâ                                      | 195/3982 [00:02<00:45, 82.83it/s]  5%|‚ñà‚ñà                                      | 206/3982 [00:02<00:44, 84.67it/s]  5%|‚ñà‚ñà‚ñè                                     | 215/3982 [00:02<00:46, 81.13it/s]  6%|‚ñà‚ñà‚ñé                                     | 225/3982 [00:02<00:55, 67.50it/s]  6%|‚ñà‚ñà‚ñç                                     | 241/3982 [00:03<00:42, 88.17it/s]  6%|‚ñà‚ñà‚ñå                                     | 251/3982 [00:03<00:42, 87.56it/s]  7%|‚ñà‚ñà‚ñå                                     | 261/3982 [00:03<00:45, 81.42it/s]  7%|‚ñà‚ñà‚ñã                                     | 270/3982 [00:03<00:46, 80.32it/s]  7%|‚ñà‚ñà‚ñä                                     | 279/3982 [00:03<00:44, 82.60it/s]  7%|‚ñà‚ñà‚ñâ                                     | 288/3982 [00:03<00:46, 79.32it/s]  7%|‚ñà‚ñà‚ñâ                                     | 297/3982 [00:03<00:56, 65.80it/s]  8%|‚ñà‚ñà‚ñà                                     | 306/3982 [00:04<01:04, 56.85it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 313/3982 [00:04<01:06, 54.87it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 320/3982 [00:04<01:06, 54.91it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 328/3982 [00:04<01:10, 52.06it/s]  8%|‚ñà‚ñà‚ñà‚ñç                                    | 336/3982 [00:04<01:11, 51.26it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 343/3982 [00:04<01:07, 53.97it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 352/3982 [00:04<01:14, 48.41it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 363/3982 [00:05<00:59, 60.86it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 370/3982 [00:05<01:00, 59.77it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 378/3982 [00:05<01:09, 51.81it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 385/3982 [00:05<01:05, 55.08it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 403/3982 [00:05<00:50, 71.22it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 413/3982 [00:05<00:54, 65.41it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 421/3982 [00:05<00:52, 67.86it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 433/3982 [00:06<00:45, 77.33it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 442/3982 [00:06<00:46, 76.05it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 453/3982 [00:06<00:42, 83.63it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 462/3982 [00:06<00:41, 84.53it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 471/3982 [00:06<00:43, 80.14it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 480/3982 [00:06<00:45, 77.75it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 488/3982 [00:06<00:53, 65.39it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 495/3982 [00:06<00:54, 63.78it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 507/3982 [00:07<00:46, 75.33it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 515/3982 [00:07<01:07, 51.03it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 522/3982 [00:07<01:13, 47.00it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 528/3982 [00:07<01:18, 44.28it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 538/3982 [00:07<01:03, 54.04it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 545/3982 [00:08<01:16, 45.16it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 557/3982 [00:08<01:14, 45.68it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 568/3982 [00:08<01:00, 56.04it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 575/3982 [00:08<00:58, 58.34it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 583/3982 [00:08<01:07, 50.50it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 591/3982 [00:08<01:01, 55.32it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 600/3982 [00:08<00:56, 60.34it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 608/3982 [00:09<00:53, 62.98it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 615/3982 [00:09<00:52, 64.26it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 624/3982 [00:09<00:52, 64.24it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 631/3982 [00:09<00:51, 65.55it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 640/3982 [00:09<00:49, 67.69it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 649/3982 [00:09<00:47, 70.41it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 657/3982 [00:09<00:48, 68.16it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 664/3982 [00:09<00:57, 57.27it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 677/3982 [00:10<00:44, 74.20it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 686/3982 [00:10<00:43, 75.39it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 695/3982 [00:10<00:41, 78.47it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 704/3982 [00:10<00:43, 76.16it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 712/3982 [00:10<00:44, 73.96it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 720/3982 [00:10<00:45, 72.06it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 728/3982 [00:10<00:45, 70.98it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 736/3982 [00:10<00:44, 72.26it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 744/3982 [00:10<00:45, 71.57it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 754/3982 [00:11<00:43, 74.41it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 763/3982 [00:11<00:41, 78.07it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 771/3982 [00:11<00:43, 73.41it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 782/3982 [00:11<00:41, 77.93it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 790/3982 [00:11<00:41, 77.80it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 799/3982 [00:11<00:40, 79.54it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 808/3982 [00:11<00:41, 76.69it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 817/3982 [00:11<00:40, 77.44it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 828/3982 [00:12<00:38, 81.95it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 837/3982 [00:12<00:38, 80.87it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 846/3982 [00:12<00:39, 80.36it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 855/3982 [00:12<00:38, 81.30it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 864/3982 [00:12<00:40, 77.69it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 874/3982 [00:12<00:37, 81.80it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 883/3982 [00:12<00:38, 81.07it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 893/3982 [00:12<00:37, 81.75it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 902/3982 [00:12<00:39, 77.42it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 913/3982 [00:13<00:35, 85.36it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 922/3982 [00:13<00:37, 81.77it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 932/3982 [00:13<00:37, 81.70it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 941/3982 [00:13<00:36, 83.61it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 950/3982 [00:13<00:36, 84.00it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 959/3982 [00:13<00:37, 80.16it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 968/3982 [00:13<00:37, 79.93it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 977/3982 [00:13<00:37, 80.45it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 986/3982 [00:13<00:37, 79.33it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 996/3982 [00:14<00:35, 83.09it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1005/3982 [00:14<00:37, 79.55it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1014/3982 [00:14<00:36, 82.04it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1023/3982 [00:14<00:36, 80.12it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1032/3982 [00:14<00:36, 80.84it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1042/3982 [00:14<00:37, 79.11it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1053/3982 [00:14<00:34, 85.33it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1062/3982 [00:14<00:37, 77.23it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1072/3982 [00:15<00:37, 77.00it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1082/3982 [00:15<00:35, 80.72it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1091/3982 [00:15<00:35, 81.13it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1100/3982 [00:15<00:35, 80.98it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1109/3982 [00:15<00:35, 80.29it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1118/3982 [00:15<00:36, 78.63it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1128/3982 [00:15<00:33, 84.30it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1137/3982 [00:15<00:34, 81.94it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1146/3982 [00:15<00:34, 82.78it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1155/3982 [00:16<00:35, 80.16it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1164/3982 [00:16<00:35, 79.32it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1174/3982 [00:16<00:34, 81.73it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1183/3982 [00:16<00:34, 80.83it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1192/3982 [00:16<00:33, 82.09it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1201/3982 [00:16<00:33, 82.46it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1210/3982 [00:16<00:34, 81.22it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1220/3982 [00:16<00:32, 85.24it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1229/3982 [00:17<00:40, 67.76it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1237/3982 [00:17<00:39, 70.34it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1246/3982 [00:17<00:37, 72.22it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1256/3982 [00:17<00:35, 77.71it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1265/3982 [00:17<00:36, 75.11it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1275/3982 [00:17<00:33, 80.17it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1284/3982 [00:17<00:34, 77.40it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1294/3982 [00:17<00:34, 78.81it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1303/3982 [00:17<00:33, 79.25it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1312/3982 [00:18<00:33, 79.44it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1322/3982 [00:18<00:32, 82.75it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1331/3982 [00:18<00:33, 79.51it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1340/3982 [00:18<00:32, 82.25it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1349/3982 [00:18<00:32, 80.34it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1358/3982 [00:18<00:32, 80.65it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1368/3982 [00:18<00:31, 83.84it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1377/3982 [00:18<00:31, 83.73it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1386/3982 [00:18<00:31, 82.95it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1395/3982 [00:19<00:32, 80.61it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1404/3982 [00:19<00:31, 83.09it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1413/3982 [00:19<00:32, 78.83it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1422/3982 [00:19<00:31, 81.82it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1431/3982 [00:19<00:33, 76.58it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1440/3982 [00:19<00:33, 75.60it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1449/3982 [00:19<00:32, 77.25it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1457/3982 [00:19<00:34, 73.35it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1465/3982 [00:20<00:34, 72.75it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1473/3982 [00:20<00:34, 73.52it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1481/3982 [00:20<00:33, 74.16it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1489/3982 [00:20<00:33, 73.75it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1497/3982 [00:20<00:35, 70.81it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1505/3982 [00:20<00:36, 67.75it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1512/3982 [00:20<00:37, 66.48it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1519/3982 [00:20<00:38, 64.29it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1526/3982 [00:20<00:39, 61.43it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1534/3982 [00:21<00:37, 65.44it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1541/3982 [00:21<00:39, 61.03it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1548/3982 [00:21<00:40, 60.61it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1556/3982 [00:21<00:38, 63.38it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1563/3982 [00:21<00:38, 62.92it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1570/3982 [00:21<00:38, 62.47it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1577/3982 [00:21<00:38, 62.48it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1584/3982 [00:21<00:38, 61.93it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1591/3982 [00:21<00:39, 61.07it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1598/3982 [00:22<00:38, 61.69it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1605/3982 [00:22<00:38, 61.28it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1612/3982 [00:22<00:37, 62.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1619/3982 [00:22<00:38, 61.53it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1626/3982 [00:22<00:48, 48.95it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1632/3982 [00:22<00:46, 50.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1638/3982 [00:22<00:50, 46.20it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1643/3982 [00:23<00:53, 44.11it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1650/3982 [00:23<00:46, 49.75it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1656/3982 [00:23<00:44, 52.19it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1663/3982 [00:23<00:41, 56.47it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1669/3982 [00:23<00:46, 49.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1675/3982 [00:23<00:44, 52.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1682/3982 [00:23<00:41, 54.94it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1689/3982 [00:23<00:40, 56.74it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1697/3982 [00:23<00:37, 60.38it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1704/3982 [00:24<00:36, 61.76it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1711/3982 [00:24<00:37, 61.14it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1718/3982 [00:24<00:36, 61.98it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1725/3982 [00:24<00:35, 63.38it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1732/3982 [00:24<00:36, 61.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1741/3982 [00:24<00:33, 67.72it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1748/3982 [00:24<00:36, 61.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1755/3982 [00:24<00:36, 61.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1762/3982 [00:24<00:35, 62.42it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1769/3982 [00:25<00:36, 59.99it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1777/3982 [00:25<00:35, 62.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1784/3982 [00:25<00:35, 61.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1791/3982 [00:25<00:34, 63.62it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1798/3982 [00:25<00:34, 63.55it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1805/3982 [00:25<00:34, 62.20it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1812/3982 [00:25<00:35, 61.01it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1819/3982 [00:25<00:35, 60.94it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1826/3982 [00:26<00:35, 61.21it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1833/3982 [00:26<00:36, 59.57it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1841/3982 [00:26<00:33, 63.70it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1848/3982 [00:26<00:34, 62.54it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1855/3982 [00:26<00:37, 55.99it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1862/3982 [00:26<00:35, 59.25it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1869/3982 [00:26<00:35, 60.27it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1876/3982 [00:26<00:35, 59.84it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1883/3982 [00:26<00:34, 61.19it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1890/3982 [00:27<00:34, 60.60it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1897/3982 [00:27<00:33, 62.52it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1904/3982 [00:27<00:34, 59.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1911/3982 [00:27<00:36, 56.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1919/3982 [00:27<00:34, 60.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1927/3982 [00:27<00:32, 63.76it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1934/3982 [00:27<00:32, 63.27it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1941/3982 [00:27<00:31, 63.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1948/3982 [00:28<00:32, 62.83it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1955/3982 [00:28<00:32, 61.52it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1962/3982 [00:28<00:33, 61.08it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1969/3982 [00:28<00:38, 52.36it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1978/3982 [00:28<00:33, 59.32it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1986/3982 [00:28<00:31, 62.40it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1993/3982 [00:28<00:32, 62.04it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2000/3982 [00:28<00:32, 60.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2007/3982 [00:29<00:32, 60.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2014/3982 [00:29<00:34, 57.53it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2021/3982 [00:29<00:33, 58.34it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2028/3982 [00:29<00:31, 61.22it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2037/3982 [00:29<00:29, 66.76it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2047/3982 [00:29<00:27, 71.05it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2056/3982 [00:29<00:26, 73.50it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2064/3982 [00:29<00:25, 74.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2072/3982 [00:29<00:26, 71.11it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2083/3982 [00:30<00:23, 80.86it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2092/3982 [00:30<00:24, 78.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2101/3982 [00:30<00:23, 78.64it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2109/3982 [00:30<00:23, 78.81it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2118/3982 [00:30<00:23, 78.73it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2128/3982 [00:30<00:22, 82.38it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2137/3982 [00:30<00:22, 82.01it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2146/3982 [00:30<00:22, 81.26it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2155/3982 [00:31<00:25, 72.31it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2164/3982 [00:31<00:25, 72.39it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2173/3982 [00:31<00:23, 76.80it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2181/3982 [00:31<00:23, 76.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2189/3982 [00:31<00:23, 76.63it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2198/3982 [00:31<00:22, 80.30it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2207/3982 [00:31<00:27, 63.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2223/3982 [00:31<00:21, 81.78it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2233/3982 [00:31<00:20, 84.47it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2242/3982 [00:32<00:21, 80.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2252/3982 [00:32<00:21, 80.98it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2262/3982 [00:32<00:21, 80.80it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2272/3982 [00:32<00:20, 83.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2281/3982 [00:32<00:20, 83.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2290/3982 [00:32<00:20, 83.00it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2299/3982 [00:32<00:23, 72.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2308/3982 [00:32<00:22, 74.97it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2316/3982 [00:33<00:22, 75.08it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2324/3982 [00:33<00:21, 75.67it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2332/3982 [00:33<00:21, 76.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2340/3982 [00:33<00:21, 75.67it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2348/3982 [00:33<00:22, 73.85it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2357/3982 [00:33<00:20, 77.94it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2365/3982 [00:33<00:24, 65.38it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2376/3982 [00:33<00:21, 76.17it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2385/3982 [00:33<00:20, 79.55it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2394/3982 [00:34<00:19, 79.46it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2403/3982 [00:34<00:20, 77.61it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2411/3982 [00:34<00:20, 77.78it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2420/3982 [00:34<00:19, 80.65it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2429/3982 [00:34<00:19, 80.99it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2438/3982 [00:34<00:19, 79.76it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2447/3982 [00:34<00:19, 79.78it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2456/3982 [00:34<00:19, 79.91it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2465/3982 [00:34<00:19, 76.48it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2473/3982 [00:35<00:19, 77.26it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2483/3982 [00:35<00:18, 80.49it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2492/3982 [00:35<00:18, 79.26it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2500/3982 [00:35<00:19, 76.53it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2509/3982 [00:35<00:18, 80.02it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2518/3982 [00:35<00:18, 79.61it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2526/3982 [00:35<00:18, 77.08it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2534/3982 [00:35<00:21, 66.57it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2547/3982 [00:36<00:17, 82.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2556/3982 [00:36<00:17, 82.30it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2565/3982 [00:36<00:17, 81.16it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2574/3982 [00:36<00:17, 81.11it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2583/3982 [00:36<00:17, 81.04it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2592/3982 [00:36<00:17, 80.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2601/3982 [00:36<00:23, 58.38it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2618/3982 [00:36<00:16, 81.82it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2628/3982 [00:37<00:16, 81.17it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2638/3982 [00:37<00:16, 80.15it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2647/3982 [00:37<00:16, 79.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2656/3982 [00:37<00:17, 77.83it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2665/3982 [00:37<00:16, 79.94it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2674/3982 [00:37<00:20, 64.69it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2682/3982 [00:37<00:22, 57.85it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2695/3982 [00:38<00:17, 72.97it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2704/3982 [00:38<00:21, 59.30it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2717/3982 [00:38<00:21, 59.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2735/3982 [00:38<00:15, 80.81it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2745/3982 [00:38<00:16, 75.88it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2755/3982 [00:38<00:15, 79.11it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2764/3982 [00:38<00:15, 77.06it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2773/3982 [00:39<00:15, 79.83it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2782/3982 [00:39<00:15, 76.41it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2790/3982 [00:39<00:15, 77.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2798/3982 [00:39<00:15, 76.03it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2807/3982 [00:39<00:14, 78.91it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2816/3982 [00:39<00:14, 79.72it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2825/3982 [00:39<00:14, 77.90it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2833/3982 [00:39<00:14, 78.20it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2842/3982 [00:39<00:14, 79.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2852/3982 [00:40<00:14, 79.87it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2861/3982 [00:40<00:13, 82.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2870/3982 [00:40<00:13, 81.74it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2879/3982 [00:40<00:14, 78.66it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2887/3982 [00:40<00:16, 67.01it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2899/3982 [00:40<00:13, 78.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2908/3982 [00:40<00:14, 75.82it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2917/3982 [00:40<00:13, 77.22it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2926/3982 [00:41<00:13, 75.78it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2936/3982 [00:41<00:12, 81.79it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2945/3982 [00:41<00:12, 81.21it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2954/3982 [00:41<00:12, 80.62it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2963/3982 [00:41<00:12, 80.58it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2972/3982 [00:41<00:12, 79.61it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2981/3982 [00:41<00:13, 75.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2991/3982 [00:41<00:12, 79.91it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3000/3982 [00:41<00:12, 78.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3009/3982 [00:42<00:11, 81.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3018/3982 [00:42<00:11, 81.59it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3027/3982 [00:42<00:12, 79.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3037/3982 [00:42<00:11, 82.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3046/3982 [00:42<00:13, 70.78it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3056/3982 [00:42<00:12, 76.04it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3065/3982 [00:42<00:11, 77.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3074/3982 [00:42<00:11, 78.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3083/3982 [00:43<00:11, 76.81it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3092/3982 [00:43<00:11, 79.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3101/3982 [00:43<00:10, 80.94it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3110/3982 [00:43<00:11, 78.93it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3119/3982 [00:43<00:10, 81.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3128/3982 [00:43<00:10, 83.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3137/3982 [00:43<00:10, 83.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3146/3982 [00:43<00:10, 82.10it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3155/3982 [00:43<00:10, 82.66it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3164/3982 [00:44<00:09, 82.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3173/3982 [00:44<00:09, 82.92it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3182/3982 [00:44<00:09, 82.28it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3191/3982 [00:44<00:09, 82.46it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3200/3982 [00:44<00:09, 79.64it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3209/3982 [00:44<00:09, 80.40it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3218/3982 [00:44<00:09, 83.03it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3227/3982 [00:44<00:09, 83.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3236/3982 [00:44<00:09, 80.59it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3245/3982 [00:45<00:09, 80.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3255/3982 [00:45<00:08, 83.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3264/3982 [00:45<00:08, 82.52it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3273/3982 [00:45<00:08, 82.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3282/3982 [00:45<00:08, 81.71it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3291/3982 [00:45<00:08, 82.26it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3300/3982 [00:45<00:08, 82.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3309/3982 [00:45<00:08, 82.01it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3318/3982 [00:45<00:08, 81.77it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3327/3982 [00:46<00:07, 82.28it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3336/3982 [00:46<00:07, 82.53it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3345/3982 [00:46<00:07, 81.82it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3354/3982 [00:46<00:07, 82.43it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3363/3982 [00:46<00:08, 73.34it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3374/3982 [00:46<00:07, 78.22it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3383/3982 [00:46<00:07, 79.05it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3392/3982 [00:46<00:07, 81.31it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3401/3982 [00:46<00:07, 82.56it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3410/3982 [00:47<00:07, 72.55it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3422/3982 [00:47<00:06, 82.67it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3431/3982 [00:47<00:06, 82.36it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3440/3982 [00:47<00:06, 82.22it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3449/3982 [00:47<00:06, 83.06it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3458/3982 [00:47<00:06, 83.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3467/3982 [00:47<00:06, 81.87it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3476/3982 [00:47<00:06, 83.94it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3485/3982 [00:47<00:05, 84.19it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3494/3982 [00:48<00:05, 82.35it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3503/3982 [00:48<00:05, 83.54it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3512/3982 [00:48<00:05, 83.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3521/3982 [00:48<00:05, 82.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3530/3982 [00:48<00:05, 82.45it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3539/3982 [00:48<00:05, 82.59it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3548/3982 [00:48<00:05, 82.41it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3557/3982 [00:48<00:05, 80.24it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3566/3982 [00:48<00:05, 82.42it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3575/3982 [00:49<00:04, 82.68it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3584/3982 [00:49<00:04, 83.39it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3593/3982 [00:49<00:04, 81.70it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3602/3982 [00:49<00:04, 82.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3611/3982 [00:49<00:04, 82.72it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3620/3982 [00:49<00:04, 79.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3630/3982 [00:49<00:04, 83.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3639/3982 [00:49<00:04, 80.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3649/3982 [00:49<00:04, 83.21it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3658/3982 [00:50<00:04, 78.89it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3666/3982 [00:50<00:04, 77.86it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3674/3982 [00:50<00:04, 76.97it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3682/3982 [00:50<00:04, 70.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3690/3982 [00:50<00:04, 66.75it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3697/3982 [00:50<00:04, 65.34it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3704/3982 [00:50<00:04, 62.78it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3711/3982 [00:50<00:04, 61.44it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3718/3982 [00:51<00:04, 59.93it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3725/3982 [00:51<00:04, 57.77it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3732/3982 [00:51<00:04, 59.35it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3738/3982 [00:51<00:04, 59.10it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3744/3982 [00:51<00:04, 57.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3750/3982 [00:51<00:04, 57.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3757/3982 [00:51<00:03, 58.44it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3763/3982 [00:51<00:03, 57.01it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3770/3982 [00:51<00:03, 59.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3776/3982 [00:52<00:03, 59.28it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3782/3982 [00:52<00:03, 59.12it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3788/3982 [00:52<00:03, 57.84it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3795/3982 [00:52<00:03, 55.33it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3803/3982 [00:52<00:03, 58.93it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3811/3982 [00:52<00:02, 60.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3820/3982 [00:52<00:02, 67.31it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3828/3982 [00:52<00:02, 70.14it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3836/3982 [00:53<00:02, 60.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3843/3982 [00:53<00:02, 61.73it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3852/3982 [00:53<00:01, 67.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3861/3982 [00:53<00:01, 69.47it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3872/3982 [00:53<00:01, 75.51it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3882/3982 [00:53<00:01, 77.99it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3892/3982 [00:53<00:01, 81.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3901/3982 [00:53<00:01, 79.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3911/3982 [00:53<00:00, 83.88it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3920/3982 [00:54<00:00, 83.66it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3929/3982 [00:54<00:00, 83.70it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3938/3982 [00:54<00:00, 81.26it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3948/3982 [00:54<00:00, 84.51it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3957/3982 [00:54<00:00, 85.21it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3966/3982 [00:54<00:00, 84.74it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3975/3982 [00:54<00:00, 81.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3982/3982 [00:54<00:00, 72.69it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.2001
Epoch 1 Step 51 Train Loss: 0.2477
Epoch 1 Step 101 Train Loss: 0.2129
Epoch 1 Step 151 Train Loss: 0.1740
Epoch 1 Step 201 Train Loss: 0.1943
Epoch 1 Step 251 Train Loss: 0.1973
Epoch 1 Step 301 Train Loss: 0.2171
Epoch 1 Step 351 Train Loss: 0.1934
Epoch 1 Step 401 Train Loss: 0.2131
Epoch 1 Step 451 Train Loss: 0.2101
Epoch 1 Step 501 Train Loss: 0.2108
Epoch 1 Step 551 Train Loss: 0.2470
Epoch 1 Step 601 Train Loss: 0.1954
Epoch 1 Step 651 Train Loss: 0.1971
Epoch 1 Step 701 Train Loss: 0.1582
Epoch 1 Step 751 Train Loss: 0.1899
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0243 Validation Top 20 DE MSE: 0.0171. 
Epoch 2 Step 1 Train Loss: 0.1575
Epoch 2 Step 51 Train Loss: 0.1637
Epoch 2 Step 101 Train Loss: 0.1788
Epoch 2 Step 151 Train Loss: 0.1754
Epoch 2 Step 201 Train Loss: 0.1714
Epoch 2 Step 251 Train Loss: 0.1595
Epoch 2 Step 301 Train Loss: 0.1538
Epoch 2 Step 351 Train Loss: 0.1711
Epoch 2 Step 401 Train Loss: 0.1846
Epoch 2 Step 451 Train Loss: 0.1755
Epoch 2 Step 501 Train Loss: 0.1976
Epoch 2 Step 551 Train Loss: 0.1820
Epoch 2 Step 601 Train Loss: 0.1425
Epoch 2 Step 651 Train Loss: 0.1466
Epoch 2 Step 701 Train Loss: 0.1300
Epoch 2 Step 751 Train Loss: 0.1533
Epoch 2: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0189 Validation Top 20 DE MSE: 0.0134. 
Epoch 3 Step 1 Train Loss: 0.1474
Epoch 3 Step 51 Train Loss: 0.1481
Epoch 3 Step 101 Train Loss: 0.1676
Epoch 3 Step 151 Train Loss: 0.1584
Epoch 3 Step 201 Train Loss: 0.1424
Epoch 3 Step 251 Train Loss: 0.2075
Epoch 3 Step 301 Train Loss: 0.1410
Epoch 3 Step 351 Train Loss: 0.1412
Epoch 3 Step 401 Train Loss: 0.1661
Epoch 3 Step 451 Train Loss: 0.1562
Epoch 3 Step 501 Train Loss: 0.1668
Epoch 3 Step 551 Train Loss: 0.1490
Epoch 3 Step 601 Train Loss: 0.1463
Epoch 3 Step 651 Train Loss: 0.1450
Epoch 3 Step 701 Train Loss: 0.1595
Epoch 3 Step 751 Train Loss: 0.1386
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0201 Validation Top 20 DE MSE: 0.0165. 
Epoch 4 Step 1 Train Loss: 0.1413
Epoch 4 Step 51 Train Loss: 0.1548
Epoch 4 Step 101 Train Loss: 0.1560
Epoch 4 Step 151 Train Loss: 0.1456
Epoch 4 Step 201 Train Loss: 0.1392
Epoch 4 Step 251 Train Loss: 0.1678
Epoch 4 Step 301 Train Loss: 0.1405
Epoch 4 Step 351 Train Loss: 0.1360
Epoch 4 Step 401 Train Loss: 0.1349
Epoch 4 Step 451 Train Loss: 0.1328
Epoch 4 Step 501 Train Loss: 0.1444
Epoch 4 Step 551 Train Loss: 0.1370
Epoch 4 Step 601 Train Loss: 0.1412
Epoch 4 Step 651 Train Loss: 0.1331
Epoch 4 Step 701 Train Loss: 0.1789
Epoch 4 Step 751 Train Loss: 0.1496
Epoch 4: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0204 Validation Top 20 DE MSE: 0.0147. 
Epoch 5 Step 1 Train Loss: 0.1430
Epoch 5 Step 51 Train Loss: 0.1797
Epoch 5 Step 101 Train Loss: 0.1466
Epoch 5 Step 151 Train Loss: 0.1309
Epoch 5 Step 201 Train Loss: 0.1317
Epoch 5 Step 251 Train Loss: 0.1605
Epoch 5 Step 301 Train Loss: 0.1554
Epoch 5 Step 351 Train Loss: 0.1595
Epoch 5 Step 401 Train Loss: 0.1817
Epoch 5 Step 451 Train Loss: 0.1437
Epoch 5 Step 501 Train Loss: 0.1583
Epoch 5 Step 551 Train Loss: 0.1563
Epoch 5 Step 601 Train Loss: 0.1329
Epoch 5 Step 651 Train Loss: 0.1536
Epoch 5 Step 701 Train Loss: 0.1516
Epoch 5 Step 751 Train Loss: 0.1444
Epoch 5: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0189 Validation Top 20 DE MSE: 0.0132. 
Epoch 6 Step 1 Train Loss: 0.1428
Epoch 6 Step 51 Train Loss: 0.1321
Epoch 6 Step 101 Train Loss: 0.1474
Epoch 6 Step 151 Train Loss: 0.1323
Epoch 6 Step 201 Train Loss: 0.1423
Epoch 6 Step 251 Train Loss: 0.1335
Epoch 6 Step 301 Train Loss: 0.1716
Epoch 6 Step 351 Train Loss: 0.1333
Epoch 6 Step 401 Train Loss: 0.1445
Epoch 6 Step 451 Train Loss: 0.1378
Epoch 6 Step 501 Train Loss: 0.1434
Epoch 6 Step 551 Train Loss: 0.1542
Epoch 6 Step 601 Train Loss: 0.1523
Epoch 6 Step 651 Train Loss: 0.1377
Epoch 6 Step 701 Train Loss: 0.2311
Epoch 6 Step 751 Train Loss: 0.1450
Epoch 6: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0201 Validation Top 20 DE MSE: 0.0144. 
Epoch 7 Step 1 Train Loss: 0.1672
Epoch 7 Step 51 Train Loss: 0.1418
Epoch 7 Step 101 Train Loss: 0.1445
Epoch 7 Step 151 Train Loss: 0.1560
Epoch 7 Step 201 Train Loss: 0.2199
Epoch 7 Step 251 Train Loss: 0.1479
Epoch 7 Step 301 Train Loss: 0.1522
Epoch 7 Step 351 Train Loss: 0.1629
Epoch 7 Step 401 Train Loss: 0.1473
Epoch 7 Step 451 Train Loss: 0.1513
Epoch 7 Step 501 Train Loss: 0.1466
Epoch 7 Step 551 Train Loss: 0.1646
Epoch 7 Step 601 Train Loss: 0.1495
Epoch 7 Step 651 Train Loss: 0.1564
Epoch 7 Step 701 Train Loss: 0.1498
Epoch 7 Step 751 Train Loss: 0.1547
Epoch 7: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0141. 
Epoch 8 Step 1 Train Loss: 0.1526
Epoch 8 Step 51 Train Loss: 0.1516
Epoch 8 Step 101 Train Loss: 0.1399
Epoch 8 Step 151 Train Loss: 0.1571
Epoch 8 Step 201 Train Loss: 0.1710
Epoch 8 Step 251 Train Loss: 0.1480
Epoch 8 Step 301 Train Loss: 0.1581
Epoch 8 Step 351 Train Loss: 0.1509
Epoch 8 Step 401 Train Loss: 0.1660
Epoch 8 Step 451 Train Loss: 0.1431
Epoch 8 Step 501 Train Loss: 0.1520
Epoch 8 Step 551 Train Loss: 0.1517
Epoch 8 Step 601 Train Loss: 0.1521
Epoch 8 Step 651 Train Loss: 0.1438
Epoch 8 Step 701 Train Loss: 0.1530
Epoch 8 Step 751 Train Loss: 0.1446
Epoch 8: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0197 Validation Top 20 DE MSE: 0.0141. 
Epoch 9 Step 1 Train Loss: 0.1392
Epoch 9 Step 51 Train Loss: 0.1656
Epoch 9 Step 101 Train Loss: 0.1869
Epoch 9 Step 151 Train Loss: 0.1443
Epoch 9 Step 201 Train Loss: 0.1518
Epoch 9 Step 251 Train Loss: 0.1432
Epoch 9 Step 301 Train Loss: 0.1415
Epoch 9 Step 351 Train Loss: 0.1477
Epoch 9 Step 401 Train Loss: 0.1425
Epoch 9 Step 451 Train Loss: 0.1558
Epoch 9 Step 501 Train Loss: 0.1519
Epoch 9 Step 551 Train Loss: 0.1499
Epoch 9 Step 601 Train Loss: 0.1388
Epoch 9 Step 651 Train Loss: 0.1455
Epoch 9 Step 701 Train Loss: 0.1482
Epoch 9 Step 751 Train Loss: 0.1625
Epoch 9: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0142. 
Epoch 10 Step 1 Train Loss: 0.1533
Epoch 10 Step 51 Train Loss: 0.1392
Epoch 10 Step 101 Train Loss: 0.1462
Epoch 10 Step 151 Train Loss: 0.1466
Epoch 10 Step 201 Train Loss: 0.1430
Epoch 10 Step 251 Train Loss: 0.1430
Epoch 10 Step 301 Train Loss: 0.1690
Epoch 10 Step 351 Train Loss: 0.1499
Epoch 10 Step 401 Train Loss: 0.1453
Epoch 10 Step 451 Train Loss: 0.1562
Epoch 10 Step 501 Train Loss: 0.1465
Epoch 10 Step 551 Train Loss: 0.1530
Epoch 10 Step 601 Train Loss: 0.1640
Epoch 10 Step 651 Train Loss: 0.1487
Epoch 10 Step 701 Train Loss: 0.1392
Epoch 10 Step 751 Train Loss: 0.1485
Epoch 10: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0199 Validation Top 20 DE MSE: 0.0142. 
Epoch 11 Step 1 Train Loss: 0.1529
Epoch 11 Step 51 Train Loss: 0.1557
Epoch 11 Step 101 Train Loss: 0.1470
Epoch 11 Step 151 Train Loss: 0.1416
Epoch 11 Step 201 Train Loss: 0.1443
Epoch 11 Step 251 Train Loss: 0.1494
Epoch 11 Step 301 Train Loss: 0.1493
Epoch 11 Step 351 Train Loss: 0.1512
Epoch 11 Step 401 Train Loss: 0.1523
Epoch 11 Step 451 Train Loss: 0.1360
Epoch 11 Step 501 Train Loss: 0.1478
Epoch 11 Step 551 Train Loss: 0.1552
Epoch 11 Step 601 Train Loss: 0.1417
Epoch 11 Step 651 Train Loss: 0.1456
Epoch 11 Step 701 Train Loss: 0.1453
Epoch 11 Step 751 Train Loss: 0.1504
Epoch 11: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0200 Validation Top 20 DE MSE: 0.0143. 
Epoch 12 Step 1 Train Loss: 0.1431
Epoch 12 Step 51 Train Loss: 0.1750
Epoch 12 Step 101 Train Loss: 0.1338
Epoch 12 Step 151 Train Loss: 0.1697
Epoch 12 Step 201 Train Loss: 0.1611
Epoch 12 Step 251 Train Loss: 0.1441
Epoch 12 Step 301 Train Loss: 0.1589
Epoch 12 Step 351 Train Loss: 0.1563
Epoch 12 Step 401 Train Loss: 0.1707
Epoch 12 Step 451 Train Loss: 0.1486
Epoch 12 Step 501 Train Loss: 0.1473
Epoch 12 Step 551 Train Loss: 0.1786
Epoch 12 Step 601 Train Loss: 0.1507
Epoch 12 Step 651 Train Loss: 0.1537
Epoch 12 Step 701 Train Loss: 0.1469
Epoch 12 Step 751 Train Loss: 0.2024
Epoch 12: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0141. 
Epoch 13 Step 1 Train Loss: 0.1781
Epoch 13 Step 51 Train Loss: 0.1841
Epoch 13 Step 101 Train Loss: 0.1644
Epoch 13 Step 151 Train Loss: 0.1602
Epoch 13 Step 201 Train Loss: 0.1377
Epoch 13 Step 251 Train Loss: 0.1556
Epoch 13 Step 301 Train Loss: 0.1534
Epoch 13 Step 351 Train Loss: 0.1758
Epoch 13 Step 401 Train Loss: 0.1474
Epoch 13 Step 451 Train Loss: 0.1487
Epoch 13 Step 501 Train Loss: 0.1518
Epoch 13 Step 551 Train Loss: 0.1515
Epoch 13 Step 601 Train Loss: 0.1366
Epoch 13 Step 651 Train Loss: 0.1450
Epoch 13 Step 701 Train Loss: 0.2294
Epoch 13 Step 751 Train Loss: 0.1561
Epoch 13: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0141. 
Epoch 14 Step 1 Train Loss: 0.1425
Epoch 14 Step 51 Train Loss: 0.1560
Epoch 14 Step 101 Train Loss: 0.1559
Epoch 14 Step 151 Train Loss: 0.1442
Epoch 14 Step 201 Train Loss: 0.1767
Epoch 14 Step 251 Train Loss: 0.1458
Epoch 14 Step 301 Train Loss: 0.1571
Epoch 14 Step 351 Train Loss: 0.1481
Epoch 14 Step 401 Train Loss: 0.1561
Epoch 14 Step 451 Train Loss: 0.1457
Epoch 14 Step 501 Train Loss: 0.1449
Epoch 14 Step 551 Train Loss: 0.1683
Epoch 14 Step 601 Train Loss: 0.1438
Epoch 14 Step 651 Train Loss: 0.1485
Epoch 14 Step 701 Train Loss: 0.1460
Epoch 14 Step 751 Train Loss: 0.1476
Epoch 14: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0198 Validation Top 20 DE MSE: 0.0141. 
Epoch 15 Step 1 Train Loss: 0.1790
Epoch 15 Step 51 Train Loss: 0.1519
Epoch 15 Step 101 Train Loss: 0.1568
Epoch 15 Step 151 Train Loss: 0.1587
Epoch 15 Step 201 Train Loss: 0.1780
Epoch 15 Step 251 Train Loss: 0.1452
Epoch 15 Step 301 Train Loss: 0.1584
Epoch 15 Step 351 Train Loss: 0.1757
Epoch 15 Step 401 Train Loss: 0.1411
Epoch 15 Step 451 Train Loss: 0.1654
Epoch 15 Step 501 Train Loss: 0.1552
Epoch 15 Step 551 Train Loss: 0.1501
Epoch 15 Step 601 Train Loss: 0.1588
Epoch 15 Step 651 Train Loss: 0.1442
Epoch 15 Step 701 Train Loss: 0.1502
Epoch 15 Step 751 Train Loss: 0.1674
Epoch 15: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0203 Validation Top 20 DE MSE: 0.0145. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0173
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0011887188
test_combo_seen0_pearson: 0.9463807731034188
test_combo_seen0_mse_de: 0.01218776
test_combo_seen0_pearson_de: 0.1531722760858938
test_combo_seen1_mse: 0.0012744329
test_combo_seen1_pearson: 0.943109494276269
test_combo_seen1_mse_de: 0.016880361
test_combo_seen1_pearson_de: 0.12438629746924783
test_combo_seen2_mse: 0.0020101066
test_combo_seen2_pearson: 0.9117707503250015
test_combo_seen2_mse_de: 0.0276651
test_combo_seen2_pearson_de: 0.03794579756614355
test_unseen_single_mse: 3.9693838e-05
test_unseen_single_pearson: 0.9981251388345941
test_unseen_single_mse_de: 0.00016017749
test_unseen_single_pearson_de: 0.7162004661990852
test_combo_seen0_pearson_delta: 0.010649253310884333
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.5309523809523811
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.6261904761904762
test_combo_seen0_mse_top20_de_non_dropout: 0.044912484
test_combo_seen1_pearson_delta: 0.04075098219045775
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.5255000000000001
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.6105000000000002
test_combo_seen1_mse_top20_de_non_dropout: 0.051570598
test_combo_seen2_pearson_delta: 0.0016445315214562227
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.5576923076923077
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.5615384615384614
test_combo_seen2_mse_top20_de_non_dropout: 0.09635811
test_unseen_single_pearson_delta: 0.19273899089354937
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4428571428571429
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9071428571428573
test_unseen_single_mse_top20_de_non_dropout: 0.00023175174
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.026 MB of 0.027 MB uploadedwandb: / 0.026 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.53095
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.62619
wandb:                                         test_combo_seen0_mse 0.00119
wandb:                                      test_combo_seen0_mse_de 0.01219
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.04491
wandb:                                     test_combo_seen0_pearson 0.94638
wandb:                                  test_combo_seen0_pearson_de 0.15317
wandb:                               test_combo_seen0_pearson_delta 0.01065
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.5255
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.6105
wandb:                                         test_combo_seen1_mse 0.00127
wandb:                                      test_combo_seen1_mse_de 0.01688
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.05157
wandb:                                     test_combo_seen1_pearson 0.94311
wandb:                                  test_combo_seen1_pearson_de 0.12439
wandb:                               test_combo_seen1_pearson_delta 0.04075
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.55769
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.56154
wandb:                                         test_combo_seen2_mse 0.00201
wandb:                                      test_combo_seen2_mse_de 0.02767
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.09636
wandb:                                     test_combo_seen2_pearson 0.91177
wandb:                                  test_combo_seen2_pearson_de 0.03795
wandb:                               test_combo_seen2_pearson_delta 0.00164
wandb:                                                  test_de_mse 0.0173
wandb:                                              test_de_pearson 0.14062
wandb:               test_frac_opposite_direction_top20_non_dropout 0.52792
wandb:                          test_frac_sigma_below_1_non_dropout 0.61786
wandb:                                                     test_mse 0.00133
wandb:                                test_mse_top20_de_non_dropout 0.05589
wandb:                                                 test_pearson 0.94077
wandb:                                           test_pearson_delta 0.03695
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.44286
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.90714
wandb:                                       test_unseen_single_mse 4e-05
wandb:                                    test_unseen_single_mse_de 0.00016
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00023
wandb:                                   test_unseen_single_pearson 0.99813
wandb:                                test_unseen_single_pearson_de 0.7162
wandb:                             test_unseen_single_pearson_delta 0.19274
wandb:                                                 train_de_mse 0.02032
wandb:                                             train_de_pearson 0.20054
wandb:                                                    train_mse 0.00119
wandb:                                                train_pearson 0.94813
wandb:                                                training_loss 0.14704
wandb:                                                   val_de_mse 0.01448
wandb:                                               val_de_pearson 0.05533
wandb:                                                      val_mse 0.00133
wandb:                                                  val_pearson 0.94205
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_iPSC_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/a0n8th7l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_041646-a0n8th7l/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:17
combo_seen1:84
combo_seen2:31
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_043716-rsb17fc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_iPSC_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/rsb17fc9
wandb: WARNING Serializing object of type ndarray that is 20562048 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1831
Epoch 1 Step 51 Train Loss: 0.2179
Epoch 1 Step 101 Train Loss: 0.1759
Epoch 1 Step 151 Train Loss: 0.1991
Epoch 1 Step 201 Train Loss: 0.2401
Epoch 1 Step 251 Train Loss: 0.1968
Epoch 1 Step 301 Train Loss: 0.1948
Epoch 1 Step 351 Train Loss: 0.2078
Epoch 1 Step 401 Train Loss: 0.1791
Epoch 1 Step 451 Train Loss: 0.2503
Epoch 1 Step 501 Train Loss: 0.1891
Epoch 1 Step 551 Train Loss: 0.1933
Epoch 1 Step 601 Train Loss: 0.2029
Epoch 1 Step 651 Train Loss: 0.1757
Epoch 1 Step 701 Train Loss: 0.1778
Epoch 1 Step 751 Train Loss: 0.1718
Epoch 1: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.0166. 
Epoch 2 Step 1 Train Loss: 0.1724
Epoch 2 Step 51 Train Loss: 0.1804
Epoch 2 Step 101 Train Loss: 0.1602
Epoch 2 Step 151 Train Loss: 0.1608
Epoch 2 Step 201 Train Loss: 0.1611
Epoch 2 Step 251 Train Loss: 0.1535
Epoch 2 Step 301 Train Loss: 0.1193
Epoch 2 Step 351 Train Loss: 0.1462
Epoch 2 Step 401 Train Loss: 0.1324
Epoch 2 Step 451 Train Loss: 0.1283
Epoch 2 Step 501 Train Loss: 0.1331
Epoch 2 Step 551 Train Loss: 0.1261
Epoch 2 Step 601 Train Loss: 0.1416
Epoch 2 Step 651 Train Loss: 0.1393
Epoch 2 Step 701 Train Loss: 0.1401
Epoch 2 Step 751 Train Loss: 0.1350
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0127 Validation Top 20 DE MSE: 0.0168. 
Epoch 3 Step 1 Train Loss: 0.1154
Epoch 3 Step 51 Train Loss: 0.1152
Epoch 3 Step 101 Train Loss: 0.1330
Epoch 3 Step 151 Train Loss: 0.1467
Epoch 3 Step 201 Train Loss: 0.1306
Epoch 3 Step 251 Train Loss: 0.1330
Epoch 3 Step 301 Train Loss: 0.1421
Epoch 3 Step 351 Train Loss: 0.1334
Epoch 3 Step 401 Train Loss: 0.1354
Epoch 3 Step 451 Train Loss: 0.1380
Epoch 3 Step 501 Train Loss: 0.1486
Epoch 3 Step 551 Train Loss: 0.1285
Epoch 3 Step 601 Train Loss: 0.1539
Epoch 3 Step 651 Train Loss: 0.1358
Epoch 3 Step 701 Train Loss: 0.1246
Epoch 3 Step 751 Train Loss: 0.1492
Epoch 3: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0149 Validation Top 20 DE MSE: 0.0195. 
Epoch 4 Step 1 Train Loss: 0.1466
Epoch 4 Step 51 Train Loss: 0.1364
Epoch 4 Step 101 Train Loss: 0.1275
Epoch 4 Step 151 Train Loss: 0.1260
Epoch 4 Step 201 Train Loss: 0.1309
Epoch 4 Step 251 Train Loss: 0.1471
Epoch 4 Step 301 Train Loss: 0.1440
Epoch 4 Step 351 Train Loss: 0.1455
Epoch 4 Step 401 Train Loss: 0.1356
Epoch 4 Step 451 Train Loss: 0.1576
Epoch 4 Step 501 Train Loss: 0.1356
Epoch 4 Step 551 Train Loss: 0.1285
Epoch 4 Step 601 Train Loss: 0.1509
Epoch 4 Step 651 Train Loss: 0.1417
Epoch 4 Step 701 Train Loss: 0.1504
Epoch 4 Step 751 Train Loss: 0.1514
Epoch 4: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0160 Validation Top 20 DE MSE: 0.0214. 
Epoch 5 Step 1 Train Loss: 0.2157
Epoch 5 Step 51 Train Loss: 0.1731
Epoch 5 Step 101 Train Loss: 0.1589
Epoch 5 Step 151 Train Loss: 0.1535
Epoch 5 Step 201 Train Loss: 0.1675
Epoch 5 Step 251 Train Loss: 0.1826
Epoch 5 Step 301 Train Loss: 0.1640
Epoch 5 Step 351 Train Loss: 0.1505
Epoch 5 Step 401 Train Loss: 0.1587
Epoch 5 Step 451 Train Loss: 0.1445
Epoch 5 Step 501 Train Loss: 0.1511
Epoch 5 Step 551 Train Loss: 0.1533
Epoch 5 Step 601 Train Loss: 0.1407
Epoch 5 Step 651 Train Loss: 0.1699
Epoch 5 Step 701 Train Loss: 0.1441
Epoch 5 Step 751 Train Loss: 0.1556
Epoch 5: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.0206. 
Epoch 6 Step 1 Train Loss: 0.1434
Epoch 6 Step 51 Train Loss: 0.1443
Epoch 6 Step 101 Train Loss: 0.1632
Epoch 6 Step 151 Train Loss: 0.1734
Epoch 6 Step 201 Train Loss: 0.1582
Epoch 6 Step 251 Train Loss: 0.1435
Epoch 6 Step 301 Train Loss: 0.1293
Epoch 6 Step 351 Train Loss: 0.1703
Epoch 6 Step 401 Train Loss: 0.1371
Epoch 6 Step 451 Train Loss: 0.1649
Epoch 6 Step 501 Train Loss: 0.1540
Epoch 6 Step 551 Train Loss: 0.1602
Epoch 6 Step 601 Train Loss: 0.1419
Epoch 6 Step 651 Train Loss: 0.1434
Epoch 6 Step 701 Train Loss: 0.1709
Epoch 6 Step 751 Train Loss: 0.1373
Epoch 6: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0155 Validation Top 20 DE MSE: 0.0205. 
Epoch 7 Step 1 Train Loss: 0.1443
Epoch 7 Step 51 Train Loss: 0.1484
Epoch 7 Step 101 Train Loss: 0.1574
Epoch 7 Step 151 Train Loss: 0.1627
Epoch 7 Step 201 Train Loss: 0.1534
Epoch 7 Step 251 Train Loss: 0.1528
Epoch 7 Step 301 Train Loss: 0.1402
Epoch 7 Step 351 Train Loss: 0.1482
Epoch 7 Step 401 Train Loss: 0.1462
Epoch 7 Step 451 Train Loss: 0.1337
Epoch 7 Step 501 Train Loss: 0.2053
Epoch 7 Step 551 Train Loss: 0.1393
Epoch 7 Step 601 Train Loss: 0.1736
Epoch 7 Step 651 Train Loss: 0.1669
Epoch 7 Step 701 Train Loss: 0.1367
Epoch 7 Step 751 Train Loss: 0.1717
Epoch 7: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0151 Validation Top 20 DE MSE: 0.0199. 
Epoch 8 Step 1 Train Loss: 0.1455
Epoch 8 Step 51 Train Loss: 0.1409
Epoch 8 Step 101 Train Loss: 0.1453
Epoch 8 Step 151 Train Loss: 0.1522
Epoch 8 Step 201 Train Loss: 0.1512
Epoch 8 Step 251 Train Loss: 0.1528
Epoch 8 Step 301 Train Loss: 0.3121
Epoch 8 Step 351 Train Loss: 0.1402
Epoch 8 Step 401 Train Loss: 0.1625
Epoch 8 Step 451 Train Loss: 0.2245
Epoch 8 Step 501 Train Loss: 0.1574
Epoch 8 Step 551 Train Loss: 0.1544
Epoch 8 Step 601 Train Loss: 0.1439
Epoch 8 Step 651 Train Loss: 0.1465
Epoch 8 Step 701 Train Loss: 0.1408
Epoch 8 Step 751 Train Loss: 0.1757
Epoch 8: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0153 Validation Top 20 DE MSE: 0.0201. 
Epoch 9 Step 1 Train Loss: 0.1607
Epoch 9 Step 51 Train Loss: 0.1483
Epoch 9 Step 101 Train Loss: 0.1410
Epoch 9 Step 151 Train Loss: 0.1457
Epoch 9 Step 201 Train Loss: 0.1525
Epoch 9 Step 251 Train Loss: 0.1398
Epoch 9 Step 301 Train Loss: 0.1418
Epoch 9 Step 351 Train Loss: 0.1565
Epoch 9 Step 401 Train Loss: 0.1576
Epoch 9 Step 451 Train Loss: 0.1483
Epoch 9 Step 501 Train Loss: 0.1482
Epoch 9 Step 551 Train Loss: 0.1558
Epoch 9 Step 601 Train Loss: 0.1526
Epoch 9 Step 651 Train Loss: 0.1625
Epoch 9 Step 701 Train Loss: 0.1806
Epoch 9 Step 751 Train Loss: 0.1585
Epoch 9: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0153 Validation Top 20 DE MSE: 0.0202. 
Epoch 10 Step 1 Train Loss: 0.1573
Epoch 10 Step 51 Train Loss: 0.1604
Epoch 10 Step 101 Train Loss: 0.1324
Epoch 10 Step 151 Train Loss: 0.1671
Epoch 10 Step 201 Train Loss: 0.1654
Epoch 10 Step 251 Train Loss: 0.1505
Epoch 10 Step 301 Train Loss: 0.1567
Epoch 10 Step 351 Train Loss: 0.1368
Epoch 10 Step 401 Train Loss: 0.1361
Epoch 10 Step 451 Train Loss: 0.1555
Epoch 10 Step 501 Train Loss: 0.1586
Epoch 10 Step 551 Train Loss: 0.1852
Epoch 10 Step 601 Train Loss: 0.1531
Epoch 10 Step 651 Train Loss: 0.1430
Epoch 10 Step 701 Train Loss: 0.1492
Epoch 10 Step 751 Train Loss: 0.1421
Epoch 10: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0150 Validation Top 20 DE MSE: 0.0198. 
Epoch 11 Step 1 Train Loss: 0.1549
Epoch 11 Step 51 Train Loss: 0.1612
Epoch 11 Step 101 Train Loss: 0.1483
Epoch 11 Step 151 Train Loss: 0.1392
Epoch 11 Step 201 Train Loss: 0.1686
Epoch 11 Step 251 Train Loss: 0.1610
Epoch 11 Step 301 Train Loss: 0.1587
Epoch 11 Step 351 Train Loss: 0.1677
Epoch 11 Step 401 Train Loss: 0.1398
Epoch 11 Step 451 Train Loss: 0.1478
Epoch 11 Step 501 Train Loss: 0.1370
Epoch 11 Step 551 Train Loss: 0.1764
Epoch 11 Step 601 Train Loss: 0.1510
Epoch 11 Step 651 Train Loss: 0.1948
Epoch 11 Step 701 Train Loss: 0.1810
Epoch 11 Step 751 Train Loss: 0.1518
Epoch 11: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0152 Validation Top 20 DE MSE: 0.0199. 
Epoch 12 Step 1 Train Loss: 0.1551
Epoch 12 Step 51 Train Loss: 0.1314
Epoch 12 Step 101 Train Loss: 0.1531
Epoch 12 Step 151 Train Loss: 0.1761
Epoch 12 Step 201 Train Loss: 0.1727
Epoch 12 Step 251 Train Loss: 0.1378
Epoch 12 Step 301 Train Loss: 0.1439
Epoch 12 Step 351 Train Loss: 0.1339
Epoch 12 Step 401 Train Loss: 0.1510
Epoch 12 Step 451 Train Loss: 0.1526
Epoch 12 Step 501 Train Loss: 0.1621
Epoch 12 Step 551 Train Loss: 0.1735
Epoch 12 Step 601 Train Loss: 0.1561
Epoch 12 Step 651 Train Loss: 0.1507
Epoch 12 Step 701 Train Loss: 0.1408
Epoch 12 Step 751 Train Loss: 0.1396
Epoch 12: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0152 Validation Top 20 DE MSE: 0.0200. 
Epoch 13 Step 1 Train Loss: 0.1647
Epoch 13 Step 51 Train Loss: 0.1438
Epoch 13 Step 101 Train Loss: 0.1458
Epoch 13 Step 151 Train Loss: 0.1668
Epoch 13 Step 201 Train Loss: 0.1484
Epoch 13 Step 251 Train Loss: 0.1480
Epoch 13 Step 301 Train Loss: 0.1523
Epoch 13 Step 351 Train Loss: 0.1557
Epoch 13 Step 401 Train Loss: 0.1430
Epoch 13 Step 451 Train Loss: 0.1514
Epoch 13 Step 501 Train Loss: 0.1569
Epoch 13 Step 551 Train Loss: 0.1467
Epoch 13 Step 601 Train Loss: 0.1596
Epoch 13 Step 651 Train Loss: 0.1435
Epoch 13 Step 701 Train Loss: 0.1726
Epoch 13 Step 751 Train Loss: 0.1483
Epoch 13: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0147 Validation Top 20 DE MSE: 0.0194. 
Epoch 14 Step 1 Train Loss: 0.1478
Epoch 14 Step 51 Train Loss: 0.1628
Epoch 14 Step 101 Train Loss: 0.1584
Epoch 14 Step 151 Train Loss: 0.1293
Epoch 14 Step 201 Train Loss: 0.1527
Epoch 14 Step 251 Train Loss: 0.1705
Epoch 14 Step 301 Train Loss: 0.1518
Epoch 14 Step 351 Train Loss: 0.1563
Epoch 14 Step 401 Train Loss: 0.1637
Epoch 14 Step 451 Train Loss: 0.1521
Epoch 14 Step 501 Train Loss: 0.1659
Epoch 14 Step 551 Train Loss: 0.1471
Epoch 14 Step 601 Train Loss: 0.1459
Epoch 14 Step 651 Train Loss: 0.2120
Epoch 14 Step 701 Train Loss: 0.1565
Epoch 14 Step 751 Train Loss: 0.1419
Epoch 14: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0152 Validation Top 20 DE MSE: 0.0200. 
Epoch 15 Step 1 Train Loss: 0.1312
Epoch 15 Step 51 Train Loss: 0.1557
Epoch 15 Step 101 Train Loss: 0.1429
Epoch 15 Step 151 Train Loss: 0.1543
Epoch 15 Step 201 Train Loss: 0.1515
Epoch 15 Step 251 Train Loss: 0.1693
Epoch 15 Step 301 Train Loss: 0.1605
Epoch 15 Step 351 Train Loss: 0.1581
Epoch 15 Step 401 Train Loss: 0.1525
Epoch 15 Step 451 Train Loss: 0.1602
Epoch 15 Step 501 Train Loss: 0.1541
Epoch 15 Step 551 Train Loss: 0.1599
Epoch 15 Step 601 Train Loss: 0.1663
Epoch 15 Step 651 Train Loss: 0.1590
Epoch 15 Step 701 Train Loss: 0.1691
Epoch 15 Step 751 Train Loss: 0.1703
Epoch 15: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0153 Validation Top 20 DE MSE: 0.0202. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0196
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0023701666
test_combo_seen0_pearson: 0.8961898143654736
test_combo_seen0_mse_de: 0.03266592
test_combo_seen0_pearson_de: 0.05564532157321807
test_combo_seen1_mse: 0.0016162037
test_combo_seen1_pearson: 0.9286245395347169
test_combo_seen1_mse_de: 0.019817356
test_combo_seen1_pearson_de: 0.03420967548377237
test_combo_seen2_mse: 0.0015301422
test_combo_seen2_pearson: 0.9326162331596359
test_combo_seen2_mse_de: 0.016255887
test_combo_seen2_pearson_de: 0.11181842764241677
test_unseen_single_mse: 0.00028905232
test_unseen_single_pearson: 0.9859335420856935
test_unseen_single_mse_de: 0.00048299818
test_unseen_single_pearson_de: 0.3904876883872873
test_combo_seen0_pearson_delta: -0.006124305282956217
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.4764705882352941
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.6147058823529411
test_combo_seen0_mse_top20_de_non_dropout: 0.09049624
test_combo_seen1_pearson_delta: -0.006548260869175023
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.5125000000000001
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.5821428571428571
test_combo_seen1_mse_top20_de_non_dropout: 0.054488216
test_combo_seen2_pearson_delta: 0.0030481677388846115
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.5354838709677419
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.5241935483870968
test_combo_seen2_mse_top20_de_non_dropout: 0.057963848
test_unseen_single_pearson_delta: 0.02780186143611982
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.5071428571428571
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6642857142857144
test_unseen_single_mse_top20_de_non_dropout: 0.000693344
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.025 MB of 0.027 MB uploadedwandb: / 0.025 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ
wandb:                                                train_pearson ‚ñÇ‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ
wandb:                                                training_loss ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ
wandb:                                               val_de_pearson ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÑ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ
wandb:                                                  val_pearson ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.47647
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.61471
wandb:                                         test_combo_seen0_mse 0.00237
wandb:                                      test_combo_seen0_mse_de 0.03267
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.0905
wandb:                                     test_combo_seen0_pearson 0.89619
wandb:                                  test_combo_seen0_pearson_de 0.05565
wandb:                               test_combo_seen0_pearson_delta -0.00612
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.5125
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.58214
wandb:                                         test_combo_seen1_mse 0.00162
wandb:                                      test_combo_seen1_mse_de 0.01982
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.05449
wandb:                                     test_combo_seen1_pearson 0.92862
wandb:                                  test_combo_seen1_pearson_de 0.03421
wandb:                               test_combo_seen1_pearson_delta -0.00655
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.53548
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.52419
wandb:                                         test_combo_seen2_mse 0.00153
wandb:                                      test_combo_seen2_mse_de 0.01626
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.05796
wandb:                                     test_combo_seen2_pearson 0.93262
wandb:                                  test_combo_seen2_pearson_de 0.11182
wandb:                               test_combo_seen2_pearson_delta 0.00305
wandb:                                                  test_de_mse 0.01962
wandb:                                              test_de_pearson 0.07208
wandb:               test_frac_opposite_direction_top20_non_dropout 0.51295
wandb:                          test_frac_sigma_below_1_non_dropout 0.57734
wandb:                                                     test_mse 0.00162
wandb:                                test_mse_top20_de_non_dropout 0.05696
wandb:                                                 test_pearson 0.92843
wandb:                                           test_pearson_delta -0.00263
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.50714
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.66429
wandb:                                       test_unseen_single_mse 0.00029
wandb:                                    test_unseen_single_mse_de 0.00048
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00069
wandb:                                   test_unseen_single_pearson 0.98593
wandb:                                test_unseen_single_pearson_de 0.39049
wandb:                             test_unseen_single_pearson_delta 0.0278
wandb:                                                 train_de_mse 0.01533
wandb:                                             train_de_pearson 0.18555
wandb:                                                    train_mse 0.00121
wandb:                                                train_pearson 0.94776
wandb:                                                training_loss 0.17029
wandb:                                                   val_de_mse 0.02016
wandb:                                               val_de_pearson 0.24161
wandb:                                                      val_mse 0.00122
wandb:                                                  val_pearson 0.94737
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_iPSC_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/rsb17fc9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_043716-rsb17fc9/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:17
combo_seen1:90
combo_seen2:29
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_045606-evo7gf0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_iPSC_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/evo7gf0t
wandb: WARNING Serializing object of type ndarray that is 20562048 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1798
Epoch 1 Step 51 Train Loss: 0.1894
Epoch 1 Step 101 Train Loss: 0.1808
Epoch 1 Step 151 Train Loss: 0.1845
Epoch 1 Step 201 Train Loss: 0.2070
Epoch 1 Step 251 Train Loss: 0.1984
Epoch 1 Step 301 Train Loss: 0.2287
Epoch 1 Step 351 Train Loss: 0.1818
Epoch 1 Step 401 Train Loss: 0.1876
Epoch 1 Step 451 Train Loss: 0.1701
Epoch 1 Step 501 Train Loss: 0.1775
Epoch 1 Step 551 Train Loss: 0.1832
Epoch 1 Step 601 Train Loss: 0.1736
Epoch 1 Step 651 Train Loss: 0.1964
Epoch 1 Step 701 Train Loss: 0.1726
Epoch 1 Step 751 Train Loss: 0.1655
Epoch 1 Step 801 Train Loss: 0.1611
Epoch 1 Step 851 Train Loss: 0.1765
Epoch 1 Step 901 Train Loss: 0.1277
Epoch 1: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0201 Validation Top 20 DE MSE: 0.0177. 
Epoch 2 Step 1 Train Loss: 0.1574
Epoch 2 Step 51 Train Loss: 0.1566
Epoch 2 Step 101 Train Loss: 0.1761
Epoch 2 Step 151 Train Loss: 0.1725
Epoch 2 Step 201 Train Loss: 0.1535
Epoch 2 Step 251 Train Loss: 0.1615
Epoch 2 Step 301 Train Loss: 0.1472
Epoch 2 Step 351 Train Loss: 0.1625
Epoch 2 Step 401 Train Loss: 0.1739
Epoch 2 Step 451 Train Loss: 0.1550
Epoch 2 Step 501 Train Loss: 0.1450
Epoch 2 Step 551 Train Loss: 0.1116
Epoch 2 Step 601 Train Loss: 0.1158
Epoch 2 Step 651 Train Loss: 0.1473
Epoch 2 Step 701 Train Loss: 0.1436
Epoch 2 Step 751 Train Loss: 0.1440
Epoch 2 Step 801 Train Loss: 0.1923
Epoch 2 Step 851 Train Loss: 0.1496
Epoch 2 Step 901 Train Loss: 0.1382
Epoch 2: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0220 Validation Top 20 DE MSE: 0.0197. 
Epoch 3 Step 1 Train Loss: 0.1970
Epoch 3 Step 51 Train Loss: 0.1229
Epoch 3 Step 101 Train Loss: 0.1379
Epoch 3 Step 151 Train Loss: 0.1587
Epoch 3 Step 201 Train Loss: 0.1637
Epoch 3 Step 251 Train Loss: 0.1896
Epoch 3 Step 301 Train Loss: 0.1401
Epoch 3 Step 351 Train Loss: 0.1312
Epoch 3 Step 401 Train Loss: 0.1179
Epoch 3 Step 451 Train Loss: 0.1188
Epoch 3 Step 501 Train Loss: 0.1459
Epoch 3 Step 551 Train Loss: 0.1415
Epoch 3 Step 601 Train Loss: 0.1422
Epoch 3 Step 651 Train Loss: 0.1395
Epoch 3 Step 701 Train Loss: 0.1354
Epoch 3 Step 751 Train Loss: 0.1497
Epoch 3 Step 801 Train Loss: 0.1247
Epoch 3 Step 851 Train Loss: 0.1294
Epoch 3 Step 901 Train Loss: 0.1367
Epoch 3: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0190 Validation Top 20 DE MSE: 0.0174. 
Epoch 4 Step 1 Train Loss: 0.1241
Epoch 4 Step 51 Train Loss: 0.1486
Epoch 4 Step 101 Train Loss: 0.1227
Epoch 4 Step 151 Train Loss: 0.1551
Epoch 4 Step 201 Train Loss: 0.1426
Epoch 4 Step 251 Train Loss: 0.1331
Epoch 4 Step 301 Train Loss: 0.1333
Epoch 4 Step 351 Train Loss: 0.1366
Epoch 4 Step 401 Train Loss: 0.1588
Epoch 4 Step 451 Train Loss: 0.1532
Epoch 4 Step 501 Train Loss: 0.1328
Epoch 4 Step 551 Train Loss: 0.1328
Epoch 4 Step 601 Train Loss: 0.1597
Epoch 4 Step 651 Train Loss: 0.3078
Epoch 4 Step 701 Train Loss: 0.1275
Epoch 4 Step 751 Train Loss: 0.1497
Epoch 4 Step 801 Train Loss: 0.1365
Epoch 4 Step 851 Train Loss: 0.1464
Epoch 4 Step 901 Train Loss: 0.1516
Epoch 4: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0197 Validation Top 20 DE MSE: 0.0174. 
Epoch 5 Step 1 Train Loss: 0.1367
Epoch 5 Step 51 Train Loss: 0.1514
Epoch 5 Step 101 Train Loss: 0.1457
Epoch 5 Step 151 Train Loss: 0.1399
Epoch 5 Step 201 Train Loss: 0.1553
Epoch 5 Step 251 Train Loss: 0.1411
Epoch 5 Step 301 Train Loss: 0.1083
Epoch 5 Step 351 Train Loss: 0.1385
Epoch 5 Step 401 Train Loss: 0.1426
Epoch 5 Step 451 Train Loss: 0.1673
Epoch 5 Step 501 Train Loss: 0.1388
Epoch 5 Step 551 Train Loss: 0.1289
Epoch 5 Step 601 Train Loss: 0.1243
Epoch 5 Step 651 Train Loss: 0.1444
Epoch 5 Step 701 Train Loss: 0.1309
Epoch 5 Step 751 Train Loss: 0.1441
Epoch 5 Step 801 Train Loss: 0.1362
Epoch 5 Step 851 Train Loss: 0.1283
Epoch 5 Step 901 Train Loss: 0.1209
Epoch 5: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0189 Validation Top 20 DE MSE: 0.0170. 
Epoch 6 Step 1 Train Loss: 0.1288
Epoch 6 Step 51 Train Loss: 0.1369
Epoch 6 Step 101 Train Loss: 0.1173
Epoch 6 Step 151 Train Loss: 0.1319
Epoch 6 Step 201 Train Loss: 0.1383
Epoch 6 Step 251 Train Loss: 0.1656
Epoch 6 Step 301 Train Loss: 0.1281
Epoch 6 Step 351 Train Loss: 0.1139
Epoch 6 Step 401 Train Loss: 0.1408
Epoch 6 Step 451 Train Loss: 0.1420
Epoch 6 Step 501 Train Loss: 0.1330
Epoch 6 Step 551 Train Loss: 0.1818
Epoch 6 Step 601 Train Loss: 0.1656
Epoch 6 Step 651 Train Loss: 0.1427
Epoch 6 Step 701 Train Loss: 0.2513
Epoch 6 Step 751 Train Loss: 0.1485
Epoch 6 Step 801 Train Loss: 0.1435
Epoch 6 Step 851 Train Loss: 0.1437
Epoch 6 Step 901 Train Loss: 0.1129
Epoch 6: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.0174. 
Epoch 7 Step 1 Train Loss: 0.1406
Epoch 7 Step 51 Train Loss: 0.1315
Epoch 7 Step 101 Train Loss: 0.1567
Epoch 7 Step 151 Train Loss: 0.1510
Epoch 7 Step 201 Train Loss: 0.1414
Epoch 7 Step 251 Train Loss: 0.1338
Epoch 7 Step 301 Train Loss: 0.1367
Epoch 7 Step 351 Train Loss: 0.1359
Epoch 7 Step 401 Train Loss: 0.1394
Epoch 7 Step 451 Train Loss: 0.1383
Epoch 7 Step 501 Train Loss: 0.1400
Epoch 7 Step 551 Train Loss: 0.1460
Epoch 7 Step 601 Train Loss: 0.1691
Epoch 7 Step 651 Train Loss: 0.1525
Epoch 7 Step 701 Train Loss: 0.1309
Epoch 7 Step 751 Train Loss: 0.1427
Epoch 7 Step 801 Train Loss: 0.1492
Epoch 7 Step 851 Train Loss: 0.1402
Epoch 7 Step 901 Train Loss: 0.1395
Epoch 7: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.0172. 
Epoch 8 Step 1 Train Loss: 0.1329
Epoch 8 Step 51 Train Loss: 0.1653
Epoch 8 Step 101 Train Loss: 0.1441
Epoch 8 Step 151 Train Loss: 0.1480
Epoch 8 Step 201 Train Loss: 0.1974
Epoch 8 Step 251 Train Loss: 0.1254
Epoch 8 Step 301 Train Loss: 0.1448
Epoch 8 Step 351 Train Loss: 0.1668
Epoch 8 Step 401 Train Loss: 0.1337
Epoch 8 Step 451 Train Loss: 0.1693
Epoch 8 Step 501 Train Loss: 0.1285
Epoch 8 Step 551 Train Loss: 0.2174
Epoch 8 Step 601 Train Loss: 0.1421
Epoch 8 Step 651 Train Loss: 0.1720
Epoch 8 Step 701 Train Loss: 0.1439
Epoch 8 Step 751 Train Loss: 0.1371
Epoch 8 Step 801 Train Loss: 0.1294
Epoch 8 Step 851 Train Loss: 0.1412
Epoch 8 Step 901 Train Loss: 0.1594
Epoch 8: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.0172. 
Epoch 9 Step 1 Train Loss: 0.1398
Epoch 9 Step 51 Train Loss: 0.1278
Epoch 9 Step 101 Train Loss: 0.1642
Epoch 9 Step 151 Train Loss: 0.1365
Epoch 9 Step 201 Train Loss: 0.1693
Epoch 9 Step 251 Train Loss: 0.1612
Epoch 9 Step 301 Train Loss: 0.1221
Epoch 9 Step 351 Train Loss: 0.1474
Epoch 9 Step 401 Train Loss: 0.1410
Epoch 9 Step 451 Train Loss: 0.1738
Epoch 9 Step 501 Train Loss: 0.1442
Epoch 9 Step 551 Train Loss: 0.1307
Epoch 9 Step 601 Train Loss: 0.1312
Epoch 9 Step 651 Train Loss: 0.1434
Epoch 9 Step 701 Train Loss: 0.1349
Epoch 9 Step 751 Train Loss: 0.1406
Epoch 9 Step 801 Train Loss: 0.1672
Epoch 9 Step 851 Train Loss: 0.1484
Epoch 9 Step 901 Train Loss: 0.1337
Epoch 9: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.0173. 
Epoch 10 Step 1 Train Loss: 0.1598
Epoch 10 Step 51 Train Loss: 0.1726
Epoch 10 Step 101 Train Loss: 0.1395
Epoch 10 Step 151 Train Loss: 0.1409
Epoch 10 Step 201 Train Loss: 0.1407
Epoch 10 Step 251 Train Loss: 0.1437
Epoch 10 Step 301 Train Loss: 0.1295
Epoch 10 Step 351 Train Loss: 0.1266
Epoch 10 Step 401 Train Loss: 0.1344
Epoch 10 Step 451 Train Loss: 0.1268
Epoch 10 Step 501 Train Loss: 0.1418
Epoch 10 Step 551 Train Loss: 0.1506
Epoch 10 Step 601 Train Loss: 0.1825
Epoch 10 Step 651 Train Loss: 0.1827
Epoch 10 Step 701 Train Loss: 0.1516
Epoch 10 Step 751 Train Loss: 0.1722
Epoch 10 Step 801 Train Loss: 0.1238
Epoch 10 Step 851 Train Loss: 0.1587
Epoch 10 Step 901 Train Loss: 0.1280
Epoch 10: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.0173. 
Epoch 11 Step 1 Train Loss: 0.1379
Epoch 11 Step 51 Train Loss: 0.1682
Epoch 11 Step 101 Train Loss: 0.1368
Epoch 11 Step 151 Train Loss: 0.1393
Epoch 11 Step 201 Train Loss: 0.1884
Epoch 11 Step 251 Train Loss: 0.1399
Epoch 11 Step 301 Train Loss: 0.1434
Epoch 11 Step 351 Train Loss: 0.1373
Epoch 11 Step 401 Train Loss: 0.1426
Epoch 11 Step 451 Train Loss: 0.1441
Epoch 11 Step 501 Train Loss: 0.1616
Epoch 11 Step 551 Train Loss: 0.1483
Epoch 11 Step 601 Train Loss: 0.1460
Epoch 11 Step 651 Train Loss: 0.1723
Epoch 11 Step 701 Train Loss: 0.1594
Epoch 11 Step 751 Train Loss: 0.1346
Epoch 11 Step 801 Train Loss: 0.1425
Epoch 11 Step 851 Train Loss: 0.1418
Epoch 11 Step 901 Train Loss: 0.2147
Epoch 11: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.0172. 
Epoch 12 Step 1 Train Loss: 0.1492
Epoch 12 Step 51 Train Loss: 0.1355
Epoch 12 Step 101 Train Loss: 0.1463
Epoch 12 Step 151 Train Loss: 0.1399
Epoch 12 Step 201 Train Loss: 0.1386
Epoch 12 Step 251 Train Loss: 0.1311
Epoch 12 Step 301 Train Loss: 0.1350
Epoch 12 Step 351 Train Loss: 0.1480
Epoch 12 Step 401 Train Loss: 0.4719
Epoch 12 Step 451 Train Loss: 0.1636
Epoch 12 Step 501 Train Loss: 0.1385
Epoch 12 Step 551 Train Loss: 0.1454
Epoch 12 Step 601 Train Loss: 0.1162
Epoch 12 Step 651 Train Loss: 0.1423
Epoch 12 Step 701 Train Loss: 0.1870
Epoch 12 Step 751 Train Loss: 0.1407
Epoch 12 Step 801 Train Loss: 0.1269
Epoch 12 Step 851 Train Loss: 0.1480
Epoch 12 Step 901 Train Loss: 0.1433
Epoch 12: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0191 Validation Top 20 DE MSE: 0.0170. 
Epoch 13 Step 1 Train Loss: 0.1276
Epoch 13 Step 51 Train Loss: 0.1455
Epoch 13 Step 101 Train Loss: 0.1660
Epoch 13 Step 151 Train Loss: 0.1310
Epoch 13 Step 201 Train Loss: 0.1538
Epoch 13 Step 251 Train Loss: 0.1723
Epoch 13 Step 301 Train Loss: 0.1550
Epoch 13 Step 351 Train Loss: 0.1515
Epoch 13 Step 401 Train Loss: 0.1446
Epoch 13 Step 451 Train Loss: 0.1485
Epoch 13 Step 501 Train Loss: 0.1464
Epoch 13 Step 551 Train Loss: 0.1401
Epoch 13 Step 601 Train Loss: 0.1295
Epoch 13 Step 651 Train Loss: 0.1454
Epoch 13 Step 701 Train Loss: 0.1493
Epoch 13 Step 751 Train Loss: 0.1498
Epoch 13 Step 801 Train Loss: 0.1360
Epoch 13 Step 851 Train Loss: 0.1539
Epoch 13 Step 901 Train Loss: 0.1475
Epoch 13: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.0173. 
Epoch 14 Step 1 Train Loss: 0.1372
Epoch 14 Step 51 Train Loss: 0.1264
Epoch 14 Step 101 Train Loss: 0.1515
Epoch 14 Step 151 Train Loss: 0.1499
Epoch 14 Step 201 Train Loss: 0.1601
Epoch 14 Step 251 Train Loss: 0.1399
Epoch 14 Step 301 Train Loss: 0.1464
Epoch 14 Step 351 Train Loss: 0.1374
Epoch 14 Step 401 Train Loss: 0.1453
Epoch 14 Step 451 Train Loss: 0.1632
Epoch 14 Step 501 Train Loss: 0.1445
Epoch 14 Step 551 Train Loss: 0.1524
Epoch 14 Step 601 Train Loss: 0.1528
Epoch 14 Step 651 Train Loss: 0.1363
Epoch 14 Step 701 Train Loss: 0.1548
Epoch 14 Step 751 Train Loss: 0.1750
Epoch 14 Step 801 Train Loss: 0.1380
Epoch 14 Step 851 Train Loss: 0.1448
Epoch 14 Step 901 Train Loss: 0.1404
Epoch 14: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.0171. 
Epoch 15 Step 1 Train Loss: 0.1471
Epoch 15 Step 51 Train Loss: 0.1313
Epoch 15 Step 101 Train Loss: 0.1472
Epoch 15 Step 151 Train Loss: 0.1502
Epoch 15 Step 201 Train Loss: 0.1385
Epoch 15 Step 251 Train Loss: 0.1522
Epoch 15 Step 301 Train Loss: 0.1545
Epoch 15 Step 351 Train Loss: 0.1154
Epoch 15 Step 401 Train Loss: 0.1564
Epoch 15 Step 451 Train Loss: 0.1237
Epoch 15 Step 501 Train Loss: 0.1306
Epoch 15 Step 551 Train Loss: 0.1370
Epoch 15 Step 601 Train Loss: 0.1213
Epoch 15 Step 651 Train Loss: 0.1487
Epoch 15 Step 701 Train Loss: 0.2102
Epoch 15 Step 751 Train Loss: 0.1264
Epoch 15 Step 801 Train Loss: 0.1449
Epoch 15 Step 851 Train Loss: 0.1518
Epoch 15 Step 901 Train Loss: 0.1451
Epoch 15: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0192 Validation Top 20 DE MSE: 0.0171. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0185
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0018697463
test_combo_seen0_pearson: 0.9179230419836204
test_combo_seen0_mse_de: 0.02811898
test_combo_seen0_pearson_de: 0.06620694434653883
test_combo_seen1_mse: 0.0013299317
test_combo_seen1_pearson: 0.9407837141631906
test_combo_seen1_mse_de: 0.01731672
test_combo_seen1_pearson_de: 0.08793083719929781
test_combo_seen2_mse: 0.0015144064
test_combo_seen2_pearson: 0.9318692351697296
test_combo_seen2_mse_de: 0.021120463
test_combo_seen2_pearson_de: 0.12804967429818087
test_unseen_single_mse: 5.0334544e-05
test_unseen_single_pearson: 0.9975442638381927
test_unseen_single_mse_de: 0.00015867711
test_unseen_single_pearson_de: 0.5668939990289463
test_combo_seen0_pearson_delta: 0.028895387021710104
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.5558823529411764
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.5705882352941176
test_combo_seen0_mse_top20_de_non_dropout: 0.07411523
test_combo_seen1_pearson_delta: 0.03743882615512406
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.5433333333333334
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.6066666666666667
test_combo_seen1_mse_top20_de_non_dropout: 0.054481167
test_combo_seen2_pearson_delta: 0.043927306042513777
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.5586206896551724
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.6103448275862069
test_combo_seen2_mse_top20_de_non_dropout: 0.064458415
test_unseen_single_pearson_delta: 0.15468295780536787
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4428571428571429
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8357142857142857
test_unseen_single_mse_top20_de_non_dropout: 0.0003344498
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.029 MB uploadedwandb: | 0.004 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedwandb: - 0.029 MB of 0.029 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.55588
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.57059
wandb:                                         test_combo_seen0_mse 0.00187
wandb:                                      test_combo_seen0_mse_de 0.02812
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.07412
wandb:                                     test_combo_seen0_pearson 0.91792
wandb:                                  test_combo_seen0_pearson_de 0.06621
wandb:                               test_combo_seen0_pearson_delta 0.0289
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.54333
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.60667
wandb:                                         test_combo_seen1_mse 0.00133
wandb:                                      test_combo_seen1_mse_de 0.01732
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.05448
wandb:                                     test_combo_seen1_pearson 0.94078
wandb:                                  test_combo_seen1_pearson_de 0.08793
wandb:                               test_combo_seen1_pearson_delta 0.03744
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.55862
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.61034
wandb:                                         test_combo_seen2_mse 0.00151
wandb:                                      test_combo_seen2_mse_de 0.02112
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.06446
wandb:                                     test_combo_seen2_pearson 0.93187
wandb:                                  test_combo_seen2_pearson_de 0.12805
wandb:                               test_combo_seen2_pearson_delta 0.04393
wandb:                                                  test_de_mse 0.01853
wandb:                                              test_de_pearson 0.11693
wandb:               test_frac_opposite_direction_top20_non_dropout 0.54301
wandb:                          test_frac_sigma_below_1_non_dropout 0.61434
wandb:                                                     test_mse 0.00137
wandb:                                test_mse_top20_de_non_dropout 0.05619
wandb:                                                 test_pearson 0.93904
wandb:                                           test_pearson_delta 0.04348
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.44286
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.83571
wandb:                                       test_unseen_single_mse 5e-05
wandb:                                    test_unseen_single_mse_de 0.00016
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00033
wandb:                                   test_unseen_single_pearson 0.99754
wandb:                                test_unseen_single_pearson_de 0.56689
wandb:                             test_unseen_single_pearson_delta 0.15468
wandb:                                                 train_de_mse 0.01918
wandb:                                             train_de_pearson 0.22678
wandb:                                                    train_mse 0.00116
wandb:                                                train_pearson 0.94895
wandb:                                                training_loss 0.13752
wandb:                                                   val_de_mse 0.01714
wandb:                                               val_de_pearson 0.19528
wandb:                                                      val_mse 0.00127
wandb:                                                  val_pearson 0.94411
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_iPSC_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/evo7gf0t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_045606-evo7gf0t/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:19
combo_seen1:92
combo_seen2:28
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_051801-xe0tp3v4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_iPSC_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/xe0tp3v4
wandb: WARNING Serializing object of type ndarray that is 20562048 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1835
Epoch 1 Step 51 Train Loss: 0.1830
Epoch 1 Step 101 Train Loss: 0.2016
Epoch 1 Step 151 Train Loss: 0.1839
Epoch 1 Step 201 Train Loss: 0.1758
Epoch 1 Step 251 Train Loss: 0.1894
Epoch 1 Step 301 Train Loss: 0.1712
Epoch 1 Step 351 Train Loss: 0.1744
Epoch 1 Step 401 Train Loss: 0.1842
Epoch 1 Step 451 Train Loss: 0.1832
Epoch 1 Step 501 Train Loss: 0.1669
Epoch 1 Step 551 Train Loss: 0.1687
Epoch 1 Step 601 Train Loss: 0.1747
Epoch 1 Step 651 Train Loss: 0.1651
Epoch 1 Step 701 Train Loss: 0.1656
Epoch 1 Step 751 Train Loss: 0.1638
Epoch 1 Step 801 Train Loss: 0.1640
Epoch 1 Step 851 Train Loss: 0.2043
Epoch 1 Step 901 Train Loss: 0.1455
Epoch 1 Step 951 Train Loss: 0.1610
Epoch 1 Step 1001 Train Loss: 0.1343
Epoch 1: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0124 Validation Top 20 DE MSE: 0.0259. 
Epoch 2 Step 1 Train Loss: 0.1436
Epoch 2 Step 51 Train Loss: 0.1393
Epoch 2 Step 101 Train Loss: 0.1268
Epoch 2 Step 151 Train Loss: 0.1517
Epoch 2 Step 201 Train Loss: 0.1435
Epoch 2 Step 251 Train Loss: 0.1497
Epoch 2 Step 301 Train Loss: 0.1300
Epoch 2 Step 351 Train Loss: 0.1212
Epoch 2 Step 401 Train Loss: 0.1326
Epoch 2 Step 451 Train Loss: 0.1310
Epoch 2 Step 501 Train Loss: 0.1412
Epoch 2 Step 551 Train Loss: 0.1517
Epoch 2 Step 601 Train Loss: 0.1236
Epoch 2 Step 651 Train Loss: 0.1165
Epoch 2 Step 701 Train Loss: 0.1149
Epoch 2 Step 751 Train Loss: 0.1281
Epoch 2 Step 801 Train Loss: 0.1412
Epoch 2 Step 851 Train Loss: 0.1316
Epoch 2 Step 901 Train Loss: 0.1223
Epoch 2 Step 951 Train Loss: 0.1205
Epoch 2 Step 1001 Train Loss: 0.1310
Epoch 2: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0123 Validation Top 20 DE MSE: 0.0270. 
Epoch 3 Step 1 Train Loss: 0.1321
Epoch 3 Step 51 Train Loss: 0.1609
Epoch 3 Step 101 Train Loss: 0.1584
Epoch 3 Step 151 Train Loss: 0.1423
Epoch 3 Step 201 Train Loss: 0.1185
Epoch 3 Step 251 Train Loss: 0.1237
Epoch 3 Step 301 Train Loss: 0.1243
Epoch 3 Step 351 Train Loss: 0.1249
Epoch 3 Step 401 Train Loss: 0.1156
Epoch 3 Step 451 Train Loss: 0.1251
Epoch 3 Step 501 Train Loss: 0.1232
Epoch 3 Step 551 Train Loss: 0.1230
Epoch 3 Step 601 Train Loss: 0.1390
Epoch 3 Step 651 Train Loss: 0.1180
Epoch 3 Step 701 Train Loss: 0.1198
Epoch 3 Step 751 Train Loss: 0.1232
Epoch 3 Step 801 Train Loss: 0.1171
Epoch 3 Step 851 Train Loss: 0.1313
Epoch 3 Step 901 Train Loss: 0.1187
Epoch 3 Step 951 Train Loss: 0.1270
Epoch 3 Step 1001 Train Loss: 0.1180
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 4 Step 1 Train Loss: 0.1188
Epoch 4 Step 51 Train Loss: 0.1196
Epoch 4 Step 101 Train Loss: 0.1284
Epoch 4 Step 151 Train Loss: 0.1847
Epoch 4 Step 201 Train Loss: 0.1210
Epoch 4 Step 251 Train Loss: 0.1271
Epoch 4 Step 301 Train Loss: 0.1261
Epoch 4 Step 351 Train Loss: 0.1186
Epoch 4 Step 401 Train Loss: 0.1185
Epoch 4 Step 451 Train Loss: 0.1305
Epoch 4 Step 501 Train Loss: 0.1186
Epoch 4 Step 551 Train Loss: 0.1238
Epoch 4 Step 601 Train Loss: 0.1146
Epoch 4 Step 651 Train Loss: 0.1369
Epoch 4 Step 701 Train Loss: 0.1253
Epoch 4 Step 751 Train Loss: 0.1232
Epoch 4 Step 801 Train Loss: 0.1409
Epoch 4 Step 851 Train Loss: 0.1196
Epoch 4 Step 901 Train Loss: 0.1260
Epoch 4 Step 951 Train Loss: 0.1236
Epoch 4 Step 1001 Train Loss: 0.1244
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0261. 
Epoch 5 Step 1 Train Loss: 0.1327
Epoch 5 Step 51 Train Loss: 0.1234
Epoch 5 Step 101 Train Loss: 0.1508
Epoch 5 Step 151 Train Loss: 0.1231
Epoch 5 Step 201 Train Loss: 0.1308
Epoch 5 Step 251 Train Loss: 0.1221
Epoch 5 Step 301 Train Loss: 0.1410
Epoch 5 Step 351 Train Loss: 0.1267
Epoch 5 Step 401 Train Loss: 0.1432
Epoch 5 Step 451 Train Loss: 0.1209
Epoch 5 Step 501 Train Loss: 0.1326
Epoch 5 Step 551 Train Loss: 0.1200
Epoch 5 Step 601 Train Loss: 0.1264
Epoch 5 Step 651 Train Loss: 0.1194
Epoch 5 Step 701 Train Loss: 0.1202
Epoch 5 Step 751 Train Loss: 0.1542
Epoch 5 Step 801 Train Loss: 0.1142
Epoch 5 Step 851 Train Loss: 0.1530
Epoch 5 Step 901 Train Loss: 0.1227
Epoch 5 Step 951 Train Loss: 0.1191
Epoch 5 Step 1001 Train Loss: 0.1455
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 6 Step 1 Train Loss: 0.1245
Epoch 6 Step 51 Train Loss: 0.1245
Epoch 6 Step 101 Train Loss: 0.1355
Epoch 6 Step 151 Train Loss: 0.1184
Epoch 6 Step 201 Train Loss: 0.1211
Epoch 6 Step 251 Train Loss: 0.1336
Epoch 6 Step 301 Train Loss: 0.1302
Epoch 6 Step 351 Train Loss: 0.1203
Epoch 6 Step 401 Train Loss: 0.1268
Epoch 6 Step 451 Train Loss: 0.1352
Epoch 6 Step 501 Train Loss: 0.1211
Epoch 6 Step 551 Train Loss: 0.1175
Epoch 6 Step 601 Train Loss: 0.1203
Epoch 6 Step 651 Train Loss: 0.1136
Epoch 6 Step 701 Train Loss: 0.1186
Epoch 6 Step 751 Train Loss: 0.1307
Epoch 6 Step 801 Train Loss: 0.1212
Epoch 6 Step 851 Train Loss: 0.1226
Epoch 6 Step 901 Train Loss: 0.1616
Epoch 6 Step 951 Train Loss: 0.1202
Epoch 6 Step 1001 Train Loss: 0.3829
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 7 Step 1 Train Loss: 0.1172
Epoch 7 Step 51 Train Loss: 0.1225
Epoch 7 Step 101 Train Loss: 0.1283
Epoch 7 Step 151 Train Loss: 0.1262
Epoch 7 Step 201 Train Loss: 0.1313
Epoch 7 Step 251 Train Loss: 0.1173
Epoch 7 Step 301 Train Loss: 0.1285
Epoch 7 Step 351 Train Loss: 0.1184
Epoch 7 Step 401 Train Loss: 0.1372
Epoch 7 Step 451 Train Loss: 0.1170
Epoch 7 Step 501 Train Loss: 0.1189
Epoch 7 Step 551 Train Loss: 0.1185
Epoch 7 Step 601 Train Loss: 0.1163
Epoch 7 Step 651 Train Loss: 0.1457
Epoch 7 Step 701 Train Loss: 0.1171
Epoch 7 Step 751 Train Loss: 0.1133
Epoch 7 Step 801 Train Loss: 0.1282
Epoch 7 Step 851 Train Loss: 0.1215
Epoch 7 Step 901 Train Loss: 0.1222
Epoch 7 Step 951 Train Loss: 0.1294
Epoch 7 Step 1001 Train Loss: 0.1189
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 8 Step 1 Train Loss: 0.1263
Epoch 8 Step 51 Train Loss: 0.1456
Epoch 8 Step 101 Train Loss: 0.1203
Epoch 8 Step 151 Train Loss: 0.1169
Epoch 8 Step 201 Train Loss: 0.1216
Epoch 8 Step 251 Train Loss: 0.1214
Epoch 8 Step 301 Train Loss: 0.1186
Epoch 8 Step 351 Train Loss: 0.1166
Epoch 8 Step 401 Train Loss: 0.1628
Epoch 8 Step 451 Train Loss: 0.1186
Epoch 8 Step 501 Train Loss: 0.1182
Epoch 8 Step 551 Train Loss: 0.1222
Epoch 8 Step 601 Train Loss: 0.1231
Epoch 8 Step 651 Train Loss: 0.1241
Epoch 8 Step 701 Train Loss: 0.1174
Epoch 8 Step 751 Train Loss: 0.1230
Epoch 8 Step 801 Train Loss: 0.1217
Epoch 8 Step 851 Train Loss: 0.1246
Epoch 8 Step 901 Train Loss: 0.1203
Epoch 8 Step 951 Train Loss: 0.1397
Epoch 8 Step 1001 Train Loss: 0.1183
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 9 Step 1 Train Loss: 0.1191
Epoch 9 Step 51 Train Loss: 0.1190
Epoch 9 Step 101 Train Loss: 0.1194
Epoch 9 Step 151 Train Loss: 0.1145
Epoch 9 Step 201 Train Loss: 0.1159
Epoch 9 Step 251 Train Loss: 0.1159
Epoch 9 Step 301 Train Loss: 0.1169
Epoch 9 Step 351 Train Loss: 0.1246
Epoch 9 Step 401 Train Loss: 0.1212
Epoch 9 Step 451 Train Loss: 0.1325
Epoch 9 Step 501 Train Loss: 0.1219
Epoch 9 Step 551 Train Loss: 0.1281
Epoch 9 Step 601 Train Loss: 0.1158
Epoch 9 Step 651 Train Loss: 0.1163
Epoch 9 Step 701 Train Loss: 0.1255
Epoch 9 Step 751 Train Loss: 0.1191
Epoch 9 Step 801 Train Loss: 0.1234
Epoch 9 Step 851 Train Loss: 0.1214
Epoch 9 Step 901 Train Loss: 0.1173
Epoch 9 Step 951 Train Loss: 0.1173
Epoch 9 Step 1001 Train Loss: 0.1161
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 10 Step 1 Train Loss: 0.1223
Epoch 10 Step 51 Train Loss: 0.1337
Epoch 10 Step 101 Train Loss: 0.1147
Epoch 10 Step 151 Train Loss: 0.1191
Epoch 10 Step 201 Train Loss: 0.1190
Epoch 10 Step 251 Train Loss: 0.1309
Epoch 10 Step 301 Train Loss: 0.1401
Epoch 10 Step 351 Train Loss: 0.1177
Epoch 10 Step 401 Train Loss: 0.1211
Epoch 10 Step 451 Train Loss: 0.1332
Epoch 10 Step 501 Train Loss: 0.1740
Epoch 10 Step 551 Train Loss: 0.2586
Epoch 10 Step 601 Train Loss: 0.1214
Epoch 10 Step 651 Train Loss: 0.1285
Epoch 10 Step 701 Train Loss: 0.1198
Epoch 10 Step 751 Train Loss: 0.1188
Epoch 10 Step 801 Train Loss: 0.1168
Epoch 10 Step 851 Train Loss: 0.1163
Epoch 10 Step 901 Train Loss: 0.1147
Epoch 10 Step 951 Train Loss: 0.1563
Epoch 10 Step 1001 Train Loss: 0.1225
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 11 Step 1 Train Loss: 0.1150
Epoch 11 Step 51 Train Loss: 0.1309
Epoch 11 Step 101 Train Loss: 0.1188
Epoch 11 Step 151 Train Loss: 0.1204
Epoch 11 Step 201 Train Loss: 0.1234
Epoch 11 Step 251 Train Loss: 0.1149
Epoch 11 Step 301 Train Loss: 0.1169
Epoch 11 Step 351 Train Loss: 0.1158
Epoch 11 Step 401 Train Loss: 0.1170
Epoch 11 Step 451 Train Loss: 0.1159
Epoch 11 Step 501 Train Loss: 0.1143
Epoch 11 Step 551 Train Loss: 0.1268
Epoch 11 Step 601 Train Loss: 0.1421
Epoch 11 Step 651 Train Loss: 0.1201
Epoch 11 Step 701 Train Loss: 0.1336
Epoch 11 Step 751 Train Loss: 0.1137
Epoch 11 Step 801 Train Loss: 0.1194
Epoch 11 Step 851 Train Loss: 0.1755
Epoch 11 Step 901 Train Loss: 0.1317
Epoch 11 Step 951 Train Loss: 0.1195
Epoch 11 Step 1001 Train Loss: 0.1155
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 12 Step 1 Train Loss: 0.1431
Epoch 12 Step 51 Train Loss: 0.1528
Epoch 12 Step 101 Train Loss: 0.1167
Epoch 12 Step 151 Train Loss: 0.1226
Epoch 12 Step 201 Train Loss: 0.1291
Epoch 12 Step 251 Train Loss: 0.1401
Epoch 12 Step 301 Train Loss: 0.1176
Epoch 12 Step 351 Train Loss: 0.1176
Epoch 12 Step 401 Train Loss: 0.1163
Epoch 12 Step 451 Train Loss: 0.1229
Epoch 12 Step 501 Train Loss: 0.1165
Epoch 12 Step 551 Train Loss: 0.1248
Epoch 12 Step 601 Train Loss: 0.1200
Epoch 12 Step 651 Train Loss: 0.1361
Epoch 12 Step 701 Train Loss: 0.1185
Epoch 12 Step 751 Train Loss: 0.1215
Epoch 12 Step 801 Train Loss: 0.1155
Epoch 12 Step 851 Train Loss: 0.1351
Epoch 12 Step 901 Train Loss: 0.1304
Epoch 12 Step 951 Train Loss: 0.1268
Epoch 12 Step 1001 Train Loss: 0.1198
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 13 Step 1 Train Loss: 0.1203
Epoch 13 Step 51 Train Loss: 0.1244
Epoch 13 Step 101 Train Loss: 0.1358
Epoch 13 Step 151 Train Loss: 0.1310
Epoch 13 Step 201 Train Loss: 0.1205
Epoch 13 Step 251 Train Loss: 0.1523
Epoch 13 Step 301 Train Loss: 0.1460
Epoch 13 Step 351 Train Loss: 0.1274
Epoch 13 Step 401 Train Loss: 0.1269
Epoch 13 Step 451 Train Loss: 0.1125
Epoch 13 Step 501 Train Loss: 0.1253
Epoch 13 Step 551 Train Loss: 0.1273
Epoch 13 Step 601 Train Loss: 0.1190
Epoch 13 Step 651 Train Loss: 0.1279
Epoch 13 Step 701 Train Loss: 0.1527
Epoch 13 Step 751 Train Loss: 0.1213
Epoch 13 Step 801 Train Loss: 0.1176
Epoch 13 Step 851 Train Loss: 0.1179
Epoch 13 Step 901 Train Loss: 0.1202
Epoch 13 Step 951 Train Loss: 0.1139
Epoch 13 Step 1001 Train Loss: 0.1358
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 14 Step 1 Train Loss: 0.1173
Epoch 14 Step 51 Train Loss: 0.1223
Epoch 14 Step 101 Train Loss: 0.1169
Epoch 14 Step 151 Train Loss: 0.1355
Epoch 14 Step 201 Train Loss: 0.1159
Epoch 14 Step 251 Train Loss: 0.1285
Epoch 14 Step 301 Train Loss: 0.1160
Epoch 14 Step 351 Train Loss: 0.1276
Epoch 14 Step 401 Train Loss: 0.1266
Epoch 14 Step 451 Train Loss: 0.1284
Epoch 14 Step 501 Train Loss: 0.1134
Epoch 14 Step 551 Train Loss: 0.1166
Epoch 14 Step 601 Train Loss: 0.1268
Epoch 14 Step 651 Train Loss: 0.1441
Epoch 14 Step 701 Train Loss: 0.1266
Epoch 14 Step 751 Train Loss: 0.1179
Epoch 14 Step 801 Train Loss: 0.1270
Epoch 14 Step 851 Train Loss: 0.1235
Epoch 14 Step 901 Train Loss: 0.1254
Epoch 14 Step 951 Train Loss: 0.1386
Epoch 14 Step 1001 Train Loss: 0.1200
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Epoch 15 Step 1 Train Loss: 0.1204
Epoch 15 Step 51 Train Loss: 0.1243
Epoch 15 Step 101 Train Loss: 0.1266
Epoch 15 Step 151 Train Loss: 0.1138
Epoch 15 Step 201 Train Loss: 0.1173
Epoch 15 Step 251 Train Loss: 0.1245
Epoch 15 Step 301 Train Loss: 0.1213
Epoch 15 Step 351 Train Loss: 0.1230
Epoch 15 Step 401 Train Loss: 0.1196
Epoch 15 Step 451 Train Loss: 0.1177
Epoch 15 Step 501 Train Loss: 0.1216
Epoch 15 Step 551 Train Loss: 0.1610
Epoch 15 Step 601 Train Loss: 0.1271
Epoch 15 Step 651 Train Loss: 0.1167
Epoch 15 Step 701 Train Loss: 0.1173
Epoch 15 Step 751 Train Loss: 0.1161
Epoch 15 Step 801 Train Loss: 0.1197
Epoch 15 Step 851 Train Loss: 0.1227
Epoch 15 Step 901 Train Loss: 0.1198
Epoch 15 Step 951 Train Loss: 0.1332
Epoch 15 Step 1001 Train Loss: 0.1233
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0260. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0186
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0019141498
test_combo_seen0_pearson: 0.9151814860922155
test_combo_seen0_mse_de: 0.022767061
test_combo_seen0_pearson_de: 0.014994093524203474
test_combo_seen1_mse: 0.0015102542
test_combo_seen1_pearson: 0.9333724172850545
test_combo_seen1_mse_de: 0.018924695
test_combo_seen1_pearson_de: 0.06703287445965411
test_combo_seen2_mse: 0.0015452033
test_combo_seen2_pearson: 0.9316700078212296
test_combo_seen2_mse_de: 0.019380482
test_combo_seen2_pearson_de: 0.0708401867623029
test_unseen_single_mse: 0.00016757932
test_unseen_single_pearson: 0.9919254026566933
test_unseen_single_mse_de: 0.0006149452
test_unseen_single_pearson_de: 0.5001932903469742
test_combo_seen0_pearson_delta: 0.0034926650809430563
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.518421052631579
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.5710526315789474
test_combo_seen0_mse_top20_de_non_dropout: 0.08670976
test_combo_seen1_pearson_delta: -0.0031791300491299753
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.5108695652173914
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.5744565217391304
test_combo_seen1_mse_top20_de_non_dropout: 0.05541882
test_combo_seen2_pearson_delta: 0.0020845729061153575
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.5357142857142857
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.6714285714285715
test_combo_seen2_mse_top20_de_non_dropout: 0.0630284
test_unseen_single_pearson_delta: 0.04919063235320119
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4357142857142858
test_unseen_single_frac_sigma_below_1_non_dropout: 0.7999999999999999
test_unseen_single_mse_top20_de_non_dropout: 0.0004636589
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.030 MB uploadedwandb: | 0.028 MB of 0.030 MB uploadedwandb: / 0.028 MB of 0.030 MB uploadedwandb: - 0.030 MB of 0.030 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:                                                   val_de_mse ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.51842
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.57105
wandb:                                         test_combo_seen0_mse 0.00191
wandb:                                      test_combo_seen0_mse_de 0.02277
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.08671
wandb:                                     test_combo_seen0_pearson 0.91518
wandb:                                  test_combo_seen0_pearson_de 0.01499
wandb:                               test_combo_seen0_pearson_delta 0.00349
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.51087
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.57446
wandb:                                         test_combo_seen1_mse 0.00151
wandb:                                      test_combo_seen1_mse_de 0.01892
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.05542
wandb:                                     test_combo_seen1_pearson 0.93337
wandb:                                  test_combo_seen1_pearson_de 0.06703
wandb:                               test_combo_seen1_pearson_delta -0.00318
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.53571
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.67143
wandb:                                         test_combo_seen2_mse 0.00155
wandb:                                      test_combo_seen2_mse_de 0.01938
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.06303
wandb:                                     test_combo_seen2_pearson 0.93167
wandb:                                  test_combo_seen2_pearson_de 0.07084
wandb:                               test_combo_seen2_pearson_delta 0.00208
wandb:                                                  test_de_mse 0.01863
wandb:                                              test_de_pearson 0.08176
wandb:               test_frac_opposite_direction_top20_non_dropout 0.51301
wandb:                          test_frac_sigma_below_1_non_dropout 0.60342
wandb:                                                     test_mse 0.00151
wandb:                                test_mse_top20_de_non_dropout 0.05832
wandb:                                                 test_pearson 0.93349
wandb:                                           test_pearson_delta 0.00121
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.43571
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.8
wandb:                                       test_unseen_single_mse 0.00017
wandb:                                    test_unseen_single_mse_de 0.00061
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00046
wandb:                                   test_unseen_single_pearson 0.99193
wandb:                                test_unseen_single_pearson_de 0.50019
wandb:                             test_unseen_single_pearson_delta 0.04919
wandb:                                                 train_de_mse 0.01193
wandb:                                             train_de_pearson 0.23075
wandb:                                                    train_mse 0.00092
wandb:                                                train_pearson 0.95939
wandb:                                                training_loss 0.12852
wandb:                                                   val_de_mse 0.02605
wandb:                                               val_de_pearson 0.07492
wandb:                                                      val_mse 0.00168
wandb:                                                  val_pearson 0.9263
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_iPSC_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/xe0tp3v4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_051801-xe0tp3v4/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:12
combo_seen1:91
combo_seen2:30
unseen_single:7
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_054125-1aizmvfz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_TianKampmann2019_iPSC_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/1aizmvfz
wandb: WARNING Serializing object of type ndarray that is 20562048 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.1910
Epoch 1 Step 51 Train Loss: 0.2094
Epoch 1 Step 101 Train Loss: 0.1859
Epoch 1 Step 151 Train Loss: 0.1918
Epoch 1 Step 201 Train Loss: 0.1879
Epoch 1 Step 251 Train Loss: 0.2108
Epoch 1 Step 301 Train Loss: 0.1858
Epoch 1 Step 351 Train Loss: 0.1833
Epoch 1 Step 401 Train Loss: 0.1882
Epoch 1 Step 451 Train Loss: 0.1890
Epoch 1 Step 501 Train Loss: 0.1924
Epoch 1 Step 551 Train Loss: 0.1834
Epoch 1 Step 601 Train Loss: 0.2075
Epoch 1 Step 651 Train Loss: 0.1847
Epoch 1 Step 701 Train Loss: 0.1863
Epoch 1 Step 751 Train Loss: 0.1791
Epoch 1 Step 801 Train Loss: 0.1654
Epoch 1 Step 851 Train Loss: 0.1579
Epoch 1: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0282. 
Epoch 2 Step 1 Train Loss: 0.1647
Epoch 2 Step 51 Train Loss: 0.1686
Epoch 2 Step 101 Train Loss: 0.1594
Epoch 2 Step 151 Train Loss: 0.1543
Epoch 2 Step 201 Train Loss: 0.1550
Epoch 2 Step 251 Train Loss: 0.1791
Epoch 2 Step 301 Train Loss: 0.1479
Epoch 2 Step 351 Train Loss: 0.1570
Epoch 2 Step 401 Train Loss: 0.1587
Epoch 2 Step 451 Train Loss: 0.1423
Epoch 2 Step 501 Train Loss: 0.1717
Epoch 2 Step 551 Train Loss: 0.1355
Epoch 2 Step 601 Train Loss: 0.1692
Epoch 2 Step 651 Train Loss: 0.1524
Epoch 2 Step 701 Train Loss: 0.1506
Epoch 2 Step 751 Train Loss: 0.1335
Epoch 2 Step 801 Train Loss: 0.1984
Epoch 2 Step 851 Train Loss: 0.1376
Epoch 2: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0316. 
Epoch 3 Step 1 Train Loss: 0.1365
Epoch 3 Step 51 Train Loss: 0.1503
Epoch 3 Step 101 Train Loss: 0.1270
Epoch 3 Step 151 Train Loss: 0.1474
Epoch 3 Step 201 Train Loss: 0.1306
Epoch 3 Step 251 Train Loss: 0.1594
Epoch 3 Step 301 Train Loss: 0.1315
Epoch 3 Step 351 Train Loss: 0.1404
Epoch 3 Step 401 Train Loss: 0.1429
Epoch 3 Step 451 Train Loss: 0.1478
Epoch 3 Step 501 Train Loss: 0.1449
Epoch 3 Step 551 Train Loss: 0.1686
Epoch 3 Step 601 Train Loss: 0.1393
Epoch 3 Step 651 Train Loss: 0.1392
Epoch 3 Step 701 Train Loss: 0.1426
Epoch 3 Step 751 Train Loss: 0.1642
Epoch 3 Step 801 Train Loss: 0.1476
Epoch 3 Step 851 Train Loss: 0.1387
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.0302. 
Epoch 4 Step 1 Train Loss: 0.1434
Epoch 4 Step 51 Train Loss: 0.1498
Epoch 4 Step 101 Train Loss: 0.1562
Epoch 4 Step 151 Train Loss: 0.1573
Epoch 4 Step 201 Train Loss: 0.1615
Epoch 4 Step 251 Train Loss: 0.1354
Epoch 4 Step 301 Train Loss: 0.1497
Epoch 4 Step 351 Train Loss: 0.1382
Epoch 4 Step 401 Train Loss: 0.1337
Epoch 4 Step 451 Train Loss: 0.1471
Epoch 4 Step 501 Train Loss: 0.1585
Epoch 4 Step 551 Train Loss: 0.1337
Epoch 4 Step 601 Train Loss: 0.1319
Epoch 4 Step 651 Train Loss: 0.1496
Epoch 4 Step 701 Train Loss: 0.1449
Epoch 4 Step 751 Train Loss: 0.1390
Epoch 4 Step 801 Train Loss: 0.1386
Epoch 4 Step 851 Train Loss: 0.1395
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0274. 
Epoch 5 Step 1 Train Loss: 0.1949
Epoch 5 Step 51 Train Loss: 0.1541
Epoch 5 Step 101 Train Loss: 0.1363
Epoch 5 Step 151 Train Loss: 0.1565
Epoch 5 Step 201 Train Loss: 0.1564
Epoch 5 Step 251 Train Loss: 0.1543
Epoch 5 Step 301 Train Loss: 0.1431
Epoch 5 Step 351 Train Loss: 0.1378
Epoch 5 Step 401 Train Loss: 0.1399
Epoch 5 Step 451 Train Loss: 0.1534
Epoch 5 Step 501 Train Loss: 0.1787
Epoch 5 Step 551 Train Loss: 0.1453
Epoch 5 Step 601 Train Loss: 0.1482
Epoch 5 Step 651 Train Loss: 0.1480
Epoch 5 Step 701 Train Loss: 0.1442
Epoch 5 Step 751 Train Loss: 0.1649
Epoch 5 Step 801 Train Loss: 0.1457
Epoch 5 Step 851 Train Loss: 0.1357
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 6 Step 1 Train Loss: 0.1694
Epoch 6 Step 51 Train Loss: 0.1501
Epoch 6 Step 101 Train Loss: 0.1410
Epoch 6 Step 151 Train Loss: 0.1415
Epoch 6 Step 201 Train Loss: 0.1398
Epoch 6 Step 251 Train Loss: 0.1432
Epoch 6 Step 301 Train Loss: 0.1389
Epoch 6 Step 351 Train Loss: 0.1382
Epoch 6 Step 401 Train Loss: 0.1370
Epoch 6 Step 451 Train Loss: 0.1438
Epoch 6 Step 501 Train Loss: 0.1519
Epoch 6 Step 551 Train Loss: 0.1379
Epoch 6 Step 601 Train Loss: 0.1428
Epoch 6 Step 651 Train Loss: 0.1546
Epoch 6 Step 701 Train Loss: 0.1406
Epoch 6 Step 751 Train Loss: 0.1493
Epoch 6 Step 801 Train Loss: 0.1383
Epoch 6 Step 851 Train Loss: 0.1396
Epoch 6: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0275. 
Epoch 7 Step 1 Train Loss: 0.1354
Epoch 7 Step 51 Train Loss: 0.1406
Epoch 7 Step 101 Train Loss: 0.1565
Epoch 7 Step 151 Train Loss: 0.1349
Epoch 7 Step 201 Train Loss: 0.1397
Epoch 7 Step 251 Train Loss: 0.1371
Epoch 7 Step 301 Train Loss: 0.1419
Epoch 7 Step 351 Train Loss: 0.1407
Epoch 7 Step 401 Train Loss: 0.1506
Epoch 7 Step 451 Train Loss: 0.1397
Epoch 7 Step 501 Train Loss: 0.1381
Epoch 7 Step 551 Train Loss: 0.1372
Epoch 7 Step 601 Train Loss: 0.1393
Epoch 7 Step 651 Train Loss: 0.1343
Epoch 7 Step 701 Train Loss: 0.1392
Epoch 7 Step 751 Train Loss: 0.1366
Epoch 7 Step 801 Train Loss: 0.1662
Epoch 7 Step 851 Train Loss: 0.1379
Epoch 7: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 8 Step 1 Train Loss: 0.1595
Epoch 8 Step 51 Train Loss: 0.1460
Epoch 8 Step 101 Train Loss: 0.1486
Epoch 8 Step 151 Train Loss: 0.1469
Epoch 8 Step 201 Train Loss: 0.1406
Epoch 8 Step 251 Train Loss: 0.1372
Epoch 8 Step 301 Train Loss: 0.1468
Epoch 8 Step 351 Train Loss: 0.1385
Epoch 8 Step 401 Train Loss: 0.1858
Epoch 8 Step 451 Train Loss: 0.1369
Epoch 8 Step 501 Train Loss: 0.1408
Epoch 8 Step 551 Train Loss: 0.1452
Epoch 8 Step 601 Train Loss: 0.1479
Epoch 8 Step 651 Train Loss: 0.1366
Epoch 8 Step 701 Train Loss: 0.1550
Epoch 8 Step 751 Train Loss: 0.1468
Epoch 8 Step 801 Train Loss: 0.1431
Epoch 8 Step 851 Train Loss: 0.1553
Epoch 8: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 9 Step 1 Train Loss: 0.1447
Epoch 9 Step 51 Train Loss: 0.2706
Epoch 9 Step 101 Train Loss: 0.1526
Epoch 9 Step 151 Train Loss: 0.1402
Epoch 9 Step 201 Train Loss: 0.1428
Epoch 9 Step 251 Train Loss: 0.1784
Epoch 9 Step 301 Train Loss: 0.1589
Epoch 9 Step 351 Train Loss: 0.1385
Epoch 9 Step 401 Train Loss: 0.1487
Epoch 9 Step 451 Train Loss: 0.1624
Epoch 9 Step 501 Train Loss: 0.1390
Epoch 9 Step 551 Train Loss: 0.1418
Epoch 9 Step 601 Train Loss: 0.1465
Epoch 9 Step 651 Train Loss: 0.1367
Epoch 9 Step 701 Train Loss: 0.1555
Epoch 9 Step 751 Train Loss: 0.1403
Epoch 9 Step 801 Train Loss: 0.1351
Epoch 9 Step 851 Train Loss: 0.1418
Epoch 9: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 10 Step 1 Train Loss: 0.2548
Epoch 10 Step 51 Train Loss: 0.1407
Epoch 10 Step 101 Train Loss: 0.1427
Epoch 10 Step 151 Train Loss: 0.1569
Epoch 10 Step 201 Train Loss: 0.1420
Epoch 10 Step 251 Train Loss: 0.1584
Epoch 10 Step 301 Train Loss: 0.1420
Epoch 10 Step 351 Train Loss: 0.1445
Epoch 10 Step 401 Train Loss: 0.1431
Epoch 10 Step 451 Train Loss: 0.1505
Epoch 10 Step 501 Train Loss: 0.1420
Epoch 10 Step 551 Train Loss: 0.1431
Epoch 10 Step 601 Train Loss: 0.1428
Epoch 10 Step 651 Train Loss: 0.1736
Epoch 10 Step 701 Train Loss: 0.1452
Epoch 10 Step 751 Train Loss: 0.1461
Epoch 10 Step 801 Train Loss: 0.1413
Epoch 10 Step 851 Train Loss: 0.1561
Epoch 10: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 11 Step 1 Train Loss: 0.1539
Epoch 11 Step 51 Train Loss: 0.1406
Epoch 11 Step 101 Train Loss: 0.1971
Epoch 11 Step 151 Train Loss: 0.1553
Epoch 11 Step 201 Train Loss: 0.1704
Epoch 11 Step 251 Train Loss: 0.1554
Epoch 11 Step 301 Train Loss: 0.1525
Epoch 11 Step 351 Train Loss: 0.1398
Epoch 11 Step 401 Train Loss: 0.1533
Epoch 11 Step 451 Train Loss: 0.1373
Epoch 11 Step 501 Train Loss: 0.1341
Epoch 11 Step 551 Train Loss: 0.1380
Epoch 11 Step 601 Train Loss: 0.1527
Epoch 11 Step 651 Train Loss: 0.1575
Epoch 11 Step 701 Train Loss: 0.1413
Epoch 11 Step 751 Train Loss: 0.1392
Epoch 11 Step 801 Train Loss: 0.1544
Epoch 11 Step 851 Train Loss: 0.1481
Epoch 11: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 12 Step 1 Train Loss: 0.2526
Epoch 12 Step 51 Train Loss: 0.1498
Epoch 12 Step 101 Train Loss: 0.1360
Epoch 12 Step 151 Train Loss: 0.1368
Epoch 12 Step 201 Train Loss: 0.1556
Epoch 12 Step 251 Train Loss: 0.1498
Epoch 12 Step 301 Train Loss: 0.1372
Epoch 12 Step 351 Train Loss: 0.1358
Epoch 12 Step 401 Train Loss: 0.1463
Epoch 12 Step 451 Train Loss: 0.1392
Epoch 12 Step 501 Train Loss: 0.1548
Epoch 12 Step 551 Train Loss: 0.1458
Epoch 12 Step 601 Train Loss: 0.1414
Epoch 12 Step 651 Train Loss: 0.1389
Epoch 12 Step 701 Train Loss: 0.1544
Epoch 12 Step 751 Train Loss: 0.1399
Epoch 12 Step 801 Train Loss: 0.1439
Epoch 12 Step 851 Train Loss: 0.1384
Epoch 12: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 13 Step 1 Train Loss: 0.1387
Epoch 13 Step 51 Train Loss: 0.1415
Epoch 13 Step 101 Train Loss: 0.1512
Epoch 13 Step 151 Train Loss: 0.1400
Epoch 13 Step 201 Train Loss: 0.1361
Epoch 13 Step 251 Train Loss: 0.1354
Epoch 13 Step 301 Train Loss: 0.1509
Epoch 13 Step 351 Train Loss: 0.1371
Epoch 13 Step 401 Train Loss: 0.1604
Epoch 13 Step 451 Train Loss: 0.1690
Epoch 13 Step 501 Train Loss: 0.1369
Epoch 13 Step 551 Train Loss: 0.1555
Epoch 13 Step 601 Train Loss: 0.1487
Epoch 13 Step 651 Train Loss: 0.1372
Epoch 13 Step 701 Train Loss: 0.1354
Epoch 13 Step 751 Train Loss: 0.1459
Epoch 13 Step 801 Train Loss: 0.1358
Epoch 13 Step 851 Train Loss: 0.1375
Epoch 13: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 14 Step 1 Train Loss: 0.1623
Epoch 14 Step 51 Train Loss: 0.1466
Epoch 14 Step 101 Train Loss: 0.1453
Epoch 14 Step 151 Train Loss: 0.1361
Epoch 14 Step 201 Train Loss: 0.1368
Epoch 14 Step 251 Train Loss: 0.1341
Epoch 14 Step 301 Train Loss: 0.1359
Epoch 14 Step 351 Train Loss: 0.1375
Epoch 14 Step 401 Train Loss: 0.1437
Epoch 14 Step 451 Train Loss: 0.1420
Epoch 14 Step 501 Train Loss: 0.1435
Epoch 14 Step 551 Train Loss: 0.1408
Epoch 14 Step 601 Train Loss: 0.1811
Epoch 14 Step 651 Train Loss: 0.1493
Epoch 14 Step 701 Train Loss: 0.1398
Epoch 14 Step 751 Train Loss: 0.1342
Epoch 14 Step 801 Train Loss: 0.1422
Epoch 14 Step 851 Train Loss: 0.1417
Epoch 14: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Epoch 15 Step 1 Train Loss: 0.1378
Epoch 15 Step 51 Train Loss: 0.1426
Epoch 15 Step 101 Train Loss: 0.1544
Epoch 15 Step 151 Train Loss: 0.1402
Epoch 15 Step 201 Train Loss: 0.1338
Epoch 15 Step 251 Train Loss: 0.1456
Epoch 15 Step 301 Train Loss: 0.1513
Epoch 15 Step 351 Train Loss: 0.1556
Epoch 15 Step 401 Train Loss: 0.1391
Epoch 15 Step 451 Train Loss: 0.1588
Epoch 15 Step 501 Train Loss: 0.1381
Epoch 15 Step 551 Train Loss: 0.1432
Epoch 15 Step 601 Train Loss: 0.1366
Epoch 15 Step 651 Train Loss: 0.1665
Epoch 15 Step 701 Train Loss: 0.2251
Epoch 15 Step 751 Train Loss: 0.1373
Epoch 15 Step 801 Train Loss: 0.1406
Epoch 15 Step 851 Train Loss: 0.1394
Epoch 15: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0275. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0206
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0009788662
test_combo_seen0_pearson: 0.9540652362181469
test_combo_seen0_mse_de: 0.011642643
test_combo_seen0_pearson_de: 0.05017917664927255
test_combo_seen1_mse: 0.0016736715
test_combo_seen1_pearson: 0.926646461903619
test_combo_seen1_mse_de: 0.021896621
test_combo_seen1_pearson_de: 0.12737237495022516
test_combo_seen2_mse: 0.0014527916
test_combo_seen2_pearson: 0.9355925941880815
test_combo_seen2_mse_de: 0.024939049
test_combo_seen2_pearson_de: 0.15403799913230723
test_unseen_single_mse: 6.586269e-05
test_unseen_single_pearson: 0.996836755666236
test_unseen_single_mse_de: 0.00023321847
test_unseen_single_pearson_de: 0.49634684846501004
test_combo_seen0_pearson_delta: 0.0465416028598477
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.4708333333333334
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.6333333333333334
test_combo_seen0_mse_top20_de_non_dropout: 0.027934045
test_combo_seen1_pearson_delta: 0.02952943471049109
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.5071428571428572
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.5857142857142857
test_combo_seen1_mse_top20_de_non_dropout: 0.07066131
test_combo_seen2_pearson_delta: 0.016322929696276404
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.5216666666666667
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.5466666666666666
test_combo_seen2_mse_top20_de_non_dropout: 0.060568098
test_unseen_single_pearson_delta: 0.0996525359469076
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4714285714285715
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8285714285714285
test_unseen_single_mse_top20_de_non_dropout: 0.00036385385
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.004 MB of 0.028 MB uploadedwandb: / 0.028 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                   val_de_mse ‚ñÇ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.47083
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.63333
wandb:                                         test_combo_seen0_mse 0.00098
wandb:                                      test_combo_seen0_mse_de 0.01164
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.02793
wandb:                                     test_combo_seen0_pearson 0.95407
wandb:                                  test_combo_seen0_pearson_de 0.05018
wandb:                               test_combo_seen0_pearson_delta 0.04654
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.50714
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.58571
wandb:                                         test_combo_seen1_mse 0.00167
wandb:                                      test_combo_seen1_mse_de 0.0219
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.07066
wandb:                                     test_combo_seen1_pearson 0.92665
wandb:                                  test_combo_seen1_pearson_de 0.12737
wandb:                               test_combo_seen1_pearson_delta 0.02953
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.52167
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.54667
wandb:                                         test_combo_seen2_mse 0.00145
wandb:                                      test_combo_seen2_mse_de 0.02494
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.06057
wandb:                                     test_combo_seen2_pearson 0.93559
wandb:                                  test_combo_seen2_pearson_de 0.15404
wandb:                               test_combo_seen2_pearson_delta 0.01632
wandb:                                                  test_de_mse 0.02059
wandb:                                              test_de_pearson 0.14492
wandb:               test_frac_opposite_direction_top20_non_dropout 0.50536
wandb:                          test_frac_sigma_below_1_non_dropout 0.59357
wandb:                                                     test_mse 0.00149
wandb:                                test_mse_top20_de_non_dropout 0.06132
wandb:                                                 test_pearson 0.93442
wandb:                                           test_pearson_delta 0.03166
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.47143
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.82857
wandb:                                       test_unseen_single_mse 7e-05
wandb:                                    test_unseen_single_mse_de 0.00023
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00036
wandb:                                   test_unseen_single_pearson 0.99684
wandb:                                test_unseen_single_pearson_de 0.49635
wandb:                             test_unseen_single_pearson_delta 0.09965
wandb:                                                 train_de_mse 0.00914
wandb:                                             train_de_pearson 0.18534
wandb:                                                    train_mse 0.00081
wandb:                                                train_pearson 0.96302
wandb:                                                training_loss 0.13914
wandb:                                                   val_de_mse 0.02752
wandb:                                               val_de_pearson 0.09455
wandb:                                                      val_mse 0.00164
wandb:                                                  val_pearson 0.92829
wandb: 
wandb: üöÄ View run geneformer_TianKampmann2019_iPSC_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/1aizmvfz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_054125-1aizmvfz/logs
