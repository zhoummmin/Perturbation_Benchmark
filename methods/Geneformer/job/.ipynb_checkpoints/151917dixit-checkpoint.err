Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_231520-sm199vix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_combined_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/sm199vix
wandb: WARNING Serializing object of type ndarray that is 20541568 bytes
  0%|                                                  | 0/3600 [00:00<?, ?it/s]  0%|                                          | 1/3600 [00:00<07:50,  7.65it/s]  0%|                                          | 5/3600 [00:00<02:27, 24.45it/s]  0%|‚ñè                                        | 11/3600 [00:00<01:38, 36.51it/s]  0%|‚ñè                                        | 18/3600 [00:00<01:14, 48.19it/s]  1%|‚ñé                                        | 32/3600 [00:00<00:48, 72.82it/s]  1%|‚ñç                                        | 41/3600 [00:00<00:47, 75.68it/s]  1%|‚ñå                                        | 49/3600 [00:00<00:50, 69.75it/s]  2%|‚ñã                                        | 56/3600 [00:00<00:56, 62.43it/s]  2%|‚ñã                                        | 63/3600 [00:01<01:06, 53.28it/s]  2%|‚ñä                                        | 71/3600 [00:01<01:00, 58.40it/s]  2%|‚ñâ                                        | 78/3600 [00:01<01:12, 48.91it/s]  2%|‚ñà                                        | 88/3600 [00:01<00:59, 59.07it/s]  3%|‚ñà                                        | 95/3600 [00:01<01:04, 54.60it/s]  3%|‚ñà                                       | 101/3600 [00:01<01:04, 54.11it/s]  3%|‚ñà‚ñè                                      | 107/3600 [00:01<01:05, 53.64it/s]  3%|‚ñà‚ñé                                      | 113/3600 [00:02<01:12, 47.88it/s]  3%|‚ñà‚ñé                                      | 121/3600 [00:02<01:11, 48.63it/s]  4%|‚ñà‚ñç                                      | 130/3600 [00:02<01:02, 55.25it/s]  4%|‚ñà‚ñå                                      | 136/3600 [00:02<01:07, 51.62it/s]  4%|‚ñà‚ñå                                      | 145/3600 [00:02<01:20, 42.76it/s]  4%|‚ñà‚ñä                                      | 159/3600 [00:02<01:02, 54.96it/s]  5%|‚ñà‚ñâ                                      | 174/3600 [00:03<00:47, 72.04it/s]  5%|‚ñà‚ñà                                      | 185/3600 [00:03<00:44, 76.73it/s]  5%|‚ñà‚ñà‚ñè                                     | 194/3600 [00:03<00:43, 78.03it/s]  6%|‚ñà‚ñà‚ñé                                     | 204/3600 [00:03<00:42, 79.31it/s]  6%|‚ñà‚ñà‚ñé                                     | 213/3600 [00:03<00:43, 77.14it/s]  6%|‚ñà‚ñà‚ñç                                     | 223/3600 [00:03<00:41, 81.13it/s]  6%|‚ñà‚ñà‚ñå                                     | 232/3600 [00:03<00:42, 78.88it/s]  7%|‚ñà‚ñà‚ñã                                     | 241/3600 [00:03<00:42, 79.88it/s]  7%|‚ñà‚ñà‚ñä                                     | 250/3600 [00:04<00:41, 80.57it/s]  7%|‚ñà‚ñà‚ñâ                                     | 260/3600 [00:04<00:40, 83.48it/s]  7%|‚ñà‚ñà‚ñâ                                     | 269/3600 [00:04<00:40, 81.41it/s]  8%|‚ñà‚ñà‚ñà                                     | 278/3600 [00:04<00:40, 81.78it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 287/3600 [00:04<00:40, 81.06it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 296/3600 [00:04<00:52, 63.24it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 313/3600 [00:04<00:38, 86.27it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 323/3600 [00:05<00:47, 69.50it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 339/3600 [00:05<00:37, 87.76it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 350/3600 [00:05<00:37, 86.95it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 360/3600 [00:05<00:37, 85.56it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 370/3600 [00:05<00:36, 87.45it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 380/3600 [00:05<00:38, 83.15it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 390/3600 [00:05<00:38, 83.14it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 399/3600 [00:05<00:40, 79.31it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 408/3600 [00:05<00:43, 73.08it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 422/3600 [00:06<00:36, 87.33it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 432/3600 [00:06<00:39, 79.89it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 441/3600 [00:06<00:42, 74.33it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 455/3600 [00:06<00:35, 87.48it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 465/3600 [00:06<00:36, 86.65it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 474/3600 [00:06<00:37, 83.49it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 484/3600 [00:06<00:37, 82.87it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 493/3600 [00:06<00:37, 83.58it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 502/3600 [00:07<00:44, 70.28it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 518/3600 [00:07<00:34, 89.38it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 528/3600 [00:07<00:35, 86.65it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 538/3600 [00:07<00:36, 82.85it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 547/3600 [00:07<00:36, 82.86it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 556/3600 [00:07<00:36, 83.80it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 565/3600 [00:07<00:36, 83.59it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 574/3600 [00:07<00:36, 82.87it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 583/3600 [00:08<00:37, 80.35it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 592/3600 [00:08<00:36, 82.50it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 601/3600 [00:08<00:36, 82.01it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 610/3600 [00:08<00:36, 82.31it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 619/3600 [00:08<00:36, 82.70it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 628/3600 [00:08<00:35, 82.69it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 637/3600 [00:08<00:35, 82.67it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 646/3600 [00:08<00:35, 83.70it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 655/3600 [00:08<00:35, 82.99it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 664/3600 [00:09<00:35, 82.55it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 673/3600 [00:09<00:35, 82.52it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 682/3600 [00:09<00:35, 82.19it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 691/3600 [00:09<00:35, 83.06it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 700/3600 [00:09<00:35, 80.90it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 709/3600 [00:09<00:35, 82.27it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 718/3600 [00:09<00:34, 82.71it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 727/3600 [00:09<00:34, 82.65it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 736/3600 [00:09<00:34, 82.98it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 746/3600 [00:10<00:33, 86.23it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 755/3600 [00:10<00:33, 83.74it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 765/3600 [00:10<00:32, 86.32it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 774/3600 [00:10<00:33, 85.18it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 783/3600 [00:10<00:34, 82.19it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 793/3600 [00:10<00:32, 85.32it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 802/3600 [00:10<00:32, 84.85it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 811/3600 [00:10<00:32, 85.17it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 820/3600 [00:10<00:32, 84.57it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 829/3600 [00:11<00:33, 83.84it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 838/3600 [00:11<00:34, 80.76it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 847/3600 [00:11<00:33, 81.16it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 856/3600 [00:11<00:33, 82.45it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 866/3600 [00:11<00:32, 85.23it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 875/3600 [00:11<00:32, 84.92it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 884/3600 [00:11<00:33, 81.80it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 893/3600 [00:11<00:32, 82.25it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 902/3600 [00:11<00:32, 82.50it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 911/3600 [00:12<00:32, 83.09it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 921/3600 [00:12<00:30, 87.01it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 930/3600 [00:12<00:31, 83.77it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 940/3600 [00:12<00:30, 86.02it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 949/3600 [00:12<00:30, 85.98it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 958/3600 [00:12<00:30, 86.10it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 967/3600 [00:12<00:30, 85.51it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 976/3600 [00:12<00:30, 85.28it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 985/3600 [00:12<00:31, 83.17it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 994/3600 [00:12<00:30, 84.08it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1004/3600 [00:13<00:30, 86.18it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1013/3600 [00:13<00:31, 82.27it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1022/3600 [00:13<00:31, 82.65it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1031/3600 [00:13<00:30, 83.67it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1041/3600 [00:13<00:29, 86.16it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1050/3600 [00:13<00:29, 86.34it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1059/3600 [00:13<00:30, 83.89it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1068/3600 [00:13<00:29, 84.73it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1078/3600 [00:13<00:29, 85.12it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1088/3600 [00:14<00:29, 86.01it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1097/3600 [00:14<00:28, 86.83it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1106/3600 [00:14<00:28, 86.71it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1116/3600 [00:14<00:28, 86.28it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1126/3600 [00:14<00:27, 88.63it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1135/3600 [00:14<00:27, 88.04it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1144/3600 [00:14<00:29, 84.51it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1153/3600 [00:14<00:29, 83.95it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1162/3600 [00:14<00:28, 84.73it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1171/3600 [00:15<00:28, 85.28it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1180/3600 [00:15<00:28, 85.21it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1189/3600 [00:15<00:28, 83.84it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1199/3600 [00:15<00:27, 86.84it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1208/3600 [00:15<00:28, 83.47it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1217/3600 [00:15<00:28, 84.12it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1227/3600 [00:15<00:27, 86.74it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1236/3600 [00:15<00:27, 85.95it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1245/3600 [00:15<00:27, 86.52it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1254/3600 [00:16<00:27, 85.56it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1263/3600 [00:16<00:27, 84.89it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1272/3600 [00:16<00:27, 86.19it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1281/3600 [00:16<00:28, 82.57it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1291/3600 [00:16<00:26, 85.83it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1300/3600 [00:16<00:26, 85.58it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1309/3600 [00:16<00:27, 82.61it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1318/3600 [00:16<00:28, 81.10it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1329/3600 [00:16<00:26, 86.84it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1338/3600 [00:17<00:26, 86.39it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1347/3600 [00:17<00:26, 86.62it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1356/3600 [00:17<00:26, 83.75it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1366/3600 [00:17<00:25, 86.68it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1375/3600 [00:17<00:25, 86.10it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1384/3600 [00:17<00:26, 83.91it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1393/3600 [00:17<00:26, 82.22it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1404/3600 [00:17<00:25, 87.42it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1413/3600 [00:17<00:25, 87.13it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1422/3600 [00:18<00:25, 86.76it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1431/3600 [00:18<00:25, 83.56it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1440/3600 [00:18<00:26, 82.94it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1449/3600 [00:18<00:28, 76.49it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1457/3600 [00:18<00:30, 69.95it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1465/3600 [00:18<00:32, 66.08it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1472/3600 [00:18<00:33, 64.31it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1479/3600 [00:18<00:34, 61.85it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1486/3600 [00:19<00:35, 59.06it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1492/3600 [00:19<00:38, 55.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1500/3600 [00:19<00:35, 58.43it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1510/3600 [00:19<00:30, 67.46it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1518/3600 [00:19<00:29, 69.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1528/3600 [00:19<00:27, 76.08it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1537/3600 [00:19<00:26, 76.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1546/3600 [00:19<00:26, 77.62it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1556/3600 [00:19<00:24, 82.29it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1565/3600 [00:20<00:24, 82.64it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1575/3600 [00:20<00:23, 85.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1584/3600 [00:20<00:24, 83.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1593/3600 [00:20<00:23, 85.01it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1603/3600 [00:20<00:22, 87.58it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1612/3600 [00:20<00:23, 83.76it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1621/3600 [00:20<00:24, 80.86it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1631/3600 [00:20<00:23, 84.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1640/3600 [00:20<00:23, 83.73it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1650/3600 [00:21<00:22, 86.62it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1659/3600 [00:21<00:22, 86.04it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1668/3600 [00:21<00:23, 81.24it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1679/3600 [00:21<00:21, 88.32it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1688/3600 [00:21<00:21, 86.92it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1697/3600 [00:21<00:22, 83.75it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1706/3600 [00:21<00:22, 84.77it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1716/3600 [00:21<00:21, 87.09it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1725/3600 [00:21<00:22, 83.23it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1735/3600 [00:22<00:22, 83.73it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1744/3600 [00:22<00:22, 83.97it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1753/3600 [00:22<00:22, 83.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1763/3600 [00:22<00:22, 83.42it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1772/3600 [00:22<00:21, 84.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1781/3600 [00:22<00:21, 84.19it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1790/3600 [00:22<00:22, 82.08it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1799/3600 [00:22<00:21, 81.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1808/3600 [00:22<00:23, 76.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1816/3600 [00:23<00:25, 70.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1824/3600 [00:23<00:25, 70.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1832/3600 [00:23<00:25, 68.99it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1840/3600 [00:23<00:24, 70.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1848/3600 [00:23<00:24, 72.53it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1858/3600 [00:23<00:22, 77.34it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1868/3600 [00:23<00:21, 79.65it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1878/3600 [00:23<00:21, 81.41it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1887/3600 [00:23<00:20, 83.71it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1896/3600 [00:24<00:20, 81.36it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1905/3600 [00:24<00:20, 82.12it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1914/3600 [00:24<00:20, 83.69it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1923/3600 [00:24<00:20, 80.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1932/3600 [00:24<00:20, 79.78it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1943/3600 [00:24<00:19, 85.86it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1952/3600 [00:24<00:19, 85.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1961/3600 [00:24<00:19, 86.19it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1970/3600 [00:24<00:19, 82.69it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1980/3600 [00:25<00:18, 86.36it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1989/3600 [00:25<00:18, 85.99it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1998/3600 [00:25<00:20, 78.31it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2010/3600 [00:25<00:17, 89.28it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2020/3600 [00:25<00:18, 85.36it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2029/3600 [00:25<00:18, 84.21it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2040/3600 [00:25<00:17, 86.75it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2050/3600 [00:25<00:17, 89.40it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2060/3600 [00:26<00:17, 90.07it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2070/3600 [00:26<00:17, 85.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2079/3600 [00:26<00:17, 86.80it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2088/3600 [00:26<00:17, 85.98it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2097/3600 [00:26<00:17, 85.41it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2107/3600 [00:26<00:17, 87.39it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2117/3600 [00:26<00:16, 88.99it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2126/3600 [00:26<00:17, 85.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2136/3600 [00:26<00:16, 89.50it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2145/3600 [00:26<00:17, 84.91it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2155/3600 [00:27<00:16, 86.42it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2165/3600 [00:27<00:16, 88.55it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2174/3600 [00:27<00:16, 84.52it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2184/3600 [00:27<00:16, 86.43it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2194/3600 [00:27<00:16, 86.31it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2203/3600 [00:27<00:16, 83.83it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2213/3600 [00:27<00:16, 86.14it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2222/3600 [00:27<00:16, 85.29it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2232/3600 [00:27<00:15, 89.14it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2241/3600 [00:28<00:16, 84.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2251/3600 [00:28<00:15, 85.25it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2261/3600 [00:28<00:15, 88.44it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2270/3600 [00:28<00:16, 80.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2280/3600 [00:28<00:15, 85.64it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2289/3600 [00:28<00:15, 85.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2298/3600 [00:28<00:15, 85.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2308/3600 [00:28<00:14, 86.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2318/3600 [00:29<00:14, 85.53it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2327/3600 [00:29<00:18, 68.05it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2345/3600 [00:29<00:13, 90.62it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2355/3600 [00:29<00:13, 92.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2365/3600 [00:29<00:13, 88.48it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2375/3600 [00:29<00:13, 89.88it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2385/3600 [00:29<00:13, 88.46it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2395/3600 [00:29<00:14, 82.82it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2404/3600 [00:30<00:15, 78.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2412/3600 [00:30<00:17, 67.16it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2422/3600 [00:30<00:16, 73.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2430/3600 [00:30<00:17, 65.11it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2440/3600 [00:30<00:16, 69.76it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2448/3600 [00:30<00:16, 70.98it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2456/3600 [00:30<00:16, 69.64it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2464/3600 [00:30<00:16, 67.56it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2471/3600 [00:31<00:16, 67.31it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2478/3600 [00:31<00:16, 66.08it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2485/3600 [00:31<00:16, 65.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2492/3600 [00:31<00:16, 65.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2499/3600 [00:31<00:16, 65.09it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2506/3600 [00:31<00:16, 66.27it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2515/3600 [00:31<00:15, 71.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2524/3600 [00:31<00:14, 75.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2532/3600 [00:31<00:15, 69.16it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2545/3600 [00:32<00:12, 84.87it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2555/3600 [00:32<00:12, 86.13it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2565/3600 [00:32<00:12, 85.68it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2574/3600 [00:32<00:11, 86.01it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2584/3600 [00:32<00:11, 89.06it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2593/3600 [00:32<00:11, 88.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2603/3600 [00:32<00:11, 84.19it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2613/3600 [00:32<00:11, 86.90it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2622/3600 [00:32<00:11, 86.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2632/3600 [00:33<00:11, 87.20it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2641/3600 [00:33<00:11, 85.43it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2650/3600 [00:33<00:11, 85.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2659/3600 [00:33<00:10, 86.66it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2668/3600 [00:33<00:10, 86.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2678/3600 [00:33<00:10, 84.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2688/3600 [00:33<00:10, 86.75it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2697/3600 [00:33<00:10, 85.74it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2706/3600 [00:33<00:10, 86.69it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2716/3600 [00:34<00:10, 86.93it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2725/3600 [00:34<00:10, 84.85it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2736/3600 [00:34<00:09, 89.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2745/3600 [00:34<00:09, 85.94it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2757/3600 [00:34<00:09, 92.98it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2767/3600 [00:34<00:09, 91.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2777/3600 [00:34<00:09, 90.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2787/3600 [00:34<00:09, 86.02it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2796/3600 [00:35<00:11, 72.60it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2806/3600 [00:35<00:10, 77.04it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2815/3600 [00:35<00:10, 74.04it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2823/3600 [00:35<00:10, 72.04it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2831/3600 [00:35<00:11, 69.87it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2839/3600 [00:35<00:11, 67.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2847/3600 [00:35<00:11, 68.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2854/3600 [00:35<00:10, 68.04it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2861/3600 [00:35<00:11, 65.49it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2869/3600 [00:36<00:10, 66.60it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2876/3600 [00:36<00:10, 67.03it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2883/3600 [00:36<00:10, 67.71it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2891/3600 [00:36<00:10, 70.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2900/3600 [00:36<00:09, 75.94it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2910/3600 [00:36<00:08, 82.87it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2919/3600 [00:36<00:08, 82.07it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2928/3600 [00:36<00:08, 83.93it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2937/3600 [00:36<00:07, 84.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2946/3600 [00:37<00:07, 82.37it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2956/3600 [00:37<00:07, 87.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2966/3600 [00:37<00:07, 87.17it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2975/3600 [00:37<00:07, 84.61it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2985/3600 [00:37<00:06, 88.34it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2994/3600 [00:37<00:06, 86.88it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3003/3600 [00:37<00:06, 86.63it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3012/3600 [00:37<00:06, 86.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3022/3600 [00:37<00:06, 86.60it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3031/3600 [00:37<00:06, 87.22it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3041/3600 [00:38<00:06, 87.49it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3050/3600 [00:38<00:06, 87.26it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3059/3600 [00:38<00:06, 87.47it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3069/3600 [00:38<00:06, 87.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3078/3600 [00:38<00:05, 87.63it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3087/3600 [00:38<00:05, 87.04it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3097/3600 [00:38<00:06, 83.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3108/3600 [00:38<00:05, 87.05it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3117/3600 [00:38<00:05, 86.23it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3126/3600 [00:39<00:05, 86.35it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3136/3600 [00:39<00:05, 86.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3145/3600 [00:39<00:05, 86.67it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3154/3600 [00:39<00:05, 87.17it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3164/3600 [00:39<00:05, 85.96it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3174/3600 [00:39<00:04, 85.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3184/3600 [00:39<00:04, 89.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3193/3600 [00:39<00:04, 88.70it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3202/3600 [00:39<00:04, 88.32it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3211/3600 [00:40<00:04, 81.35it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3222/3600 [00:40<00:04, 88.02it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3231/3600 [00:40<00:04, 83.61it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3240/3600 [00:40<00:04, 84.10it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3249/3600 [00:40<00:04, 79.32it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3260/3600 [00:40<00:04, 83.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3269/3600 [00:40<00:03, 83.12it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3278/3600 [00:40<00:03, 82.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3288/3600 [00:40<00:03, 85.35it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3297/3600 [00:41<00:03, 82.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3307/3600 [00:41<00:03, 85.80it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3316/3600 [00:41<00:03, 85.48it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3325/3600 [00:41<00:03, 82.89it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3335/3600 [00:41<00:03, 83.48it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3344/3600 [00:41<00:03, 83.88it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3354/3600 [00:41<00:02, 84.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3363/3600 [00:41<00:02, 84.48it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3373/3600 [00:41<00:02, 87.32it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3382/3600 [00:42<00:02, 83.68it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3391/3600 [00:42<00:02, 83.87it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3400/3600 [00:42<00:02, 83.95it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3409/3600 [00:42<00:02, 85.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3418/3600 [00:42<00:02, 77.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3427/3600 [00:42<00:02, 76.40it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3437/3600 [00:42<00:01, 81.91it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3447/3600 [00:42<00:01, 85.23it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3456/3600 [00:43<00:01, 85.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3465/3600 [00:43<00:01, 82.52it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3475/3600 [00:43<00:01, 83.57it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3485/3600 [00:43<00:01, 84.16it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3494/3600 [00:43<00:01, 84.02it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3503/3600 [00:43<00:01, 82.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3512/3600 [00:43<00:01, 78.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3520/3600 [00:43<00:01, 68.57it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3528/3600 [00:43<00:01, 67.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3537/3600 [00:44<00:00, 70.95it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3546/3600 [00:44<00:00, 70.77it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3554/3600 [00:44<00:00, 73.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3564/3600 [00:44<00:00, 77.98it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3573/3600 [00:44<00:00, 78.95it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3582/3600 [00:44<00:00, 79.36it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3591/3600 [00:44<00:00, 80.28it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3600/3600 [00:44<00:00, 80.24it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.5399
Epoch 1 Step 51 Train Loss: 0.5695
Epoch 1 Step 101 Train Loss: 0.5658
Epoch 1 Step 151 Train Loss: 0.6282
Epoch 1 Step 201 Train Loss: 0.4751
Epoch 1 Step 251 Train Loss: 0.6201
Epoch 1 Step 301 Train Loss: 0.4278
Epoch 1 Step 351 Train Loss: 0.6388
Epoch 1 Step 401 Train Loss: 0.5713
Epoch 1 Step 451 Train Loss: 0.5418
Epoch 1 Step 501 Train Loss: 0.5110
Epoch 1 Step 551 Train Loss: 0.5280
Epoch 1 Step 601 Train Loss: 0.6207
Epoch 1 Step 651 Train Loss: 0.5132
Epoch 1 Step 701 Train Loss: 0.5999
Epoch 1 Step 751 Train Loss: 0.4078
Epoch 1 Step 801 Train Loss: 0.4797
Epoch 1 Step 851 Train Loss: 0.5270
Epoch 1 Step 901 Train Loss: 0.4704
Epoch 1 Step 951 Train Loss: 0.4912
Epoch 1 Step 1001 Train Loss: 0.5333
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0200 Validation Top 20 DE MSE: 0.0252. 
Epoch 2 Step 1 Train Loss: 0.5085
Epoch 2 Step 51 Train Loss: 0.4810
Epoch 2 Step 101 Train Loss: 0.4657
Epoch 2 Step 151 Train Loss: 0.5497
Epoch 2 Step 201 Train Loss: 0.4375
Epoch 2 Step 251 Train Loss: 0.6313
Epoch 2 Step 301 Train Loss: 0.5352
Epoch 2 Step 351 Train Loss: 0.6642
Epoch 2 Step 401 Train Loss: 0.4694
Epoch 2 Step 451 Train Loss: 0.5337
Epoch 2 Step 501 Train Loss: 0.5945
Epoch 2 Step 551 Train Loss: 0.6231
Epoch 2 Step 601 Train Loss: 0.4452
Epoch 2 Step 651 Train Loss: 0.5212
Epoch 2 Step 701 Train Loss: 0.5158
Epoch 2 Step 751 Train Loss: 0.5708
Epoch 2 Step 801 Train Loss: 0.5205
Epoch 2 Step 851 Train Loss: 0.4815
Epoch 2 Step 901 Train Loss: 0.5638
Epoch 2 Step 951 Train Loss: 0.5055
Epoch 2 Step 1001 Train Loss: 0.5977
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0146 Validation Top 20 DE MSE: 0.0974. 
Epoch 3 Step 1 Train Loss: 0.5060
Epoch 3 Step 51 Train Loss: 0.5372
Epoch 3 Step 101 Train Loss: 0.5655
Epoch 3 Step 151 Train Loss: 0.4315
Epoch 3 Step 201 Train Loss: 0.5284
Epoch 3 Step 251 Train Loss: 0.5985
Epoch 3 Step 301 Train Loss: 0.5035
Epoch 3 Step 351 Train Loss: 0.6107
Epoch 3 Step 401 Train Loss: 0.6273
Epoch 3 Step 451 Train Loss: 0.5357
Epoch 3 Step 501 Train Loss: 0.5015
Epoch 3 Step 551 Train Loss: 0.5315
Epoch 3 Step 601 Train Loss: 0.4834
Epoch 3 Step 651 Train Loss: 0.4342
Epoch 3 Step 701 Train Loss: 0.4999
Epoch 3 Step 751 Train Loss: 0.5327
Epoch 3 Step 801 Train Loss: 0.4735
Epoch 3 Step 851 Train Loss: 0.5642
Epoch 3 Step 901 Train Loss: 0.4418
Epoch 3 Step 951 Train Loss: 0.6729
Epoch 3 Step 1001 Train Loss: 0.5403
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0171 Validation Top 20 DE MSE: 0.1382. 
Epoch 4 Step 1 Train Loss: 0.4148
Epoch 4 Step 51 Train Loss: 0.4925
Epoch 4 Step 101 Train Loss: 0.4976
Epoch 4 Step 151 Train Loss: 0.4315
Epoch 4 Step 201 Train Loss: 0.4385
Epoch 4 Step 251 Train Loss: 0.5076
Epoch 4 Step 301 Train Loss: 0.4763
Epoch 4 Step 351 Train Loss: 0.5326
Epoch 4 Step 401 Train Loss: 0.4801
Epoch 4 Step 451 Train Loss: 0.5689
Epoch 4 Step 501 Train Loss: 0.4966
Epoch 4 Step 551 Train Loss: 0.5865
Epoch 4 Step 601 Train Loss: 0.5250
Epoch 4 Step 651 Train Loss: 0.4845
Epoch 4 Step 701 Train Loss: 0.6098
Epoch 4 Step 751 Train Loss: 0.5467
Epoch 4 Step 801 Train Loss: 0.4388
Epoch 4 Step 851 Train Loss: 0.5375
Epoch 4 Step 901 Train Loss: 0.4795
Epoch 4 Step 951 Train Loss: 0.5329
Epoch 4 Step 1001 Train Loss: 0.4456
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0161. 
Epoch 5 Step 1 Train Loss: 0.5465
Epoch 5 Step 51 Train Loss: 0.4716
Epoch 5 Step 101 Train Loss: 0.4380
Epoch 5 Step 151 Train Loss: 0.5559
Epoch 5 Step 201 Train Loss: 0.4538
Epoch 5 Step 251 Train Loss: 0.5105
Epoch 5 Step 301 Train Loss: 0.4501
Epoch 5 Step 351 Train Loss: 0.6285
Epoch 5 Step 401 Train Loss: 0.5300
Epoch 5 Step 451 Train Loss: 0.6293
Epoch 5 Step 501 Train Loss: 0.4957
Epoch 5 Step 551 Train Loss: 0.4985
Epoch 5 Step 601 Train Loss: 0.4755
Epoch 5 Step 651 Train Loss: 0.4579
Epoch 5 Step 701 Train Loss: 0.4631
Epoch 5 Step 751 Train Loss: 0.4428
Epoch 5 Step 801 Train Loss: 0.5032
Epoch 5 Step 851 Train Loss: 0.4523
Epoch 5 Step 901 Train Loss: 0.4156
Epoch 5 Step 951 Train Loss: 0.5200
Epoch 5 Step 1001 Train Loss: 0.5343
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0131 Validation Top 20 DE MSE: 0.0097. 
Epoch 6 Step 1 Train Loss: 0.7420
Epoch 6 Step 51 Train Loss: 0.4409
Epoch 6 Step 101 Train Loss: 0.4455
Epoch 6 Step 151 Train Loss: 0.4979
Epoch 6 Step 201 Train Loss: 0.5688
Epoch 6 Step 251 Train Loss: 0.4612
Epoch 6 Step 301 Train Loss: 0.5237
Epoch 6 Step 351 Train Loss: 0.5483
Epoch 6 Step 401 Train Loss: 0.5315
Epoch 6 Step 451 Train Loss: 0.4256
Epoch 6 Step 501 Train Loss: 0.4481
Epoch 6 Step 551 Train Loss: 0.5107
Epoch 6 Step 601 Train Loss: 0.6080
Epoch 6 Step 651 Train Loss: 0.6044
Epoch 6 Step 701 Train Loss: 0.5784
Epoch 6 Step 751 Train Loss: 0.5429
Epoch 6 Step 801 Train Loss: 0.5133
Epoch 6 Step 851 Train Loss: 0.4834
Epoch 6 Step 901 Train Loss: 0.4522
Epoch 6 Step 951 Train Loss: 0.5365
Epoch 6 Step 1001 Train Loss: 0.5925
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0103. 
Epoch 7 Step 1 Train Loss: 0.5651
Epoch 7 Step 51 Train Loss: 0.5877
Epoch 7 Step 101 Train Loss: 0.6170
Epoch 7 Step 151 Train Loss: 0.4988
Epoch 7 Step 201 Train Loss: 0.6202
Epoch 7 Step 251 Train Loss: 0.5860
Epoch 7 Step 301 Train Loss: 0.4593
Epoch 7 Step 351 Train Loss: 0.5305
Epoch 7 Step 401 Train Loss: 0.5200
Epoch 7 Step 451 Train Loss: 0.5091
Epoch 7 Step 501 Train Loss: 0.4448
Epoch 7 Step 551 Train Loss: 0.4344
Epoch 7 Step 601 Train Loss: 0.4878
Epoch 7 Step 651 Train Loss: 0.4925
Epoch 7 Step 701 Train Loss: 0.5929
Epoch 7 Step 751 Train Loss: 0.7270
Epoch 7 Step 801 Train Loss: 0.5709
Epoch 7 Step 851 Train Loss: 0.5903
Epoch 7 Step 901 Train Loss: 0.4734
Epoch 7 Step 951 Train Loss: 0.5389
Epoch 7 Step 1001 Train Loss: 0.4194
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0123 Validation Top 20 DE MSE: 0.0109. 
Epoch 8 Step 1 Train Loss: 0.4765
Epoch 8 Step 51 Train Loss: 0.5665
Epoch 8 Step 101 Train Loss: 0.6082
Epoch 8 Step 151 Train Loss: 0.4634
Epoch 8 Step 201 Train Loss: 0.4949
Epoch 8 Step 251 Train Loss: 0.6457
Epoch 8 Step 301 Train Loss: 0.5046
Epoch 8 Step 351 Train Loss: 0.4710
Epoch 8 Step 401 Train Loss: 0.4468
Epoch 8 Step 451 Train Loss: 0.4863
Epoch 8 Step 501 Train Loss: 0.5590
Epoch 8 Step 551 Train Loss: 0.5850
Epoch 8 Step 601 Train Loss: 0.4714
Epoch 8 Step 651 Train Loss: 0.4663
Epoch 8 Step 701 Train Loss: 0.7600
Epoch 8 Step 751 Train Loss: 0.4630
Epoch 8 Step 801 Train Loss: 0.5290
Epoch 8 Step 851 Train Loss: 0.5785
Epoch 8 Step 901 Train Loss: 0.6014
Epoch 8 Step 951 Train Loss: 0.5139
Epoch 8 Step 1001 Train Loss: 0.5106
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0058. 
Epoch 9 Step 1 Train Loss: 0.4831
Epoch 9 Step 51 Train Loss: 0.6038
Epoch 9 Step 101 Train Loss: 0.6307
Epoch 9 Step 151 Train Loss: 0.4871
Epoch 9 Step 201 Train Loss: 0.5567
Epoch 9 Step 251 Train Loss: 0.5187
Epoch 9 Step 301 Train Loss: 0.4362
Epoch 9 Step 351 Train Loss: 0.6885
Epoch 9 Step 401 Train Loss: 0.5448
Epoch 9 Step 451 Train Loss: 0.8066
Epoch 9 Step 501 Train Loss: 0.4696
Epoch 9 Step 551 Train Loss: 0.4120
Epoch 9 Step 601 Train Loss: 0.5287
Epoch 9 Step 651 Train Loss: 0.5978
Epoch 9 Step 701 Train Loss: 0.4549
Epoch 9 Step 751 Train Loss: 0.5201
Epoch 9 Step 801 Train Loss: 0.5202
Epoch 9 Step 851 Train Loss: 0.5489
Epoch 9 Step 901 Train Loss: 0.4359
Epoch 9 Step 951 Train Loss: 0.4621
Epoch 9 Step 1001 Train Loss: 0.4642
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0103. 
Epoch 10 Step 1 Train Loss: 0.5542
Epoch 10 Step 51 Train Loss: 0.6562
Epoch 10 Step 101 Train Loss: 0.5765
Epoch 10 Step 151 Train Loss: 0.6372
Epoch 10 Step 201 Train Loss: 0.5716
Epoch 10 Step 251 Train Loss: 0.4705
Epoch 10 Step 301 Train Loss: 0.4818
Epoch 10 Step 351 Train Loss: 0.5553
Epoch 10 Step 401 Train Loss: 0.4067
Epoch 10 Step 451 Train Loss: 0.4717
Epoch 10 Step 501 Train Loss: 0.4562
Epoch 10 Step 551 Train Loss: 0.6711
Epoch 10 Step 601 Train Loss: 0.4270
Epoch 10 Step 651 Train Loss: 0.5100
Epoch 10 Step 701 Train Loss: 0.5011
Epoch 10 Step 751 Train Loss: 0.6339
Epoch 10 Step 801 Train Loss: 0.4502
Epoch 10 Step 851 Train Loss: 0.5716
Epoch 10 Step 901 Train Loss: 0.4651
Epoch 10 Step 951 Train Loss: 0.5199
Epoch 10 Step 1001 Train Loss: 0.5805
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.0079. 
Epoch 11 Step 1 Train Loss: 0.5780
Epoch 11 Step 51 Train Loss: 0.5506
Epoch 11 Step 101 Train Loss: 0.5667
Epoch 11 Step 151 Train Loss: 0.4736
Epoch 11 Step 201 Train Loss: 0.4434
Epoch 11 Step 251 Train Loss: 0.5438
Epoch 11 Step 301 Train Loss: 0.5015
Epoch 11 Step 351 Train Loss: 0.5885
Epoch 11 Step 401 Train Loss: 0.4311
Epoch 11 Step 451 Train Loss: 0.4693
Epoch 11 Step 501 Train Loss: 0.5228
Epoch 11 Step 551 Train Loss: 0.5833
Epoch 11 Step 601 Train Loss: 0.4970
Epoch 11 Step 651 Train Loss: 0.4305
Epoch 11 Step 701 Train Loss: 0.4438
Epoch 11 Step 751 Train Loss: 0.5602
Epoch 11 Step 801 Train Loss: 0.5505
Epoch 11 Step 851 Train Loss: 0.4951
Epoch 11 Step 901 Train Loss: 0.5451
Epoch 11 Step 951 Train Loss: 0.5830
Epoch 11 Step 1001 Train Loss: 0.6303
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0065. 
Epoch 12 Step 1 Train Loss: 0.5658
Epoch 12 Step 51 Train Loss: 0.6674
Epoch 12 Step 101 Train Loss: 0.7216
Epoch 12 Step 151 Train Loss: 0.4538
Epoch 12 Step 201 Train Loss: 0.5461
Epoch 12 Step 251 Train Loss: 0.4375
Epoch 12 Step 301 Train Loss: 0.5393
Epoch 12 Step 351 Train Loss: 0.4807
Epoch 12 Step 401 Train Loss: 0.4887
Epoch 12 Step 451 Train Loss: 0.5576
Epoch 12 Step 501 Train Loss: 0.4606
Epoch 12 Step 551 Train Loss: 0.5353
Epoch 12 Step 601 Train Loss: 0.5148
Epoch 12 Step 651 Train Loss: 0.5400
Epoch 12 Step 701 Train Loss: 0.5307
Epoch 12 Step 751 Train Loss: 0.4879
Epoch 12 Step 801 Train Loss: 0.3962
Epoch 12 Step 851 Train Loss: 0.5053
Epoch 12 Step 901 Train Loss: 0.5836
Epoch 12 Step 951 Train Loss: 0.6952
Epoch 12 Step 1001 Train Loss: 0.6319
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0065. 
Epoch 13 Step 1 Train Loss: 0.5824
Epoch 13 Step 51 Train Loss: 0.3928
Epoch 13 Step 101 Train Loss: 0.5245
Epoch 13 Step 151 Train Loss: 0.4988
Epoch 13 Step 201 Train Loss: 0.6004
Epoch 13 Step 251 Train Loss: 0.5357
Epoch 13 Step 301 Train Loss: 0.4266
Epoch 13 Step 351 Train Loss: 0.5194
Epoch 13 Step 401 Train Loss: 0.4752
Epoch 13 Step 451 Train Loss: 0.5885
Epoch 13 Step 501 Train Loss: 0.4649
Epoch 13 Step 551 Train Loss: 0.5019
Epoch 13 Step 601 Train Loss: 0.5609
Epoch 13 Step 651 Train Loss: 0.5804
Epoch 13 Step 701 Train Loss: 0.6235
Epoch 13 Step 751 Train Loss: 0.5579
Epoch 13 Step 801 Train Loss: 0.4698
Epoch 13 Step 851 Train Loss: 0.4930
Epoch 13 Step 901 Train Loss: 0.4437
Epoch 13 Step 951 Train Loss: 0.5431
Epoch 13 Step 1001 Train Loss: 0.5375
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0096. 
Epoch 14 Step 1 Train Loss: 0.5672
Epoch 14 Step 51 Train Loss: 0.4718
Epoch 14 Step 101 Train Loss: 0.5169
Epoch 14 Step 151 Train Loss: 0.4516
Epoch 14 Step 201 Train Loss: 0.6520
Epoch 14 Step 251 Train Loss: 0.5506
Epoch 14 Step 301 Train Loss: 0.4873
Epoch 14 Step 351 Train Loss: 0.4174
Epoch 14 Step 401 Train Loss: 0.4173
Epoch 14 Step 451 Train Loss: 0.4483
Epoch 14 Step 501 Train Loss: 0.5670
Epoch 14 Step 551 Train Loss: 0.4656
Epoch 14 Step 601 Train Loss: 0.5188
Epoch 14 Step 651 Train Loss: 0.5015
Epoch 14 Step 701 Train Loss: 0.5949
Epoch 14 Step 751 Train Loss: 0.4008
Epoch 14 Step 801 Train Loss: 0.4110
Epoch 14 Step 851 Train Loss: 0.4995
Epoch 14 Step 901 Train Loss: 0.4892
Epoch 14 Step 951 Train Loss: 0.5812
Epoch 14 Step 1001 Train Loss: 0.4723
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0109 Validation Top 20 DE MSE: 0.0076. 
Epoch 15 Step 1 Train Loss: 0.4236
Epoch 15 Step 51 Train Loss: 0.5297
Epoch 15 Step 101 Train Loss: 0.6480
Epoch 15 Step 151 Train Loss: 0.4018
Epoch 15 Step 201 Train Loss: 0.5245
Epoch 15 Step 251 Train Loss: 0.5587
Epoch 15 Step 301 Train Loss: 0.4312
Epoch 15 Step 351 Train Loss: 0.5827
Epoch 15 Step 401 Train Loss: 0.4739
Epoch 15 Step 451 Train Loss: 0.7390
Epoch 15 Step 501 Train Loss: 0.7006
Epoch 15 Step 551 Train Loss: 0.5069
Epoch 15 Step 601 Train Loss: 0.5061
Epoch 15 Step 651 Train Loss: 0.4918
Epoch 15 Step 701 Train Loss: 0.4413
Epoch 15 Step 751 Train Loss: 0.7026
Epoch 15 Step 801 Train Loss: 0.4306
Epoch 15 Step 851 Train Loss: 0.5423
Epoch 15 Step 901 Train Loss: 0.5376
Epoch 15 Step 951 Train Loss: 0.4663
Epoch 15 Step 1001 Train Loss: 0.5240
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0064. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0078
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00050999015
test_unseen_single_pearson: 0.9991327815104283
test_unseen_single_mse_de: 0.0077549317
test_unseen_single_pearson_de: 0.9975586445900374
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.9343897724747917
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9666666666666668
test_unseen_single_mse_top20_de_non_dropout: 0.00765715355062955
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.029 MB uploadedwandb: - 0.024 MB of 0.029 MB uploadedwandb: \ 0.029 MB of 0.029 MB uploadedwandb: | 0.029 MB of 0.029 MB uploadedwandb: / 0.029 MB of 0.029 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñá‚ñà‚ñá‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñà‚ñÅ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ
wandb:                                                   val_de_mse ‚ñÇ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñá‚ñÉ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÉ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÜ‚ñÇ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00775
wandb:                                              test_de_pearson 0.99756
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.96667
wandb:                                                     test_mse 0.00051
wandb:                                test_mse_top20_de_non_dropout 0.00766
wandb:                                                 test_pearson 0.99913
wandb:                                           test_pearson_delta 0.93439
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.96667
wandb:                                       test_unseen_single_mse 0.00051
wandb:                                    test_unseen_single_mse_de 0.00775
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00766
wandb:                                   test_unseen_single_pearson 0.99913
wandb:                                test_unseen_single_pearson_de 0.99756
wandb:                             test_unseen_single_pearson_delta 0.93439
wandb:                                                 train_de_mse 0.00945
wandb:                                             train_de_pearson 0.99711
wandb:                                                    train_mse 0.0006
wandb:                                                train_pearson 0.99904
wandb:                                                training_loss 0.59698
wandb:                                                   val_de_mse 0.00641
wandb:                                               val_de_pearson 0.99878
wandb:                                                      val_mse 0.00052
wandb:                                                  val_pearson 0.99917
wandb: 
wandb: üöÄ View run geneformer_Dixit_combined_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/sm199vix
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_231520-sm199vix/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_233815-rrqrk5r6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_combined_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/rrqrk5r6
wandb: WARNING Serializing object of type ndarray that is 20541568 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5711
Epoch 1 Step 51 Train Loss: 0.5115
Epoch 1 Step 101 Train Loss: 0.4701
Epoch 1 Step 151 Train Loss: 0.4805
Epoch 1 Step 201 Train Loss: 0.6759
Epoch 1 Step 251 Train Loss: 0.5441
Epoch 1 Step 301 Train Loss: 0.4669
Epoch 1 Step 351 Train Loss: 0.4715
Epoch 1 Step 401 Train Loss: 0.4554
Epoch 1 Step 451 Train Loss: 0.5005
Epoch 1 Step 501 Train Loss: 0.5001
Epoch 1 Step 551 Train Loss: 0.5058
Epoch 1 Step 601 Train Loss: 0.5162
Epoch 1 Step 651 Train Loss: 0.7330
Epoch 1 Step 701 Train Loss: 0.4858
Epoch 1 Step 751 Train Loss: 0.4191
Epoch 1 Step 801 Train Loss: 0.4739
Epoch 1 Step 851 Train Loss: 0.7967
Epoch 1 Step 901 Train Loss: 0.5867
Epoch 1: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0254 Validation Top 20 DE MSE: 0.0313. 
Epoch 2 Step 1 Train Loss: 0.5248
Epoch 2 Step 51 Train Loss: 0.5423
Epoch 2 Step 101 Train Loss: 0.4277
Epoch 2 Step 151 Train Loss: 0.5948
Epoch 2 Step 201 Train Loss: 0.5854
Epoch 2 Step 251 Train Loss: 0.5370
Epoch 2 Step 301 Train Loss: 0.5698
Epoch 2 Step 351 Train Loss: 0.5753
Epoch 2 Step 401 Train Loss: 0.5628
Epoch 2 Step 451 Train Loss: 0.6472
Epoch 2 Step 501 Train Loss: 0.5280
Epoch 2 Step 551 Train Loss: 0.4758
Epoch 2 Step 601 Train Loss: 0.6855
Epoch 2 Step 651 Train Loss: 0.3972
Epoch 2 Step 701 Train Loss: 0.4779
Epoch 2 Step 751 Train Loss: 0.4287
Epoch 2 Step 801 Train Loss: 0.5562
Epoch 2 Step 851 Train Loss: 0.5761
Epoch 2 Step 901 Train Loss: 0.3971
Epoch 2: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0157 Validation Top 20 DE MSE: 0.0489. 
Epoch 3 Step 1 Train Loss: 0.5170
Epoch 3 Step 51 Train Loss: 0.4250
Epoch 3 Step 101 Train Loss: 0.4030
Epoch 3 Step 151 Train Loss: 0.6224
Epoch 3 Step 201 Train Loss: 0.4562
Epoch 3 Step 251 Train Loss: 0.4874
Epoch 3 Step 301 Train Loss: 0.4908
Epoch 3 Step 351 Train Loss: 0.5908
Epoch 3 Step 401 Train Loss: 0.6374
Epoch 3 Step 451 Train Loss: 0.6563
Epoch 3 Step 501 Train Loss: 0.5197
Epoch 3 Step 551 Train Loss: 0.5152
Epoch 3 Step 601 Train Loss: 0.6443
Epoch 3 Step 651 Train Loss: 0.5368
Epoch 3 Step 701 Train Loss: 0.5235
Epoch 3 Step 751 Train Loss: 0.4752
Epoch 3 Step 801 Train Loss: 0.5867
Epoch 3 Step 851 Train Loss: 0.4939
Epoch 3 Step 901 Train Loss: 0.4282
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.2727. 
Epoch 4 Step 1 Train Loss: 0.5581
Epoch 4 Step 51 Train Loss: 0.5067
Epoch 4 Step 101 Train Loss: 0.6478
Epoch 4 Step 151 Train Loss: 0.3751
Epoch 4 Step 201 Train Loss: 0.5107
Epoch 4 Step 251 Train Loss: 0.4469
Epoch 4 Step 301 Train Loss: 0.5324
Epoch 4 Step 351 Train Loss: 0.5032
Epoch 4 Step 401 Train Loss: 0.5765
Epoch 4 Step 451 Train Loss: 0.6120
Epoch 4 Step 501 Train Loss: 0.4491
Epoch 4 Step 551 Train Loss: 0.5468
Epoch 4 Step 601 Train Loss: 0.5105
Epoch 4 Step 651 Train Loss: 0.4318
Epoch 4 Step 701 Train Loss: 0.4940
Epoch 4 Step 751 Train Loss: 0.5815
Epoch 4 Step 801 Train Loss: 0.5727
Epoch 4 Step 851 Train Loss: 0.4482
Epoch 4 Step 901 Train Loss: 0.5223
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0078. 
Train Top 20 DE MSE: 0.0087 Validation Top 20 DE MSE: 0.4143. 
Epoch 5 Step 1 Train Loss: 0.6259
Epoch 5 Step 51 Train Loss: 0.5514
Epoch 5 Step 101 Train Loss: 0.6137
Epoch 5 Step 151 Train Loss: 0.7858
Epoch 5 Step 201 Train Loss: 0.5142
Epoch 5 Step 251 Train Loss: 0.4719
Epoch 5 Step 301 Train Loss: 0.5114
Epoch 5 Step 351 Train Loss: 0.4490
Epoch 5 Step 401 Train Loss: 0.5071
Epoch 5 Step 451 Train Loss: 0.4419
Epoch 5 Step 501 Train Loss: 0.5321
Epoch 5 Step 551 Train Loss: 0.4590
Epoch 5 Step 601 Train Loss: 0.5942
Epoch 5 Step 651 Train Loss: 0.6860
Epoch 5 Step 701 Train Loss: 0.4589
Epoch 5 Step 751 Train Loss: 0.6212
Epoch 5 Step 801 Train Loss: 0.5622
Epoch 5 Step 851 Train Loss: 0.4281
Epoch 5 Step 901 Train Loss: 0.5663
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0078. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.4276. 
Epoch 6 Step 1 Train Loss: 0.4643
Epoch 6 Step 51 Train Loss: 0.5080
Epoch 6 Step 101 Train Loss: 0.4993
Epoch 6 Step 151 Train Loss: 0.5366
Epoch 6 Step 201 Train Loss: 0.6241
Epoch 6 Step 251 Train Loss: 0.4813
Epoch 6 Step 301 Train Loss: 0.4851
Epoch 6 Step 351 Train Loss: 0.7295
Epoch 6 Step 401 Train Loss: 0.5224
Epoch 6 Step 451 Train Loss: 0.5929
Epoch 6 Step 501 Train Loss: 0.5168
Epoch 6 Step 551 Train Loss: 0.6142
Epoch 6 Step 601 Train Loss: 0.6495
Epoch 6 Step 651 Train Loss: 0.6036
Epoch 6 Step 701 Train Loss: 0.4365
Epoch 6 Step 751 Train Loss: 0.5346
Epoch 6 Step 801 Train Loss: 0.5751
Epoch 6 Step 851 Train Loss: 0.5284
Epoch 6 Step 901 Train Loss: 0.5064
Epoch 6: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0074. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.4037. 
Epoch 7 Step 1 Train Loss: 0.4517
Epoch 7 Step 51 Train Loss: 0.4978
Epoch 7 Step 101 Train Loss: 0.6264
Epoch 7 Step 151 Train Loss: 0.5385
Epoch 7 Step 201 Train Loss: 0.5239
Epoch 7 Step 251 Train Loss: 0.5631
Epoch 7 Step 301 Train Loss: 0.5647
Epoch 7 Step 351 Train Loss: 0.5167
Epoch 7 Step 401 Train Loss: 0.4336
Epoch 7 Step 451 Train Loss: 0.4310
Epoch 7 Step 501 Train Loss: 0.6206
Epoch 7 Step 551 Train Loss: 0.4983
Epoch 7 Step 601 Train Loss: 0.5456
Epoch 7 Step 651 Train Loss: 0.6510
Epoch 7 Step 701 Train Loss: 0.4286
Epoch 7 Step 751 Train Loss: 0.5071
Epoch 7 Step 801 Train Loss: 0.5248
Epoch 7 Step 851 Train Loss: 0.4988
Epoch 7 Step 901 Train Loss: 0.4376
Epoch 7: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0075. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.4101. 
Epoch 8 Step 1 Train Loss: 0.5641
Epoch 8 Step 51 Train Loss: 0.4909
Epoch 8 Step 101 Train Loss: 0.4320
Epoch 8 Step 151 Train Loss: 0.5826
Epoch 8 Step 201 Train Loss: 0.4505
Epoch 8 Step 251 Train Loss: 0.4849
Epoch 8 Step 301 Train Loss: 0.4254
Epoch 8 Step 351 Train Loss: 0.6046
Epoch 8 Step 401 Train Loss: 0.6015
Epoch 8 Step 451 Train Loss: 0.4332
Epoch 8 Step 501 Train Loss: 0.5325
Epoch 8 Step 551 Train Loss: 0.5379
Epoch 8 Step 601 Train Loss: 0.5301
Epoch 8 Step 651 Train Loss: 0.7105
Epoch 8 Step 701 Train Loss: 0.4027
Epoch 8 Step 751 Train Loss: 0.4240
Epoch 8 Step 801 Train Loss: 0.5485
Epoch 8 Step 851 Train Loss: 0.6292
Epoch 8 Step 901 Train Loss: 0.4829
Epoch 8: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.4133. 
Epoch 9 Step 1 Train Loss: 0.5395
Epoch 9 Step 51 Train Loss: 0.4602
Epoch 9 Step 101 Train Loss: 0.4701
Epoch 9 Step 151 Train Loss: 0.4796
Epoch 9 Step 201 Train Loss: 0.4587
Epoch 9 Step 251 Train Loss: 0.5001
Epoch 9 Step 301 Train Loss: 0.4944
Epoch 9 Step 351 Train Loss: 0.4858
Epoch 9 Step 401 Train Loss: 0.5134
Epoch 9 Step 451 Train Loss: 0.4032
Epoch 9 Step 501 Train Loss: 0.4690
Epoch 9 Step 551 Train Loss: 0.5698
Epoch 9 Step 601 Train Loss: 0.5339
Epoch 9 Step 651 Train Loss: 0.6231
Epoch 9 Step 701 Train Loss: 0.4321
Epoch 9 Step 751 Train Loss: 0.5263
Epoch 9 Step 801 Train Loss: 0.4286
Epoch 9 Step 851 Train Loss: 0.3900
Epoch 9 Step 901 Train Loss: 0.4704
Epoch 9: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0077. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.4259. 
Epoch 10 Step 1 Train Loss: 0.5294
Epoch 10 Step 51 Train Loss: 0.4722
Epoch 10 Step 101 Train Loss: 0.4970
Epoch 10 Step 151 Train Loss: 0.6146
Epoch 10 Step 201 Train Loss: 0.4420
Epoch 10 Step 251 Train Loss: 0.4768
Epoch 10 Step 301 Train Loss: 0.4384
Epoch 10 Step 351 Train Loss: 0.4983
Epoch 10 Step 401 Train Loss: 0.6525
Epoch 10 Step 451 Train Loss: 0.5535
Epoch 10 Step 501 Train Loss: 0.5667
Epoch 10 Step 551 Train Loss: 0.5665
Epoch 10 Step 601 Train Loss: 0.3718
Epoch 10 Step 651 Train Loss: 0.5723
Epoch 10 Step 701 Train Loss: 0.4725
Epoch 10 Step 751 Train Loss: 0.7453
Epoch 10 Step 801 Train Loss: 0.4292
Epoch 10 Step 851 Train Loss: 0.4790
Epoch 10 Step 901 Train Loss: 0.5434
Epoch 10: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0074. 
Train Top 20 DE MSE: 0.0115 Validation Top 20 DE MSE: 0.4025. 
Epoch 11 Step 1 Train Loss: 0.5430
Epoch 11 Step 51 Train Loss: 0.5762
Epoch 11 Step 101 Train Loss: 0.4527
Epoch 11 Step 151 Train Loss: 0.4857
Epoch 11 Step 201 Train Loss: 0.4712
Epoch 11 Step 251 Train Loss: 0.6039
Epoch 11 Step 301 Train Loss: 0.5280
Epoch 11 Step 351 Train Loss: 0.5606
Epoch 11 Step 401 Train Loss: 0.4677
Epoch 11 Step 451 Train Loss: 0.5901
Epoch 11 Step 501 Train Loss: 0.5691
Epoch 11 Step 551 Train Loss: 0.5376
Epoch 11 Step 601 Train Loss: 0.5487
Epoch 11 Step 651 Train Loss: 0.4599
Epoch 11 Step 701 Train Loss: 0.5381
Epoch 11 Step 751 Train Loss: 0.5558
Epoch 11 Step 801 Train Loss: 0.5478
Epoch 11 Step 851 Train Loss: 0.6302
Epoch 11 Step 901 Train Loss: 0.5675
Epoch 11: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0074. 
Train Top 20 DE MSE: 0.0120 Validation Top 20 DE MSE: 0.4042. 
Epoch 12 Step 1 Train Loss: 0.6154
Epoch 12 Step 51 Train Loss: 0.4693
Epoch 12 Step 101 Train Loss: 0.5174
Epoch 12 Step 151 Train Loss: 0.6982
Epoch 12 Step 201 Train Loss: 0.5875
Epoch 12 Step 251 Train Loss: 0.6350
Epoch 12 Step 301 Train Loss: 0.4993
Epoch 12 Step 351 Train Loss: 0.6258
Epoch 12 Step 401 Train Loss: 0.5106
Epoch 12 Step 451 Train Loss: 0.4703
Epoch 12 Step 501 Train Loss: 0.5200
Epoch 12 Step 551 Train Loss: 0.5334
Epoch 12 Step 601 Train Loss: 0.5008
Epoch 12 Step 651 Train Loss: 0.3980
Epoch 12 Step 701 Train Loss: 0.4961
Epoch 12 Step 751 Train Loss: 0.6033
Epoch 12 Step 801 Train Loss: 0.4481
Epoch 12 Step 851 Train Loss: 0.5280
Epoch 12 Step 901 Train Loss: 0.4227
Epoch 12: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0075. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.4100. 
Epoch 13 Step 1 Train Loss: 0.4970
Epoch 13 Step 51 Train Loss: 0.5298
Epoch 13 Step 101 Train Loss: 0.5270
Epoch 13 Step 151 Train Loss: 0.5928
Epoch 13 Step 201 Train Loss: 0.3749
Epoch 13 Step 251 Train Loss: 0.5141
Epoch 13 Step 301 Train Loss: 0.4835
Epoch 13 Step 351 Train Loss: 0.4743
Epoch 13 Step 401 Train Loss: 0.7028
Epoch 13 Step 451 Train Loss: 0.5017
Epoch 13 Step 501 Train Loss: 0.4913
Epoch 13 Step 551 Train Loss: 0.5626
Epoch 13 Step 601 Train Loss: 0.3970
Epoch 13 Step 651 Train Loss: 0.5359
Epoch 13 Step 701 Train Loss: 0.4859
Epoch 13 Step 751 Train Loss: 0.4592
Epoch 13 Step 801 Train Loss: 0.4906
Epoch 13 Step 851 Train Loss: 0.5582
Epoch 13 Step 901 Train Loss: 0.6351
Epoch 13: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.4177. 
Epoch 14 Step 1 Train Loss: 0.4691
Epoch 14 Step 51 Train Loss: 0.4539
Epoch 14 Step 101 Train Loss: 0.5059
Epoch 14 Step 151 Train Loss: 0.5569
Epoch 14 Step 201 Train Loss: 0.4866
Epoch 14 Step 251 Train Loss: 0.5126
Epoch 14 Step 301 Train Loss: 0.5232
Epoch 14 Step 351 Train Loss: 0.4281
Epoch 14 Step 401 Train Loss: 0.4652
Epoch 14 Step 451 Train Loss: 0.5611
Epoch 14 Step 501 Train Loss: 0.5201
Epoch 14 Step 551 Train Loss: 0.6000
Epoch 14 Step 601 Train Loss: 0.5563
Epoch 14 Step 651 Train Loss: 0.5772
Epoch 14 Step 701 Train Loss: 0.6903
Epoch 14 Step 751 Train Loss: 0.4798
Epoch 14 Step 801 Train Loss: 0.4972
Epoch 14 Step 851 Train Loss: 0.5812
Epoch 14 Step 901 Train Loss: 0.6587
Epoch 14: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0075. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.4106. 
Epoch 15 Step 1 Train Loss: 0.5520
Epoch 15 Step 51 Train Loss: 0.5675
Epoch 15 Step 101 Train Loss: 0.4578
Epoch 15 Step 151 Train Loss: 0.4154
Epoch 15 Step 201 Train Loss: 0.4918
Epoch 15 Step 251 Train Loss: 0.4541
Epoch 15 Step 301 Train Loss: 0.4823
Epoch 15 Step 351 Train Loss: 0.5348
Epoch 15 Step 401 Train Loss: 0.5447
Epoch 15 Step 451 Train Loss: 0.4572
Epoch 15 Step 501 Train Loss: 0.6395
Epoch 15 Step 551 Train Loss: 0.5128
Epoch 15 Step 601 Train Loss: 0.6402
Epoch 15 Step 651 Train Loss: 0.5157
Epoch 15 Step 701 Train Loss: 0.4750
Epoch 15 Step 751 Train Loss: 0.4597
Epoch 15 Step 801 Train Loss: 0.4523
Epoch 15 Step 851 Train Loss: 0.4601
Epoch 15 Step 901 Train Loss: 0.4718
Epoch 15: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0078. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.4266. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0732
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0017663072
test_unseen_single_pearson: 0.996749284681636
test_unseen_single_mse_de: 0.07318371
test_unseen_single_pearson_de: 0.9779665133117307
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.2650641192372377
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.19166666666666665
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9833333333333334
test_unseen_single_mse_top20_de_non_dropout: 0.07318370537512932
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.001 MB uploadedwandb: - 0.001 MB of 0.028 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.017 MB of 0.028 MB uploadedwandb: / 0.024 MB of 0.028 MB uploadedwandb: - 0.024 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÉ
wandb:                                                   val_de_mse ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                               val_de_pearson ‚ñà‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                  val_pearson ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.07318
wandb:                                              test_de_pearson 0.97797
wandb:               test_frac_opposite_direction_top20_non_dropout 0.19167
wandb:                          test_frac_sigma_below_1_non_dropout 0.98333
wandb:                                                     test_mse 0.00177
wandb:                                test_mse_top20_de_non_dropout 0.07318
wandb:                                                 test_pearson 0.99675
wandb:                                           test_pearson_delta 0.26506
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.19167
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98333
wandb:                                       test_unseen_single_mse 0.00177
wandb:                                    test_unseen_single_mse_de 0.07318
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.07318
wandb:                                   test_unseen_single_pearson 0.99675
wandb:                                test_unseen_single_pearson_de 0.97797
wandb:                             test_unseen_single_pearson_delta 0.26506
wandb:                                                 train_de_mse 0.00928
wandb:                                             train_de_pearson 0.99713
wandb:                                                    train_mse 0.00069
wandb:                                                train_pearson 0.99891
wandb:                                                training_loss 0.67352
wandb:                                                   val_de_mse 0.42655
wandb:                                               val_de_pearson 0.88382
wandb:                                                      val_mse 0.00777
wandb:                                                  val_pearson 0.98635
wandb: 
wandb: üöÄ View run geneformer_Dixit_combined_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/rrqrk5r6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_233815-rrqrk5r6/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_235748-zww4v9v9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_combined_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zww4v9v9
wandb: WARNING Serializing object of type ndarray that is 20541568 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5520
Epoch 1 Step 51 Train Loss: 0.5716
Epoch 1 Step 101 Train Loss: 0.5245
Epoch 1 Step 151 Train Loss: 0.5062
Epoch 1 Step 201 Train Loss: 0.6478
Epoch 1 Step 251 Train Loss: 0.4533
Epoch 1 Step 301 Train Loss: 0.7176
Epoch 1 Step 351 Train Loss: 0.4811
Epoch 1 Step 401 Train Loss: 0.6841
Epoch 1 Step 451 Train Loss: 0.5016
Epoch 1 Step 501 Train Loss: 0.6317
Epoch 1 Step 551 Train Loss: 0.6044
Epoch 1 Step 601 Train Loss: 0.5029
Epoch 1 Step 651 Train Loss: 0.4573
Epoch 1 Step 701 Train Loss: 0.5230
Epoch 1 Step 751 Train Loss: 0.5142
Epoch 1 Step 801 Train Loss: 0.5629
Epoch 1 Step 851 Train Loss: 0.4944
Epoch 1 Step 901 Train Loss: 0.4936
Epoch 1 Step 951 Train Loss: 0.6087
Epoch 1: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0418 Validation Top 20 DE MSE: 0.0191. 
Epoch 2 Step 1 Train Loss: 0.5736
Epoch 2 Step 51 Train Loss: 0.5010
Epoch 2 Step 101 Train Loss: 0.6104
Epoch 2 Step 151 Train Loss: 0.4945
Epoch 2 Step 201 Train Loss: 0.5760
Epoch 2 Step 251 Train Loss: 0.6683
Epoch 2 Step 301 Train Loss: 0.4701
Epoch 2 Step 351 Train Loss: 0.5048
Epoch 2 Step 401 Train Loss: 0.6063
Epoch 2 Step 451 Train Loss: 0.5318
Epoch 2 Step 501 Train Loss: 0.4285
Epoch 2 Step 551 Train Loss: 0.5126
Epoch 2 Step 601 Train Loss: 0.6470
Epoch 2 Step 651 Train Loss: 0.6707
Epoch 2 Step 701 Train Loss: 0.5863
Epoch 2 Step 751 Train Loss: 0.6399
Epoch 2 Step 801 Train Loss: 0.5264
Epoch 2 Step 851 Train Loss: 0.5292
Epoch 2 Step 901 Train Loss: 0.5437
Epoch 2 Step 951 Train Loss: 0.5404
Epoch 2: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0306 Validation Top 20 DE MSE: 0.0306. 
Epoch 3 Step 1 Train Loss: 0.5114
Epoch 3 Step 51 Train Loss: 0.4970
Epoch 3 Step 101 Train Loss: 0.4986
Epoch 3 Step 151 Train Loss: 0.4747
Epoch 3 Step 201 Train Loss: 0.4458
Epoch 3 Step 251 Train Loss: 0.5545
Epoch 3 Step 301 Train Loss: 0.5898
Epoch 3 Step 351 Train Loss: 0.4222
Epoch 3 Step 401 Train Loss: 0.5292
Epoch 3 Step 451 Train Loss: 0.5097
Epoch 3 Step 501 Train Loss: 0.5120
Epoch 3 Step 551 Train Loss: 0.3968
Epoch 3 Step 601 Train Loss: 0.4204
Epoch 3 Step 651 Train Loss: 0.4730
Epoch 3 Step 701 Train Loss: 0.5427
Epoch 3 Step 751 Train Loss: 0.6482
Epoch 3 Step 801 Train Loss: 0.4529
Epoch 3 Step 851 Train Loss: 0.4865
Epoch 3 Step 901 Train Loss: 0.5174
Epoch 3 Step 951 Train Loss: 0.4045
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0128. 
Epoch 4 Step 1 Train Loss: 0.4986
Epoch 4 Step 51 Train Loss: 0.6156
Epoch 4 Step 101 Train Loss: 0.4800
Epoch 4 Step 151 Train Loss: 0.6456
Epoch 4 Step 201 Train Loss: 0.5192
Epoch 4 Step 251 Train Loss: 0.5028
Epoch 4 Step 301 Train Loss: 0.6702
Epoch 4 Step 351 Train Loss: 0.5814
Epoch 4 Step 401 Train Loss: 0.4157
Epoch 4 Step 451 Train Loss: 0.5956
Epoch 4 Step 501 Train Loss: 0.5747
Epoch 4 Step 551 Train Loss: 0.5755
Epoch 4 Step 601 Train Loss: 0.5154
Epoch 4 Step 651 Train Loss: 0.5304
Epoch 4 Step 701 Train Loss: 0.5387
Epoch 4 Step 751 Train Loss: 0.5657
Epoch 4 Step 801 Train Loss: 0.5090
Epoch 4 Step 851 Train Loss: 0.5888
Epoch 4 Step 901 Train Loss: 0.4407
Epoch 4 Step 951 Train Loss: 0.5839
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0109 Validation Top 20 DE MSE: 0.0109. 
Epoch 5 Step 1 Train Loss: 0.6102
Epoch 5 Step 51 Train Loss: 0.5211
Epoch 5 Step 101 Train Loss: 0.4973
Epoch 5 Step 151 Train Loss: 0.5320
Epoch 5 Step 201 Train Loss: 0.4602
Epoch 5 Step 251 Train Loss: 0.4400
Epoch 5 Step 301 Train Loss: 0.6074
Epoch 5 Step 351 Train Loss: 0.5026
Epoch 5 Step 401 Train Loss: 0.4294
Epoch 5 Step 451 Train Loss: 0.4473
Epoch 5 Step 501 Train Loss: 0.5103
Epoch 5 Step 551 Train Loss: 0.4558
Epoch 5 Step 601 Train Loss: 0.4776
Epoch 5 Step 651 Train Loss: 0.5027
Epoch 5 Step 701 Train Loss: 0.4174
Epoch 5 Step 751 Train Loss: 0.5143
Epoch 5 Step 801 Train Loss: 0.4921
Epoch 5 Step 851 Train Loss: 0.5396
Epoch 5 Step 901 Train Loss: 0.4501
Epoch 5 Step 951 Train Loss: 0.4251
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0117. 
Epoch 6 Step 1 Train Loss: 0.5309
Epoch 6 Step 51 Train Loss: 0.5771
Epoch 6 Step 101 Train Loss: 0.5002
Epoch 6 Step 151 Train Loss: 0.5946
Epoch 6 Step 201 Train Loss: 0.5709
Epoch 6 Step 251 Train Loss: 0.4971
Epoch 6 Step 301 Train Loss: 0.4456
Epoch 6 Step 351 Train Loss: 0.4111
Epoch 6 Step 401 Train Loss: 0.5061
Epoch 6 Step 451 Train Loss: 0.5414
Epoch 6 Step 501 Train Loss: 0.6500
Epoch 6 Step 551 Train Loss: 0.6000
Epoch 6 Step 601 Train Loss: 0.6161
Epoch 6 Step 651 Train Loss: 0.5479
Epoch 6 Step 701 Train Loss: 0.5177
Epoch 6 Step 751 Train Loss: 0.4822
Epoch 6 Step 801 Train Loss: 0.4978
Epoch 6 Step 851 Train Loss: 0.5251
Epoch 6 Step 901 Train Loss: 0.4369
Epoch 6 Step 951 Train Loss: 0.5055
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0079 Validation Top 20 DE MSE: 0.0108. 
Epoch 7 Step 1 Train Loss: 0.4465
Epoch 7 Step 51 Train Loss: 0.5941
Epoch 7 Step 101 Train Loss: 0.6139
Epoch 7 Step 151 Train Loss: 0.4168
Epoch 7 Step 201 Train Loss: 0.5178
Epoch 7 Step 251 Train Loss: 0.6256
Epoch 7 Step 301 Train Loss: 0.4844
Epoch 7 Step 351 Train Loss: 0.5136
Epoch 7 Step 401 Train Loss: 0.5259
Epoch 7 Step 451 Train Loss: 0.5193
Epoch 7 Step 501 Train Loss: 0.5491
Epoch 7 Step 551 Train Loss: 0.4778
Epoch 7 Step 601 Train Loss: 0.6202
Epoch 7 Step 651 Train Loss: 0.4684
Epoch 7 Step 701 Train Loss: 0.6654
Epoch 7 Step 751 Train Loss: 0.5812
Epoch 7 Step 801 Train Loss: 0.6552
Epoch 7 Step 851 Train Loss: 0.5038
Epoch 7 Step 901 Train Loss: 0.5037
Epoch 7 Step 951 Train Loss: 0.5179
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.0109. 
Epoch 8 Step 1 Train Loss: 0.4338
Epoch 8 Step 51 Train Loss: 0.4152
Epoch 8 Step 101 Train Loss: 0.6115
Epoch 8 Step 151 Train Loss: 0.6125
Epoch 8 Step 201 Train Loss: 0.4587
Epoch 8 Step 251 Train Loss: 0.7043
Epoch 8 Step 301 Train Loss: 0.4562
Epoch 8 Step 351 Train Loss: 0.5863
Epoch 8 Step 401 Train Loss: 0.5122
Epoch 8 Step 451 Train Loss: 0.6192
Epoch 8 Step 501 Train Loss: 0.4594
Epoch 8 Step 551 Train Loss: 0.4671
Epoch 8 Step 601 Train Loss: 0.5773
Epoch 8 Step 651 Train Loss: 0.5206
Epoch 8 Step 701 Train Loss: 0.4833
Epoch 8 Step 751 Train Loss: 0.4912
Epoch 8 Step 801 Train Loss: 0.6440
Epoch 8 Step 851 Train Loss: 0.5018
Epoch 8 Step 901 Train Loss: 0.5932
Epoch 8 Step 951 Train Loss: 0.5130
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.0125. 
Epoch 9 Step 1 Train Loss: 0.5215
Epoch 9 Step 51 Train Loss: 0.4914
Epoch 9 Step 101 Train Loss: 0.5651
Epoch 9 Step 151 Train Loss: 0.7373
Epoch 9 Step 201 Train Loss: 0.4811
Epoch 9 Step 251 Train Loss: 0.5824
Epoch 9 Step 301 Train Loss: 0.4836
Epoch 9 Step 351 Train Loss: 0.5350
Epoch 9 Step 401 Train Loss: 0.4945
Epoch 9 Step 451 Train Loss: 0.4886
Epoch 9 Step 501 Train Loss: 0.6334
Epoch 9 Step 551 Train Loss: 0.4515
Epoch 9 Step 601 Train Loss: 0.5675
Epoch 9 Step 651 Train Loss: 0.4810
Epoch 9 Step 701 Train Loss: 0.4387
Epoch 9 Step 751 Train Loss: 0.5949
Epoch 9 Step 801 Train Loss: 0.5505
Epoch 9 Step 851 Train Loss: 0.5673
Epoch 9 Step 901 Train Loss: 0.4358
Epoch 9 Step 951 Train Loss: 0.5988
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0111 Validation Top 20 DE MSE: 0.0110. 
Epoch 10 Step 1 Train Loss: 0.5924
Epoch 10 Step 51 Train Loss: 0.4923
Epoch 10 Step 101 Train Loss: 0.4467
Epoch 10 Step 151 Train Loss: 0.5296
Epoch 10 Step 201 Train Loss: 0.5399
Epoch 10 Step 251 Train Loss: 0.4604
Epoch 10 Step 301 Train Loss: 0.6228
Epoch 10 Step 351 Train Loss: 0.4360
Epoch 10 Step 401 Train Loss: 0.6014
Epoch 10 Step 451 Train Loss: 0.6677
Epoch 10 Step 501 Train Loss: 0.5209
Epoch 10 Step 551 Train Loss: 0.6084
Epoch 10 Step 601 Train Loss: 0.5733
Epoch 10 Step 651 Train Loss: 0.5318
Epoch 10 Step 701 Train Loss: 0.4875
Epoch 10 Step 751 Train Loss: 0.4865
Epoch 10 Step 801 Train Loss: 0.4811
Epoch 10 Step 851 Train Loss: 0.6539
Epoch 10 Step 901 Train Loss: 0.6629
Epoch 10 Step 951 Train Loss: 0.4249
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0071 Validation Top 20 DE MSE: 0.0127. 
Epoch 11 Step 1 Train Loss: 0.5259
Epoch 11 Step 51 Train Loss: 0.5168
Epoch 11 Step 101 Train Loss: 0.4675
Epoch 11 Step 151 Train Loss: 0.4082
Epoch 11 Step 201 Train Loss: 0.5902
Epoch 11 Step 251 Train Loss: 0.4466
Epoch 11 Step 301 Train Loss: 0.5916
Epoch 11 Step 351 Train Loss: 0.5610
Epoch 11 Step 401 Train Loss: 0.5342
Epoch 11 Step 451 Train Loss: 0.5808
Epoch 11 Step 501 Train Loss: 0.3977
Epoch 11 Step 551 Train Loss: 0.6479
Epoch 11 Step 601 Train Loss: 0.4949
Epoch 11 Step 651 Train Loss: 0.4136
Epoch 11 Step 701 Train Loss: 0.4963
Epoch 11 Step 751 Train Loss: 0.5294
Epoch 11 Step 801 Train Loss: 0.6133
Epoch 11 Step 851 Train Loss: 0.5642
Epoch 11 Step 901 Train Loss: 0.4626
Epoch 11 Step 951 Train Loss: 0.4318
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0122. 
Epoch 12 Step 1 Train Loss: 0.4285
Epoch 12 Step 51 Train Loss: 0.4516
Epoch 12 Step 101 Train Loss: 0.5196
Epoch 12 Step 151 Train Loss: 0.6066
Epoch 12 Step 201 Train Loss: 0.5902
Epoch 12 Step 251 Train Loss: 0.4875
Epoch 12 Step 301 Train Loss: 0.7257
Epoch 12 Step 351 Train Loss: 0.5185
Epoch 12 Step 401 Train Loss: 0.4416
Epoch 12 Step 451 Train Loss: 0.5171
Epoch 12 Step 501 Train Loss: 0.6695
Epoch 12 Step 551 Train Loss: 0.4303
Epoch 12 Step 601 Train Loss: 0.5312
Epoch 12 Step 651 Train Loss: 0.5083
Epoch 12 Step 701 Train Loss: 0.5590
Epoch 12 Step 751 Train Loss: 0.6073
Epoch 12 Step 801 Train Loss: 0.6779
Epoch 12 Step 851 Train Loss: 0.5140
Epoch 12 Step 901 Train Loss: 0.5906
Epoch 12 Step 951 Train Loss: 0.4833
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0079 Validation Top 20 DE MSE: 0.0134. 
Epoch 13 Step 1 Train Loss: 0.6076
Epoch 13 Step 51 Train Loss: 0.4694
Epoch 13 Step 101 Train Loss: 0.5321
Epoch 13 Step 151 Train Loss: 0.4676
Epoch 13 Step 201 Train Loss: 0.4861
Epoch 13 Step 251 Train Loss: 0.5377
Epoch 13 Step 301 Train Loss: 0.4498
Epoch 13 Step 351 Train Loss: 0.3661
Epoch 13 Step 401 Train Loss: 0.4991
Epoch 13 Step 451 Train Loss: 0.5366
Epoch 13 Step 501 Train Loss: 0.5477
Epoch 13 Step 551 Train Loss: 0.5001
Epoch 13 Step 601 Train Loss: 0.4624
Epoch 13 Step 651 Train Loss: 0.4719
Epoch 13 Step 701 Train Loss: 0.5093
Epoch 13 Step 751 Train Loss: 0.4258
Epoch 13 Step 801 Train Loss: 0.4700
Epoch 13 Step 851 Train Loss: 0.4898
Epoch 13 Step 901 Train Loss: 0.4669
Epoch 13 Step 951 Train Loss: 0.4565
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0076 Validation Top 20 DE MSE: 0.0130. 
Epoch 14 Step 1 Train Loss: 0.4129
Epoch 14 Step 51 Train Loss: 0.5282
Epoch 14 Step 101 Train Loss: 0.5312
Epoch 14 Step 151 Train Loss: 0.5061
Epoch 14 Step 201 Train Loss: 0.4949
Epoch 14 Step 251 Train Loss: 0.4309
Epoch 14 Step 301 Train Loss: 0.4847
Epoch 14 Step 351 Train Loss: 0.4529
Epoch 14 Step 401 Train Loss: 0.3887
Epoch 14 Step 451 Train Loss: 0.5989
Epoch 14 Step 501 Train Loss: 0.5044
Epoch 14 Step 551 Train Loss: 0.5652
Epoch 14 Step 601 Train Loss: 0.6383
Epoch 14 Step 651 Train Loss: 0.4834
Epoch 14 Step 701 Train Loss: 0.4549
Epoch 14 Step 751 Train Loss: 0.4139
Epoch 14 Step 801 Train Loss: 0.5252
Epoch 14 Step 851 Train Loss: 0.6375
Epoch 14 Step 901 Train Loss: 0.5052
Epoch 14 Step 951 Train Loss: 0.5668
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0080 Validation Top 20 DE MSE: 0.0131. 
Epoch 15 Step 1 Train Loss: 0.6501
Epoch 15 Step 51 Train Loss: 0.5429
Epoch 15 Step 101 Train Loss: 0.4377
Epoch 15 Step 151 Train Loss: 0.5050
Epoch 15 Step 201 Train Loss: 0.7013
Epoch 15 Step 251 Train Loss: 0.5739
Epoch 15 Step 301 Train Loss: 0.4965
Epoch 15 Step 351 Train Loss: 0.4568
Epoch 15 Step 401 Train Loss: 0.5345
Epoch 15 Step 451 Train Loss: 0.4557
Epoch 15 Step 501 Train Loss: 0.5147
Epoch 15 Step 551 Train Loss: 0.5361
Epoch 15 Step 601 Train Loss: 0.4566
Epoch 15 Step 651 Train Loss: 0.6022
Epoch 15 Step 701 Train Loss: 0.5118
Epoch 15 Step 751 Train Loss: 0.4181
Epoch 15 Step 801 Train Loss: 0.4780
Epoch 15 Step 851 Train Loss: 0.4490
Epoch 15 Step 901 Train Loss: 0.4917
Epoch 15 Step 951 Train Loss: 0.4899
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0127. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0718
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0017019846
test_unseen_single_pearson: 0.9969923641810358
test_unseen_single_mse_de: 0.07184621
test_unseen_single_pearson_de: 0.9779364748298702
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4745399944909317
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9249999999999999
test_unseen_single_mse_top20_de_non_dropout: 0.07184621100912236
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.001 MB of 0.028 MB uploadedwandb: / 0.027 MB of 0.028 MB uploadedwandb: - 0.027 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ
wandb:                                                   val_de_mse ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÖ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.07185
wandb:                                              test_de_pearson 0.97794
wandb:               test_frac_opposite_direction_top20_non_dropout 0.25
wandb:                          test_frac_sigma_below_1_non_dropout 0.925
wandb:                                                     test_mse 0.0017
wandb:                                test_mse_top20_de_non_dropout 0.07185
wandb:                                                 test_pearson 0.99699
wandb:                                           test_pearson_delta 0.47454
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.925
wandb:                                       test_unseen_single_mse 0.0017
wandb:                                    test_unseen_single_mse_de 0.07185
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.07185
wandb:                                   test_unseen_single_pearson 0.99699
wandb:                                test_unseen_single_pearson_de 0.97794
wandb:                             test_unseen_single_pearson_delta 0.47454
wandb:                                                 train_de_mse 0.00923
wandb:                                             train_de_pearson 0.99735
wandb:                                                    train_mse 0.00058
wandb:                                                train_pearson 0.99906
wandb:                                                training_loss 0.49163
wandb:                                                   val_de_mse 0.01266
wandb:                                               val_de_pearson 0.99707
wandb:                                                      val_mse 0.00044
wandb:                                                  val_pearson 0.99918
wandb: 
wandb: üöÄ View run geneformer_Dixit_combined_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/zww4v9v9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_235748-zww4v9v9/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_001851-adnd1tss
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_combined_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/adnd1tss
wandb: WARNING Serializing object of type ndarray that is 20541568 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5687
Epoch 1 Step 51 Train Loss: 0.5204
Epoch 1 Step 101 Train Loss: 0.5931
Epoch 1 Step 151 Train Loss: 0.5344
Epoch 1 Step 201 Train Loss: 0.5499
Epoch 1 Step 251 Train Loss: 0.4937
Epoch 1 Step 301 Train Loss: 0.4523
Epoch 1 Step 351 Train Loss: 0.5467
Epoch 1 Step 401 Train Loss: 0.6065
Epoch 1 Step 451 Train Loss: 0.6465
Epoch 1 Step 501 Train Loss: 0.6821
Epoch 1 Step 551 Train Loss: 0.4685
Epoch 1 Step 601 Train Loss: 0.4992
Epoch 1 Step 651 Train Loss: 0.5487
Epoch 1 Step 701 Train Loss: 0.4637
Epoch 1 Step 751 Train Loss: 0.5294
Epoch 1 Step 801 Train Loss: 0.4537
Epoch 1 Step 851 Train Loss: 0.4307
Epoch 1 Step 901 Train Loss: 0.5532
Epoch 1 Step 951 Train Loss: 0.3897
Epoch 1 Step 1001 Train Loss: 0.4719
Epoch 1 Step 1051 Train Loss: 0.6553
Epoch 1 Step 1101 Train Loss: 0.5010
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0482 Validation Top 20 DE MSE: 0.0595. 
Epoch 2 Step 1 Train Loss: 0.7275
Epoch 2 Step 51 Train Loss: 0.4549
Epoch 2 Step 101 Train Loss: 0.4590
Epoch 2 Step 151 Train Loss: 0.4739
Epoch 2 Step 201 Train Loss: 0.4643
Epoch 2 Step 251 Train Loss: 0.6171
Epoch 2 Step 301 Train Loss: 0.5895
Epoch 2 Step 351 Train Loss: 0.5334
Epoch 2 Step 401 Train Loss: 0.4662
Epoch 2 Step 451 Train Loss: 0.4097
Epoch 2 Step 501 Train Loss: 0.5719
Epoch 2 Step 551 Train Loss: 0.6314
Epoch 2 Step 601 Train Loss: 0.6884
Epoch 2 Step 651 Train Loss: 0.6141
Epoch 2 Step 701 Train Loss: 0.4659
Epoch 2 Step 751 Train Loss: 0.4364
Epoch 2 Step 801 Train Loss: 0.6142
Epoch 2 Step 851 Train Loss: 0.5244
Epoch 2 Step 901 Train Loss: 0.5595
Epoch 2 Step 951 Train Loss: 0.4111
Epoch 2 Step 1001 Train Loss: 0.4473
Epoch 2 Step 1051 Train Loss: 0.4460
Epoch 2 Step 1101 Train Loss: 0.4373
Epoch 2: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0262 Validation Top 20 DE MSE: 0.0237. 
Epoch 3 Step 1 Train Loss: 0.4950
Epoch 3 Step 51 Train Loss: 0.5211
Epoch 3 Step 101 Train Loss: 0.5123
Epoch 3 Step 151 Train Loss: 0.5537
Epoch 3 Step 201 Train Loss: 0.4655
Epoch 3 Step 251 Train Loss: 0.4298
Epoch 3 Step 301 Train Loss: 0.5132
Epoch 3 Step 351 Train Loss: 0.4243
Epoch 3 Step 401 Train Loss: 0.5833
Epoch 3 Step 451 Train Loss: 0.5323
Epoch 3 Step 501 Train Loss: 0.6297
Epoch 3 Step 551 Train Loss: 0.5109
Epoch 3 Step 601 Train Loss: 0.4762
Epoch 3 Step 651 Train Loss: 0.4830
Epoch 3 Step 701 Train Loss: 0.5091
Epoch 3 Step 751 Train Loss: 0.5274
Epoch 3 Step 801 Train Loss: 0.3983
Epoch 3 Step 851 Train Loss: 0.3826
Epoch 3 Step 901 Train Loss: 0.4823
Epoch 3 Step 951 Train Loss: 0.4818
Epoch 3 Step 1001 Train Loss: 0.4490
Epoch 3 Step 1051 Train Loss: 0.6830
Epoch 3 Step 1101 Train Loss: 0.4593
Epoch 3: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0400 Validation Top 20 DE MSE: 0.0309. 
Epoch 4 Step 1 Train Loss: 0.5576
Epoch 4 Step 51 Train Loss: 0.5350
Epoch 4 Step 101 Train Loss: 0.4704
Epoch 4 Step 151 Train Loss: 0.5116
Epoch 4 Step 201 Train Loss: 0.5844
Epoch 4 Step 251 Train Loss: 0.4746
Epoch 4 Step 301 Train Loss: 0.5794
Epoch 4 Step 351 Train Loss: 0.5178
Epoch 4 Step 401 Train Loss: 0.6234
Epoch 4 Step 451 Train Loss: 0.4915
Epoch 4 Step 501 Train Loss: 0.5263
Epoch 4 Step 551 Train Loss: 0.4974
Epoch 4 Step 601 Train Loss: 0.5723
Epoch 4 Step 651 Train Loss: 0.5090
Epoch 4 Step 701 Train Loss: 0.4173
Epoch 4 Step 751 Train Loss: 0.4909
Epoch 4 Step 801 Train Loss: 0.4841
Epoch 4 Step 851 Train Loss: 0.5791
Epoch 4 Step 901 Train Loss: 0.4505
Epoch 4 Step 951 Train Loss: 0.6533
Epoch 4 Step 1001 Train Loss: 0.5359
Epoch 4 Step 1051 Train Loss: 0.5097
Epoch 4 Step 1101 Train Loss: 0.6210
Epoch 4: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0073 Validation Top 20 DE MSE: 0.0082. 
Epoch 5 Step 1 Train Loss: 0.5443
Epoch 5 Step 51 Train Loss: 0.6781
Epoch 5 Step 101 Train Loss: 0.4497
Epoch 5 Step 151 Train Loss: 0.5907
Epoch 5 Step 201 Train Loss: 0.5449
Epoch 5 Step 251 Train Loss: 0.5455
Epoch 5 Step 301 Train Loss: 0.4671
Epoch 5 Step 351 Train Loss: 0.5544
Epoch 5 Step 401 Train Loss: 0.5041
Epoch 5 Step 451 Train Loss: 0.4919
Epoch 5 Step 501 Train Loss: 0.5056
Epoch 5 Step 551 Train Loss: 0.5222
Epoch 5 Step 601 Train Loss: 0.5147
Epoch 5 Step 651 Train Loss: 0.4780
Epoch 5 Step 701 Train Loss: 0.6944
Epoch 5 Step 751 Train Loss: 0.5052
Epoch 5 Step 801 Train Loss: 0.5130
Epoch 5 Step 851 Train Loss: 0.5371
Epoch 5 Step 901 Train Loss: 0.5365
Epoch 5 Step 951 Train Loss: 0.4742
Epoch 5 Step 1001 Train Loss: 0.4703
Epoch 5 Step 1051 Train Loss: 0.5800
Epoch 5 Step 1101 Train Loss: 0.5192
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.0071. 
Epoch 6 Step 1 Train Loss: 0.4494
Epoch 6 Step 51 Train Loss: 0.5448
Epoch 6 Step 101 Train Loss: 0.4962
Epoch 6 Step 151 Train Loss: 0.5103
Epoch 6 Step 201 Train Loss: 0.6318
Epoch 6 Step 251 Train Loss: 0.4517
Epoch 6 Step 301 Train Loss: 0.5234
Epoch 6 Step 351 Train Loss: 0.5862
Epoch 6 Step 401 Train Loss: 0.4300
Epoch 6 Step 451 Train Loss: 0.6231
Epoch 6 Step 501 Train Loss: 0.4411
Epoch 6 Step 551 Train Loss: 0.4753
Epoch 6 Step 601 Train Loss: 0.5019
Epoch 6 Step 651 Train Loss: 0.4115
Epoch 6 Step 701 Train Loss: 0.6141
Epoch 6 Step 751 Train Loss: 0.5377
Epoch 6 Step 801 Train Loss: 0.5474
Epoch 6 Step 851 Train Loss: 0.5524
Epoch 6 Step 901 Train Loss: 0.4399
Epoch 6 Step 951 Train Loss: 0.4826
Epoch 6 Step 1001 Train Loss: 0.4837
Epoch 6 Step 1051 Train Loss: 0.5095
Epoch 6 Step 1101 Train Loss: 0.4988
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0072. 
Epoch 7 Step 1 Train Loss: 0.5831
Epoch 7 Step 51 Train Loss: 0.6222
Epoch 7 Step 101 Train Loss: 0.5239
Epoch 7 Step 151 Train Loss: 0.6507
Epoch 7 Step 201 Train Loss: 0.5322
Epoch 7 Step 251 Train Loss: 0.4980
Epoch 7 Step 301 Train Loss: 0.6076
Epoch 7 Step 351 Train Loss: 0.3891
Epoch 7 Step 401 Train Loss: 0.4852
Epoch 7 Step 451 Train Loss: 0.4887
Epoch 7 Step 501 Train Loss: 0.5770
Epoch 7 Step 551 Train Loss: 0.5048
Epoch 7 Step 601 Train Loss: 0.4577
Epoch 7 Step 651 Train Loss: 0.4471
Epoch 7 Step 701 Train Loss: 0.5149
Epoch 7 Step 751 Train Loss: 0.5483
Epoch 7 Step 801 Train Loss: 0.4723
Epoch 7 Step 851 Train Loss: 0.4890
Epoch 7 Step 901 Train Loss: 0.4612
Epoch 7 Step 951 Train Loss: 0.4701
Epoch 7 Step 1001 Train Loss: 0.5395
Epoch 7 Step 1051 Train Loss: 0.4467
Epoch 7 Step 1101 Train Loss: 0.4854
Epoch 7: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0079. 
Epoch 8 Step 1 Train Loss: 0.4814
Epoch 8 Step 51 Train Loss: 0.5566
Epoch 8 Step 101 Train Loss: 0.4594
Epoch 8 Step 151 Train Loss: 0.3939
Epoch 8 Step 201 Train Loss: 0.6171
Epoch 8 Step 251 Train Loss: 0.4952
Epoch 8 Step 301 Train Loss: 0.6838
Epoch 8 Step 351 Train Loss: 0.3663
Epoch 8 Step 401 Train Loss: 0.5861
Epoch 8 Step 451 Train Loss: 0.5859
Epoch 8 Step 501 Train Loss: 0.4543
Epoch 8 Step 551 Train Loss: 0.4838
Epoch 8 Step 601 Train Loss: 0.6166
Epoch 8 Step 651 Train Loss: 0.5285
Epoch 8 Step 701 Train Loss: 0.5485
Epoch 8 Step 751 Train Loss: 0.4664
Epoch 8 Step 801 Train Loss: 0.5602
Epoch 8 Step 851 Train Loss: 0.5209
Epoch 8 Step 901 Train Loss: 0.5017
Epoch 8 Step 951 Train Loss: 0.5281
Epoch 8 Step 1001 Train Loss: 0.6510
Epoch 8 Step 1051 Train Loss: 0.5182
Epoch 8 Step 1101 Train Loss: 0.5663
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0072. 
Epoch 9 Step 1 Train Loss: 0.4992
Epoch 9 Step 51 Train Loss: 0.4714
Epoch 9 Step 101 Train Loss: 0.4371
Epoch 9 Step 151 Train Loss: 0.5279
Epoch 9 Step 201 Train Loss: 0.5443
Epoch 9 Step 251 Train Loss: 0.5608
Epoch 9 Step 301 Train Loss: 0.5415
Epoch 9 Step 351 Train Loss: 0.4786
Epoch 9 Step 401 Train Loss: 0.5673
Epoch 9 Step 451 Train Loss: 0.4282
Epoch 9 Step 501 Train Loss: 0.5396
Epoch 9 Step 551 Train Loss: 0.5949
Epoch 9 Step 601 Train Loss: 0.4303
Epoch 9 Step 651 Train Loss: 0.5329
Epoch 9 Step 701 Train Loss: 0.5069
Epoch 9 Step 751 Train Loss: 0.4815
Epoch 9 Step 801 Train Loss: 0.4540
Epoch 9 Step 851 Train Loss: 0.6868
Epoch 9 Step 901 Train Loss: 0.5071
Epoch 9 Step 951 Train Loss: 0.4123
Epoch 9 Step 1001 Train Loss: 0.4900
Epoch 9 Step 1051 Train Loss: 0.4530
Epoch 9 Step 1101 Train Loss: 0.4935
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0079 Validation Top 20 DE MSE: 0.0061. 
Epoch 10 Step 1 Train Loss: 0.4443
Epoch 10 Step 51 Train Loss: 0.5486
Epoch 10 Step 101 Train Loss: 0.5279
Epoch 10 Step 151 Train Loss: 0.4366
Epoch 10 Step 201 Train Loss: 0.4409
Epoch 10 Step 251 Train Loss: 0.4216
Epoch 10 Step 301 Train Loss: 0.4980
Epoch 10 Step 351 Train Loss: 0.5537
Epoch 10 Step 401 Train Loss: 0.4946
Epoch 10 Step 451 Train Loss: 0.5040
Epoch 10 Step 501 Train Loss: 0.4499
Epoch 10 Step 551 Train Loss: 0.5845
Epoch 10 Step 601 Train Loss: 0.4441
Epoch 10 Step 651 Train Loss: 0.4957
Epoch 10 Step 701 Train Loss: 0.4548
Epoch 10 Step 751 Train Loss: 0.5348
Epoch 10 Step 801 Train Loss: 0.7409
Epoch 10 Step 851 Train Loss: 0.5177
Epoch 10 Step 901 Train Loss: 0.6119
Epoch 10 Step 951 Train Loss: 0.5228
Epoch 10 Step 1001 Train Loss: 0.5008
Epoch 10 Step 1051 Train Loss: 0.4726
Epoch 10 Step 1101 Train Loss: 0.6178
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.0060. 
Epoch 11 Step 1 Train Loss: 0.4287
Epoch 11 Step 51 Train Loss: 0.5051
Epoch 11 Step 101 Train Loss: 0.5766
Epoch 11 Step 151 Train Loss: 0.4663
Epoch 11 Step 201 Train Loss: 0.6180
Epoch 11 Step 251 Train Loss: 0.4521
Epoch 11 Step 301 Train Loss: 0.5279
Epoch 11 Step 351 Train Loss: 0.4716
Epoch 11 Step 401 Train Loss: 0.5556
Epoch 11 Step 451 Train Loss: 0.4230
Epoch 11 Step 501 Train Loss: 0.5581
Epoch 11 Step 551 Train Loss: 0.5468
Epoch 11 Step 601 Train Loss: 0.4532
Epoch 11 Step 651 Train Loss: 0.5824
Epoch 11 Step 701 Train Loss: 0.7299
Epoch 11 Step 751 Train Loss: 0.5197
Epoch 11 Step 801 Train Loss: 0.4415
Epoch 11 Step 851 Train Loss: 0.4809
Epoch 11 Step 901 Train Loss: 0.6354
Epoch 11 Step 951 Train Loss: 0.3876
Epoch 11 Step 1001 Train Loss: 0.4371
Epoch 11 Step 1051 Train Loss: 0.5551
Epoch 11 Step 1101 Train Loss: 0.5706
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0081. 
Epoch 12 Step 1 Train Loss: 0.5017
Epoch 12 Step 51 Train Loss: 0.5070
Epoch 12 Step 101 Train Loss: 0.5669
Epoch 12 Step 151 Train Loss: 0.5844
Epoch 12 Step 201 Train Loss: 0.6896
Epoch 12 Step 251 Train Loss: 0.6007
Epoch 12 Step 301 Train Loss: 0.7294
Epoch 12 Step 351 Train Loss: 0.4444
Epoch 12 Step 401 Train Loss: 0.5504
Epoch 12 Step 451 Train Loss: 0.4431
Epoch 12 Step 501 Train Loss: 0.5366
Epoch 12 Step 551 Train Loss: 0.5200
Epoch 12 Step 601 Train Loss: 0.4341
Epoch 12 Step 651 Train Loss: 0.6803
Epoch 12 Step 701 Train Loss: 0.5159
Epoch 12 Step 751 Train Loss: 0.5954
Epoch 12 Step 801 Train Loss: 0.5509
Epoch 12 Step 851 Train Loss: 0.6284
Epoch 12 Step 901 Train Loss: 0.5856
Epoch 12 Step 951 Train Loss: 0.5552
Epoch 12 Step 1001 Train Loss: 0.5278
Epoch 12 Step 1051 Train Loss: 0.5569
Epoch 12 Step 1101 Train Loss: 0.5245
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0076. 
Epoch 13 Step 1 Train Loss: 0.5045
Epoch 13 Step 51 Train Loss: 0.5639
Epoch 13 Step 101 Train Loss: 0.5520
Epoch 13 Step 151 Train Loss: 0.4871
Epoch 13 Step 201 Train Loss: 0.5009
Epoch 13 Step 251 Train Loss: 0.4641
Epoch 13 Step 301 Train Loss: 0.4630
Epoch 13 Step 351 Train Loss: 0.5292
Epoch 13 Step 401 Train Loss: 0.5394
Epoch 13 Step 451 Train Loss: 0.4409
Epoch 13 Step 501 Train Loss: 0.5032
Epoch 13 Step 551 Train Loss: 0.7357
Epoch 13 Step 601 Train Loss: 0.5151
Epoch 13 Step 651 Train Loss: 0.4487
Epoch 13 Step 701 Train Loss: 0.3949
Epoch 13 Step 751 Train Loss: 0.5925
Epoch 13 Step 801 Train Loss: 0.5082
Epoch 13 Step 851 Train Loss: 0.5079
Epoch 13 Step 901 Train Loss: 0.4372
Epoch 13 Step 951 Train Loss: 0.5373
Epoch 13 Step 1001 Train Loss: 0.5430
Epoch 13 Step 1051 Train Loss: 0.5156
Epoch 13 Step 1101 Train Loss: 0.4316
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0078 Validation Top 20 DE MSE: 0.0065. 
Epoch 14 Step 1 Train Loss: 0.5691
Epoch 14 Step 51 Train Loss: 0.5361
Epoch 14 Step 101 Train Loss: 0.4752
Epoch 14 Step 151 Train Loss: 0.4508
Epoch 14 Step 201 Train Loss: 0.4697
Epoch 14 Step 251 Train Loss: 0.6127
Epoch 14 Step 301 Train Loss: 0.4689
Epoch 14 Step 351 Train Loss: 0.4569
Epoch 14 Step 401 Train Loss: 0.4490
Epoch 14 Step 451 Train Loss: 0.5830
Epoch 14 Step 501 Train Loss: 0.5063
Epoch 14 Step 551 Train Loss: 0.5174
Epoch 14 Step 601 Train Loss: 0.4756
Epoch 14 Step 651 Train Loss: 0.4690
Epoch 14 Step 701 Train Loss: 0.4830
Epoch 14 Step 751 Train Loss: 0.6444
Epoch 14 Step 801 Train Loss: 0.4886
Epoch 14 Step 851 Train Loss: 0.5683
Epoch 14 Step 901 Train Loss: 0.4490
Epoch 14 Step 951 Train Loss: 0.7529
Epoch 14 Step 1001 Train Loss: 0.4125
Epoch 14 Step 1051 Train Loss: 0.4938
Epoch 14 Step 1101 Train Loss: 0.4534
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0071 Validation Top 20 DE MSE: 0.0062. 
Epoch 15 Step 1 Train Loss: 0.6219
Epoch 15 Step 51 Train Loss: 0.4441
Epoch 15 Step 101 Train Loss: 0.4456
Epoch 15 Step 151 Train Loss: 0.4347
Epoch 15 Step 201 Train Loss: 0.4457
Epoch 15 Step 251 Train Loss: 0.4696
Epoch 15 Step 301 Train Loss: 0.4388
Epoch 15 Step 351 Train Loss: 0.4966
Epoch 15 Step 401 Train Loss: 0.5496
Epoch 15 Step 451 Train Loss: 0.5549
Epoch 15 Step 501 Train Loss: 0.5773
Epoch 15 Step 551 Train Loss: 0.5326
Epoch 15 Step 601 Train Loss: 0.4930
Epoch 15 Step 651 Train Loss: 0.3987
Epoch 15 Step 701 Train Loss: 0.6284
Epoch 15 Step 751 Train Loss: 0.4422
Epoch 15 Step 801 Train Loss: 0.5074
Epoch 15 Step 851 Train Loss: 0.4022
Epoch 15 Step 901 Train Loss: 0.4731
Epoch 15 Step 951 Train Loss: 0.4435
Epoch 15 Step 1001 Train Loss: 0.4640
Epoch 15 Step 1051 Train Loss: 0.4610
Epoch 15 Step 1101 Train Loss: 0.5036
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0076. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1240
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0026349947
test_unseen_single_pearson: 0.9953819573860138
test_unseen_single_mse_de: 0.123971425
test_unseen_single_pearson_de: 0.9663523916051169
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.6163516902318638
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.16666666666666666
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8250000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.12384035255475606
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.026 MB uploadedwandb: | 0.001 MB of 0.030 MB uploadedwandb: / 0.007 MB of 0.030 MB uploadedwandb: - 0.030 MB of 0.030 MB uploadedwandb: \ 0.030 MB of 0.030 MB uploadedwandb: | 0.030 MB of 0.030 MB uploadedwandb: / 0.030 MB of 0.030 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñá‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñÖ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.12397
wandb:                                              test_de_pearson 0.96635
wandb:               test_frac_opposite_direction_top20_non_dropout 0.16667
wandb:                          test_frac_sigma_below_1_non_dropout 0.825
wandb:                                                     test_mse 0.00263
wandb:                                test_mse_top20_de_non_dropout 0.12384
wandb:                                                 test_pearson 0.99538
wandb:                                           test_pearson_delta 0.61635
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.16667
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.825
wandb:                                       test_unseen_single_mse 0.00263
wandb:                                    test_unseen_single_mse_de 0.12397
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.12384
wandb:                                   test_unseen_single_pearson 0.99538
wandb:                                test_unseen_single_pearson_de 0.96635
wandb:                             test_unseen_single_pearson_delta 0.61635
wandb:                                                 train_de_mse 0.0091
wandb:                                             train_de_pearson 0.99734
wandb:                                                    train_mse 0.0005
wandb:                                                train_pearson 0.99917
wandb:                                                training_loss 0.52404
wandb:                                                   val_de_mse 0.00758
wandb:                                               val_de_pearson 0.99764
wandb:                                                      val_mse 0.00064
wandb:                                                  val_pearson 0.99893
wandb: 
wandb: üöÄ View run geneformer_Dixit_combined_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/adnd1tss
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_001851-adnd1tss/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_004144-hdm9n31g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_combined_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/hdm9n31g
wandb: WARNING Serializing object of type ndarray that is 20541568 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5664
Epoch 1 Step 51 Train Loss: 0.6527
Epoch 1 Step 101 Train Loss: 0.5859
Epoch 1 Step 151 Train Loss: 0.6169
Epoch 1 Step 201 Train Loss: 0.5487
Epoch 1 Step 251 Train Loss: 0.5289
Epoch 1 Step 301 Train Loss: 0.6802
Epoch 1 Step 351 Train Loss: 0.4814
Epoch 1 Step 401 Train Loss: 0.5790
Epoch 1 Step 451 Train Loss: 0.4663
Epoch 1 Step 501 Train Loss: 0.4662
Epoch 1 Step 551 Train Loss: 0.7501
Epoch 1 Step 601 Train Loss: 0.5995
Epoch 1 Step 651 Train Loss: 0.5667
Epoch 1 Step 701 Train Loss: 0.5434
Epoch 1 Step 751 Train Loss: 0.6959
Epoch 1 Step 801 Train Loss: 0.5177
Epoch 1 Step 851 Train Loss: 0.5752
Epoch 1 Step 901 Train Loss: 0.5147
Epoch 1 Step 951 Train Loss: 0.5927
Epoch 1 Step 1001 Train Loss: 0.4712
Epoch 1 Step 1051 Train Loss: 0.4860
Epoch 1 Step 1101 Train Loss: 0.4394
Epoch 1: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0657 Validation Top 20 DE MSE: 0.4920. 
Epoch 2 Step 1 Train Loss: 0.6359
Epoch 2 Step 51 Train Loss: 0.5352
Epoch 2 Step 101 Train Loss: 0.4421
Epoch 2 Step 151 Train Loss: 0.4986
Epoch 2 Step 201 Train Loss: 0.6190
Epoch 2 Step 251 Train Loss: 0.4999
Epoch 2 Step 301 Train Loss: 0.5536
Epoch 2 Step 351 Train Loss: 0.5675
Epoch 2 Step 401 Train Loss: 0.4411
Epoch 2 Step 451 Train Loss: 0.4233
Epoch 2 Step 501 Train Loss: 0.5347
Epoch 2 Step 551 Train Loss: 0.5318
Epoch 2 Step 601 Train Loss: 0.6154
Epoch 2 Step 651 Train Loss: 0.5046
Epoch 2 Step 701 Train Loss: 0.4733
Epoch 2 Step 751 Train Loss: 0.5751
Epoch 2 Step 801 Train Loss: 0.4281
Epoch 2 Step 851 Train Loss: 0.6845
Epoch 2 Step 901 Train Loss: 0.6597
Epoch 2 Step 951 Train Loss: 0.4731
Epoch 2 Step 1001 Train Loss: 0.4353
Epoch 2 Step 1051 Train Loss: 0.6353
Epoch 2 Step 1101 Train Loss: 0.7346
Epoch 2: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0183 Validation Top 20 DE MSE: 0.0769. 
Epoch 3 Step 1 Train Loss: 0.7132
Epoch 3 Step 51 Train Loss: 0.4731
Epoch 3 Step 101 Train Loss: 0.6896
Epoch 3 Step 151 Train Loss: 0.4830
Epoch 3 Step 201 Train Loss: 0.5050
Epoch 3 Step 251 Train Loss: 0.5115
Epoch 3 Step 301 Train Loss: 0.5506
Epoch 3 Step 351 Train Loss: 0.4915
Epoch 3 Step 401 Train Loss: 0.7297
Epoch 3 Step 451 Train Loss: 0.4935
Epoch 3 Step 501 Train Loss: 0.7234
Epoch 3 Step 551 Train Loss: 0.5146
Epoch 3 Step 601 Train Loss: 0.6276
Epoch 3 Step 651 Train Loss: 0.6126
Epoch 3 Step 701 Train Loss: 0.4171
Epoch 3 Step 751 Train Loss: 0.4223
Epoch 3 Step 801 Train Loss: 0.5723
Epoch 3 Step 851 Train Loss: 0.5008
Epoch 3 Step 901 Train Loss: 0.5041
Epoch 3 Step 951 Train Loss: 0.4462
Epoch 3 Step 1001 Train Loss: 0.4744
Epoch 3 Step 1051 Train Loss: 0.4486
Epoch 3 Step 1101 Train Loss: 0.4744
Epoch 3: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0281. 
Epoch 4 Step 1 Train Loss: 0.6590
Epoch 4 Step 51 Train Loss: 0.4493
Epoch 4 Step 101 Train Loss: 0.5185
Epoch 4 Step 151 Train Loss: 0.4554
Epoch 4 Step 201 Train Loss: 0.5488
Epoch 4 Step 251 Train Loss: 0.4542
Epoch 4 Step 301 Train Loss: 0.4967
Epoch 4 Step 351 Train Loss: 0.5437
Epoch 4 Step 401 Train Loss: 0.5572
Epoch 4 Step 451 Train Loss: 0.4543
Epoch 4 Step 501 Train Loss: 0.6040
Epoch 4 Step 551 Train Loss: 0.4695
Epoch 4 Step 601 Train Loss: 0.4954
Epoch 4 Step 651 Train Loss: 0.7016
Epoch 4 Step 701 Train Loss: 0.5052
Epoch 4 Step 751 Train Loss: 0.4959
Epoch 4 Step 801 Train Loss: 0.6808
Epoch 4 Step 851 Train Loss: 0.5581
Epoch 4 Step 901 Train Loss: 0.5163
Epoch 4 Step 951 Train Loss: 0.4903
Epoch 4 Step 1001 Train Loss: 0.5136
Epoch 4 Step 1051 Train Loss: 0.4742
Epoch 4 Step 1101 Train Loss: 0.5060
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.0334. 
Epoch 5 Step 1 Train Loss: 0.5473
Epoch 5 Step 51 Train Loss: 0.4896
Epoch 5 Step 101 Train Loss: 0.5527
Epoch 5 Step 151 Train Loss: 0.4279
Epoch 5 Step 201 Train Loss: 0.5252
Epoch 5 Step 251 Train Loss: 0.5539
Epoch 5 Step 301 Train Loss: 0.5303
Epoch 5 Step 351 Train Loss: 0.5399
Epoch 5 Step 401 Train Loss: 0.4389
Epoch 5 Step 451 Train Loss: 0.4993
Epoch 5 Step 501 Train Loss: 0.3939
Epoch 5 Step 551 Train Loss: 0.4689
Epoch 5 Step 601 Train Loss: 0.4810
Epoch 5 Step 651 Train Loss: 0.5283
Epoch 5 Step 701 Train Loss: 0.6496
Epoch 5 Step 751 Train Loss: 0.4297
Epoch 5 Step 801 Train Loss: 0.5405
Epoch 5 Step 851 Train Loss: 0.4442
Epoch 5 Step 901 Train Loss: 0.5330
Epoch 5 Step 951 Train Loss: 0.5552
Epoch 5 Step 1001 Train Loss: 0.4766
Epoch 5 Step 1051 Train Loss: 0.6291
Epoch 5 Step 1101 Train Loss: 0.6266
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0190 Validation Top 20 DE MSE: 0.1353. 
Epoch 6 Step 1 Train Loss: 0.5255
Epoch 6 Step 51 Train Loss: 0.5743
Epoch 6 Step 101 Train Loss: 0.5150
Epoch 6 Step 151 Train Loss: 0.4990
Epoch 6 Step 201 Train Loss: 0.6544
Epoch 6 Step 251 Train Loss: 0.4035
Epoch 6 Step 301 Train Loss: 0.5758
Epoch 6 Step 351 Train Loss: 0.5401
Epoch 6 Step 401 Train Loss: 0.4726
Epoch 6 Step 451 Train Loss: 0.4743
Epoch 6 Step 501 Train Loss: 0.5202
Epoch 6 Step 551 Train Loss: 0.4678
Epoch 6 Step 601 Train Loss: 0.4741
Epoch 6 Step 651 Train Loss: 0.4166
Epoch 6 Step 701 Train Loss: 0.5173
Epoch 6 Step 751 Train Loss: 0.4913
Epoch 6 Step 801 Train Loss: 0.5105
Epoch 6 Step 851 Train Loss: 0.5003
Epoch 6 Step 901 Train Loss: 0.4594
Epoch 6 Step 951 Train Loss: 0.4809
Epoch 6 Step 1001 Train Loss: 0.3970
Epoch 6 Step 1051 Train Loss: 0.4711
Epoch 6 Step 1101 Train Loss: 0.4853
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0076 Validation Top 20 DE MSE: 0.0181. 
Epoch 7 Step 1 Train Loss: 0.6007
Epoch 7 Step 51 Train Loss: 0.4866
Epoch 7 Step 101 Train Loss: 0.4550
Epoch 7 Step 151 Train Loss: 0.5935
Epoch 7 Step 201 Train Loss: 0.5239
Epoch 7 Step 251 Train Loss: 0.6127
Epoch 7 Step 301 Train Loss: 0.4259
Epoch 7 Step 351 Train Loss: 0.4366
Epoch 7 Step 401 Train Loss: 0.8555
Epoch 7 Step 451 Train Loss: 0.4954
Epoch 7 Step 501 Train Loss: 0.4898
Epoch 7 Step 551 Train Loss: 0.4859
Epoch 7 Step 601 Train Loss: 0.4733
Epoch 7 Step 651 Train Loss: 0.5224
Epoch 7 Step 701 Train Loss: 0.5380
Epoch 7 Step 751 Train Loss: 0.5496
Epoch 7 Step 801 Train Loss: 0.5399
Epoch 7 Step 851 Train Loss: 0.4865
Epoch 7 Step 901 Train Loss: 0.5163
Epoch 7 Step 951 Train Loss: 0.5314
Epoch 7 Step 1001 Train Loss: 0.4909
Epoch 7 Step 1051 Train Loss: 0.4703
Epoch 7 Step 1101 Train Loss: 0.4163
Epoch 7: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0076 Validation Top 20 DE MSE: 0.0208. 
Epoch 8 Step 1 Train Loss: 0.5447
Epoch 8 Step 51 Train Loss: 0.4890
Epoch 8 Step 101 Train Loss: 0.5783
Epoch 8 Step 151 Train Loss: 0.4818
Epoch 8 Step 201 Train Loss: 0.4405
Epoch 8 Step 251 Train Loss: 0.5355
Epoch 8 Step 301 Train Loss: 0.4883
Epoch 8 Step 351 Train Loss: 0.4789
Epoch 8 Step 401 Train Loss: 0.4833
Epoch 8 Step 451 Train Loss: 0.5870
Epoch 8 Step 501 Train Loss: 0.4411
Epoch 8 Step 551 Train Loss: 0.4707
Epoch 8 Step 601 Train Loss: 0.5173
Epoch 8 Step 651 Train Loss: 0.6684
Epoch 8 Step 701 Train Loss: 0.4612
Epoch 8 Step 751 Train Loss: 0.4137
Epoch 8 Step 801 Train Loss: 0.4951
Epoch 8 Step 851 Train Loss: 0.5370
Epoch 8 Step 901 Train Loss: 0.4746
Epoch 8 Step 951 Train Loss: 0.4405
Epoch 8 Step 1001 Train Loss: 0.5555
Epoch 8 Step 1051 Train Loss: 0.5843
Epoch 8 Step 1101 Train Loss: 0.5657
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0193. 
Epoch 9 Step 1 Train Loss: 0.4553
Epoch 9 Step 51 Train Loss: 0.5737
Epoch 9 Step 101 Train Loss: 0.4440
Epoch 9 Step 151 Train Loss: 0.4649
Epoch 9 Step 201 Train Loss: 0.6171
Epoch 9 Step 251 Train Loss: 0.4916
Epoch 9 Step 301 Train Loss: 0.4994
Epoch 9 Step 351 Train Loss: 0.4062
Epoch 9 Step 401 Train Loss: 0.5149
Epoch 9 Step 451 Train Loss: 0.5152
Epoch 9 Step 501 Train Loss: 0.5210
Epoch 9 Step 551 Train Loss: 0.4524
Epoch 9 Step 601 Train Loss: 0.5903
Epoch 9 Step 651 Train Loss: 0.4921
Epoch 9 Step 701 Train Loss: 0.4795
Epoch 9 Step 751 Train Loss: 0.5404
Epoch 9 Step 801 Train Loss: 0.6156
Epoch 9 Step 851 Train Loss: 0.4395
Epoch 9 Step 901 Train Loss: 0.4337
Epoch 9 Step 951 Train Loss: 0.5223
Epoch 9 Step 1001 Train Loss: 0.5999
Epoch 9 Step 1051 Train Loss: 0.5527
Epoch 9 Step 1101 Train Loss: 0.6602
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0085 Validation Top 20 DE MSE: 0.0232. 
Epoch 10 Step 1 Train Loss: 0.5415
Epoch 10 Step 51 Train Loss: 0.4502
Epoch 10 Step 101 Train Loss: 0.4487
Epoch 10 Step 151 Train Loss: 0.6047
Epoch 10 Step 201 Train Loss: 0.4683
Epoch 10 Step 251 Train Loss: 0.3934
Epoch 10 Step 301 Train Loss: 0.5966
Epoch 10 Step 351 Train Loss: 0.8774
Epoch 10 Step 401 Train Loss: 0.5927
Epoch 10 Step 451 Train Loss: 0.4128
Epoch 10 Step 501 Train Loss: 0.5104
Epoch 10 Step 551 Train Loss: 0.6446
Epoch 10 Step 601 Train Loss: 0.6160
Epoch 10 Step 651 Train Loss: 0.5901
Epoch 10 Step 701 Train Loss: 0.6035
Epoch 10 Step 751 Train Loss: 0.5735
Epoch 10 Step 801 Train Loss: 0.5571
Epoch 10 Step 851 Train Loss: 0.4138
Epoch 10 Step 901 Train Loss: 0.5291
Epoch 10 Step 951 Train Loss: 0.6505
Epoch 10 Step 1001 Train Loss: 0.4830
Epoch 10 Step 1051 Train Loss: 0.3742
Epoch 10 Step 1101 Train Loss: 0.6324
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.0852. 
Epoch 11 Step 1 Train Loss: 0.5248
Epoch 11 Step 51 Train Loss: 0.4619
Epoch 11 Step 101 Train Loss: 0.4488
Epoch 11 Step 151 Train Loss: 0.3943
Epoch 11 Step 201 Train Loss: 0.4301
Epoch 11 Step 251 Train Loss: 0.5715
Epoch 11 Step 301 Train Loss: 0.4868
Epoch 11 Step 351 Train Loss: 0.5641
Epoch 11 Step 401 Train Loss: 0.6146
Epoch 11 Step 451 Train Loss: 0.4145
Epoch 11 Step 501 Train Loss: 0.5740
Epoch 11 Step 551 Train Loss: 0.5077
Epoch 11 Step 601 Train Loss: 0.4589
Epoch 11 Step 651 Train Loss: 0.5974
Epoch 11 Step 701 Train Loss: 0.7097
Epoch 11 Step 751 Train Loss: 0.4302
Epoch 11 Step 801 Train Loss: 0.6252
Epoch 11 Step 851 Train Loss: 0.4753
Epoch 11 Step 901 Train Loss: 0.4658
Epoch 11 Step 951 Train Loss: 0.4753
Epoch 11 Step 1001 Train Loss: 0.5110
Epoch 11 Step 1051 Train Loss: 0.5360
Epoch 11 Step 1101 Train Loss: 0.4584
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.1403. 
Epoch 12 Step 1 Train Loss: 0.4974
Epoch 12 Step 51 Train Loss: 0.5522
Epoch 12 Step 101 Train Loss: 0.4619
Epoch 12 Step 151 Train Loss: 0.5627
Epoch 12 Step 201 Train Loss: 0.5729
Epoch 12 Step 251 Train Loss: 0.4442
Epoch 12 Step 301 Train Loss: 0.3833
Epoch 12 Step 351 Train Loss: 0.5854
Epoch 12 Step 401 Train Loss: 0.5599
Epoch 12 Step 451 Train Loss: 0.5650
Epoch 12 Step 501 Train Loss: 0.4330
Epoch 12 Step 551 Train Loss: 0.4399
Epoch 12 Step 601 Train Loss: 0.4596
Epoch 12 Step 651 Train Loss: 0.5497
Epoch 12 Step 701 Train Loss: 0.4606
Epoch 12 Step 751 Train Loss: 0.5230
Epoch 12 Step 801 Train Loss: 0.6245
Epoch 12 Step 851 Train Loss: 0.4842
Epoch 12 Step 901 Train Loss: 0.6491
Epoch 12 Step 951 Train Loss: 0.5005
Epoch 12 Step 1001 Train Loss: 0.6003
Epoch 12 Step 1051 Train Loss: 0.6465
Epoch 12 Step 1101 Train Loss: 0.5551
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0204. 
Epoch 13 Step 1 Train Loss: 0.4371
Epoch 13 Step 51 Train Loss: 0.5421
Epoch 13 Step 101 Train Loss: 0.4638
Epoch 13 Step 151 Train Loss: 0.5583
Epoch 13 Step 201 Train Loss: 0.6273
Epoch 13 Step 251 Train Loss: 0.4453
Epoch 13 Step 301 Train Loss: 0.5159
Epoch 13 Step 351 Train Loss: 0.5219
Epoch 13 Step 401 Train Loss: 0.4911
Epoch 13 Step 451 Train Loss: 0.6715
Epoch 13 Step 501 Train Loss: 0.4391
Epoch 13 Step 551 Train Loss: 0.4705
Epoch 13 Step 601 Train Loss: 0.4199
Epoch 13 Step 651 Train Loss: 0.4477
Epoch 13 Step 701 Train Loss: 0.4747
Epoch 13 Step 751 Train Loss: 0.7293
Epoch 13 Step 801 Train Loss: 0.5499
Epoch 13 Step 851 Train Loss: 0.5579
Epoch 13 Step 901 Train Loss: 0.4650
Epoch 13 Step 951 Train Loss: 0.5156
Epoch 13 Step 1001 Train Loss: 0.4939
Epoch 13 Step 1051 Train Loss: 0.5527
Epoch 13 Step 1101 Train Loss: 0.6042
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0250. 
Epoch 14 Step 1 Train Loss: 0.4369
Epoch 14 Step 51 Train Loss: 0.5266
Epoch 14 Step 101 Train Loss: 0.5370
Epoch 14 Step 151 Train Loss: 0.4922
Epoch 14 Step 201 Train Loss: 0.4916
Epoch 14 Step 251 Train Loss: 0.4524
Epoch 14 Step 301 Train Loss: 0.5598
Epoch 14 Step 351 Train Loss: 0.4736
Epoch 14 Step 401 Train Loss: 0.4966
Epoch 14 Step 451 Train Loss: 0.4036
Epoch 14 Step 501 Train Loss: 0.6182
Epoch 14 Step 551 Train Loss: 0.4159
Epoch 14 Step 601 Train Loss: 0.5008
Epoch 14 Step 651 Train Loss: 0.7028
Epoch 14 Step 701 Train Loss: 0.5337
Epoch 14 Step 751 Train Loss: 0.4085
Epoch 14 Step 801 Train Loss: 0.5315
Epoch 14 Step 851 Train Loss: 0.6349
Epoch 14 Step 901 Train Loss: 0.4803
Epoch 14 Step 951 Train Loss: 0.4579
Epoch 14 Step 1001 Train Loss: 0.4196
Epoch 14 Step 1051 Train Loss: 0.4664
Epoch 14 Step 1101 Train Loss: 0.4584
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0534. 
Epoch 15 Step 1 Train Loss: 0.5208
Epoch 15 Step 51 Train Loss: 0.4409
Epoch 15 Step 101 Train Loss: 0.5237
Epoch 15 Step 151 Train Loss: 0.7462
Epoch 15 Step 201 Train Loss: 0.5517
Epoch 15 Step 251 Train Loss: 0.5300
Epoch 15 Step 301 Train Loss: 0.5518
Epoch 15 Step 351 Train Loss: 0.5469
Epoch 15 Step 401 Train Loss: 0.4130
Epoch 15 Step 451 Train Loss: 0.5258
Epoch 15 Step 501 Train Loss: 0.5070
Epoch 15 Step 551 Train Loss: 0.4158
Epoch 15 Step 601 Train Loss: 0.6128
Epoch 15 Step 651 Train Loss: 0.5047
Epoch 15 Step 701 Train Loss: 0.4432
Epoch 15 Step 751 Train Loss: 0.6288
Epoch 15 Step 801 Train Loss: 0.4739
Epoch 15 Step 851 Train Loss: 0.6036
Epoch 15 Step 901 Train Loss: 0.4485
Epoch 15 Step 951 Train Loss: 0.4692
Epoch 15 Step 1001 Train Loss: 0.5327
Epoch 15 Step 1051 Train Loss: 0.5090
Epoch 15 Step 1101 Train Loss: 0.5507
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0086 Validation Top 20 DE MSE: 0.0356. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0077
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0004673842
test_unseen_single_pearson: 0.9992020485970098
test_unseen_single_mse_de: 0.0077278283
test_unseen_single_pearson_de: 0.9981973637464794
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.9262161764005802
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9833333333333334
test_unseen_single_mse_top20_de_non_dropout: 0.007727828791961953
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.030 MB uploadedwandb: | 0.003 MB of 0.030 MB uploadedwandb: / 0.030 MB of 0.030 MB uploadedwandb: - 0.030 MB of 0.030 MB uploadedwandb: \ 0.030 MB of 0.030 MB uploadedwandb: | 0.030 MB of 0.030 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÅ
wandb:                                                   val_de_mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00773
wandb:                                              test_de_pearson 0.9982
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.98333
wandb:                                                     test_mse 0.00047
wandb:                                test_mse_top20_de_non_dropout 0.00773
wandb:                                                 test_pearson 0.9992
wandb:                                           test_pearson_delta 0.92622
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98333
wandb:                                       test_unseen_single_mse 0.00047
wandb:                                    test_unseen_single_mse_de 0.00773
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00773
wandb:                                   test_unseen_single_pearson 0.9992
wandb:                                test_unseen_single_pearson_de 0.9982
wandb:                             test_unseen_single_pearson_delta 0.92622
wandb:                                                 train_de_mse 0.00855
wandb:                                             train_de_pearson 0.99742
wandb:                                                    train_mse 0.00053
wandb:                                                train_pearson 0.99913
wandb:                                                training_loss 0.52908
wandb:                                                   val_de_mse 0.03559
wandb:                                               val_de_pearson 0.99139
wandb:                                                      val_mse 0.00106
wandb:                                                  val_pearson 0.9982
wandb: 
wandb: üöÄ View run geneformer_Dixit_combined_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/hdm9n31g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_004144-hdm9n31g/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_010457-hf56wmvw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396858_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/hf56wmvw
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
  0%|                                                  | 0/3576 [00:00<?, ?it/s]  0%|                                          | 4/3576 [00:00<01:55, 30.91it/s]  0%|                                          | 8/3576 [00:00<01:43, 34.46it/s]  0%|‚ñè                                        | 16/3576 [00:00<01:09, 51.33it/s]  1%|‚ñé                                        | 24/3576 [00:00<01:02, 57.20it/s]  1%|‚ñç                                        | 40/3576 [00:00<00:40, 88.24it/s]  1%|‚ñå                                        | 50/3576 [00:00<00:39, 89.23it/s]  2%|‚ñã                                        | 60/3576 [00:00<00:40, 86.78it/s]  2%|‚ñä                                        | 71/3576 [00:00<00:38, 91.00it/s]  2%|‚ñâ                                        | 81/3576 [00:01<00:38, 91.24it/s]  3%|‚ñà                                        | 91/3576 [00:01<00:37, 91.81it/s]  3%|‚ñà‚ñè                                      | 101/3576 [00:01<00:43, 80.07it/s]  3%|‚ñà‚ñè                                      | 110/3576 [00:01<00:44, 77.67it/s]  3%|‚ñà‚ñé                                      | 119/3576 [00:01<00:49, 69.24it/s]  4%|‚ñà‚ñå                                      | 136/3576 [00:01<00:41, 82.51it/s]  4%|‚ñà‚ñã                                      | 146/3576 [00:01<00:40, 84.11it/s]  4%|‚ñà‚ñã                                      | 156/3576 [00:01<00:39, 86.92it/s]  5%|‚ñà‚ñä                                      | 165/3576 [00:02<00:40, 84.05it/s]  5%|‚ñà‚ñâ                                      | 174/3576 [00:02<00:40, 83.59it/s]  5%|‚ñà‚ñà                                      | 183/3576 [00:02<00:44, 76.10it/s]  6%|‚ñà‚ñà‚ñè                                     | 198/3576 [00:02<00:37, 90.20it/s]  6%|‚ñà‚ñà‚ñé                                     | 209/3576 [00:02<00:42, 79.45it/s]  6%|‚ñà‚ñà‚ñå                                     | 224/3576 [00:02<00:35, 93.82it/s]  7%|‚ñà‚ñà‚ñå                                     | 234/3576 [00:02<00:35, 92.84it/s]  7%|‚ñà‚ñà‚ñã                                     | 244/3576 [00:02<00:37, 88.55it/s]  7%|‚ñà‚ñà‚ñä                                     | 254/3576 [00:03<00:42, 77.42it/s]  7%|‚ñà‚ñà‚ñâ                                     | 267/3576 [00:03<00:37, 87.47it/s]  8%|‚ñà‚ñà‚ñà                                     | 278/3576 [00:03<00:37, 88.58it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 288/3576 [00:03<00:40, 80.27it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 297/3576 [00:03<00:41, 79.18it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 309/3576 [00:03<00:37, 87.10it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 318/3576 [00:03<00:37, 87.60it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 328/3576 [00:03<00:37, 86.41it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 339/3576 [00:04<00:35, 90.70it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 349/3576 [00:04<00:36, 88.13it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 358/3576 [00:04<00:40, 80.39it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 371/3576 [00:04<00:34, 92.85it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 381/3576 [00:04<00:34, 92.40it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 391/3576 [00:04<00:34, 92.74it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 401/3576 [00:04<00:35, 89.48it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 411/3576 [00:04<00:34, 92.26it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 421/3576 [00:05<00:34, 91.04it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 431/3576 [00:05<00:34, 91.24it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 441/3576 [00:05<00:34, 91.56it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 451/3576 [00:05<00:34, 91.03it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 461/3576 [00:05<00:35, 88.83it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 471/3576 [00:05<00:33, 91.87it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 481/3576 [00:05<00:33, 91.71it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 491/3576 [00:05<00:33, 91.13it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 501/3576 [00:05<00:33, 91.11it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 511/3576 [00:05<00:33, 91.16it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 521/3576 [00:06<00:33, 90.08it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 531/3576 [00:06<00:33, 89.85it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 540/3576 [00:06<00:34, 88.82it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 549/3576 [00:06<00:36, 83.64it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 558/3576 [00:06<00:39, 75.88it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 573/3576 [00:06<00:31, 94.51it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 583/3576 [00:06<00:32, 92.98it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 593/3576 [00:06<00:32, 91.74it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 603/3576 [00:07<00:33, 88.83it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 613/3576 [00:07<00:36, 80.91it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 624/3576 [00:07<00:33, 87.72it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 635/3576 [00:07<00:31, 93.28it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 645/3576 [00:07<00:31, 91.75it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 655/3576 [00:07<00:31, 91.33it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 665/3576 [00:07<00:33, 87.93it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 675/3576 [00:07<00:32, 88.75it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 685/3576 [00:07<00:31, 91.09it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 695/3576 [00:08<00:33, 85.12it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 705/3576 [00:08<00:32, 88.56it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 714/3576 [00:08<00:33, 85.17it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 724/3576 [00:08<00:33, 85.52it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 734/3576 [00:08<00:31, 88.91it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 743/3576 [00:08<00:31, 88.64it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 753/3576 [00:08<00:32, 86.81it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 764/3576 [00:08<00:31, 90.57it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 774/3576 [00:08<00:31, 90.09it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 784/3576 [00:09<00:30, 90.30it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 794/3576 [00:09<00:30, 90.51it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 804/3576 [00:09<00:30, 90.74it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 814/3576 [00:09<00:32, 85.65it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 824/3576 [00:09<00:30, 89.41it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 834/3576 [00:09<00:30, 89.00it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 843/3576 [00:09<00:30, 88.81it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 853/3576 [00:09<00:38, 70.56it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 871/3576 [00:10<00:28, 96.04it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 882/3576 [00:10<00:32, 84.18it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 895/3576 [00:10<00:32, 82.09it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 910/3576 [00:10<00:27, 95.45it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 921/3576 [00:10<00:28, 91.93it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 931/3576 [00:10<00:29, 90.33it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 941/3576 [00:10<00:29, 88.06it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 952/3576 [00:10<00:28, 91.33it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 962/3576 [00:11<00:28, 90.63it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 972/3576 [00:11<00:28, 91.13it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 982/3576 [00:11<00:28, 90.75it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 992/3576 [00:11<00:29, 86.52it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1001/3576 [00:11<00:32, 78.49it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1010/3576 [00:11<00:35, 72.96it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1018/3576 [00:11<00:35, 71.81it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1026/3576 [00:11<00:37, 68.05it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1033/3576 [00:12<00:40, 62.05it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1040/3576 [00:12<00:42, 60.29it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1047/3576 [00:12<00:40, 62.19it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1054/3576 [00:12<00:48, 51.89it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1062/3576 [00:12<00:43, 58.26it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1069/3576 [00:12<00:41, 60.54it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1076/3576 [00:13<01:07, 37.13it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1112/3576 [00:13<00:30, 79.78it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1121/3576 [00:13<00:35, 69.97it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1143/3576 [00:13<00:25, 94.29it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1156/3576 [00:13<00:24, 98.42it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1168/3576 [00:13<00:26, 91.90it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1183/3576 [00:14<00:23, 102.36it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1195/3576 [00:14<00:23, 101.55it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1206/3576 [00:14<00:23, 101.40it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1217/3576 [00:14<00:25, 90.74it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1231/3576 [00:14<00:22, 102.14it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1242/3576 [00:14<00:29, 80.04it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1257/3576 [00:14<00:24, 94.57it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1268/3576 [00:14<00:25, 89.94it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1280/3576 [00:15<00:23, 95.99it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1291/3576 [00:15<00:23, 98.33it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1302/3576 [00:15<00:23, 96.42it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1313/3576 [00:15<00:23, 95.44it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1324/3576 [00:15<00:23, 96.57it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1334/3576 [00:15<00:26, 84.67it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1350/3576 [00:15<00:22, 100.84it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1361/3576 [00:15<00:24, 89.82it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1376/3576 [00:16<00:21, 102.05it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1387/3576 [00:16<00:21, 101.72it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1398/3576 [00:16<00:24, 88.33it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1413/3576 [00:16<00:21, 101.27it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1424/3576 [00:16<00:23, 92.30it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1434/3576 [00:16<00:22, 93.97it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1447/3576 [00:16<00:22, 95.55it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1458/3576 [00:16<00:21, 96.54it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1468/3576 [00:17<00:21, 97.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1478/3576 [00:17<00:22, 93.20it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1490/3576 [00:17<00:21, 99.01it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1501/3576 [00:17<00:21, 96.11it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1511/3576 [00:17<00:24, 84.52it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1525/3576 [00:17<00:21, 97.17it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1536/3576 [00:17<00:21, 94.09it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1546/3576 [00:17<00:21, 95.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1557/3576 [00:17<00:21, 94.35it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1567/3576 [00:18<00:21, 95.52it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1577/3576 [00:18<00:21, 94.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1587/3576 [00:18<00:21, 94.25it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1597/3576 [00:18<00:20, 95.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1607/3576 [00:18<00:20, 94.63it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1617/3576 [00:18<00:23, 83.86it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1627/3576 [00:18<00:22, 86.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1637/3576 [00:18<00:21, 88.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1647/3576 [00:18<00:21, 91.17it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1657/3576 [00:19<00:21, 91.36it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1668/3576 [00:19<00:25, 75.84it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1685/3576 [00:19<00:19, 97.55it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1696/3576 [00:19<00:19, 97.15it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1707/3576 [00:19<00:19, 96.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1718/3576 [00:19<00:19, 96.86it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1728/3576 [00:19<00:19, 95.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1738/3576 [00:19<00:19, 95.46it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1749/3576 [00:20<00:18, 98.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1760/3576 [00:20<00:18, 96.67it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1770/3576 [00:20<00:19, 94.76it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1780/3576 [00:20<00:19, 94.21it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1791/3576 [00:20<00:18, 97.66it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1801/3576 [00:20<00:18, 95.15it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1812/3576 [00:20<00:18, 97.77it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1822/3576 [00:20<00:19, 90.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1834/3576 [00:21<00:27, 63.67it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1862/3576 [00:21<00:16, 103.30it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1875/3576 [00:21<00:16, 103.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1887/3576 [00:21<00:19, 87.26it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1904/3576 [00:21<00:16, 102.24it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1916/3576 [00:21<00:16, 100.05it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1927/3576 [00:22<00:20, 79.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1945/3576 [00:22<00:16, 99.42it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1957/3576 [00:22<00:16, 99.64it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1969/3576 [00:22<00:16, 99.31it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1980/3576 [00:22<00:16, 94.60it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1991/3576 [00:22<00:16, 95.57it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2002/3576 [00:22<00:16, 97.99it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2013/3576 [00:22<00:16, 93.72it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2023/3576 [00:23<00:20, 76.74it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2041/3576 [00:23<00:15, 98.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2052/3576 [00:23<00:15, 96.81it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2063/3576 [00:23<00:16, 94.00it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2074/3576 [00:23<00:15, 96.14it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2085/3576 [00:23<00:15, 97.69it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2096/3576 [00:23<00:15, 97.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2106/3576 [00:23<00:15, 92.36it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2116/3576 [00:23<00:15, 93.73it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2126/3576 [00:24<00:15, 93.33it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2136/3576 [00:24<00:15, 93.61it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2146/3576 [00:24<00:18, 76.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2163/3576 [00:24<00:14, 96.17it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2174/3576 [00:24<00:14, 97.09it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2185/3576 [00:24<00:14, 98.33it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2196/3576 [00:24<00:14, 94.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2206/3576 [00:24<00:14, 92.75it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2217/3576 [00:25<00:14, 92.72it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2227/3576 [00:25<00:14, 92.20it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2237/3576 [00:25<00:14, 92.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2248/3576 [00:25<00:13, 95.08it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2258/3576 [00:25<00:14, 91.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2269/3576 [00:25<00:14, 92.29it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2280/3576 [00:25<00:14, 92.27it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2290/3576 [00:25<00:13, 92.94it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2301/3576 [00:25<00:13, 92.94it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2311/3576 [00:26<00:13, 92.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2321/3576 [00:26<00:13, 89.66it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2331/3576 [00:26<00:13, 90.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2341/3576 [00:26<00:13, 91.06it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2351/3576 [00:26<00:13, 91.07it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2362/3576 [00:26<00:13, 91.80it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2372/3576 [00:26<00:13, 86.95it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2385/3576 [00:26<00:12, 96.32it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2395/3576 [00:27<00:14, 82.58it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2410/3576 [00:27<00:12, 95.23it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2420/3576 [00:27<00:12, 94.35it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2430/3576 [00:27<00:12, 94.42it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2441/3576 [00:27<00:12, 93.91it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2451/3576 [00:27<00:11, 94.27it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2461/3576 [00:27<00:13, 83.25it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2473/3576 [00:27<00:12, 88.43it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2484/3576 [00:27<00:11, 92.92it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2494/3576 [00:28<00:11, 93.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2505/3576 [00:28<00:11, 93.97it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2515/3576 [00:28<00:11, 90.93it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2525/3576 [00:28<00:13, 80.59it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2540/3576 [00:28<00:10, 97.19it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2551/3576 [00:28<00:12, 82.10it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2568/3576 [00:28<00:09, 101.58it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2580/3576 [00:29<00:10, 99.25it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2591/3576 [00:29<00:10, 98.39it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2602/3576 [00:29<00:09, 99.08it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2613/3576 [00:29<00:10, 95.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2624/3576 [00:29<00:10, 94.36it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2635/3576 [00:29<00:09, 94.40it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2646/3576 [00:29<00:09, 93.54it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2657/3576 [00:29<00:09, 95.89it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2667/3576 [00:29<00:09, 92.57it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2678/3576 [00:30<00:09, 92.32it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2688/3576 [00:30<00:09, 93.24it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2699/3576 [00:30<00:09, 90.72it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2711/3576 [00:30<00:09, 94.81it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2721/3576 [00:30<00:09, 91.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2732/3576 [00:30<00:09, 93.27it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2742/3576 [00:30<00:08, 92.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2753/3576 [00:30<00:08, 95.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2763/3576 [00:30<00:08, 92.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2774/3576 [00:31<00:08, 92.25it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2784/3576 [00:31<00:08, 93.75it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2794/3576 [00:31<00:08, 93.63it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2804/3576 [00:31<00:08, 93.27it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2815/3576 [00:31<00:08, 93.39it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2825/3576 [00:31<00:08, 91.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2836/3576 [00:31<00:07, 96.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2846/3576 [00:31<00:07, 91.83it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2857/3576 [00:31<00:07, 93.30it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2868/3576 [00:32<00:07, 96.05it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2878/3576 [00:32<00:07, 95.60it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2888/3576 [00:32<00:07, 95.99it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2898/3576 [00:32<00:07, 95.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2908/3576 [00:32<00:07, 94.22it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2918/3576 [00:32<00:06, 94.26it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2929/3576 [00:32<00:06, 97.04it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2939/3576 [00:32<00:06, 93.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2949/3576 [00:32<00:06, 93.72it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2960/3576 [00:33<00:06, 94.42it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2970/3576 [00:33<00:06, 95.47it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2980/3576 [00:33<00:06, 96.10it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2990/3576 [00:33<00:06, 90.10it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3001/3576 [00:33<00:06, 93.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3011/3576 [00:33<00:05, 95.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3021/3576 [00:33<00:06, 92.42it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3031/3576 [00:33<00:05, 92.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3041/3576 [00:33<00:05, 92.94it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3052/3576 [00:34<00:05, 95.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3062/3576 [00:34<00:05, 92.88it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3072/3576 [00:34<00:05, 93.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3083/3576 [00:34<00:05, 93.89it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3093/3576 [00:34<00:05, 93.97it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3103/3576 [00:34<00:05, 93.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3114/3576 [00:34<00:04, 93.94it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3125/3576 [00:34<00:04, 96.54it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3135/3576 [00:34<00:04, 96.02it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3145/3576 [00:35<00:04, 93.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3156/3576 [00:35<00:04, 96.22it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3166/3576 [00:35<00:04, 93.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3177/3576 [00:35<00:04, 95.95it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3187/3576 [00:35<00:04, 93.78it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3197/3576 [00:35<00:04, 90.63it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3208/3576 [00:35<00:03, 95.48it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3218/3576 [00:35<00:03, 94.60it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3229/3576 [00:35<00:03, 96.23it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3239/3576 [00:36<00:03, 91.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3250/3576 [00:36<00:03, 94.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3260/3576 [00:36<00:03, 92.96it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3271/3576 [00:36<00:03, 95.90it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3282/3576 [00:36<00:03, 95.78it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3292/3576 [00:36<00:03, 94.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3302/3576 [00:36<00:02, 93.83it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3313/3576 [00:36<00:02, 98.05it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3323/3576 [00:36<00:02, 93.37it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3333/3576 [00:37<00:02, 94.58it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3344/3576 [00:37<00:02, 93.80it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3355/3576 [00:37<00:02, 96.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3365/3576 [00:37<00:02, 93.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3376/3576 [00:37<00:02, 95.69it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3386/3576 [00:37<00:01, 96.34it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3396/3576 [00:37<00:01, 92.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3407/3576 [00:37<00:01, 93.64it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3418/3576 [00:37<00:01, 91.77it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3430/3576 [00:38<00:01, 97.85it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3440/3576 [00:38<00:01, 96.28it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3450/3576 [00:38<00:01, 96.90it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3460/3576 [00:38<00:01, 97.70it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3470/3576 [00:38<00:01, 93.77it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3481/3576 [00:38<00:00, 95.40it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3492/3576 [00:38<00:00, 93.76it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3503/3576 [00:38<00:00, 98.19it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3514/3576 [00:38<00:00, 95.83it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3525/3576 [00:39<00:00, 95.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3536/3576 [00:39<00:00, 98.11it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3546/3576 [00:39<00:00, 95.77it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3556/3576 [00:39<00:00, 92.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3567/3576 [00:39<00:00, 96.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3576/3576 [00:39<00:00, 90.40it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.3987
Epoch 1 Step 51 Train Loss: 0.4104
Epoch 1 Step 101 Train Loss: 0.4528
Epoch 1 Step 151 Train Loss: 0.4061
Epoch 1 Step 201 Train Loss: 0.4430
Epoch 1 Step 251 Train Loss: 0.3971
Epoch 1 Step 301 Train Loss: 0.3965
Epoch 1 Step 351 Train Loss: 0.4599
Epoch 1 Step 401 Train Loss: 0.4315
Epoch 1 Step 451 Train Loss: 0.3841
Epoch 1 Step 501 Train Loss: 0.4201
Epoch 1 Step 551 Train Loss: 0.4331
Epoch 1 Step 601 Train Loss: 0.3660
Epoch 1 Step 651 Train Loss: 0.3951
Epoch 1: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0018. 
Epoch 2 Step 1 Train Loss: 0.3962
Epoch 2 Step 51 Train Loss: 0.4286
Epoch 2 Step 101 Train Loss: 0.4543
Epoch 2 Step 151 Train Loss: 0.3311
Epoch 2 Step 201 Train Loss: 0.4785
Epoch 2 Step 251 Train Loss: 0.3977
Epoch 2 Step 301 Train Loss: 0.4324
Epoch 2 Step 351 Train Loss: 0.4063
Epoch 2 Step 401 Train Loss: 0.3894
Epoch 2 Step 451 Train Loss: 0.3995
Epoch 2 Step 501 Train Loss: 0.3781
Epoch 2 Step 551 Train Loss: 0.4355
Epoch 2 Step 601 Train Loss: 0.4744
Epoch 2 Step 651 Train Loss: 0.3647
Epoch 2: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.0029. 
Epoch 3 Step 1 Train Loss: 0.4980
Epoch 3 Step 51 Train Loss: 0.4409
Epoch 3 Step 101 Train Loss: 0.4077
Epoch 3 Step 151 Train Loss: 0.3770
Epoch 3 Step 201 Train Loss: 0.4242
Epoch 3 Step 251 Train Loss: 0.4320
Epoch 3 Step 301 Train Loss: 0.3905
Epoch 3 Step 351 Train Loss: 0.4248
Epoch 3 Step 401 Train Loss: 0.3624
Epoch 3 Step 451 Train Loss: 0.4295
Epoch 3 Step 501 Train Loss: 0.3931
Epoch 3 Step 551 Train Loss: 0.4654
Epoch 3 Step 601 Train Loss: 0.4279
Epoch 3 Step 651 Train Loss: 0.4314
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0018. 
Epoch 4 Step 1 Train Loss: 0.3501
Epoch 4 Step 51 Train Loss: 0.3464
Epoch 4 Step 101 Train Loss: 0.3881
Epoch 4 Step 151 Train Loss: 0.4997
Epoch 4 Step 201 Train Loss: 0.3750
Epoch 4 Step 251 Train Loss: 0.4517
Epoch 4 Step 301 Train Loss: 0.4703
Epoch 4 Step 351 Train Loss: 0.4080
Epoch 4 Step 401 Train Loss: 0.3335
Epoch 4 Step 451 Train Loss: 0.3376
Epoch 4 Step 501 Train Loss: 0.4243
Epoch 4 Step 551 Train Loss: 0.3440
Epoch 4 Step 601 Train Loss: 0.4034
Epoch 4 Step 651 Train Loss: 0.4441
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0022. 
Epoch 5 Step 1 Train Loss: 0.3393
Epoch 5 Step 51 Train Loss: 0.3955
Epoch 5 Step 101 Train Loss: 0.3515
Epoch 5 Step 151 Train Loss: 0.3484
Epoch 5 Step 201 Train Loss: 0.3388
Epoch 5 Step 251 Train Loss: 0.3228
Epoch 5 Step 301 Train Loss: 0.4624
Epoch 5 Step 351 Train Loss: 0.4078
Epoch 5 Step 401 Train Loss: 0.3711
Epoch 5 Step 451 Train Loss: 0.3693
Epoch 5 Step 501 Train Loss: 0.3612
Epoch 5 Step 551 Train Loss: 0.4359
Epoch 5 Step 601 Train Loss: 0.3277
Epoch 5 Step 651 Train Loss: 0.3968
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0039. 
Epoch 6 Step 1 Train Loss: 0.4093
Epoch 6 Step 51 Train Loss: 0.3621
Epoch 6 Step 101 Train Loss: 0.3441
Epoch 6 Step 151 Train Loss: 0.3481
Epoch 6 Step 201 Train Loss: 0.4670
Epoch 6 Step 251 Train Loss: 0.5298
Epoch 6 Step 301 Train Loss: 0.3089
Epoch 6 Step 351 Train Loss: 0.4001
Epoch 6 Step 401 Train Loss: 0.4874
Epoch 6 Step 451 Train Loss: 0.3329
Epoch 6 Step 501 Train Loss: 0.3464
Epoch 6 Step 551 Train Loss: 0.4274
Epoch 6 Step 601 Train Loss: 0.4634
Epoch 6 Step 651 Train Loss: 0.3903
Epoch 6: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0038. 
Epoch 7 Step 1 Train Loss: 0.3380
Epoch 7 Step 51 Train Loss: 0.3355
Epoch 7 Step 101 Train Loss: 0.4070
Epoch 7 Step 151 Train Loss: 0.4924
Epoch 7 Step 201 Train Loss: 0.4100
Epoch 7 Step 251 Train Loss: 0.4282
Epoch 7 Step 301 Train Loss: 0.4722
Epoch 7 Step 351 Train Loss: 0.3531
Epoch 7 Step 401 Train Loss: 0.3890
Epoch 7 Step 451 Train Loss: 0.4526
Epoch 7 Step 501 Train Loss: 0.3699
Epoch 7 Step 551 Train Loss: 0.3872
Epoch 7 Step 601 Train Loss: 0.3472
Epoch 7 Step 651 Train Loss: 0.4387
Epoch 7: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0035. 
Epoch 8 Step 1 Train Loss: 0.4101
Epoch 8 Step 51 Train Loss: 0.2857
Epoch 8 Step 101 Train Loss: 0.4074
Epoch 8 Step 151 Train Loss: 0.3326
Epoch 8 Step 201 Train Loss: 0.3795
Epoch 8 Step 251 Train Loss: 0.3793
Epoch 8 Step 301 Train Loss: 0.4877
Epoch 8 Step 351 Train Loss: 0.4548
Epoch 8 Step 401 Train Loss: 0.3570
Epoch 8 Step 451 Train Loss: 0.5622
Epoch 8 Step 501 Train Loss: 0.3448
Epoch 8 Step 551 Train Loss: 0.3338
Epoch 8 Step 601 Train Loss: 0.3767
Epoch 8 Step 651 Train Loss: 0.3601
Epoch 8: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0041. 
Epoch 9 Step 1 Train Loss: 0.4861
Epoch 9 Step 51 Train Loss: 0.4316
Epoch 9 Step 101 Train Loss: 0.3486
Epoch 9 Step 151 Train Loss: 0.5455
Epoch 9 Step 201 Train Loss: 0.3408
Epoch 9 Step 251 Train Loss: 0.4163
Epoch 9 Step 301 Train Loss: 0.3546
Epoch 9 Step 351 Train Loss: 0.4321
Epoch 9 Step 401 Train Loss: 0.3599
Epoch 9 Step 451 Train Loss: 0.3835
Epoch 9 Step 501 Train Loss: 0.3633
Epoch 9 Step 551 Train Loss: 0.4024
Epoch 9 Step 601 Train Loss: 0.3645
Epoch 9 Step 651 Train Loss: 0.4849
Epoch 9: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0039. 
Epoch 10 Step 1 Train Loss: 0.3813
Epoch 10 Step 51 Train Loss: 0.4098
Epoch 10 Step 101 Train Loss: 0.3745
Epoch 10 Step 151 Train Loss: 0.4287
Epoch 10 Step 201 Train Loss: 0.3928
Epoch 10 Step 251 Train Loss: 0.3485
Epoch 10 Step 301 Train Loss: 0.3359
Epoch 10 Step 351 Train Loss: 0.4540
Epoch 10 Step 401 Train Loss: 0.3293
Epoch 10 Step 451 Train Loss: 0.3825
Epoch 10 Step 501 Train Loss: 0.4297
Epoch 10 Step 551 Train Loss: 0.4184
Epoch 10 Step 601 Train Loss: 0.4023
Epoch 10 Step 651 Train Loss: 0.3577
Epoch 10: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0041. 
Epoch 11 Step 1 Train Loss: 0.3986
Epoch 11 Step 51 Train Loss: 0.3736
Epoch 11 Step 101 Train Loss: 0.4310
Epoch 11 Step 151 Train Loss: 0.3712
Epoch 11 Step 201 Train Loss: 0.3564
Epoch 11 Step 251 Train Loss: 0.4412
Epoch 11 Step 301 Train Loss: 0.4314
Epoch 11 Step 351 Train Loss: 0.3749
Epoch 11 Step 401 Train Loss: 0.4007
Epoch 11 Step 451 Train Loss: 0.3578
Epoch 11 Step 501 Train Loss: 0.5087
Epoch 11 Step 551 Train Loss: 0.4182
Epoch 11 Step 601 Train Loss: 0.4231
Epoch 11 Step 651 Train Loss: 0.3984
Epoch 11: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0040. 
Epoch 12 Step 1 Train Loss: 0.4041
Epoch 12 Step 51 Train Loss: 0.4171
Epoch 12 Step 101 Train Loss: 0.3063
Epoch 12 Step 151 Train Loss: 0.4132
Epoch 12 Step 201 Train Loss: 0.4719
Epoch 12 Step 251 Train Loss: 0.4074
Epoch 12 Step 301 Train Loss: 0.3720
Epoch 12 Step 351 Train Loss: 0.4194
Epoch 12 Step 401 Train Loss: 0.4058
Epoch 12 Step 451 Train Loss: 0.3623
Epoch 12 Step 501 Train Loss: 0.3589
Epoch 12 Step 551 Train Loss: 0.3787
Epoch 12 Step 601 Train Loss: 0.3886
Epoch 12 Step 651 Train Loss: 0.5719
Epoch 12: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0042. 
Epoch 13 Step 1 Train Loss: 0.3749
Epoch 13 Step 51 Train Loss: 0.4219
Epoch 13 Step 101 Train Loss: 0.4125
Epoch 13 Step 151 Train Loss: 0.3657
Epoch 13 Step 201 Train Loss: 0.3473
Epoch 13 Step 251 Train Loss: 0.4139
Epoch 13 Step 301 Train Loss: 0.3905
Epoch 13 Step 351 Train Loss: 0.3616
Epoch 13 Step 401 Train Loss: 0.4356
Epoch 13 Step 451 Train Loss: 0.3269
Epoch 13 Step 501 Train Loss: 0.4466
Epoch 13 Step 551 Train Loss: 0.4648
Epoch 13 Step 601 Train Loss: 0.3177
Epoch 13 Step 651 Train Loss: 0.3451
Epoch 13: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0039. 
Epoch 14 Step 1 Train Loss: 0.3802
Epoch 14 Step 51 Train Loss: 0.3585
Epoch 14 Step 101 Train Loss: 0.4205
Epoch 14 Step 151 Train Loss: 0.4393
Epoch 14 Step 201 Train Loss: 0.3396
Epoch 14 Step 251 Train Loss: 0.3937
Epoch 14 Step 301 Train Loss: 0.3888
Epoch 14 Step 351 Train Loss: 0.4095
Epoch 14 Step 401 Train Loss: 0.4107
Epoch 14 Step 451 Train Loss: 0.3201
Epoch 14 Step 501 Train Loss: 0.3665
Epoch 14 Step 551 Train Loss: 0.3712
Epoch 14 Step 601 Train Loss: 0.3117
Epoch 14 Step 651 Train Loss: 0.5630
Epoch 14: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0041. 
Epoch 15 Step 1 Train Loss: 0.4115
Epoch 15 Step 51 Train Loss: 0.4032
Epoch 15 Step 101 Train Loss: 0.3963
Epoch 15 Step 151 Train Loss: 0.3567
Epoch 15 Step 201 Train Loss: 0.3419
Epoch 15 Step 251 Train Loss: 0.4240
Epoch 15 Step 301 Train Loss: 0.4102
Epoch 15 Step 351 Train Loss: 0.4343
Epoch 15 Step 401 Train Loss: 0.4742
Epoch 15 Step 451 Train Loss: 0.3938
Epoch 15 Step 501 Train Loss: 0.3610
Epoch 15 Step 551 Train Loss: 0.4548
Epoch 15 Step 601 Train Loss: 0.3521
Epoch 15 Step 651 Train Loss: 0.3998
Epoch 15: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0042. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0028
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00073324936
test_unseen_single_pearson: 0.9986138037005041
test_unseen_single_mse_de: 0.0028301247
test_unseen_single_pearson_de: 0.9996194315124893
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.2384866113975179
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.09999999999999999
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9833333333333334
test_unseen_single_mse_top20_de_non_dropout: 0.00296621734537859
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.026 MB uploadedwandb: - 0.020 MB of 0.026 MB uploadedwandb: \ 0.020 MB of 0.026 MB uploadedwandb: | 0.026 MB of 0.026 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÅ‚ñà
wandb:                                                   val_de_mse ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:                                               val_de_pearson ‚ñá‚ñÜ‚ñà‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00283
wandb:                                              test_de_pearson 0.99962
wandb:               test_frac_opposite_direction_top20_non_dropout 0.1
wandb:                          test_frac_sigma_below_1_non_dropout 0.98333
wandb:                                                     test_mse 0.00073
wandb:                                test_mse_top20_de_non_dropout 0.00297
wandb:                                                 test_pearson 0.99861
wandb:                                           test_pearson_delta 0.23849
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.1
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.98333
wandb:                                       test_unseen_single_mse 0.00073
wandb:                                    test_unseen_single_mse_de 0.00283
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00297
wandb:                                   test_unseen_single_pearson 0.99861
wandb:                                test_unseen_single_pearson_de 0.99962
wandb:                             test_unseen_single_pearson_delta 0.23849
wandb:                                                 train_de_mse 0.00319
wandb:                                             train_de_pearson 0.99951
wandb:                                                    train_mse 0.00013
wandb:                                                train_pearson 0.99976
wandb:                                                training_loss 0.43601
wandb:                                                   val_de_mse 0.00419
wandb:                                               val_de_pearson 0.9995
wandb:                                                      val_mse 0.00036
wandb:                                                  val_pearson 0.9993
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396858_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/hf56wmvw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_010457-hf56wmvw/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_011759-c77tbrjx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396858_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/c77tbrjx
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4323
Epoch 1 Step 51 Train Loss: 0.4602
Epoch 1 Step 101 Train Loss: 0.4252
Epoch 1 Step 151 Train Loss: 0.3732
Epoch 1 Step 201 Train Loss: 0.3889
Epoch 1 Step 251 Train Loss: 0.3648
Epoch 1 Step 301 Train Loss: 0.4645
Epoch 1 Step 351 Train Loss: 0.4029
Epoch 1 Step 401 Train Loss: 0.3379
Epoch 1 Step 451 Train Loss: 0.3782
Epoch 1 Step 501 Train Loss: 0.3909
Epoch 1 Step 551 Train Loss: 0.4853
Epoch 1 Step 601 Train Loss: 0.4425
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0023. 
Epoch 2 Step 1 Train Loss: 0.3743
Epoch 2 Step 51 Train Loss: 0.4064
Epoch 2 Step 101 Train Loss: 0.3698
Epoch 2 Step 151 Train Loss: 0.4162
Epoch 2 Step 201 Train Loss: 0.3696
Epoch 2 Step 251 Train Loss: 0.3692
Epoch 2 Step 301 Train Loss: 0.4353
Epoch 2 Step 351 Train Loss: 0.4874
Epoch 2 Step 401 Train Loss: 0.5268
Epoch 2 Step 451 Train Loss: 0.4009
Epoch 2 Step 501 Train Loss: 0.3367
Epoch 2 Step 551 Train Loss: 0.3987
Epoch 2 Step 601 Train Loss: 0.3755
Epoch 2: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0027. 
Epoch 3 Step 1 Train Loss: 0.4112
Epoch 3 Step 51 Train Loss: 0.4130
Epoch 3 Step 101 Train Loss: 0.4649
Epoch 3 Step 151 Train Loss: 0.4179
Epoch 3 Step 201 Train Loss: 0.4180
Epoch 3 Step 251 Train Loss: 0.3919
Epoch 3 Step 301 Train Loss: 0.3811
Epoch 3 Step 351 Train Loss: 0.3646
Epoch 3 Step 401 Train Loss: 0.4216
Epoch 3 Step 451 Train Loss: 0.4198
Epoch 3 Step 501 Train Loss: 0.3741
Epoch 3 Step 551 Train Loss: 0.4903
Epoch 3 Step 601 Train Loss: 0.4513
Epoch 3: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0033. 
Epoch 4 Step 1 Train Loss: 0.4009
Epoch 4 Step 51 Train Loss: 0.3597
Epoch 4 Step 101 Train Loss: 0.3843
Epoch 4 Step 151 Train Loss: 0.3736
Epoch 4 Step 201 Train Loss: 0.5011
Epoch 4 Step 251 Train Loss: 0.3663
Epoch 4 Step 301 Train Loss: 0.4279
Epoch 4 Step 351 Train Loss: 0.3374
Epoch 4 Step 401 Train Loss: 0.4716
Epoch 4 Step 451 Train Loss: 0.4016
Epoch 4 Step 501 Train Loss: 0.4500
Epoch 4 Step 551 Train Loss: 0.6262
Epoch 4 Step 601 Train Loss: 0.4120
Epoch 4: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0031. 
Epoch 5 Step 1 Train Loss: 0.3267
Epoch 5 Step 51 Train Loss: 0.3500
Epoch 5 Step 101 Train Loss: 0.3858
Epoch 5 Step 151 Train Loss: 0.4226
Epoch 5 Step 201 Train Loss: 0.3187
Epoch 5 Step 251 Train Loss: 0.4398
Epoch 5 Step 301 Train Loss: 0.3894
Epoch 5 Step 351 Train Loss: 0.2956
Epoch 5 Step 401 Train Loss: 0.3875
Epoch 5 Step 451 Train Loss: 0.3049
Epoch 5 Step 501 Train Loss: 0.3829
Epoch 5 Step 551 Train Loss: 0.3877
Epoch 5 Step 601 Train Loss: 0.3445
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0030. 
Epoch 6 Step 1 Train Loss: 0.4979
Epoch 6 Step 51 Train Loss: 0.5002
Epoch 6 Step 101 Train Loss: 0.3477
Epoch 6 Step 151 Train Loss: 0.3640
Epoch 6 Step 201 Train Loss: 0.4162
Epoch 6 Step 251 Train Loss: 0.3291
Epoch 6 Step 301 Train Loss: 0.4519
Epoch 6 Step 351 Train Loss: 0.3352
Epoch 6 Step 401 Train Loss: 0.4059
Epoch 6 Step 451 Train Loss: 0.3663
Epoch 6 Step 501 Train Loss: 0.4326
Epoch 6 Step 551 Train Loss: 0.3336
Epoch 6 Step 601 Train Loss: 0.3324
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 7 Step 1 Train Loss: 0.4153
Epoch 7 Step 51 Train Loss: 0.3088
Epoch 7 Step 101 Train Loss: 0.4420
Epoch 7 Step 151 Train Loss: 0.4371
Epoch 7 Step 201 Train Loss: 0.3149
Epoch 7 Step 251 Train Loss: 0.3638
Epoch 7 Step 301 Train Loss: 0.4780
Epoch 7 Step 351 Train Loss: 0.3900
Epoch 7 Step 401 Train Loss: 0.4181
Epoch 7 Step 451 Train Loss: 0.3835
Epoch 7 Step 501 Train Loss: 0.4175
Epoch 7 Step 551 Train Loss: 0.4111
Epoch 7 Step 601 Train Loss: 0.3874
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 8 Step 1 Train Loss: 0.5254
Epoch 8 Step 51 Train Loss: 0.3043
Epoch 8 Step 101 Train Loss: 0.4008
Epoch 8 Step 151 Train Loss: 0.4128
Epoch 8 Step 201 Train Loss: 0.3553
Epoch 8 Step 251 Train Loss: 0.4770
Epoch 8 Step 301 Train Loss: 0.5797
Epoch 8 Step 351 Train Loss: 0.3220
Epoch 8 Step 401 Train Loss: 0.4741
Epoch 8 Step 451 Train Loss: 0.4174
Epoch 8 Step 501 Train Loss: 0.3235
Epoch 8 Step 551 Train Loss: 0.4766
Epoch 8 Step 601 Train Loss: 0.3090
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 9 Step 1 Train Loss: 0.3346
Epoch 9 Step 51 Train Loss: 0.4219
Epoch 9 Step 101 Train Loss: 0.3202
Epoch 9 Step 151 Train Loss: 0.4461
Epoch 9 Step 201 Train Loss: 0.4597
Epoch 9 Step 251 Train Loss: 0.3732
Epoch 9 Step 301 Train Loss: 0.3181
Epoch 9 Step 351 Train Loss: 0.4281
Epoch 9 Step 401 Train Loss: 0.3660
Epoch 9 Step 451 Train Loss: 0.3109
Epoch 9 Step 501 Train Loss: 0.3587
Epoch 9 Step 551 Train Loss: 0.3138
Epoch 9 Step 601 Train Loss: 0.3817
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 10 Step 1 Train Loss: 0.5061
Epoch 10 Step 51 Train Loss: 0.3218
Epoch 10 Step 101 Train Loss: 0.3884
Epoch 10 Step 151 Train Loss: 0.3504
Epoch 10 Step 201 Train Loss: 0.5749
Epoch 10 Step 251 Train Loss: 0.5038
Epoch 10 Step 301 Train Loss: 0.4615
Epoch 10 Step 351 Train Loss: 0.3918
Epoch 10 Step 401 Train Loss: 0.3748
Epoch 10 Step 451 Train Loss: 0.5280
Epoch 10 Step 501 Train Loss: 0.3648
Epoch 10 Step 551 Train Loss: 0.3110
Epoch 10 Step 601 Train Loss: 0.4091
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 11 Step 1 Train Loss: 0.3786
Epoch 11 Step 51 Train Loss: 0.2926
Epoch 11 Step 101 Train Loss: 0.4580
Epoch 11 Step 151 Train Loss: 0.3957
Epoch 11 Step 201 Train Loss: 0.4281
Epoch 11 Step 251 Train Loss: 0.4977
Epoch 11 Step 301 Train Loss: 0.3341
Epoch 11 Step 351 Train Loss: 0.3595
Epoch 11 Step 401 Train Loss: 0.3494
Epoch 11 Step 451 Train Loss: 0.2907
Epoch 11 Step 501 Train Loss: 0.3638
Epoch 11 Step 551 Train Loss: 0.3555
Epoch 11 Step 601 Train Loss: 0.3921
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 12 Step 1 Train Loss: 0.4092
Epoch 12 Step 51 Train Loss: 0.3757
Epoch 12 Step 101 Train Loss: 0.5426
Epoch 12 Step 151 Train Loss: 0.4958
Epoch 12 Step 201 Train Loss: 0.3733
Epoch 12 Step 251 Train Loss: 0.3768
Epoch 12 Step 301 Train Loss: 0.4130
Epoch 12 Step 351 Train Loss: 0.3928
Epoch 12 Step 401 Train Loss: 0.4174
Epoch 12 Step 451 Train Loss: 0.4053
Epoch 12 Step 501 Train Loss: 0.3636
Epoch 12 Step 551 Train Loss: 0.4081
Epoch 12 Step 601 Train Loss: 0.3911
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 13 Step 1 Train Loss: 0.4023
Epoch 13 Step 51 Train Loss: 0.4160
Epoch 13 Step 101 Train Loss: 0.4009
Epoch 13 Step 151 Train Loss: 0.4360
Epoch 13 Step 201 Train Loss: 0.3266
Epoch 13 Step 251 Train Loss: 0.5105
Epoch 13 Step 301 Train Loss: 0.3227
Epoch 13 Step 351 Train Loss: 0.3252
Epoch 13 Step 401 Train Loss: 0.3576
Epoch 13 Step 451 Train Loss: 0.3471
Epoch 13 Step 501 Train Loss: 0.3295
Epoch 13 Step 551 Train Loss: 0.3549
Epoch 13 Step 601 Train Loss: 0.4263
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 14 Step 1 Train Loss: 0.4901
Epoch 14 Step 51 Train Loss: 0.4669
Epoch 14 Step 101 Train Loss: 0.3693
Epoch 14 Step 151 Train Loss: 0.3361
Epoch 14 Step 201 Train Loss: 0.3314
Epoch 14 Step 251 Train Loss: 0.3148
Epoch 14 Step 301 Train Loss: 0.3527
Epoch 14 Step 351 Train Loss: 0.4062
Epoch 14 Step 401 Train Loss: 0.4732
Epoch 14 Step 451 Train Loss: 0.3913
Epoch 14 Step 501 Train Loss: 0.3722
Epoch 14 Step 551 Train Loss: 0.5196
Epoch 14 Step 601 Train Loss: 0.4679
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0015 Validation Top 20 DE MSE: 0.0031. 
Epoch 15 Step 1 Train Loss: 0.3681
Epoch 15 Step 51 Train Loss: 0.4050
Epoch 15 Step 101 Train Loss: 0.3205
Epoch 15 Step 151 Train Loss: 0.3563
Epoch 15 Step 201 Train Loss: 0.4186
Epoch 15 Step 251 Train Loss: 0.4098
Epoch 15 Step 301 Train Loss: 0.2991
Epoch 15 Step 351 Train Loss: 0.3674
Epoch 15 Step 401 Train Loss: 0.3576
Epoch 15 Step 451 Train Loss: 0.3759
Epoch 15 Step 501 Train Loss: 0.3569
Epoch 15 Step 551 Train Loss: 0.4720
Epoch 15 Step 601 Train Loss: 0.3034
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0014 Validation Top 20 DE MSE: 0.0031. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0089
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0004929285
test_unseen_single_pearson: 0.9990466359006067
test_unseen_single_mse_de: 0.0088661285
test_unseen_single_pearson_de: 0.998649142360041
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.262211741560401
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.19999999999999998
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.008903859879943665
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.023 MB of 0.025 MB uploadedwandb: / 0.023 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÇ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÜ‚ñÅ‚ñÉ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ
wandb:                                                   val_de_mse ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                               val_de_pearson ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                                      val_mse ‚ñÖ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÑ‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00887
wandb:                                              test_de_pearson 0.99865
wandb:               test_frac_opposite_direction_top20_non_dropout 0.2
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00049
wandb:                                test_mse_top20_de_non_dropout 0.0089
wandb:                                                 test_pearson 0.99905
wandb:                                           test_pearson_delta 0.26221
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.2
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00049
wandb:                                    test_unseen_single_mse_de 0.00887
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0089
wandb:                                   test_unseen_single_pearson 0.99905
wandb:                                test_unseen_single_pearson_de 0.99865
wandb:                             test_unseen_single_pearson_delta 0.26221
wandb:                                                 train_de_mse 0.00144
wandb:                                             train_de_pearson 0.9998
wandb:                                                    train_mse 0.00016
wandb:                                                train_pearson 0.99973
wandb:                                                training_loss 0.37258
wandb:                                                   val_de_mse 0.00308
wandb:                                               val_de_pearson 0.99951
wandb:                                                      val_mse 0.00021
wandb:                                                  val_pearson 0.99963
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396858_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/c77tbrjx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_011759-c77tbrjx/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_012914-9ua71u2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396858_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/9ua71u2c
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4976
Epoch 1 Step 51 Train Loss: 0.3839
Epoch 1 Step 101 Train Loss: 0.4040
Epoch 1 Step 151 Train Loss: 0.4068
Epoch 1 Step 201 Train Loss: 0.4415
Epoch 1 Step 251 Train Loss: 0.4700
Epoch 1 Step 301 Train Loss: 0.4205
Epoch 1 Step 351 Train Loss: 0.4294
Epoch 1 Step 401 Train Loss: 0.3749
Epoch 1 Step 451 Train Loss: 0.5066
Epoch 1 Step 501 Train Loss: 0.5042
Epoch 1: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0065 Validation Top 20 DE MSE: 0.0073. 
Epoch 2 Step 1 Train Loss: 0.4486
Epoch 2 Step 51 Train Loss: 0.4166
Epoch 2 Step 101 Train Loss: 0.3485
Epoch 2 Step 151 Train Loss: 0.4291
Epoch 2 Step 201 Train Loss: 0.4419
Epoch 2 Step 251 Train Loss: 0.4763
Epoch 2 Step 301 Train Loss: 0.5268
Epoch 2 Step 351 Train Loss: 0.4345
Epoch 2 Step 401 Train Loss: 0.4094
Epoch 2 Step 451 Train Loss: 0.4551
Epoch 2 Step 501 Train Loss: 0.4585
Epoch 2: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0071 Validation Top 20 DE MSE: 0.0076. 
Epoch 3 Step 1 Train Loss: 0.4753
Epoch 3 Step 51 Train Loss: 0.4261
Epoch 3 Step 101 Train Loss: 0.3998
Epoch 3 Step 151 Train Loss: 0.4808
Epoch 3 Step 201 Train Loss: 0.3803
Epoch 3 Step 251 Train Loss: 0.4225
Epoch 3 Step 301 Train Loss: 0.4446
Epoch 3 Step 351 Train Loss: 0.4875
Epoch 3 Step 401 Train Loss: 0.3878
Epoch 3 Step 451 Train Loss: 0.4096
Epoch 3 Step 501 Train Loss: 0.3560
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0052. 
Epoch 4 Step 1 Train Loss: 0.4593
Epoch 4 Step 51 Train Loss: 0.4486
Epoch 4 Step 101 Train Loss: 0.3750
Epoch 4 Step 151 Train Loss: 0.4759
Epoch 4 Step 201 Train Loss: 0.5305
Epoch 4 Step 251 Train Loss: 0.4393
Epoch 4 Step 301 Train Loss: 0.5103
Epoch 4 Step 351 Train Loss: 0.4087
Epoch 4 Step 401 Train Loss: 0.4223
Epoch 4 Step 451 Train Loss: 0.5675
Epoch 4 Step 501 Train Loss: 0.3370
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0032. 
Epoch 5 Step 1 Train Loss: 0.4637
Epoch 5 Step 51 Train Loss: 0.5387
Epoch 5 Step 101 Train Loss: 0.4791
Epoch 5 Step 151 Train Loss: 0.4809
Epoch 5 Step 201 Train Loss: 0.4330
Epoch 5 Step 251 Train Loss: 0.3792
Epoch 5 Step 301 Train Loss: 0.4257
Epoch 5 Step 351 Train Loss: 0.3584
Epoch 5 Step 401 Train Loss: 0.3955
Epoch 5 Step 451 Train Loss: 0.5555
Epoch 5 Step 501 Train Loss: 0.4647
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0022. 
Epoch 6 Step 1 Train Loss: 0.3554
Epoch 6 Step 51 Train Loss: 0.4824
Epoch 6 Step 101 Train Loss: 0.6361
Epoch 6 Step 151 Train Loss: 0.4300
Epoch 6 Step 201 Train Loss: 0.5094
Epoch 6 Step 251 Train Loss: 0.3540
Epoch 6 Step 301 Train Loss: 0.4163
Epoch 6 Step 351 Train Loss: 0.5100
Epoch 6 Step 401 Train Loss: 0.4594
Epoch 6 Step 451 Train Loss: 0.4756
Epoch 6 Step 501 Train Loss: 0.4869
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0025. 
Epoch 7 Step 1 Train Loss: 0.4004
Epoch 7 Step 51 Train Loss: 0.3962
Epoch 7 Step 101 Train Loss: 0.4978
Epoch 7 Step 151 Train Loss: 0.4290
Epoch 7 Step 201 Train Loss: 0.4880
Epoch 7 Step 251 Train Loss: 0.5267
Epoch 7 Step 301 Train Loss: 0.4643
Epoch 7 Step 351 Train Loss: 0.5034
Epoch 7 Step 401 Train Loss: 0.4125
Epoch 7 Step 451 Train Loss: 0.4052
Epoch 7 Step 501 Train Loss: 0.4739
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0023. 
Epoch 8 Step 1 Train Loss: 0.3572
Epoch 8 Step 51 Train Loss: 0.4979
Epoch 8 Step 101 Train Loss: 0.4279
Epoch 8 Step 151 Train Loss: 0.4217
Epoch 8 Step 201 Train Loss: 0.4950
Epoch 8 Step 251 Train Loss: 0.5022
Epoch 8 Step 301 Train Loss: 0.4056
Epoch 8 Step 351 Train Loss: 0.4056
Epoch 8 Step 401 Train Loss: 0.4265
Epoch 8 Step 451 Train Loss: 0.4138
Epoch 8 Step 501 Train Loss: 0.4682
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0023. 
Epoch 9 Step 1 Train Loss: 0.3846
Epoch 9 Step 51 Train Loss: 0.4607
Epoch 9 Step 101 Train Loss: 0.5389
Epoch 9 Step 151 Train Loss: 0.4916
Epoch 9 Step 201 Train Loss: 0.4029
Epoch 9 Step 251 Train Loss: 0.3892
Epoch 9 Step 301 Train Loss: 0.3957
Epoch 9 Step 351 Train Loss: 0.4907
Epoch 9 Step 401 Train Loss: 0.5430
Epoch 9 Step 451 Train Loss: 0.3996
Epoch 9 Step 501 Train Loss: 0.4858
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0026. 
Epoch 10 Step 1 Train Loss: 0.5245
Epoch 10 Step 51 Train Loss: 0.4816
Epoch 10 Step 101 Train Loss: 0.3537
Epoch 10 Step 151 Train Loss: 0.4167
Epoch 10 Step 201 Train Loss: 0.4797
Epoch 10 Step 251 Train Loss: 0.3576
Epoch 10 Step 301 Train Loss: 0.3571
Epoch 10 Step 351 Train Loss: 0.4050
Epoch 10 Step 401 Train Loss: 0.4473
Epoch 10 Step 451 Train Loss: 0.3558
Epoch 10 Step 501 Train Loss: 0.4362
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0025. 
Epoch 11 Step 1 Train Loss: 0.5021
Epoch 11 Step 51 Train Loss: 0.4517
Epoch 11 Step 101 Train Loss: 0.3992
Epoch 11 Step 151 Train Loss: 0.3546
Epoch 11 Step 201 Train Loss: 0.3599
Epoch 11 Step 251 Train Loss: 0.5362
Epoch 11 Step 301 Train Loss: 0.3823
Epoch 11 Step 351 Train Loss: 0.4223
Epoch 11 Step 401 Train Loss: 0.5358
Epoch 11 Step 451 Train Loss: 0.4457
Epoch 11 Step 501 Train Loss: 0.4763
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0026. 
Epoch 12 Step 1 Train Loss: 0.3968
Epoch 12 Step 51 Train Loss: 0.4611
Epoch 12 Step 101 Train Loss: 0.3917
Epoch 12 Step 151 Train Loss: 0.4029
Epoch 12 Step 201 Train Loss: 0.4355
Epoch 12 Step 251 Train Loss: 0.4812
Epoch 12 Step 301 Train Loss: 0.3234
Epoch 12 Step 351 Train Loss: 0.4008
Epoch 12 Step 401 Train Loss: 0.4145
Epoch 12 Step 451 Train Loss: 0.7056
Epoch 12 Step 501 Train Loss: 0.4942
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0024. 
Epoch 13 Step 1 Train Loss: 0.4165
Epoch 13 Step 51 Train Loss: 0.3452
Epoch 13 Step 101 Train Loss: 0.4185
Epoch 13 Step 151 Train Loss: 0.4163
Epoch 13 Step 201 Train Loss: 0.5375
Epoch 13 Step 251 Train Loss: 0.5283
Epoch 13 Step 301 Train Loss: 0.4399
Epoch 13 Step 351 Train Loss: 0.4431
Epoch 13 Step 401 Train Loss: 0.4615
Epoch 13 Step 451 Train Loss: 0.4022
Epoch 13 Step 501 Train Loss: 0.4368
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0025. 
Epoch 14 Step 1 Train Loss: 0.4962
Epoch 14 Step 51 Train Loss: 0.5102
Epoch 14 Step 101 Train Loss: 0.4964
Epoch 14 Step 151 Train Loss: 0.4110
Epoch 14 Step 201 Train Loss: 0.3977
Epoch 14 Step 251 Train Loss: 0.4013
Epoch 14 Step 301 Train Loss: 0.4159
Epoch 14 Step 351 Train Loss: 0.3503
Epoch 14 Step 401 Train Loss: 0.4093
Epoch 14 Step 451 Train Loss: 0.3439
Epoch 14 Step 501 Train Loss: 0.5582
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0024. 
Epoch 15 Step 1 Train Loss: 0.4138
Epoch 15 Step 51 Train Loss: 0.4437
Epoch 15 Step 101 Train Loss: 0.4884
Epoch 15 Step 151 Train Loss: 0.4195
Epoch 15 Step 201 Train Loss: 0.4139
Epoch 15 Step 251 Train Loss: 0.3749
Epoch 15 Step 301 Train Loss: 0.5342
Epoch 15 Step 351 Train Loss: 0.3134
Epoch 15 Step 401 Train Loss: 0.4249
Epoch 15 Step 451 Train Loss: 0.4344
Epoch 15 Step 501 Train Loss: 0.3484
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0023. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0026
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00025863797
test_unseen_single_pearson: 0.9995375867144917
test_unseen_single_mse_de: 0.0025550833
test_unseen_single_pearson_de: 0.9995001859080085
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5264966068388536
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.03333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002555083172208835
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                  val_pearson ‚ñÑ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00256
wandb:                                              test_de_pearson 0.9995
wandb:               test_frac_opposite_direction_top20_non_dropout 0.03333
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00026
wandb:                                test_mse_top20_de_non_dropout 0.00256
wandb:                                                 test_pearson 0.99954
wandb:                                           test_pearson_delta 0.5265
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.03333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00026
wandb:                                    test_unseen_single_mse_de 0.00256
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00256
wandb:                                   test_unseen_single_pearson 0.99954
wandb:                                test_unseen_single_pearson_de 0.9995
wandb:                             test_unseen_single_pearson_delta 0.5265
wandb:                                                 train_de_mse 0.00239
wandb:                                             train_de_pearson 0.99971
wandb:                                                    train_mse 0.00022
wandb:                                                train_pearson 0.99961
wandb:                                                training_loss 0.36169
wandb:                                                   val_de_mse 0.00231
wandb:                                               val_de_pearson 0.99958
wandb:                                                      val_mse 0.00026
wandb:                                                  val_pearson 0.99954
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396858_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/9ua71u2c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_012914-9ua71u2c/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_013851-743tmwwb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396858_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/743tmwwb
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4126
Epoch 1 Step 51 Train Loss: 0.4978
Epoch 1 Step 101 Train Loss: 0.4269
Epoch 1 Step 151 Train Loss: 0.3475
Epoch 1 Step 201 Train Loss: 0.3818
Epoch 1 Step 251 Train Loss: 0.4058
Epoch 1 Step 301 Train Loss: 0.4977
Epoch 1 Step 351 Train Loss: 0.4876
Epoch 1 Step 401 Train Loss: 0.3624
Epoch 1 Step 451 Train Loss: 0.4215
Epoch 1 Step 501 Train Loss: 0.4209
Epoch 1 Step 551 Train Loss: 0.3791
Epoch 1 Step 601 Train Loss: 0.4640
Epoch 1: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0065 Validation Top 20 DE MSE: 0.0032. 
Epoch 2 Step 1 Train Loss: 0.4044
Epoch 2 Step 51 Train Loss: 0.3965
Epoch 2 Step 101 Train Loss: 0.3396
Epoch 2 Step 151 Train Loss: 0.4052
Epoch 2 Step 201 Train Loss: 0.3621
Epoch 2 Step 251 Train Loss: 0.4928
Epoch 2 Step 301 Train Loss: 0.4238
Epoch 2 Step 351 Train Loss: 0.3837
Epoch 2 Step 401 Train Loss: 0.4160
Epoch 2 Step 451 Train Loss: 0.4349
Epoch 2 Step 501 Train Loss: 0.3305
Epoch 2 Step 551 Train Loss: 0.3343
Epoch 2 Step 601 Train Loss: 0.3895
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0051 Validation Top 20 DE MSE: 0.0013. 
Epoch 3 Step 1 Train Loss: 0.3927
Epoch 3 Step 51 Train Loss: 0.3524
Epoch 3 Step 101 Train Loss: 0.4789
Epoch 3 Step 151 Train Loss: 0.4392
Epoch 3 Step 201 Train Loss: 0.4039
Epoch 3 Step 251 Train Loss: 0.4971
Epoch 3 Step 301 Train Loss: 0.3402
Epoch 3 Step 351 Train Loss: 0.3922
Epoch 3 Step 401 Train Loss: 0.4234
Epoch 3 Step 451 Train Loss: 0.3642
Epoch 3 Step 501 Train Loss: 0.4073
Epoch 3 Step 551 Train Loss: 0.3942
Epoch 3 Step 601 Train Loss: 0.3317
Epoch 3: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0014. 
Epoch 4 Step 1 Train Loss: 0.4378
Epoch 4 Step 51 Train Loss: 0.4414
Epoch 4 Step 101 Train Loss: 0.3826
Epoch 4 Step 151 Train Loss: 0.4639
Epoch 4 Step 201 Train Loss: 0.3666
Epoch 4 Step 251 Train Loss: 0.4860
Epoch 4 Step 301 Train Loss: 0.4143
Epoch 4 Step 351 Train Loss: 0.4224
Epoch 4 Step 401 Train Loss: 0.3408
Epoch 4 Step 451 Train Loss: 0.4416
Epoch 4 Step 501 Train Loss: 0.4290
Epoch 4 Step 551 Train Loss: 0.3660
Epoch 4 Step 601 Train Loss: 0.4631
Epoch 4: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0010. 
Epoch 5 Step 1 Train Loss: 0.4055
Epoch 5 Step 51 Train Loss: 0.4122
Epoch 5 Step 101 Train Loss: 0.3736
Epoch 5 Step 151 Train Loss: 0.3703
Epoch 5 Step 201 Train Loss: 0.3980
Epoch 5 Step 251 Train Loss: 0.3786
Epoch 5 Step 301 Train Loss: 0.3317
Epoch 5 Step 351 Train Loss: 0.4449
Epoch 5 Step 401 Train Loss: 0.4601
Epoch 5 Step 451 Train Loss: 0.3385
Epoch 5 Step 501 Train Loss: 0.3947
Epoch 5 Step 551 Train Loss: 0.4225
Epoch 5 Step 601 Train Loss: 0.3608
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0014. 
Epoch 6 Step 1 Train Loss: 0.5650
Epoch 6 Step 51 Train Loss: 0.3915
Epoch 6 Step 101 Train Loss: 0.3690
Epoch 6 Step 151 Train Loss: 0.4520
Epoch 6 Step 201 Train Loss: 0.4112
Epoch 6 Step 251 Train Loss: 0.4718
Epoch 6 Step 301 Train Loss: 0.4346
Epoch 6 Step 351 Train Loss: 0.3174
Epoch 6 Step 401 Train Loss: 0.4372
Epoch 6 Step 451 Train Loss: 0.4336
Epoch 6 Step 501 Train Loss: 0.4868
Epoch 6 Step 551 Train Loss: 0.3404
Epoch 6 Step 601 Train Loss: 0.3456
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0012. 
Epoch 7 Step 1 Train Loss: 0.4865
Epoch 7 Step 51 Train Loss: 0.3684
Epoch 7 Step 101 Train Loss: 0.4370
Epoch 7 Step 151 Train Loss: 0.3528
Epoch 7 Step 201 Train Loss: 0.3972
Epoch 7 Step 251 Train Loss: 0.3769
Epoch 7 Step 301 Train Loss: 0.3777
Epoch 7 Step 351 Train Loss: 0.3304
Epoch 7 Step 401 Train Loss: 0.5374
Epoch 7 Step 451 Train Loss: 0.3784
Epoch 7 Step 501 Train Loss: 0.3340
Epoch 7 Step 551 Train Loss: 0.3930
Epoch 7 Step 601 Train Loss: 0.3660
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0010. 
Epoch 8 Step 1 Train Loss: 0.3743
Epoch 8 Step 51 Train Loss: 0.4108
Epoch 8 Step 101 Train Loss: 0.3650
Epoch 8 Step 151 Train Loss: 0.3697
Epoch 8 Step 201 Train Loss: 0.4050
Epoch 8 Step 251 Train Loss: 0.4994
Epoch 8 Step 301 Train Loss: 0.4266
Epoch 8 Step 351 Train Loss: 0.3703
Epoch 8 Step 401 Train Loss: 0.4178
Epoch 8 Step 451 Train Loss: 0.3990
Epoch 8 Step 501 Train Loss: 0.3869
Epoch 8 Step 551 Train Loss: 0.3037
Epoch 8 Step 601 Train Loss: 0.4148
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0011. 
Epoch 9 Step 1 Train Loss: 0.3690
Epoch 9 Step 51 Train Loss: 0.3497
Epoch 9 Step 101 Train Loss: 0.4159
Epoch 9 Step 151 Train Loss: 0.4237
Epoch 9 Step 201 Train Loss: 0.3938
Epoch 9 Step 251 Train Loss: 0.5298
Epoch 9 Step 301 Train Loss: 0.3161
Epoch 9 Step 351 Train Loss: 0.5033
Epoch 9 Step 401 Train Loss: 0.3482
Epoch 9 Step 451 Train Loss: 0.3368
Epoch 9 Step 501 Train Loss: 0.3687
Epoch 9 Step 551 Train Loss: 0.3662
Epoch 9 Step 601 Train Loss: 0.4198
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0009. 
Epoch 10 Step 1 Train Loss: 0.3677
Epoch 10 Step 51 Train Loss: 0.4063
Epoch 10 Step 101 Train Loss: 0.3844
Epoch 10 Step 151 Train Loss: 0.3932
Epoch 10 Step 201 Train Loss: 0.4855
Epoch 10 Step 251 Train Loss: 0.4427
Epoch 10 Step 301 Train Loss: 0.3602
Epoch 10 Step 351 Train Loss: 0.2841
Epoch 10 Step 401 Train Loss: 0.3763
Epoch 10 Step 451 Train Loss: 0.3768
Epoch 10 Step 501 Train Loss: 0.3954
Epoch 10 Step 551 Train Loss: 0.5030
Epoch 10 Step 601 Train Loss: 0.3483
Epoch 10: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0010. 
Epoch 11 Step 1 Train Loss: 0.5902
Epoch 11 Step 51 Train Loss: 0.3238
Epoch 11 Step 101 Train Loss: 0.3888
Epoch 11 Step 151 Train Loss: 0.2926
Epoch 11 Step 201 Train Loss: 0.4658
Epoch 11 Step 251 Train Loss: 0.3988
Epoch 11 Step 301 Train Loss: 0.4018
Epoch 11 Step 351 Train Loss: 0.4381
Epoch 11 Step 401 Train Loss: 0.3188
Epoch 11 Step 451 Train Loss: 0.3855
Epoch 11 Step 501 Train Loss: 0.3880
Epoch 11 Step 551 Train Loss: 0.3419
Epoch 11 Step 601 Train Loss: 0.4672
Epoch 11: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0009. 
Epoch 12 Step 1 Train Loss: 0.3748
Epoch 12 Step 51 Train Loss: 0.5505
Epoch 12 Step 101 Train Loss: 0.3773
Epoch 12 Step 151 Train Loss: 0.3609
Epoch 12 Step 201 Train Loss: 0.3767
Epoch 12 Step 251 Train Loss: 0.3975
Epoch 12 Step 301 Train Loss: 0.4769
Epoch 12 Step 351 Train Loss: 0.4134
Epoch 12 Step 401 Train Loss: 0.4133
Epoch 12 Step 451 Train Loss: 0.4756
Epoch 12 Step 501 Train Loss: 0.4836
Epoch 12 Step 551 Train Loss: 0.3365
Epoch 12 Step 601 Train Loss: 0.3781
Epoch 12: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0010. 
Epoch 13 Step 1 Train Loss: 0.4333
Epoch 13 Step 51 Train Loss: 0.3781
Epoch 13 Step 101 Train Loss: 0.4196
Epoch 13 Step 151 Train Loss: 0.3099
Epoch 13 Step 201 Train Loss: 0.3382
Epoch 13 Step 251 Train Loss: 0.3216
Epoch 13 Step 301 Train Loss: 0.3217
Epoch 13 Step 351 Train Loss: 0.3615
Epoch 13 Step 401 Train Loss: 0.3812
Epoch 13 Step 451 Train Loss: 0.4298
Epoch 13 Step 501 Train Loss: 0.3498
Epoch 13 Step 551 Train Loss: 0.3134
Epoch 13 Step 601 Train Loss: 0.4262
Epoch 13: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0009. 
Epoch 14 Step 1 Train Loss: 0.3110
Epoch 14 Step 51 Train Loss: 0.4346
Epoch 14 Step 101 Train Loss: 0.3837
Epoch 14 Step 151 Train Loss: 0.3635
Epoch 14 Step 201 Train Loss: 0.4892
Epoch 14 Step 251 Train Loss: 0.3537
Epoch 14 Step 301 Train Loss: 0.4322
Epoch 14 Step 351 Train Loss: 0.4032
Epoch 14 Step 401 Train Loss: 0.4533
Epoch 14 Step 451 Train Loss: 0.4621
Epoch 14 Step 501 Train Loss: 0.3323
Epoch 14 Step 551 Train Loss: 0.3672
Epoch 14 Step 601 Train Loss: 0.5979
Epoch 14: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0010. 
Epoch 15 Step 1 Train Loss: 0.3175
Epoch 15 Step 51 Train Loss: 0.3778
Epoch 15 Step 101 Train Loss: 0.3690
Epoch 15 Step 151 Train Loss: 0.3197
Epoch 15 Step 201 Train Loss: 0.3880
Epoch 15 Step 251 Train Loss: 0.4442
Epoch 15 Step 301 Train Loss: 0.4099
Epoch 15 Step 351 Train Loss: 0.3895
Epoch 15 Step 401 Train Loss: 0.3698
Epoch 15 Step 451 Train Loss: 0.4617
Epoch 15 Step 501 Train Loss: 0.3595
Epoch 15 Step 551 Train Loss: 0.3607
Epoch 15 Step 601 Train Loss: 0.3944
Epoch 15: Train Overall MSE: 0.0001 Validation Overall MSE: 0.0001. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0010. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0013
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0002151397
test_unseen_single_pearson: 0.9996001626036376
test_unseen_single_mse_de: 0.0012660647
test_unseen_single_pearson_de: 0.9998538681283858
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.47397528481084955
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.08333333333333333
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.001624417978440603
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.025 MB uploadedwandb: | 0.003 MB of 0.025 MB uploadedwandb: / 0.025 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ
wandb:                                                   val_de_mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00127
wandb:                                              test_de_pearson 0.99985
wandb:               test_frac_opposite_direction_top20_non_dropout 0.08333
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00022
wandb:                                test_mse_top20_de_non_dropout 0.00162
wandb:                                                 test_pearson 0.9996
wandb:                                           test_pearson_delta 0.47398
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.08333
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00022
wandb:                                    test_unseen_single_mse_de 0.00127
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00162
wandb:                                   test_unseen_single_pearson 0.9996
wandb:                                test_unseen_single_pearson_de 0.99985
wandb:                             test_unseen_single_pearson_delta 0.47398
wandb:                                                 train_de_mse 0.00315
wandb:                                             train_de_pearson 0.99951
wandb:                                                    train_mse 0.00015
wandb:                                                train_pearson 0.99973
wandb:                                                training_loss 0.38064
wandb:                                                   val_de_mse 0.00096
wandb:                                               val_de_pearson 0.99976
wandb:                                                      val_mse 0.00015
wandb:                                                  val_pearson 0.99974
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396858_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/743tmwwb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_013851-743tmwwb/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:3
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_014956-lf3p3pjz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396858_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/lf3p3pjz
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4945
Epoch 1 Step 51 Train Loss: 0.4785
Epoch 1 Step 101 Train Loss: 0.4180
Epoch 1 Step 151 Train Loss: 0.5534
Epoch 1 Step 201 Train Loss: 0.4513
Epoch 1 Step 251 Train Loss: 0.5831
Epoch 1 Step 301 Train Loss: 0.4452
Epoch 1 Step 351 Train Loss: 0.4063
Epoch 1 Step 401 Train Loss: 0.4615
Epoch 1 Step 451 Train Loss: 0.4044
Epoch 1 Step 501 Train Loss: 0.4846
Epoch 1: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0053 Validation Top 20 DE MSE: 0.0048. 
Epoch 2 Step 1 Train Loss: 0.4381
Epoch 2 Step 51 Train Loss: 0.4247
Epoch 2 Step 101 Train Loss: 0.4063
Epoch 2 Step 151 Train Loss: 0.4908
Epoch 2 Step 201 Train Loss: 0.3812
Epoch 2 Step 251 Train Loss: 0.4548
Epoch 2 Step 301 Train Loss: 0.4900
Epoch 2 Step 351 Train Loss: 0.4664
Epoch 2 Step 401 Train Loss: 0.5045
Epoch 2 Step 451 Train Loss: 0.4181
Epoch 2 Step 501 Train Loss: 0.3820
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0048. 
Epoch 3 Step 1 Train Loss: 0.4128
Epoch 3 Step 51 Train Loss: 0.5388
Epoch 3 Step 101 Train Loss: 0.4100
Epoch 3 Step 151 Train Loss: 0.3764
Epoch 3 Step 201 Train Loss: 0.5113
Epoch 3 Step 251 Train Loss: 0.3947
Epoch 3 Step 301 Train Loss: 0.4010
Epoch 3 Step 351 Train Loss: 0.4114
Epoch 3 Step 401 Train Loss: 0.3632
Epoch 3 Step 451 Train Loss: 0.4174
Epoch 3 Step 501 Train Loss: 0.4830
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0052 Validation Top 20 DE MSE: 0.0038. 
Epoch 4 Step 1 Train Loss: 0.5311
Epoch 4 Step 51 Train Loss: 0.4928
Epoch 4 Step 101 Train Loss: 0.3268
Epoch 4 Step 151 Train Loss: 0.5282
Epoch 4 Step 201 Train Loss: 0.4379
Epoch 4 Step 251 Train Loss: 0.4750
Epoch 4 Step 301 Train Loss: 0.4366
Epoch 4 Step 351 Train Loss: 0.3974
Epoch 4 Step 401 Train Loss: 0.3736
Epoch 4 Step 451 Train Loss: 0.3424
Epoch 4 Step 501 Train Loss: 0.4315
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0031. 
Epoch 5 Step 1 Train Loss: 0.3933
Epoch 5 Step 51 Train Loss: 0.4762
Epoch 5 Step 101 Train Loss: 0.5114
Epoch 5 Step 151 Train Loss: 0.4076
Epoch 5 Step 201 Train Loss: 0.4012
Epoch 5 Step 251 Train Loss: 0.5319
Epoch 5 Step 301 Train Loss: 0.5888
Epoch 5 Step 351 Train Loss: 0.3955
Epoch 5 Step 401 Train Loss: 0.4537
Epoch 5 Step 451 Train Loss: 0.3613
Epoch 5 Step 501 Train Loss: 0.3531
Epoch 5: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0030 Validation Top 20 DE MSE: 0.0030. 
Epoch 6 Step 1 Train Loss: 0.3747
Epoch 6 Step 51 Train Loss: 0.4491
Epoch 6 Step 101 Train Loss: 0.4174
Epoch 6 Step 151 Train Loss: 0.5034
Epoch 6 Step 201 Train Loss: 0.4458
Epoch 6 Step 251 Train Loss: 0.4050
Epoch 6 Step 301 Train Loss: 0.4689
Epoch 6 Step 351 Train Loss: 0.4224
Epoch 6 Step 401 Train Loss: 0.4401
Epoch 6 Step 451 Train Loss: 0.4122
Epoch 6 Step 501 Train Loss: 0.4480
Epoch 6: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0032. 
Epoch 7 Step 1 Train Loss: 0.4122
Epoch 7 Step 51 Train Loss: 0.4371
Epoch 7 Step 101 Train Loss: 0.4946
Epoch 7 Step 151 Train Loss: 0.4360
Epoch 7 Step 201 Train Loss: 0.5312
Epoch 7 Step 251 Train Loss: 0.4719
Epoch 7 Step 301 Train Loss: 0.4199
Epoch 7 Step 351 Train Loss: 0.4084
Epoch 7 Step 401 Train Loss: 0.5161
Epoch 7 Step 451 Train Loss: 0.5877
Epoch 7 Step 501 Train Loss: 0.4447
Epoch 7: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0032. 
Epoch 8 Step 1 Train Loss: 0.3553
Epoch 8 Step 51 Train Loss: 0.3658
Epoch 8 Step 101 Train Loss: 0.4619
Epoch 8 Step 151 Train Loss: 0.3630
Epoch 8 Step 201 Train Loss: 0.5577
Epoch 8 Step 251 Train Loss: 0.4035
Epoch 8 Step 301 Train Loss: 0.4022
Epoch 8 Step 351 Train Loss: 0.4814
Epoch 8 Step 401 Train Loss: 0.4047
Epoch 8 Step 451 Train Loss: 0.4366
Epoch 8 Step 501 Train Loss: 0.5008
Epoch 8: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0030. 
Epoch 9 Step 1 Train Loss: 0.4752
Epoch 9 Step 51 Train Loss: 0.5287
Epoch 9 Step 101 Train Loss: 0.3923
Epoch 9 Step 151 Train Loss: 0.3762
Epoch 9 Step 201 Train Loss: 0.4610
Epoch 9 Step 251 Train Loss: 0.3940
Epoch 9 Step 301 Train Loss: 0.3750
Epoch 9 Step 351 Train Loss: 0.5462
Epoch 9 Step 401 Train Loss: 0.3569
Epoch 9 Step 451 Train Loss: 0.4305
Epoch 9 Step 501 Train Loss: 0.3838
Epoch 9: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0031. 
Epoch 10 Step 1 Train Loss: 0.4174
Epoch 10 Step 51 Train Loss: 0.5332
Epoch 10 Step 101 Train Loss: 0.3836
Epoch 10 Step 151 Train Loss: 0.4624
Epoch 10 Step 201 Train Loss: 0.3608
Epoch 10 Step 251 Train Loss: 0.3507
Epoch 10 Step 301 Train Loss: 0.4100
Epoch 10 Step 351 Train Loss: 0.4935
Epoch 10 Step 401 Train Loss: 0.4160
Epoch 10 Step 451 Train Loss: 0.4586
Epoch 10 Step 501 Train Loss: 0.4327
Epoch 10: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0031. 
Epoch 11 Step 1 Train Loss: 0.4012
Epoch 11 Step 51 Train Loss: 0.5435
Epoch 11 Step 101 Train Loss: 0.3956
Epoch 11 Step 151 Train Loss: 0.4268
Epoch 11 Step 201 Train Loss: 0.5289
Epoch 11 Step 251 Train Loss: 0.4897
Epoch 11 Step 301 Train Loss: 0.5021
Epoch 11 Step 351 Train Loss: 0.4784
Epoch 11 Step 401 Train Loss: 0.5160
Epoch 11 Step 451 Train Loss: 0.4785
Epoch 11 Step 501 Train Loss: 0.4328
Epoch 11: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0030. 
Epoch 12 Step 1 Train Loss: 0.3832
Epoch 12 Step 51 Train Loss: 0.4480
Epoch 12 Step 101 Train Loss: 0.3638
Epoch 12 Step 151 Train Loss: 0.4649
Epoch 12 Step 201 Train Loss: 0.3957
Epoch 12 Step 251 Train Loss: 0.4232
Epoch 12 Step 301 Train Loss: 0.4911
Epoch 12 Step 351 Train Loss: 0.3968
Epoch 12 Step 401 Train Loss: 0.5149
Epoch 12 Step 451 Train Loss: 0.4395
Epoch 12 Step 501 Train Loss: 0.5524
Epoch 12: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0031. 
Epoch 13 Step 1 Train Loss: 0.3592
Epoch 13 Step 51 Train Loss: 0.3998
Epoch 13 Step 101 Train Loss: 0.3657
Epoch 13 Step 151 Train Loss: 0.5016
Epoch 13 Step 201 Train Loss: 0.4368
Epoch 13 Step 251 Train Loss: 0.4314
Epoch 13 Step 301 Train Loss: 0.3609
Epoch 13 Step 351 Train Loss: 0.4383
Epoch 13 Step 401 Train Loss: 0.6182
Epoch 13 Step 451 Train Loss: 0.3366
Epoch 13 Step 501 Train Loss: 0.3969
Epoch 13: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0032. 
Epoch 14 Step 1 Train Loss: 0.4665
Epoch 14 Step 51 Train Loss: 0.4364
Epoch 14 Step 101 Train Loss: 0.4040
Epoch 14 Step 151 Train Loss: 0.4218
Epoch 14 Step 201 Train Loss: 0.3643
Epoch 14 Step 251 Train Loss: 0.5150
Epoch 14 Step 301 Train Loss: 0.3322
Epoch 14 Step 351 Train Loss: 0.3455
Epoch 14 Step 401 Train Loss: 0.3892
Epoch 14 Step 451 Train Loss: 0.4932
Epoch 14 Step 501 Train Loss: 0.3739
Epoch 14: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0032. 
Epoch 15 Step 1 Train Loss: 0.4805
Epoch 15 Step 51 Train Loss: 0.5142
Epoch 15 Step 101 Train Loss: 0.4240
Epoch 15 Step 151 Train Loss: 0.3757
Epoch 15 Step 201 Train Loss: 0.4212
Epoch 15 Step 251 Train Loss: 0.4079
Epoch 15 Step 301 Train Loss: 0.4786
Epoch 15 Step 351 Train Loss: 0.3208
Epoch 15 Step 401 Train Loss: 0.4479
Epoch 15 Step 451 Train Loss: 0.3938
Epoch 15 Step 501 Train Loss: 0.4200
Epoch 15: Train Overall MSE: 0.0002 Validation Overall MSE: 0.0002. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0032. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0017
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00027429973
test_unseen_single_pearson: 0.9995039989080144
test_unseen_single_mse_de: 0.0016527214
test_unseen_single_pearson_de: 0.999708491618439
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5075131862203833
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.0017283359509654428
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.005 MB of 0.024 MB uploadedwandb: / 0.005 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ
wandb:                                                   val_de_mse ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00165
wandb:                                              test_de_pearson 0.99971
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00027
wandb:                                test_mse_top20_de_non_dropout 0.00173
wandb:                                                 test_pearson 0.9995
wandb:                                           test_pearson_delta 0.50751
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00027
wandb:                                    test_unseen_single_mse_de 0.00165
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00173
wandb:                                   test_unseen_single_pearson 0.9995
wandb:                                test_unseen_single_pearson_de 0.99971
wandb:                             test_unseen_single_pearson_delta 0.50751
wandb:                                                 train_de_mse 0.00157
wandb:                                             train_de_pearson 0.99979
wandb:                                                    train_mse 0.0002
wandb:                                                train_pearson 0.99965
wandb:                                                training_loss 0.40357
wandb:                                                   val_de_mse 0.00317
wandb:                                               val_de_pearson 0.99943
wandb:                                                      val_mse 0.00022
wandb:                                                  val_pearson 0.9996
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396858_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/lf3p3pjz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_014956-lf3p3pjz/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_020027-mcc83xtm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396861_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/mcc83xtm
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
  0%|                                                  | 0/3671 [00:00<?, ?it/s]  0%|                                          | 7/3671 [00:00<00:54, 67.58it/s]  0%|‚ñè                                        | 15/3671 [00:00<00:51, 70.65it/s]  1%|‚ñé                                        | 23/3671 [00:00<00:49, 73.00it/s]  1%|‚ñé                                        | 33/3671 [00:00<00:46, 79.09it/s]  1%|‚ñç                                        | 43/3671 [00:00<00:42, 85.82it/s]  1%|‚ñå                                        | 52/3671 [00:00<00:42, 84.57it/s]  2%|‚ñã                                        | 62/3671 [00:00<00:41, 86.52it/s]  2%|‚ñä                                        | 71/3671 [00:00<00:41, 87.37it/s]  2%|‚ñâ                                        | 82/3671 [00:00<00:39, 91.51it/s]  3%|‚ñà                                        | 92/3671 [00:01<00:39, 91.04it/s]  3%|‚ñà                                       | 102/3671 [00:01<00:39, 90.47it/s]  3%|‚ñà‚ñè                                      | 112/3671 [00:01<00:39, 90.35it/s]  3%|‚ñà‚ñé                                      | 122/3671 [00:01<00:39, 89.87it/s]  4%|‚ñà‚ñç                                      | 132/3671 [00:01<00:39, 89.87it/s]  4%|‚ñà‚ñå                                      | 141/3671 [00:01<00:39, 88.80it/s]  4%|‚ñà‚ñã                                      | 150/3671 [00:01<00:42, 83.10it/s]  4%|‚ñà‚ñä                                      | 162/3671 [00:01<00:39, 87.99it/s]  5%|‚ñà‚ñä                                      | 172/3671 [00:01<00:39, 88.21it/s]  5%|‚ñà‚ñâ                                      | 183/3671 [00:02<00:37, 91.79it/s]  5%|‚ñà‚ñà                                      | 193/3671 [00:02<00:38, 91.13it/s]  6%|‚ñà‚ñà‚ñè                                     | 203/3671 [00:02<00:38, 91.08it/s]  6%|‚ñà‚ñà‚ñé                                     | 213/3671 [00:02<00:38, 90.81it/s]  6%|‚ñà‚ñà‚ñç                                     | 223/3671 [00:02<00:38, 90.00it/s]  6%|‚ñà‚ñà‚ñå                                     | 233/3671 [00:02<00:38, 90.27it/s]  7%|‚ñà‚ñà‚ñã                                     | 243/3671 [00:02<00:37, 90.32it/s]  7%|‚ñà‚ñà‚ñä                                     | 253/3671 [00:02<00:37, 90.29it/s]  7%|‚ñà‚ñà‚ñä                                     | 263/3671 [00:02<00:38, 88.77it/s]  7%|‚ñà‚ñà‚ñâ                                     | 272/3671 [00:03<00:39, 85.78it/s]  8%|‚ñà‚ñà‚ñà                                     | 281/3671 [00:03<00:39, 86.58it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 291/3671 [00:03<00:37, 89.40it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 301/3671 [00:03<00:38, 86.92it/s]  8%|‚ñà‚ñà‚ñà‚ñç                                    | 311/3671 [00:03<00:37, 89.20it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 320/3671 [00:03<00:39, 84.96it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 330/3671 [00:03<00:37, 88.31it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 339/3671 [00:03<00:37, 87.96it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 348/3671 [00:03<00:37, 88.27it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 358/3671 [00:04<00:38, 86.69it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 368/3671 [00:04<00:36, 89.63it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 377/3671 [00:04<00:36, 89.33it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 386/3671 [00:04<00:36, 89.43it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 395/3671 [00:04<00:37, 86.46it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 405/3671 [00:04<00:36, 89.99it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 415/3671 [00:04<00:36, 89.40it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 424/3671 [00:04<00:36, 89.31it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 433/3671 [00:04<00:36, 88.83it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 442/3671 [00:05<00:36, 87.80it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 451/3671 [00:05<00:36, 87.69it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 460/3671 [00:05<00:37, 85.37it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 470/3671 [00:05<00:36, 86.73it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 480/3671 [00:05<00:36, 88.08it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 491/3671 [00:05<00:34, 91.80it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 501/3671 [00:05<00:34, 91.66it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 511/3671 [00:05<00:34, 91.61it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 521/3671 [00:05<00:35, 89.03it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 531/3671 [00:06<00:34, 90.10it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 542/3671 [00:06<00:34, 90.27it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 552/3671 [00:06<00:34, 89.69it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 562/3671 [00:06<00:34, 89.89it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 571/3671 [00:06<00:34, 89.27it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 581/3671 [00:06<00:34, 89.25it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 591/3671 [00:06<00:33, 92.08it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 601/3671 [00:06<00:33, 91.38it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 611/3671 [00:06<00:34, 87.83it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 621/3671 [00:07<00:34, 87.59it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 630/3671 [00:07<00:34, 88.05it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 640/3671 [00:07<00:34, 87.94it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 651/3671 [00:07<00:32, 92.09it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 661/3671 [00:07<00:33, 91.14it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 671/3671 [00:07<00:33, 88.47it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 681/3671 [00:07<00:32, 91.27it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 691/3671 [00:07<00:33, 90.27it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 701/3671 [00:07<00:33, 88.13it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 711/3671 [00:08<00:32, 90.90it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 721/3671 [00:08<00:33, 88.09it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 731/3671 [00:08<00:32, 91.12it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 741/3671 [00:08<00:32, 90.52it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 751/3671 [00:08<00:33, 87.23it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 762/3671 [00:08<00:32, 88.44it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 772/3671 [00:08<00:32, 89.46it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 783/3671 [00:08<00:30, 93.19it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 793/3671 [00:08<00:30, 93.19it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 803/3671 [00:09<00:31, 91.09it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 814/3671 [00:09<00:31, 91.38it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 824/3671 [00:09<00:30, 92.20it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 835/3671 [00:09<00:30, 93.15it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 846/3671 [00:09<00:29, 95.51it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 856/3671 [00:09<00:30, 92.94it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 866/3671 [00:09<00:30, 92.07it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 876/3671 [00:09<00:30, 91.57it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 887/3671 [00:09<00:29, 94.90it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 897/3671 [00:10<00:29, 93.75it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 907/3671 [00:10<00:29, 93.09it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 917/3671 [00:10<00:30, 91.58it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 927/3671 [00:10<00:30, 90.44it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 937/3671 [00:10<00:30, 88.45it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 947/3671 [00:10<00:30, 89.17it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 957/3671 [00:10<00:29, 92.12it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 967/3671 [00:10<00:29, 92.06it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 977/3671 [00:10<00:29, 91.64it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 987/3671 [00:11<00:29, 91.15it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 997/3671 [00:11<00:29, 90.96it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1007/3671 [00:11<00:30, 87.77it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1017/3671 [00:11<00:29, 89.14it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1027/3671 [00:11<00:29, 89.31it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1036/3671 [00:11<00:29, 88.89it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1046/3671 [00:11<00:28, 90.54it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1056/3671 [00:11<00:28, 90.33it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1066/3671 [00:11<00:28, 90.21it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1076/3671 [00:12<00:28, 90.48it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1086/3671 [00:12<00:28, 90.63it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1096/3671 [00:12<00:28, 90.00it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1106/3671 [00:12<00:28, 90.21it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1116/3671 [00:12<00:29, 87.79it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1126/3671 [00:12<00:28, 88.71it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1137/3671 [00:12<00:28, 89.27it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1147/3671 [00:12<00:28, 89.78it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1157/3671 [00:12<00:27, 90.32it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1167/3671 [00:13<00:27, 92.43it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1177/3671 [00:13<00:27, 91.34it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1187/3671 [00:13<00:27, 90.97it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1197/3671 [00:13<00:27, 90.92it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1207/3671 [00:13<00:28, 87.62it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1217/3671 [00:13<00:27, 90.46it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1227/3671 [00:13<00:28, 87.04it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1238/3671 [00:13<00:27, 88.31it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1249/3671 [00:13<00:26, 92.14it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1259/3671 [00:14<00:26, 92.08it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1269/3671 [00:14<00:26, 92.01it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1279/3671 [00:14<00:26, 91.99it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1289/3671 [00:14<00:26, 88.87it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1300/3671 [00:14<00:25, 92.01it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1310/3671 [00:14<00:25, 91.43it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1320/3671 [00:14<00:25, 91.02it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1330/3671 [00:14<00:26, 88.12it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1340/3671 [00:14<00:25, 90.65it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1350/3671 [00:15<00:25, 89.87it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1360/3671 [00:15<00:26, 87.06it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1371/3671 [00:15<00:26, 88.34it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1381/3671 [00:15<00:25, 89.20it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1392/3671 [00:15<00:24, 92.84it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1402/3671 [00:15<00:24, 92.00it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1412/3671 [00:15<00:24, 91.05it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1422/3671 [00:15<00:25, 88.39it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1432/3671 [00:15<00:25, 89.11it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1442/3671 [00:16<00:24, 89.86it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1452/3671 [00:16<00:24, 89.62it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1462/3671 [00:16<00:24, 89.18it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1471/3671 [00:16<00:24, 89.20it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1481/3671 [00:16<00:24, 89.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1491/3671 [00:16<00:24, 89.81it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1500/3671 [00:16<00:24, 86.87it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1511/3671 [00:16<00:23, 91.89it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1521/3671 [00:16<00:24, 87.28it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1530/3671 [00:17<00:24, 87.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1540/3671 [00:17<00:23, 90.71it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1550/3671 [00:17<00:23, 91.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1560/3671 [00:17<00:23, 89.39it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1571/3671 [00:17<00:23, 90.66it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1582/3671 [00:17<00:22, 94.61it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1592/3671 [00:17<00:22, 91.20it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1603/3671 [00:17<00:21, 94.51it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1613/3671 [00:17<00:21, 93.75it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1623/3671 [00:18<00:22, 91.24it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1634/3671 [00:18<00:22, 91.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1645/3671 [00:18<00:21, 93.73it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1655/3671 [00:18<00:22, 90.99it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1665/3671 [00:18<00:21, 91.50it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1676/3671 [00:18<00:21, 92.51it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1687/3671 [00:18<00:21, 92.99it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1698/3671 [00:18<00:20, 96.21it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1708/3671 [00:18<00:20, 96.08it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1718/3671 [00:19<00:20, 95.42it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1728/3671 [00:19<00:20, 94.79it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1738/3671 [00:19<00:20, 94.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1748/3671 [00:19<00:20, 94.02it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1758/3671 [00:19<00:20, 93.91it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1768/3671 [00:19<00:20, 90.62it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1778/3671 [00:19<00:20, 91.58it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1788/3671 [00:19<00:20, 91.89it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1799/3671 [00:19<00:19, 95.28it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1809/3671 [00:20<00:20, 91.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1819/3671 [00:20<00:19, 93.80it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1829/3671 [00:20<00:20, 90.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1840/3671 [00:20<00:19, 94.44it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1850/3671 [00:20<00:19, 94.42it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1860/3671 [00:20<00:19, 94.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1870/3671 [00:20<00:19, 94.12it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1880/3671 [00:20<00:19, 94.23it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1890/3671 [00:20<00:19, 91.49it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1901/3671 [00:21<00:19, 92.42it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1912/3671 [00:21<00:18, 95.29it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1922/3671 [00:21<00:18, 92.20it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1932/3671 [00:21<00:18, 92.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1943/3671 [00:21<00:18, 95.05it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1953/3671 [00:21<00:18, 91.97it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1964/3671 [00:21<00:18, 94.72it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1974/3671 [00:21<00:17, 94.78it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1984/3671 [00:21<00:18, 92.57it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1995/3671 [00:22<00:17, 95.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2005/3671 [00:22<00:17, 95.90it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2015/3671 [00:22<00:17, 92.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2026/3671 [00:22<00:17, 93.51it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2037/3671 [00:22<00:16, 96.53it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2047/3671 [00:22<00:17, 93.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2058/3671 [00:22<00:16, 96.63it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2068/3671 [00:22<00:17, 92.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2079/3671 [00:22<00:16, 93.74it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2089/3671 [00:23<00:16, 93.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2100/3671 [00:23<00:16, 93.94it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2111/3671 [00:23<00:16, 93.59it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2121/3671 [00:23<00:16, 93.84it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2132/3671 [00:23<00:15, 97.49it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2142/3671 [00:23<00:15, 97.60it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2152/3671 [00:23<00:15, 97.85it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2162/3671 [00:23<00:15, 97.59it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2172/3671 [00:23<00:15, 97.80it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2182/3671 [00:24<00:15, 97.86it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2193/3671 [00:24<00:15, 98.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2203/3671 [00:24<00:15, 97.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2213/3671 [00:24<00:15, 96.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2223/3671 [00:24<00:15, 93.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2233/3671 [00:24<00:15, 92.85it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2244/3671 [00:24<00:14, 95.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2254/3671 [00:24<00:15, 92.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2264/3671 [00:24<00:15, 92.71it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2274/3671 [00:25<00:14, 93.26it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2285/3671 [00:25<00:14, 93.14it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2296/3671 [00:25<00:14, 93.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2306/3671 [00:25<00:14, 93.61it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2317/3671 [00:25<00:14, 93.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2327/3671 [00:25<00:14, 93.80it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2337/3671 [00:25<00:14, 93.60it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2348/3671 [00:25<00:13, 96.41it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2358/3671 [00:25<00:13, 95.42it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2368/3671 [00:26<00:14, 92.09it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2378/3671 [00:26<00:13, 92.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2388/3671 [00:26<00:13, 91.90it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2399/3671 [00:26<00:13, 95.42it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2409/3671 [00:26<00:13, 93.91it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2419/3671 [00:26<00:13, 90.55it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2429/3671 [00:26<00:13, 90.95it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2440/3671 [00:26<00:13, 93.70it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2450/3671 [00:26<00:13, 93.86it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2460/3671 [00:26<00:12, 93.45it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2470/3671 [00:27<00:13, 91.49it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2481/3671 [00:27<00:12, 94.69it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2491/3671 [00:27<00:12, 94.14it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2501/3671 [00:27<00:12, 94.40it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2511/3671 [00:27<00:12, 94.14it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2521/3671 [00:27<00:12, 94.12it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2531/3671 [00:27<00:12, 90.28it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2542/3671 [00:27<00:12, 90.94it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2552/3671 [00:27<00:12, 91.36it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2563/3671 [00:28<00:12, 91.67it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2574/3671 [00:28<00:11, 95.06it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2584/3671 [00:28<00:11, 91.95it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2595/3671 [00:28<00:11, 95.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2605/3671 [00:28<00:11, 95.06it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2615/3671 [00:28<00:11, 94.72it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2625/3671 [00:28<00:11, 94.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2635/3671 [00:28<00:10, 94.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2645/3671 [00:28<00:11, 91.43it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2655/3671 [00:29<00:11, 92.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2665/3671 [00:29<00:10, 92.14it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2675/3671 [00:29<00:30, 32.66it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2707/3671 [00:30<00:17, 56.70it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2722/3671 [00:30<00:16, 56.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2730/3671 [00:30<00:20, 45.74it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2788/3671 [00:30<00:07, 111.37it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2809/3671 [00:31<00:08, 103.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2826/3671 [00:31<00:08, 102.10it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2841/3671 [00:31<00:08, 99.42it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2854/3671 [00:31<00:08, 95.22it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2866/3671 [00:31<00:08, 95.70it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2877/3671 [00:31<00:08, 94.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2888/3671 [00:32<00:08, 92.74it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2898/3671 [00:32<00:08, 92.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2908/3671 [00:32<00:08, 91.78it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2918/3671 [00:32<00:08, 91.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2928/3671 [00:32<00:08, 91.24it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2938/3671 [00:32<00:08, 90.97it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2948/3671 [00:32<00:07, 91.02it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2958/3671 [00:32<00:07, 90.78it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2968/3671 [00:32<00:07, 88.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2977/3671 [00:33<00:07, 88.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2987/3671 [00:33<00:07, 91.05it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2997/3671 [00:33<00:07, 87.54it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3007/3671 [00:33<00:07, 90.59it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3017/3671 [00:33<00:07, 90.78it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3027/3671 [00:33<00:07, 87.02it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3036/3671 [00:33<00:07, 84.00it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3045/3671 [00:33<00:07, 82.43it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3054/3671 [00:33<00:07, 80.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3063/3671 [00:34<00:07, 80.20it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3072/3671 [00:34<00:07, 77.18it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3081/3671 [00:34<00:07, 78.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3091/3671 [00:34<00:07, 81.20it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3100/3671 [00:34<00:07, 77.76it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3109/3671 [00:34<00:07, 78.85it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3118/3671 [00:34<00:06, 80.38it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3127/3671 [00:34<00:06, 82.38it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3137/3671 [00:34<00:06, 86.63it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3146/3671 [00:35<00:06, 84.58it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3156/3671 [00:35<00:05, 88.59it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3165/3671 [00:35<00:06, 84.22it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3175/3671 [00:35<00:05, 83.46it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3184/3671 [00:35<00:05, 83.25it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3193/3671 [00:35<00:05, 84.57it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3202/3671 [00:35<00:05, 82.17it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3211/3671 [00:35<00:05, 83.52it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3220/3671 [00:35<00:05, 85.31it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3230/3671 [00:36<00:05, 86.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3239/3671 [00:36<00:04, 86.41it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3249/3671 [00:36<00:04, 89.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3258/3671 [00:36<00:04, 89.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3267/3671 [00:36<00:04, 88.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3276/3671 [00:36<00:04, 87.92it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3286/3671 [00:36<00:04, 88.96it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3295/3671 [00:36<00:04, 86.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3305/3671 [00:36<00:04, 87.71it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3315/3671 [00:37<00:03, 90.63it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3325/3671 [00:37<00:03, 87.42it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3335/3671 [00:37<00:03, 88.26it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3345/3671 [00:37<00:03, 90.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3355/3671 [00:37<00:03, 87.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3365/3671 [00:37<00:03, 90.27it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3375/3671 [00:37<00:03, 89.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3385/3671 [00:37<00:03, 88.20it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3395/3671 [00:37<00:03, 90.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3405/3671 [00:38<00:03, 88.12it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3415/3671 [00:38<00:02, 88.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3425/3671 [00:38<00:02, 88.34it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3435/3671 [00:38<00:02, 90.00it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3445/3671 [00:38<00:02, 90.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3455/3671 [00:38<00:02, 91.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3465/3671 [00:38<00:02, 88.70it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3477/3671 [00:38<00:02, 92.21it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3488/3671 [00:38<00:01, 95.31it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3498/3671 [00:39<00:01, 93.91it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3508/3671 [00:39<00:01, 91.83it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3519/3671 [00:39<00:01, 94.19it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3529/3671 [00:39<00:01, 93.63it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3539/3671 [00:39<00:01, 91.07it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3550/3671 [00:39<00:01, 93.57it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3560/3671 [00:39<00:01, 91.31it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3570/3671 [00:39<00:01, 91.10it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3580/3671 [00:39<00:00, 91.80it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3590/3671 [00:40<00:00, 91.70it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3600/3671 [00:40<00:00, 79.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3610/3671 [00:40<00:00, 72.27it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3628/3671 [00:40<00:00, 94.39it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3639/3671 [00:40<00:00, 94.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3649/3671 [00:40<00:00, 95.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3659/3671 [00:40<00:00, 86.65it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3671/3671 [00:40<00:00, 89.61it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.4720
Epoch 1 Step 51 Train Loss: 0.4807
Epoch 1 Step 101 Train Loss: 0.4717
Epoch 1 Step 151 Train Loss: 0.4324
Epoch 1 Step 201 Train Loss: 0.6078
Epoch 1 Step 251 Train Loss: 0.5410
Epoch 1 Step 301 Train Loss: 0.4987
Epoch 1 Step 351 Train Loss: 0.4640
Epoch 1 Step 401 Train Loss: 0.5023
Epoch 1: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0046. 
Epoch 2 Step 1 Train Loss: 0.5687
Epoch 2 Step 51 Train Loss: 0.4501
Epoch 2 Step 101 Train Loss: 0.5651
Epoch 2 Step 151 Train Loss: 0.4115
Epoch 2 Step 201 Train Loss: 0.6307
Epoch 2 Step 251 Train Loss: 0.4957
Epoch 2 Step 301 Train Loss: 0.5848
Epoch 2 Step 351 Train Loss: 0.3493
Epoch 2 Step 401 Train Loss: 0.5283
Epoch 2: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0036. 
Epoch 3 Step 1 Train Loss: 0.4843
Epoch 3 Step 51 Train Loss: 0.4607
Epoch 3 Step 101 Train Loss: 0.5900
Epoch 3 Step 151 Train Loss: 0.5772
Epoch 3 Step 201 Train Loss: 0.4693
Epoch 3 Step 251 Train Loss: 0.5147
Epoch 3 Step 301 Train Loss: 0.4908
Epoch 3 Step 351 Train Loss: 0.4801
Epoch 3 Step 401 Train Loss: 0.5239
Epoch 3: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0025. 
Epoch 4 Step 1 Train Loss: 0.3871
Epoch 4 Step 51 Train Loss: 0.5241
Epoch 4 Step 101 Train Loss: 0.3622
Epoch 4 Step 151 Train Loss: 0.6110
Epoch 4 Step 201 Train Loss: 0.5347
Epoch 4 Step 251 Train Loss: 0.4622
Epoch 4 Step 301 Train Loss: 0.5687
Epoch 4 Step 351 Train Loss: 0.4834
Epoch 4 Step 401 Train Loss: 0.4353
Epoch 4: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0024. 
Epoch 5 Step 1 Train Loss: 0.4412
Epoch 5 Step 51 Train Loss: 0.4880
Epoch 5 Step 101 Train Loss: 0.4869
Epoch 5 Step 151 Train Loss: 0.3880
Epoch 5 Step 201 Train Loss: 0.5035
Epoch 5 Step 251 Train Loss: 0.4689
Epoch 5 Step 301 Train Loss: 0.5233
Epoch 5 Step 351 Train Loss: 0.6905
Epoch 5 Step 401 Train Loss: 0.4388
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0026. 
Epoch 6 Step 1 Train Loss: 0.3805
Epoch 6 Step 51 Train Loss: 0.6216
Epoch 6 Step 101 Train Loss: 0.4823
Epoch 6 Step 151 Train Loss: 0.6278
Epoch 6 Step 201 Train Loss: 0.6771
Epoch 6 Step 251 Train Loss: 0.4048
Epoch 6 Step 301 Train Loss: 0.5174
Epoch 6 Step 351 Train Loss: 0.4909
Epoch 6 Step 401 Train Loss: 0.4059
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0030. 
Epoch 7 Step 1 Train Loss: 0.5675
Epoch 7 Step 51 Train Loss: 0.5626
Epoch 7 Step 101 Train Loss: 0.4601
Epoch 7 Step 151 Train Loss: 0.4184
Epoch 7 Step 201 Train Loss: 0.4481
Epoch 7 Step 251 Train Loss: 0.4966
Epoch 7 Step 301 Train Loss: 0.4474
Epoch 7 Step 351 Train Loss: 0.5142
Epoch 7 Step 401 Train Loss: 0.5534
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0028. 
Epoch 8 Step 1 Train Loss: 0.5182
Epoch 8 Step 51 Train Loss: 0.5416
Epoch 8 Step 101 Train Loss: 0.6199
Epoch 8 Step 151 Train Loss: 0.4610
Epoch 8 Step 201 Train Loss: 0.4416
Epoch 8 Step 251 Train Loss: 0.4455
Epoch 8 Step 301 Train Loss: 0.5514
Epoch 8 Step 351 Train Loss: 0.4148
Epoch 8 Step 401 Train Loss: 0.5345
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0029. 
Epoch 9 Step 1 Train Loss: 0.7109
Epoch 9 Step 51 Train Loss: 0.4494
Epoch 9 Step 101 Train Loss: 0.6313
Epoch 9 Step 151 Train Loss: 0.5408
Epoch 9 Step 201 Train Loss: 0.5065
Epoch 9 Step 251 Train Loss: 0.4553
Epoch 9 Step 301 Train Loss: 0.6477
Epoch 9 Step 351 Train Loss: 0.3834
Epoch 9 Step 401 Train Loss: 0.5232
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0028. 
Epoch 10 Step 1 Train Loss: 0.4503
Epoch 10 Step 51 Train Loss: 0.4925
Epoch 10 Step 101 Train Loss: 0.5519
Epoch 10 Step 151 Train Loss: 0.5793
Epoch 10 Step 201 Train Loss: 0.4903
Epoch 10 Step 251 Train Loss: 0.3939
Epoch 10 Step 301 Train Loss: 0.4124
Epoch 10 Step 351 Train Loss: 0.4360
Epoch 10 Step 401 Train Loss: 0.5119
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0027. 
Epoch 11 Step 1 Train Loss: 0.4520
Epoch 11 Step 51 Train Loss: 0.4708
Epoch 11 Step 101 Train Loss: 0.4371
Epoch 11 Step 151 Train Loss: 0.4078
Epoch 11 Step 201 Train Loss: 0.7630
Epoch 11 Step 251 Train Loss: 0.5072
Epoch 11 Step 301 Train Loss: 0.4512
Epoch 11 Step 351 Train Loss: 0.5036
Epoch 11 Step 401 Train Loss: 0.5880
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0026. 
Epoch 12 Step 1 Train Loss: 0.5121
Epoch 12 Step 51 Train Loss: 0.3869
Epoch 12 Step 101 Train Loss: 0.4726
Epoch 12 Step 151 Train Loss: 0.5325
Epoch 12 Step 201 Train Loss: 0.6231
Epoch 12 Step 251 Train Loss: 0.5072
Epoch 12 Step 301 Train Loss: 0.5467
Epoch 12 Step 351 Train Loss: 0.4761
Epoch 12 Step 401 Train Loss: 0.4730
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0029. 
Epoch 13 Step 1 Train Loss: 0.4714
Epoch 13 Step 51 Train Loss: 0.4911
Epoch 13 Step 101 Train Loss: 0.4066
Epoch 13 Step 151 Train Loss: 0.5459
Epoch 13 Step 201 Train Loss: 0.4502
Epoch 13 Step 251 Train Loss: 0.4623
Epoch 13 Step 301 Train Loss: 0.5962
Epoch 13 Step 351 Train Loss: 0.5381
Epoch 13 Step 401 Train Loss: 0.7383
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0026. 
Epoch 14 Step 1 Train Loss: 0.5452
Epoch 14 Step 51 Train Loss: 0.4508
Epoch 14 Step 101 Train Loss: 0.5178
Epoch 14 Step 151 Train Loss: 0.5639
Epoch 14 Step 201 Train Loss: 0.4828
Epoch 14 Step 251 Train Loss: 0.4754
Epoch 14 Step 301 Train Loss: 0.5549
Epoch 14 Step 351 Train Loss: 0.4328
Epoch 14 Step 401 Train Loss: 0.5950
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0030. 
Epoch 15 Step 1 Train Loss: 0.4881
Epoch 15 Step 51 Train Loss: 0.4739
Epoch 15 Step 101 Train Loss: 0.4854
Epoch 15 Step 151 Train Loss: 0.5326
Epoch 15 Step 201 Train Loss: 0.5667
Epoch 15 Step 251 Train Loss: 0.4364
Epoch 15 Step 301 Train Loss: 0.4951
Epoch 15 Step 351 Train Loss: 0.5115
Epoch 15 Step 401 Train Loss: 0.5069
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0026. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0023
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0004455311
test_unseen_single_pearson: 0.9992766911098709
test_unseen_single_mse_de: 0.0022774362
test_unseen_single_pearson_de: 0.9993430542153385
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5286660287083362
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.05
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.0025969755558146587
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.003 MB of 0.023 MB uploadedwandb: / 0.003 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00228
wandb:                                              test_de_pearson 0.99934
wandb:               test_frac_opposite_direction_top20_non_dropout 0.05
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00045
wandb:                                test_mse_top20_de_non_dropout 0.0026
wandb:                                                 test_pearson 0.99928
wandb:                                           test_pearson_delta 0.52867
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.05
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00045
wandb:                                    test_unseen_single_mse_de 0.00228
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0026
wandb:                                   test_unseen_single_pearson 0.99928
wandb:                                test_unseen_single_pearson_de 0.99934
wandb:                             test_unseen_single_pearson_delta 0.52867
wandb:                                                 train_de_mse 0.00188
wandb:                                             train_de_pearson 0.99955
wandb:                                                    train_mse 0.00041
wandb:                                                train_pearson 0.99934
wandb:                                                training_loss 0.54655
wandb:                                                   val_de_mse 0.00264
wandb:                                               val_de_pearson 0.99914
wandb:                                                      val_mse 0.00038
wandb:                                                  val_pearson 0.99939
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396861_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/mcc83xtm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_020027-mcc83xtm/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_021348-jremhpsi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396861_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/jremhpsi
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4910
Epoch 1 Step 51 Train Loss: 0.4699
Epoch 1 Step 101 Train Loss: 0.4831
Epoch 1 Step 151 Train Loss: 0.4124
Epoch 1 Step 201 Train Loss: 0.4517
Epoch 1 Step 251 Train Loss: 0.5045
Epoch 1 Step 301 Train Loss: 0.4609
Epoch 1 Step 351 Train Loss: 0.4754
Epoch 1 Step 401 Train Loss: 0.4627
Epoch 1 Step 451 Train Loss: 0.4040
Epoch 1: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0007. 
Epoch 2 Step 1 Train Loss: 0.5455
Epoch 2 Step 51 Train Loss: 0.5033
Epoch 2 Step 101 Train Loss: 0.5104
Epoch 2 Step 151 Train Loss: 0.4325
Epoch 2 Step 201 Train Loss: 0.5135
Epoch 2 Step 251 Train Loss: 0.4444
Epoch 2 Step 301 Train Loss: 0.5356
Epoch 2 Step 351 Train Loss: 0.4588
Epoch 2 Step 401 Train Loss: 0.4585
Epoch 2 Step 451 Train Loss: 0.5866
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0012. 
Epoch 3 Step 1 Train Loss: 0.5179
Epoch 3 Step 51 Train Loss: 0.4586
Epoch 3 Step 101 Train Loss: 0.4337
Epoch 3 Step 151 Train Loss: 0.5026
Epoch 3 Step 201 Train Loss: 0.5582
Epoch 3 Step 251 Train Loss: 0.4864
Epoch 3 Step 301 Train Loss: 0.4085
Epoch 3 Step 351 Train Loss: 0.4953
Epoch 3 Step 401 Train Loss: 0.4802
Epoch 3 Step 451 Train Loss: 0.6221
Epoch 3: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0010. 
Epoch 4 Step 1 Train Loss: 0.4668
Epoch 4 Step 51 Train Loss: 0.4819
Epoch 4 Step 101 Train Loss: 0.5486
Epoch 4 Step 151 Train Loss: 0.5779
Epoch 4 Step 201 Train Loss: 0.4721
Epoch 4 Step 251 Train Loss: 0.3910
Epoch 4 Step 301 Train Loss: 0.4592
Epoch 4 Step 351 Train Loss: 0.4533
Epoch 4 Step 401 Train Loss: 0.5287
Epoch 4 Step 451 Train Loss: 0.4193
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0007. 
Epoch 5 Step 1 Train Loss: 0.4658
Epoch 5 Step 51 Train Loss: 0.4040
Epoch 5 Step 101 Train Loss: 0.5265
Epoch 5 Step 151 Train Loss: 0.5028
Epoch 5 Step 201 Train Loss: 0.4242
Epoch 5 Step 251 Train Loss: 0.5304
Epoch 5 Step 301 Train Loss: 0.5010
Epoch 5 Step 351 Train Loss: 0.5186
Epoch 5 Step 401 Train Loss: 0.3929
Epoch 5 Step 451 Train Loss: 0.4298
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0006. 
Epoch 6 Step 1 Train Loss: 0.4956
Epoch 6 Step 51 Train Loss: 0.5276
Epoch 6 Step 101 Train Loss: 0.4561
Epoch 6 Step 151 Train Loss: 0.4354
Epoch 6 Step 201 Train Loss: 0.5576
Epoch 6 Step 251 Train Loss: 0.4044
Epoch 6 Step 301 Train Loss: 0.4694
Epoch 6 Step 351 Train Loss: 0.5282
Epoch 6 Step 401 Train Loss: 0.4994
Epoch 6 Step 451 Train Loss: 0.5404
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0006. 
Epoch 7 Step 1 Train Loss: 0.5773
Epoch 7 Step 51 Train Loss: 0.4898
Epoch 7 Step 101 Train Loss: 0.4374
Epoch 7 Step 151 Train Loss: 0.6212
Epoch 7 Step 201 Train Loss: 0.4622
Epoch 7 Step 251 Train Loss: 0.4862
Epoch 7 Step 301 Train Loss: 0.4549
Epoch 7 Step 351 Train Loss: 0.4921
Epoch 7 Step 401 Train Loss: 0.4706
Epoch 7 Step 451 Train Loss: 0.6223
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0006. 
Epoch 8 Step 1 Train Loss: 0.4334
Epoch 8 Step 51 Train Loss: 0.5611
Epoch 8 Step 101 Train Loss: 0.4300
Epoch 8 Step 151 Train Loss: 0.5504
Epoch 8 Step 201 Train Loss: 0.4079
Epoch 8 Step 251 Train Loss: 0.5265
Epoch 8 Step 301 Train Loss: 0.4637
Epoch 8 Step 351 Train Loss: 0.4422
Epoch 8 Step 401 Train Loss: 0.4220
Epoch 8 Step 451 Train Loss: 0.5053
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0006. 
Epoch 9 Step 1 Train Loss: 0.4410
Epoch 9 Step 51 Train Loss: 0.4513
Epoch 9 Step 101 Train Loss: 0.4193
Epoch 9 Step 151 Train Loss: 0.4360
Epoch 9 Step 201 Train Loss: 0.4838
Epoch 9 Step 251 Train Loss: 0.4561
Epoch 9 Step 301 Train Loss: 0.4436
Epoch 9 Step 351 Train Loss: 0.4925
Epoch 9 Step 401 Train Loss: 0.4973
Epoch 9 Step 451 Train Loss: 0.4184
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0006. 
Epoch 10 Step 1 Train Loss: 0.4062
Epoch 10 Step 51 Train Loss: 0.4287
Epoch 10 Step 101 Train Loss: 0.4956
Epoch 10 Step 151 Train Loss: 0.4930
Epoch 10 Step 201 Train Loss: 0.4384
Epoch 10 Step 251 Train Loss: 0.5110
Epoch 10 Step 301 Train Loss: 0.4813
Epoch 10 Step 351 Train Loss: 0.4202
Epoch 10 Step 401 Train Loss: 0.5078
Epoch 10 Step 451 Train Loss: 0.5094
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0006. 
Epoch 11 Step 1 Train Loss: 0.4572
Epoch 11 Step 51 Train Loss: 0.4902
Epoch 11 Step 101 Train Loss: 0.4722
Epoch 11 Step 151 Train Loss: 0.5816
Epoch 11 Step 201 Train Loss: 0.4260
Epoch 11 Step 251 Train Loss: 0.4863
Epoch 11 Step 301 Train Loss: 0.4444
Epoch 11 Step 351 Train Loss: 0.4115
Epoch 11 Step 401 Train Loss: 0.4414
Epoch 11 Step 451 Train Loss: 0.4997
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 12 Step 1 Train Loss: 0.4712
Epoch 12 Step 51 Train Loss: 0.5118
Epoch 12 Step 101 Train Loss: 0.5495
Epoch 12 Step 151 Train Loss: 0.4615
Epoch 12 Step 201 Train Loss: 0.4408
Epoch 12 Step 251 Train Loss: 0.5184
Epoch 12 Step 301 Train Loss: 0.5149
Epoch 12 Step 351 Train Loss: 0.6096
Epoch 12 Step 401 Train Loss: 0.6311
Epoch 12 Step 451 Train Loss: 0.4670
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0006. 
Epoch 13 Step 1 Train Loss: 0.5223
Epoch 13 Step 51 Train Loss: 0.4806
Epoch 13 Step 101 Train Loss: 0.4147
Epoch 13 Step 151 Train Loss: 0.4542
Epoch 13 Step 201 Train Loss: 0.4378
Epoch 13 Step 251 Train Loss: 0.4440
Epoch 13 Step 301 Train Loss: 0.4117
Epoch 13 Step 351 Train Loss: 0.5617
Epoch 13 Step 401 Train Loss: 0.5154
Epoch 13 Step 451 Train Loss: 0.4040
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0006. 
Epoch 14 Step 1 Train Loss: 0.6386
Epoch 14 Step 51 Train Loss: 0.4695
Epoch 14 Step 101 Train Loss: 0.5467
Epoch 14 Step 151 Train Loss: 0.4714
Epoch 14 Step 201 Train Loss: 0.3962
Epoch 14 Step 251 Train Loss: 0.4757
Epoch 14 Step 301 Train Loss: 0.5645
Epoch 14 Step 351 Train Loss: 0.4401
Epoch 14 Step 401 Train Loss: 0.5830
Epoch 14 Step 451 Train Loss: 0.4921
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0006. 
Epoch 15 Step 1 Train Loss: 0.4641
Epoch 15 Step 51 Train Loss: 0.5311
Epoch 15 Step 101 Train Loss: 0.5499
Epoch 15 Step 151 Train Loss: 0.4644
Epoch 15 Step 201 Train Loss: 0.6162
Epoch 15 Step 251 Train Loss: 0.4521
Epoch 15 Step 301 Train Loss: 0.4947
Epoch 15 Step 351 Train Loss: 0.5036
Epoch 15 Step 401 Train Loss: 0.5739
Epoch 15 Step 451 Train Loss: 0.6071
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0006. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0019
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00037400943
test_unseen_single_pearson: 0.999398731953972
test_unseen_single_mse_de: 0.0019160733
test_unseen_single_pearson_de: 0.9995588040972117
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5811837194409517
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.037500000000000006
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002447740712399708
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.001 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÜ
wandb:                                                   val_de_mse ‚ñÇ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñá‚ñÅ‚ñÉ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÜ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÉ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00192
wandb:                                              test_de_pearson 0.99956
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0375
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00037
wandb:                                test_mse_top20_de_non_dropout 0.00245
wandb:                                                 test_pearson 0.9994
wandb:                                           test_pearson_delta 0.58118
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00037
wandb:                                    test_unseen_single_mse_de 0.00192
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00245
wandb:                                   test_unseen_single_pearson 0.9994
wandb:                                test_unseen_single_pearson_de 0.99956
wandb:                             test_unseen_single_pearson_delta 0.58118
wandb:                                                 train_de_mse 0.00191
wandb:                                             train_de_pearson 0.99945
wandb:                                                    train_mse 0.00038
wandb:                                                train_pearson 0.99939
wandb:                                                training_loss 0.59119
wandb:                                                   val_de_mse 0.00059
wandb:                                               val_de_pearson 0.99988
wandb:                                                      val_mse 0.00039
wandb:                                                  val_pearson 0.99937
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396861_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/jremhpsi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_021348-jremhpsi/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_022742-kgayzrqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396861_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/kgayzrqq
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4425
Epoch 1 Step 51 Train Loss: 0.4498
Epoch 1 Step 101 Train Loss: 0.6776
Epoch 1 Step 151 Train Loss: 0.4918
Epoch 1 Step 201 Train Loss: 0.3962
Epoch 1 Step 251 Train Loss: 0.5479
Epoch 1 Step 301 Train Loss: 0.5601
Epoch 1 Step 351 Train Loss: 0.4859
Epoch 1 Step 401 Train Loss: 0.6221
Epoch 1: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0046. 
Epoch 2 Step 1 Train Loss: 0.6634
Epoch 2 Step 51 Train Loss: 0.5015
Epoch 2 Step 101 Train Loss: 0.5200
Epoch 2 Step 151 Train Loss: 0.5567
Epoch 2 Step 201 Train Loss: 0.5129
Epoch 2 Step 251 Train Loss: 0.5683
Epoch 2 Step 301 Train Loss: 0.6102
Epoch 2 Step 351 Train Loss: 0.4049
Epoch 2 Step 401 Train Loss: 0.6563
Epoch 2: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0035. 
Epoch 3 Step 1 Train Loss: 0.4702
Epoch 3 Step 51 Train Loss: 0.4891
Epoch 3 Step 101 Train Loss: 0.4744
Epoch 3 Step 151 Train Loss: 0.5216
Epoch 3 Step 201 Train Loss: 0.4445
Epoch 3 Step 251 Train Loss: 0.5372
Epoch 3 Step 301 Train Loss: 0.4504
Epoch 3 Step 351 Train Loss: 0.4628
Epoch 3 Step 401 Train Loss: 0.4632
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0027. 
Epoch 4 Step 1 Train Loss: 0.5679
Epoch 4 Step 51 Train Loss: 0.4744
Epoch 4 Step 101 Train Loss: 0.4288
Epoch 4 Step 151 Train Loss: 0.5312
Epoch 4 Step 201 Train Loss: 0.4564
Epoch 4 Step 251 Train Loss: 0.4745
Epoch 4 Step 301 Train Loss: 0.5017
Epoch 4 Step 351 Train Loss: 0.3959
Epoch 4 Step 401 Train Loss: 0.5427
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0032. 
Epoch 5 Step 1 Train Loss: 0.6242
Epoch 5 Step 51 Train Loss: 0.4918
Epoch 5 Step 101 Train Loss: 0.6648
Epoch 5 Step 151 Train Loss: 0.5155
Epoch 5 Step 201 Train Loss: 0.4146
Epoch 5 Step 251 Train Loss: 0.4837
Epoch 5 Step 301 Train Loss: 0.6949
Epoch 5 Step 351 Train Loss: 0.4804
Epoch 5 Step 401 Train Loss: 0.5923
Epoch 5: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0026. 
Epoch 6 Step 1 Train Loss: 0.5185
Epoch 6 Step 51 Train Loss: 0.4709
Epoch 6 Step 101 Train Loss: 0.5474
Epoch 6 Step 151 Train Loss: 0.4943
Epoch 6 Step 201 Train Loss: 0.4475
Epoch 6 Step 251 Train Loss: 0.4786
Epoch 6 Step 301 Train Loss: 0.4018
Epoch 6 Step 351 Train Loss: 0.4248
Epoch 6 Step 401 Train Loss: 0.4967
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0027. 
Epoch 7 Step 1 Train Loss: 0.5693
Epoch 7 Step 51 Train Loss: 0.4433
Epoch 7 Step 101 Train Loss: 0.6264
Epoch 7 Step 151 Train Loss: 0.5849
Epoch 7 Step 201 Train Loss: 0.5753
Epoch 7 Step 251 Train Loss: 0.4924
Epoch 7 Step 301 Train Loss: 0.4080
Epoch 7 Step 351 Train Loss: 0.4799
Epoch 7 Step 401 Train Loss: 0.5586
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0028. 
Epoch 8 Step 1 Train Loss: 0.5862
Epoch 8 Step 51 Train Loss: 0.5430
Epoch 8 Step 101 Train Loss: 0.4883
Epoch 8 Step 151 Train Loss: 0.4365
Epoch 8 Step 201 Train Loss: 0.4041
Epoch 8 Step 251 Train Loss: 0.6190
Epoch 8 Step 301 Train Loss: 0.4973
Epoch 8 Step 351 Train Loss: 0.6051
Epoch 8 Step 401 Train Loss: 0.5414
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0029. 
Epoch 9 Step 1 Train Loss: 0.4280
Epoch 9 Step 51 Train Loss: 0.4641
Epoch 9 Step 101 Train Loss: 0.4837
Epoch 9 Step 151 Train Loss: 0.4258
Epoch 9 Step 201 Train Loss: 0.5126
Epoch 9 Step 251 Train Loss: 0.5007
Epoch 9 Step 301 Train Loss: 0.5170
Epoch 9 Step 351 Train Loss: 0.5181
Epoch 9 Step 401 Train Loss: 0.3509
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0028. 
Epoch 10 Step 1 Train Loss: 0.7411
Epoch 10 Step 51 Train Loss: 0.6202
Epoch 10 Step 101 Train Loss: 0.4601
Epoch 10 Step 151 Train Loss: 0.5018
Epoch 10 Step 201 Train Loss: 0.5470
Epoch 10 Step 251 Train Loss: 0.5533
Epoch 10 Step 301 Train Loss: 0.4769
Epoch 10 Step 351 Train Loss: 0.5016
Epoch 10 Step 401 Train Loss: 0.3928
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0027. 
Epoch 11 Step 1 Train Loss: 0.4459
Epoch 11 Step 51 Train Loss: 0.5596
Epoch 11 Step 101 Train Loss: 0.5421
Epoch 11 Step 151 Train Loss: 0.5114
Epoch 11 Step 201 Train Loss: 0.4511
Epoch 11 Step 251 Train Loss: 0.5265
Epoch 11 Step 301 Train Loss: 0.6159
Epoch 11 Step 351 Train Loss: 0.4429
Epoch 11 Step 401 Train Loss: 0.5664
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0022 Validation Top 20 DE MSE: 0.0030. 
Epoch 12 Step 1 Train Loss: 0.5754
Epoch 12 Step 51 Train Loss: 0.6388
Epoch 12 Step 101 Train Loss: 0.5596
Epoch 12 Step 151 Train Loss: 0.5807
Epoch 12 Step 201 Train Loss: 0.4974
Epoch 12 Step 251 Train Loss: 0.4131
Epoch 12 Step 301 Train Loss: 0.4405
Epoch 12 Step 351 Train Loss: 0.4630
Epoch 12 Step 401 Train Loss: 0.5722
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0028. 
Epoch 13 Step 1 Train Loss: 0.4843
Epoch 13 Step 51 Train Loss: 0.4519
Epoch 13 Step 101 Train Loss: 0.4846
Epoch 13 Step 151 Train Loss: 0.4255
Epoch 13 Step 201 Train Loss: 0.4364
Epoch 13 Step 251 Train Loss: 0.5280
Epoch 13 Step 301 Train Loss: 0.3809
Epoch 13 Step 351 Train Loss: 0.6455
Epoch 13 Step 401 Train Loss: 0.5024
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0028. 
Epoch 14 Step 1 Train Loss: 0.4839
Epoch 14 Step 51 Train Loss: 0.5001
Epoch 14 Step 101 Train Loss: 0.5009
Epoch 14 Step 151 Train Loss: 0.5495
Epoch 14 Step 201 Train Loss: 0.6261
Epoch 14 Step 251 Train Loss: 0.5752
Epoch 14 Step 301 Train Loss: 0.5091
Epoch 14 Step 351 Train Loss: 0.4107
Epoch 14 Step 401 Train Loss: 0.4566
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0029. 
Epoch 15 Step 1 Train Loss: 0.4227
Epoch 15 Step 51 Train Loss: 0.4389
Epoch 15 Step 101 Train Loss: 0.8908
Epoch 15 Step 151 Train Loss: 0.4372
Epoch 15 Step 201 Train Loss: 0.3990
Epoch 15 Step 251 Train Loss: 0.5296
Epoch 15 Step 301 Train Loss: 0.3862
Epoch 15 Step 351 Train Loss: 0.5072
Epoch 15 Step 401 Train Loss: 0.4806
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0028. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0022
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00040728258
test_unseen_single_pearson: 0.9993484772193644
test_unseen_single_mse_de: 0.002206518
test_unseen_single_pearson_de: 0.9993294316613567
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5562106608842998
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0625
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.002206517963266139
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.021 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÜ‚ñÑ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñá‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñà
wandb:                                                   val_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00221
wandb:                                              test_de_pearson 0.99933
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0625
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00041
wandb:                                test_mse_top20_de_non_dropout 0.00221
wandb:                                                 test_pearson 0.99935
wandb:                                           test_pearson_delta 0.55621
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0625
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00041
wandb:                                    test_unseen_single_mse_de 0.00221
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00221
wandb:                                   test_unseen_single_pearson 0.99935
wandb:                                test_unseen_single_pearson_de 0.99933
wandb:                             test_unseen_single_pearson_delta 0.55621
wandb:                                                 train_de_mse 0.00204
wandb:                                             train_de_pearson 0.99949
wandb:                                                    train_mse 0.0004
wandb:                                                train_pearson 0.99936
wandb:                                                training_loss 0.55244
wandb:                                                   val_de_mse 0.00275
wandb:                                               val_de_pearson 0.99965
wandb:                                                      val_mse 0.00063
wandb:                                                  val_pearson 0.999
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396861_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/kgayzrqq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_022742-kgayzrqq/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_023851-bcdlgm40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396861_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/bcdlgm40
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5216
Epoch 1 Step 51 Train Loss: 0.4833
Epoch 1 Step 101 Train Loss: 0.5696
Epoch 1 Step 151 Train Loss: 0.4929
Epoch 1 Step 201 Train Loss: 0.6995
Epoch 1 Step 251 Train Loss: 0.4808
Epoch 1 Step 301 Train Loss: 0.4854
Epoch 1 Step 351 Train Loss: 0.6588
Epoch 1 Step 401 Train Loss: 0.5041
Epoch 1: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0016. 
Epoch 2 Step 1 Train Loss: 0.4918
Epoch 2 Step 51 Train Loss: 0.6109
Epoch 2 Step 101 Train Loss: 0.4588
Epoch 2 Step 151 Train Loss: 0.4735
Epoch 2 Step 201 Train Loss: 0.4990
Epoch 2 Step 251 Train Loss: 0.6103
Epoch 2 Step 301 Train Loss: 0.4438
Epoch 2 Step 351 Train Loss: 0.4336
Epoch 2 Step 401 Train Loss: 0.4244
Epoch 2: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0010. 
Epoch 3 Step 1 Train Loss: 0.5005
Epoch 3 Step 51 Train Loss: 0.4772
Epoch 3 Step 101 Train Loss: 0.4325
Epoch 3 Step 151 Train Loss: 0.4651
Epoch 3 Step 201 Train Loss: 0.5362
Epoch 3 Step 251 Train Loss: 0.6125
Epoch 3 Step 301 Train Loss: 0.7386
Epoch 3 Step 351 Train Loss: 0.4775
Epoch 3 Step 401 Train Loss: 0.5930
Epoch 3: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0004. 
Epoch 4 Step 1 Train Loss: 0.4315
Epoch 4 Step 51 Train Loss: 0.5189
Epoch 4 Step 101 Train Loss: 0.4414
Epoch 4 Step 151 Train Loss: 0.6253
Epoch 4 Step 201 Train Loss: 0.3964
Epoch 4 Step 251 Train Loss: 0.5851
Epoch 4 Step 301 Train Loss: 0.4286
Epoch 4 Step 351 Train Loss: 0.4341
Epoch 4 Step 401 Train Loss: 0.5776
Epoch 4: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0006. 
Epoch 5 Step 1 Train Loss: 0.5341
Epoch 5 Step 51 Train Loss: 0.5757
Epoch 5 Step 101 Train Loss: 0.6029
Epoch 5 Step 151 Train Loss: 0.4505
Epoch 5 Step 201 Train Loss: 0.5549
Epoch 5 Step 251 Train Loss: 0.5116
Epoch 5 Step 301 Train Loss: 0.4899
Epoch 5 Step 351 Train Loss: 0.3936
Epoch 5 Step 401 Train Loss: 0.5466
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0006. 
Epoch 6 Step 1 Train Loss: 0.5902
Epoch 6 Step 51 Train Loss: 0.5748
Epoch 6 Step 101 Train Loss: 0.6517
Epoch 6 Step 151 Train Loss: 0.5783
Epoch 6 Step 201 Train Loss: 0.4248
Epoch 6 Step 251 Train Loss: 0.6572
Epoch 6 Step 301 Train Loss: 0.4830
Epoch 6 Step 351 Train Loss: 0.3972
Epoch 6 Step 401 Train Loss: 0.5197
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 7 Step 1 Train Loss: 0.3881
Epoch 7 Step 51 Train Loss: 0.5603
Epoch 7 Step 101 Train Loss: 0.4216
Epoch 7 Step 151 Train Loss: 0.5335
Epoch 7 Step 201 Train Loss: 0.4448
Epoch 7 Step 251 Train Loss: 0.4019
Epoch 7 Step 301 Train Loss: 0.4398
Epoch 7 Step 351 Train Loss: 0.3924
Epoch 7 Step 401 Train Loss: 0.5618
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0006. 
Epoch 8 Step 1 Train Loss: 0.4349
Epoch 8 Step 51 Train Loss: 0.4282
Epoch 8 Step 101 Train Loss: 0.4514
Epoch 8 Step 151 Train Loss: 0.5180
Epoch 8 Step 201 Train Loss: 0.5252
Epoch 8 Step 251 Train Loss: 0.5169
Epoch 8 Step 301 Train Loss: 0.4988
Epoch 8 Step 351 Train Loss: 0.3946
Epoch 8 Step 401 Train Loss: 0.6211
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 9 Step 1 Train Loss: 0.4943
Epoch 9 Step 51 Train Loss: 0.5014
Epoch 9 Step 101 Train Loss: 0.5289
Epoch 9 Step 151 Train Loss: 0.4916
Epoch 9 Step 201 Train Loss: 0.4897
Epoch 9 Step 251 Train Loss: 0.5143
Epoch 9 Step 301 Train Loss: 0.5508
Epoch 9 Step 351 Train Loss: 0.4939
Epoch 9 Step 401 Train Loss: 0.5898
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 10 Step 1 Train Loss: 0.6365
Epoch 10 Step 51 Train Loss: 0.6409
Epoch 10 Step 101 Train Loss: 0.5422
Epoch 10 Step 151 Train Loss: 0.4875
Epoch 10 Step 201 Train Loss: 0.4813
Epoch 10 Step 251 Train Loss: 0.6410
Epoch 10 Step 301 Train Loss: 0.4929
Epoch 10 Step 351 Train Loss: 0.5909
Epoch 10 Step 401 Train Loss: 0.5433
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 11 Step 1 Train Loss: 0.5692
Epoch 11 Step 51 Train Loss: 0.4313
Epoch 11 Step 101 Train Loss: 0.4363
Epoch 11 Step 151 Train Loss: 0.5407
Epoch 11 Step 201 Train Loss: 0.4926
Epoch 11 Step 251 Train Loss: 0.5134
Epoch 11 Step 301 Train Loss: 0.5074
Epoch 11 Step 351 Train Loss: 0.4416
Epoch 11 Step 401 Train Loss: 0.7245
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 12 Step 1 Train Loss: 0.4494
Epoch 12 Step 51 Train Loss: 0.5906
Epoch 12 Step 101 Train Loss: 0.4665
Epoch 12 Step 151 Train Loss: 0.6282
Epoch 12 Step 201 Train Loss: 0.5167
Epoch 12 Step 251 Train Loss: 0.4490
Epoch 12 Step 301 Train Loss: 0.4430
Epoch 12 Step 351 Train Loss: 0.5178
Epoch 12 Step 401 Train Loss: 0.5534
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0006. 
Epoch 13 Step 1 Train Loss: 0.4325
Epoch 13 Step 51 Train Loss: 0.4859
Epoch 13 Step 101 Train Loss: 0.6821
Epoch 13 Step 151 Train Loss: 0.4536
Epoch 13 Step 201 Train Loss: 0.5915
Epoch 13 Step 251 Train Loss: 0.4268
Epoch 13 Step 301 Train Loss: 0.6124
Epoch 13 Step 351 Train Loss: 0.4646
Epoch 13 Step 401 Train Loss: 0.4211
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0007. 
Epoch 14 Step 1 Train Loss: 0.4901
Epoch 14 Step 51 Train Loss: 0.4079
Epoch 14 Step 101 Train Loss: 0.4866
Epoch 14 Step 151 Train Loss: 0.5476
Epoch 14 Step 201 Train Loss: 0.4074
Epoch 14 Step 251 Train Loss: 0.4941
Epoch 14 Step 301 Train Loss: 0.5522
Epoch 14 Step 351 Train Loss: 0.5444
Epoch 14 Step 401 Train Loss: 0.5296
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0016 Validation Top 20 DE MSE: 0.0007. 
Epoch 15 Step 1 Train Loss: 0.4856
Epoch 15 Step 51 Train Loss: 0.4824
Epoch 15 Step 101 Train Loss: 0.4559
Epoch 15 Step 151 Train Loss: 0.4215
Epoch 15 Step 201 Train Loss: 0.4815
Epoch 15 Step 251 Train Loss: 0.5081
Epoch 15 Step 301 Train Loss: 0.4514
Epoch 15 Step 351 Train Loss: 0.5653
Epoch 15 Step 401 Train Loss: 0.5210
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0006. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0021
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0005980723
test_unseen_single_pearson: 0.9990406589495694
test_unseen_single_mse_de: 0.002120596
test_unseen_single_pearson_de: 0.9994268859508191
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.484568343552923
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0875
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.00239858559008358
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñÑ‚ñÖ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00212
wandb:                                              test_de_pearson 0.99943
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0875
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.0006
wandb:                                test_mse_top20_de_non_dropout 0.0024
wandb:                                                 test_pearson 0.99904
wandb:                                           test_pearson_delta 0.48457
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0875
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.0006
wandb:                                    test_unseen_single_mse_de 0.00212
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0024
wandb:                                   test_unseen_single_pearson 0.99904
wandb:                                test_unseen_single_pearson_de 0.99943
wandb:                             test_unseen_single_pearson_delta 0.48457
wandb:                                                 train_de_mse 0.00176
wandb:                                             train_de_pearson 0.9996
wandb:                                                    train_mse 0.00029
wandb:                                                train_pearson 0.99953
wandb:                                                training_loss 0.56134
wandb:                                                   val_de_mse 0.00061
wandb:                                               val_de_pearson 0.99987
wandb:                                                      val_mse 0.00043
wandb:                                                  val_pearson 0.99929
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396861_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/bcdlgm40
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_023851-bcdlgm40/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:4
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_024945-v1dneldb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_Dixit_GSM2396861_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/v1dneldb
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5677
Epoch 1 Step 51 Train Loss: 0.5113
Epoch 1 Step 101 Train Loss: 0.3923
Epoch 1 Step 151 Train Loss: 0.5047
Epoch 1 Step 201 Train Loss: 0.6272
Epoch 1 Step 251 Train Loss: 0.5758
Epoch 1 Step 301 Train Loss: 0.5244
Epoch 1 Step 351 Train Loss: 0.5018
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0056 Validation Top 20 DE MSE: 0.0097. 
Epoch 2 Step 1 Train Loss: 0.6990
Epoch 2 Step 51 Train Loss: 0.4670
Epoch 2 Step 101 Train Loss: 0.5273
Epoch 2 Step 151 Train Loss: 0.5095
Epoch 2 Step 201 Train Loss: 0.5370
Epoch 2 Step 251 Train Loss: 0.5170
Epoch 2 Step 301 Train Loss: 0.5036
Epoch 2 Step 351 Train Loss: 0.5660
Epoch 2: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0079. 
Epoch 3 Step 1 Train Loss: 0.4464
Epoch 3 Step 51 Train Loss: 0.5341
Epoch 3 Step 101 Train Loss: 0.4936
Epoch 3 Step 151 Train Loss: 0.5472
Epoch 3 Step 201 Train Loss: 0.4919
Epoch 3 Step 251 Train Loss: 0.6134
Epoch 3 Step 301 Train Loss: 0.5367
Epoch 3 Step 351 Train Loss: 0.7666
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0033. 
Epoch 4 Step 1 Train Loss: 0.4948
Epoch 4 Step 51 Train Loss: 0.4678
Epoch 4 Step 101 Train Loss: 0.4731
Epoch 4 Step 151 Train Loss: 0.5340
Epoch 4 Step 201 Train Loss: 0.4335
Epoch 4 Step 251 Train Loss: 0.4818
Epoch 4 Step 301 Train Loss: 0.4293
Epoch 4 Step 351 Train Loss: 0.4464
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0020 Validation Top 20 DE MSE: 0.0036. 
Epoch 5 Step 1 Train Loss: 0.5086
Epoch 5 Step 51 Train Loss: 0.4812
Epoch 5 Step 101 Train Loss: 0.6363
Epoch 5 Step 151 Train Loss: 0.5178
Epoch 5 Step 201 Train Loss: 0.4047
Epoch 5 Step 251 Train Loss: 0.6451
Epoch 5 Step 301 Train Loss: 0.4939
Epoch 5 Step 351 Train Loss: 0.4341
Epoch 5: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0023 Validation Top 20 DE MSE: 0.0046. 
Epoch 6 Step 1 Train Loss: 0.4664
Epoch 6 Step 51 Train Loss: 0.4281
Epoch 6 Step 101 Train Loss: 0.5220
Epoch 6 Step 151 Train Loss: 0.4749
Epoch 6 Step 201 Train Loss: 0.4624
Epoch 6 Step 251 Train Loss: 0.4728
Epoch 6 Step 301 Train Loss: 0.5591
Epoch 6 Step 351 Train Loss: 0.5022
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0021 Validation Top 20 DE MSE: 0.0044. 
Epoch 7 Step 1 Train Loss: 0.4945
Epoch 7 Step 51 Train Loss: 0.5301
Epoch 7 Step 101 Train Loss: 0.5776
Epoch 7 Step 151 Train Loss: 0.6072
Epoch 7 Step 201 Train Loss: 0.5245
Epoch 7 Step 251 Train Loss: 0.3876
Epoch 7 Step 301 Train Loss: 0.3891
Epoch 7 Step 351 Train Loss: 0.5435
Epoch 7: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0029. 
Epoch 8 Step 1 Train Loss: 0.5434
Epoch 8 Step 51 Train Loss: 0.5080
Epoch 8 Step 101 Train Loss: 0.4410
Epoch 8 Step 151 Train Loss: 0.4674
Epoch 8 Step 201 Train Loss: 0.4952
Epoch 8 Step 251 Train Loss: 0.4443
Epoch 8 Step 301 Train Loss: 0.5351
Epoch 8 Step 351 Train Loss: 0.4157
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0017 Validation Top 20 DE MSE: 0.0028. 
Epoch 9 Step 1 Train Loss: 0.5063
Epoch 9 Step 51 Train Loss: 0.5333
Epoch 9 Step 101 Train Loss: 0.7669
Epoch 9 Step 151 Train Loss: 0.5153
Epoch 9 Step 201 Train Loss: 0.6471
Epoch 9 Step 251 Train Loss: 0.5610
Epoch 9 Step 301 Train Loss: 0.5619
Epoch 9 Step 351 Train Loss: 0.5687
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0031. 
Epoch 10 Step 1 Train Loss: 0.4722
Epoch 10 Step 51 Train Loss: 0.5901
Epoch 10 Step 101 Train Loss: 0.4192
Epoch 10 Step 151 Train Loss: 0.4895
Epoch 10 Step 201 Train Loss: 0.4904
Epoch 10 Step 251 Train Loss: 0.4396
Epoch 10 Step 301 Train Loss: 0.4105
Epoch 10 Step 351 Train Loss: 0.4393
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0032. 
Epoch 11 Step 1 Train Loss: 0.5496
Epoch 11 Step 51 Train Loss: 0.4747
Epoch 11 Step 101 Train Loss: 0.6909
Epoch 11 Step 151 Train Loss: 0.4381
Epoch 11 Step 201 Train Loss: 0.4399
Epoch 11 Step 251 Train Loss: 0.4795
Epoch 11 Step 301 Train Loss: 0.4784
Epoch 11 Step 351 Train Loss: 0.5243
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0031. 
Epoch 12 Step 1 Train Loss: 0.4809
Epoch 12 Step 51 Train Loss: 0.4023
Epoch 12 Step 101 Train Loss: 0.5464
Epoch 12 Step 151 Train Loss: 0.4913
Epoch 12 Step 201 Train Loss: 0.5019
Epoch 12 Step 251 Train Loss: 0.6468
Epoch 12 Step 301 Train Loss: 0.5528
Epoch 12 Step 351 Train Loss: 0.5087
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0019 Validation Top 20 DE MSE: 0.0032. 
Epoch 13 Step 1 Train Loss: 0.5500
Epoch 13 Step 51 Train Loss: 0.6070
Epoch 13 Step 101 Train Loss: 0.5603
Epoch 13 Step 151 Train Loss: 0.4254
Epoch 13 Step 201 Train Loss: 0.4577
Epoch 13 Step 251 Train Loss: 0.5800
Epoch 13 Step 301 Train Loss: 0.5470
Epoch 13 Step 351 Train Loss: 0.5079
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0031. 
Epoch 14 Step 1 Train Loss: 0.4509
Epoch 14 Step 51 Train Loss: 0.6929
Epoch 14 Step 101 Train Loss: 0.4234
Epoch 14 Step 151 Train Loss: 0.4971
Epoch 14 Step 201 Train Loss: 0.5356
Epoch 14 Step 251 Train Loss: 0.6559
Epoch 14 Step 301 Train Loss: 0.5207
Epoch 14 Step 351 Train Loss: 0.6391
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0029. 
Epoch 15 Step 1 Train Loss: 0.5291
Epoch 15 Step 51 Train Loss: 0.5695
Epoch 15 Step 101 Train Loss: 0.5212
Epoch 15 Step 151 Train Loss: 0.5147
Epoch 15 Step 201 Train Loss: 0.6490
Epoch 15 Step 251 Train Loss: 0.4867
Epoch 15 Step 301 Train Loss: 0.5122
Epoch 15 Step 351 Train Loss: 0.3972
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0018 Validation Top 20 DE MSE: 0.0029. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0023
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00051593373
test_unseen_single_pearson: 0.9991680653992281
test_unseen_single_mse_de: 0.002254605
test_unseen_single_pearson_de: 0.9993842477228241
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5477338951173342
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.037500000000000006
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9875
test_unseen_single_mse_top20_de_non_dropout: 0.00250093288664902
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.001 MB uploadedwandb: - 0.001 MB of 0.022 MB uploadedwandb: \ 0.007 MB of 0.022 MB uploadedwandb: | 0.007 MB of 0.022 MB uploadedwandb: / 0.007 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00225
wandb:                                              test_de_pearson 0.99938
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0375
wandb:                          test_frac_sigma_below_1_non_dropout 0.9875
wandb:                                                     test_mse 0.00052
wandb:                                test_mse_top20_de_non_dropout 0.0025
wandb:                                                 test_pearson 0.99917
wandb:                                           test_pearson_delta 0.54773
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9875
wandb:                                       test_unseen_single_mse 0.00052
wandb:                                    test_unseen_single_mse_de 0.00225
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0025
wandb:                                   test_unseen_single_pearson 0.99917
wandb:                                test_unseen_single_pearson_de 0.99938
wandb:                             test_unseen_single_pearson_delta 0.54773
wandb:                                                 train_de_mse 0.00181
wandb:                                             train_de_pearson 0.99957
wandb:                                                    train_mse 0.00051
wandb:                                                train_pearson 0.99918
wandb:                                                training_loss 0.41685
wandb:                                                   val_de_mse 0.00295
wandb:                                               val_de_pearson 0.99921
wandb:                                                      val_mse 0.00039
wandb:                                                  val_pearson 0.99936
wandb: 
wandb: üöÄ View run geneformer_Dixit_GSM2396861_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/v1dneldb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_024945-v1dneldb/logs
