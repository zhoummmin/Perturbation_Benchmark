Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_030331-ebpo5ekv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_XuCao2023_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/ebpo5ekv
wandb: WARNING Serializing object of type ndarray that is 21184640 bytes
  0%|                                                  | 0/2098 [00:00<?, ?it/s]  1%|‚ñè                                       | 12/2098 [00:00<00:20, 103.44it/s]  1%|‚ñç                                       | 25/2098 [00:00<00:18, 113.80it/s]  2%|‚ñä                                       | 43/2098 [00:00<00:14, 137.43it/s]  3%|‚ñà‚ñè                                      | 60/2098 [00:00<00:13, 147.60it/s]  4%|‚ñà‚ñç                                      | 76/2098 [00:00<00:13, 146.84it/s]  4%|‚ñà‚ñä                                      | 93/2098 [00:00<00:13, 153.31it/s]  5%|‚ñà‚ñà                                     | 109/2098 [00:00<00:12, 153.76it/s]  6%|‚ñà‚ñà‚ñé                                    | 125/2098 [00:00<00:13, 151.65it/s]  7%|‚ñà‚ñà‚ñã                                    | 142/2098 [00:00<00:12, 156.80it/s]  8%|‚ñà‚ñà‚ñâ                                    | 158/2098 [00:01<00:12, 156.41it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                   | 174/2098 [00:01<00:12, 156.80it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                   | 190/2098 [00:01<00:12, 156.68it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                   | 206/2098 [00:01<00:12, 149.93it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 224/2098 [00:01<00:11, 158.32it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 240/2098 [00:01<00:11, 158.03it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 256/2098 [00:01<00:11, 156.63it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 272/2098 [00:01<00:11, 156.13it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 288/2098 [00:01<00:11, 154.11it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 305/2098 [00:02<00:11, 157.66it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 321/2098 [00:02<00:11, 157.12it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 337/2098 [00:02<00:11, 154.54it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 354/2098 [00:02<00:11, 157.72it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 370/2098 [00:02<00:11, 156.98it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 386/2098 [00:02<00:11, 154.36it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 402/2098 [00:02<00:10, 154.76it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 419/2098 [00:02<00:10, 158.19it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 435/2098 [00:02<00:10, 154.77it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 452/2098 [00:02<00:10, 155.41it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 469/2098 [00:03<00:10, 157.65it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 485/2098 [00:03<00:10, 154.76it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 501/2098 [00:03<00:10, 155.63it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 518/2098 [00:03<00:09, 158.24it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 534/2098 [00:03<00:10, 155.50it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 551/2098 [00:03<00:09, 159.42it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 567/2098 [00:03<00:09, 157.96it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 583/2098 [00:03<00:09, 154.24it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 600/2098 [00:03<00:09, 155.71it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 616/2098 [00:04<00:09, 151.44it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 632/2098 [00:04<00:10, 139.90it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 647/2098 [00:04<00:10, 134.49it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 661/2098 [00:04<00:10, 131.37it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 675/2098 [00:04<00:11, 128.67it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 688/2098 [00:04<00:11, 120.49it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 703/2098 [00:04<00:10, 127.23it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 716/2098 [00:04<00:11, 125.35it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 729/2098 [00:04<00:11, 121.77it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 743/2098 [00:05<00:10, 124.29it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 756/2098 [00:05<00:10, 123.24it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 769/2098 [00:05<00:11, 120.57it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 782/2098 [00:05<00:10, 122.98it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 795/2098 [00:05<00:10, 123.52it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 808/2098 [00:05<00:10, 120.03it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 821/2098 [00:05<00:10, 116.86it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 836/2098 [00:05<00:10, 123.33it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 849/2098 [00:05<00:10, 123.49it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 862/2098 [00:06<00:09, 123.78it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 875/2098 [00:06<00:10, 120.07it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 889/2098 [00:06<00:09, 122.66it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 902/2098 [00:06<00:09, 122.57it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 915/2098 [00:06<00:09, 122.25it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 928/2098 [00:06<00:09, 120.11it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 942/2098 [00:06<00:09, 121.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 955/2098 [00:06<00:09, 123.66it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 968/2098 [00:06<00:09, 122.68it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 981/2098 [00:07<00:09, 120.38it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 994/2098 [00:07<00:09, 120.19it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1007/2098 [00:07<00:09, 119.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1021/2098 [00:07<00:08, 123.31it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1034/2098 [00:07<00:09, 117.76it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1049/2098 [00:07<00:08, 125.34it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1062/2098 [00:07<00:08, 125.00it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1075/2098 [00:07<00:08, 125.07it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1088/2098 [00:07<00:08, 122.95it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1102/2098 [00:07<00:07, 127.21it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1115/2098 [00:08<00:07, 126.88it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1128/2098 [00:08<00:07, 126.82it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1141/2098 [00:08<00:07, 126.63it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1154/2098 [00:08<00:07, 126.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 1167/2098 [00:08<00:07, 124.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 1181/2098 [00:08<00:07, 127.51it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 1194/2098 [00:08<00:07, 126.78it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 1207/2098 [00:08<00:07, 126.47it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1220/2098 [00:08<00:07, 123.56it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 1234/2098 [00:09<00:06, 127.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 1247/2098 [00:09<00:06, 126.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 1260/2098 [00:09<00:06, 126.20it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1273/2098 [00:09<00:06, 125.94it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 1286/2098 [00:09<00:06, 125.77it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 1299/2098 [00:09<00:06, 122.91it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 1313/2098 [00:09<00:06, 123.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 1327/2098 [00:09<00:06, 127.03it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 1340/2098 [00:09<00:05, 126.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 1353/2098 [00:09<00:06, 123.84it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 1367/2098 [00:10<00:05, 124.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 1381/2098 [00:10<00:05, 127.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 1394/2098 [00:10<00:05, 124.17it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 1407/2098 [00:10<00:05, 124.28it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 1420/2098 [00:10<00:05, 125.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 1434/2098 [00:10<00:05, 125.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 1448/2098 [00:10<00:05, 127.83it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 1461/2098 [00:10<00:05, 124.78it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 1474/2098 [00:10<00:05, 121.57it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 1489/2098 [00:11<00:04, 127.92it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 1502/2098 [00:11<00:04, 124.59it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 1516/2098 [00:11<00:04, 124.41it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 1529/2098 [00:11<00:04, 124.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 1543/2098 [00:11<00:04, 127.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 1556/2098 [00:11<00:04, 127.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 1569/2098 [00:11<00:04, 124.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 1583/2098 [00:11<00:04, 127.08it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 1596/2098 [00:11<00:03, 126.30it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 1609/2098 [00:12<00:03, 123.14it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 1623/2098 [00:12<00:03, 123.67it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 1637/2098 [00:12<00:03, 124.26it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 1651/2098 [00:12<00:03, 127.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 1664/2098 [00:12<00:03, 127.29it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 1677/2098 [00:12<00:03, 127.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 1690/2098 [00:12<00:03, 126.86it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 1703/2098 [00:12<00:03, 123.98it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 1717/2098 [00:12<00:03, 124.77it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 1731/2098 [00:12<00:02, 128.38it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 1744/2098 [00:13<00:02, 128.10it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 1757/2098 [00:13<00:02, 125.22it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 1770/2098 [00:13<00:02, 125.50it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 1783/2098 [00:13<00:02, 126.06it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 1797/2098 [00:13<00:02, 128.90it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 1810/2098 [00:13<00:02, 123.24it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 1824/2098 [00:13<00:02, 126.85it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1837/2098 [00:13<00:02, 124.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1852/2098 [00:13<00:01, 130.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1866/2098 [00:14<00:01, 129.30it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1879/2098 [00:14<00:01, 128.67it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1892/2098 [00:14<00:01, 128.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1905/2098 [00:14<00:01, 127.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1918/2098 [00:14<00:01, 127.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1931/2098 [00:14<00:01, 127.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1944/2098 [00:14<00:01, 127.11it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1957/2098 [00:14<00:01, 126.95it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1970/2098 [00:14<00:01, 124.23it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1984/2098 [00:14<00:00, 125.08it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1997/2098 [00:15<00:00, 125.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2010/2098 [00:15<00:00, 125.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2023/2098 [00:15<00:00, 125.71it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2036/2098 [00:15<00:00, 125.83it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2049/2098 [00:15<00:00, 126.04it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 2062/2098 [00:15<00:00, 125.92it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2075/2098 [00:15<00:00, 125.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2088/2098 [00:15<00:00, 125.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2098/2098 [00:15<00:00, 132.10it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.2595
Epoch 1 Step 51 Train Loss: 0.2522
Epoch 1 Step 101 Train Loss: 0.2581
Epoch 1 Step 151 Train Loss: 0.2398
Epoch 1 Step 201 Train Loss: 0.2411
Epoch 1 Step 251 Train Loss: 0.2337
Epoch 1 Step 301 Train Loss: 0.2545
Epoch 1 Step 351 Train Loss: 0.2632
Epoch 1 Step 401 Train Loss: 0.2120
Epoch 1 Step 451 Train Loss: 0.2544
Epoch 1 Step 501 Train Loss: 0.2327
Epoch 1 Step 551 Train Loss: 0.2367
Epoch 1 Step 601 Train Loss: 0.2413
Epoch 1 Step 651 Train Loss: 0.2443
Epoch 1 Step 701 Train Loss: 0.2471
Epoch 1 Step 751 Train Loss: 0.2291
Epoch 1 Step 801 Train Loss: 0.2380
Epoch 1 Step 851 Train Loss: 0.2304
Epoch 1 Step 901 Train Loss: 0.2533
Epoch 1 Step 951 Train Loss: 0.2343
Epoch 1 Step 1001 Train Loss: 0.2211
Epoch 1 Step 1051 Train Loss: 0.2001
Epoch 1 Step 1101 Train Loss: 0.2486
Epoch 1 Step 1151 Train Loss: 0.2188
Epoch 1 Step 1201 Train Loss: 0.2282
Epoch 1 Step 1251 Train Loss: 0.2445
Epoch 1 Step 1301 Train Loss: 0.2247
Epoch 1 Step 1351 Train Loss: 0.2558
Epoch 1 Step 1401 Train Loss: 0.2205
Epoch 1 Step 1451 Train Loss: 0.2356
Epoch 1 Step 1501 Train Loss: 0.1974
Epoch 1 Step 1551 Train Loss: 0.2231
Epoch 1 Step 1601 Train Loss: 0.2392
Epoch 1 Step 1651 Train Loss: 0.2130
Epoch 1 Step 1701 Train Loss: 0.2350
Epoch 1 Step 1751 Train Loss: 0.2488
Epoch 1 Step 1801 Train Loss: 0.2294
Epoch 1 Step 1851 Train Loss: 0.2180
Epoch 1: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0083. 
Epoch 2 Step 1 Train Loss: 0.2065
Epoch 2 Step 51 Train Loss: 0.2355
Epoch 2 Step 101 Train Loss: 0.2301
Epoch 2 Step 151 Train Loss: 0.2205
Epoch 2 Step 201 Train Loss: 0.2315
Epoch 2 Step 251 Train Loss: 0.2335
Epoch 2 Step 301 Train Loss: 0.1970
Epoch 2 Step 351 Train Loss: 0.2153
Epoch 2 Step 401 Train Loss: 0.2068
Epoch 2 Step 451 Train Loss: 0.2165
Epoch 2 Step 501 Train Loss: 0.2194
Epoch 2 Step 551 Train Loss: 0.2329
Epoch 2 Step 601 Train Loss: 0.2252
Epoch 2 Step 651 Train Loss: 0.2397
Epoch 2 Step 701 Train Loss: 0.2540
Epoch 2 Step 751 Train Loss: 0.2076
Epoch 2 Step 801 Train Loss: 0.2222
Epoch 2 Step 851 Train Loss: 0.2114
Epoch 2 Step 901 Train Loss: 0.2316
Epoch 2 Step 951 Train Loss: 0.2336
Epoch 2 Step 1001 Train Loss: 0.2497
Epoch 2 Step 1051 Train Loss: 0.2082
Epoch 2 Step 1101 Train Loss: 0.2159
Epoch 2 Step 1151 Train Loss: 0.2282
Epoch 2 Step 1201 Train Loss: 0.2184
Epoch 2 Step 1251 Train Loss: 0.2172
Epoch 2 Step 1301 Train Loss: 0.2493
Epoch 2 Step 1351 Train Loss: 0.2364
Epoch 2 Step 1401 Train Loss: 0.2555
Epoch 2 Step 1451 Train Loss: 0.2424
Epoch 2 Step 1501 Train Loss: 0.2603
Epoch 2 Step 1551 Train Loss: 0.2287
Epoch 2 Step 1601 Train Loss: 0.2187
Epoch 2 Step 1651 Train Loss: 0.2342
Epoch 2 Step 1701 Train Loss: 0.2246
Epoch 2 Step 1751 Train Loss: 0.2039
Epoch 2 Step 1801 Train Loss: 0.2147
Epoch 2 Step 1851 Train Loss: 0.2262
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0083. 
Epoch 3 Step 1 Train Loss: 0.2153
Epoch 3 Step 51 Train Loss: 0.2272
Epoch 3 Step 101 Train Loss: 0.2328
Epoch 3 Step 151 Train Loss: 0.2257
Epoch 3 Step 201 Train Loss: 0.2487
Epoch 3 Step 251 Train Loss: 0.2341
Epoch 3 Step 301 Train Loss: 0.2100
Epoch 3 Step 351 Train Loss: 0.2409
Epoch 3 Step 401 Train Loss: 0.2372
Epoch 3 Step 451 Train Loss: 0.2303
Epoch 3 Step 501 Train Loss: 0.2212
Epoch 3 Step 551 Train Loss: 0.2195
Epoch 3 Step 601 Train Loss: 0.2279
Epoch 3 Step 651 Train Loss: 0.2183
Epoch 3 Step 701 Train Loss: 0.2235
Epoch 3 Step 751 Train Loss: 0.2298
Epoch 3 Step 801 Train Loss: 0.2370
Epoch 3 Step 851 Train Loss: 0.2150
Epoch 3 Step 901 Train Loss: 0.2405
Epoch 3 Step 951 Train Loss: 0.2402
Epoch 3 Step 1001 Train Loss: 0.2338
Epoch 3 Step 1051 Train Loss: 0.2298
Epoch 3 Step 1101 Train Loss: 0.2508
Epoch 3 Step 1151 Train Loss: 0.2150
Epoch 3 Step 1201 Train Loss: 0.2408
Epoch 3 Step 1251 Train Loss: 0.2248
Epoch 3 Step 1301 Train Loss: 0.2247
Epoch 3 Step 1351 Train Loss: 0.2479
Epoch 3 Step 1401 Train Loss: 0.2245
Epoch 3 Step 1451 Train Loss: 0.2305
Epoch 3 Step 1501 Train Loss: 0.2307
Epoch 3 Step 1551 Train Loss: 0.2303
Epoch 3 Step 1601 Train Loss: 0.2404
Epoch 3 Step 1651 Train Loss: 0.2433
Epoch 3 Step 1701 Train Loss: 0.2319
Epoch 3 Step 1751 Train Loss: 0.2265
Epoch 3 Step 1801 Train Loss: 0.2180
Epoch 3 Step 1851 Train Loss: 0.2468
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 4 Step 1 Train Loss: 0.2173
Epoch 4 Step 51 Train Loss: 0.2352
Epoch 4 Step 101 Train Loss: 0.2374
Epoch 4 Step 151 Train Loss: 0.2331
Epoch 4 Step 201 Train Loss: 0.2297
Epoch 4 Step 251 Train Loss: 0.2706
Epoch 4 Step 301 Train Loss: 0.2277
Epoch 4 Step 351 Train Loss: 0.2426
Epoch 4 Step 401 Train Loss: 0.2460
Epoch 4 Step 451 Train Loss: 0.2447
Epoch 4 Step 501 Train Loss: 0.2319
Epoch 4 Step 551 Train Loss: 0.2588
Epoch 4 Step 601 Train Loss: 0.2446
Epoch 4 Step 651 Train Loss: 0.2335
Epoch 4 Step 701 Train Loss: 0.2409
Epoch 4 Step 751 Train Loss: 0.2200
Epoch 4 Step 801 Train Loss: 0.2195
Epoch 4 Step 851 Train Loss: 0.2276
Epoch 4 Step 901 Train Loss: 0.2184
Epoch 4 Step 951 Train Loss: 0.2495
Epoch 4 Step 1001 Train Loss: 0.2419
Epoch 4 Step 1051 Train Loss: 0.2434
Epoch 4 Step 1101 Train Loss: 0.2476
Epoch 4 Step 1151 Train Loss: 0.2334
Epoch 4 Step 1201 Train Loss: 0.2364
Epoch 4 Step 1251 Train Loss: 0.2269
Epoch 4 Step 1301 Train Loss: 0.2484
Epoch 4 Step 1351 Train Loss: 0.2320
Epoch 4 Step 1401 Train Loss: 0.2123
Epoch 4 Step 1451 Train Loss: 0.2600
Epoch 4 Step 1501 Train Loss: 0.2462
Epoch 4 Step 1551 Train Loss: 0.2461
Epoch 4 Step 1601 Train Loss: 0.2252
Epoch 4 Step 1651 Train Loss: 0.2263
Epoch 4 Step 1701 Train Loss: 0.2222
Epoch 4 Step 1751 Train Loss: 0.2343
Epoch 4 Step 1801 Train Loss: 0.2332
Epoch 4 Step 1851 Train Loss: 0.2605
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 5 Step 1 Train Loss: 0.2449
Epoch 5 Step 51 Train Loss: 0.2592
Epoch 5 Step 101 Train Loss: 0.2420
Epoch 5 Step 151 Train Loss: 0.2430
Epoch 5 Step 201 Train Loss: 0.2324
Epoch 5 Step 251 Train Loss: 0.2490
Epoch 5 Step 301 Train Loss: 0.2501
Epoch 5 Step 351 Train Loss: 0.2346
Epoch 5 Step 401 Train Loss: 0.2451
Epoch 5 Step 451 Train Loss: 0.2323
Epoch 5 Step 501 Train Loss: 0.2655
Epoch 5 Step 551 Train Loss: 0.2364
Epoch 5 Step 601 Train Loss: 0.2244
Epoch 5 Step 651 Train Loss: 0.2389
Epoch 5 Step 701 Train Loss: 0.2297
Epoch 5 Step 751 Train Loss: 0.2322
Epoch 5 Step 801 Train Loss: 0.2508
Epoch 5 Step 851 Train Loss: 0.2202
Epoch 5 Step 901 Train Loss: 0.2446
Epoch 5 Step 951 Train Loss: 0.2412
Epoch 5 Step 1001 Train Loss: 0.2627
Epoch 5 Step 1051 Train Loss: 0.2368
Epoch 5 Step 1101 Train Loss: 0.2564
Epoch 5 Step 1151 Train Loss: 0.2524
Epoch 5 Step 1201 Train Loss: 0.2459
Epoch 5 Step 1251 Train Loss: 0.2339
Epoch 5 Step 1301 Train Loss: 0.2459
Epoch 5 Step 1351 Train Loss: 0.2505
Epoch 5 Step 1401 Train Loss: 0.2249
Epoch 5 Step 1451 Train Loss: 0.2496
Epoch 5 Step 1501 Train Loss: 0.2431
Epoch 5 Step 1551 Train Loss: 0.2468
Epoch 5 Step 1601 Train Loss: 0.2483
Epoch 5 Step 1651 Train Loss: 0.2361
Epoch 5 Step 1701 Train Loss: 0.2285
Epoch 5 Step 1751 Train Loss: 0.2164
Epoch 5 Step 1801 Train Loss: 0.2434
Epoch 5 Step 1851 Train Loss: 0.2131
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 6 Step 1 Train Loss: 0.2288
Epoch 6 Step 51 Train Loss: 0.2458
Epoch 6 Step 101 Train Loss: 0.2452
Epoch 6 Step 151 Train Loss: 0.2466
Epoch 6 Step 201 Train Loss: 0.2510
Epoch 6 Step 251 Train Loss: 0.2201
Epoch 6 Step 301 Train Loss: 0.2245
Epoch 6 Step 351 Train Loss: 0.2388
Epoch 6 Step 401 Train Loss: 0.2417
Epoch 6 Step 451 Train Loss: 0.2179
Epoch 6 Step 501 Train Loss: 0.2406
Epoch 6 Step 551 Train Loss: 0.2399
Epoch 6 Step 601 Train Loss: 0.2507
Epoch 6 Step 651 Train Loss: 0.2542
Epoch 6 Step 701 Train Loss: 0.2706
Epoch 6 Step 751 Train Loss: 0.2341
Epoch 6 Step 801 Train Loss: 0.2260
Epoch 6 Step 851 Train Loss: 0.2508
Epoch 6 Step 901 Train Loss: 0.2205
Epoch 6 Step 951 Train Loss: 0.2395
Epoch 6 Step 1001 Train Loss: 0.2401
Epoch 6 Step 1051 Train Loss: 0.2331
Epoch 6 Step 1101 Train Loss: 0.2506
Epoch 6 Step 1151 Train Loss: 0.2649
Epoch 6 Step 1201 Train Loss: 0.2359
Epoch 6 Step 1251 Train Loss: 0.2354
Epoch 6 Step 1301 Train Loss: 0.2512
Epoch 6 Step 1351 Train Loss: 0.2483
Epoch 6 Step 1401 Train Loss: 0.2408
Epoch 6 Step 1451 Train Loss: 0.2451
Epoch 6 Step 1501 Train Loss: 0.2268
Epoch 6 Step 1551 Train Loss: 0.2410
Epoch 6 Step 1601 Train Loss: 0.2389
Epoch 6 Step 1651 Train Loss: 0.2385
Epoch 6 Step 1701 Train Loss: 0.2300
Epoch 6 Step 1751 Train Loss: 0.2377
Epoch 6 Step 1801 Train Loss: 0.2427
Epoch 6 Step 1851 Train Loss: 0.2547
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 7 Step 1 Train Loss: 0.2343
Epoch 7 Step 51 Train Loss: 0.2218
Epoch 7 Step 101 Train Loss: 0.2324
Epoch 7 Step 151 Train Loss: 0.2492
Epoch 7 Step 201 Train Loss: 0.2304
Epoch 7 Step 251 Train Loss: 0.2437
Epoch 7 Step 301 Train Loss: 0.2352
Epoch 7 Step 351 Train Loss: 0.2315
Epoch 7 Step 401 Train Loss: 0.2338
Epoch 7 Step 451 Train Loss: 0.2342
Epoch 7 Step 501 Train Loss: 0.2549
Epoch 7 Step 551 Train Loss: 0.2455
Epoch 7 Step 601 Train Loss: 0.2364
Epoch 7 Step 651 Train Loss: 0.2431
Epoch 7 Step 701 Train Loss: 0.2524
Epoch 7 Step 751 Train Loss: 0.2326
Epoch 7 Step 801 Train Loss: 0.2143
Epoch 7 Step 851 Train Loss: 0.2207
Epoch 7 Step 901 Train Loss: 0.2398
Epoch 7 Step 951 Train Loss: 0.2457
Epoch 7 Step 1001 Train Loss: 0.2344
Epoch 7 Step 1051 Train Loss: 0.2363
Epoch 7 Step 1101 Train Loss: 0.2495
Epoch 7 Step 1151 Train Loss: 0.2354
Epoch 7 Step 1201 Train Loss: 0.2461
Epoch 7 Step 1251 Train Loss: 0.2273
Epoch 7 Step 1301 Train Loss: 0.2374
Epoch 7 Step 1351 Train Loss: 0.2254
Epoch 7 Step 1401 Train Loss: 0.2436
Epoch 7 Step 1451 Train Loss: 0.2390
Epoch 7 Step 1501 Train Loss: 0.2436
Epoch 7 Step 1551 Train Loss: 0.2281
Epoch 7 Step 1601 Train Loss: 0.2394
Epoch 7 Step 1651 Train Loss: 0.2406
Epoch 7 Step 1701 Train Loss: 0.2294
Epoch 7 Step 1751 Train Loss: 0.2180
Epoch 7 Step 1801 Train Loss: 0.2560
Epoch 7 Step 1851 Train Loss: 0.2358
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Epoch 8 Step 1 Train Loss: 0.2180
Epoch 8 Step 51 Train Loss: 0.2295
Epoch 8 Step 101 Train Loss: 0.2291
Epoch 8 Step 151 Train Loss: 0.2303
Epoch 8 Step 201 Train Loss: 0.2332
Epoch 8 Step 251 Train Loss: 0.2573
Epoch 8 Step 301 Train Loss: 0.2422
Epoch 8 Step 351 Train Loss: 0.2390
Epoch 8 Step 401 Train Loss: 0.2244
Epoch 8 Step 451 Train Loss: 0.2445
Epoch 8 Step 501 Train Loss: 0.2342
Epoch 8 Step 551 Train Loss: 0.2369
Epoch 8 Step 601 Train Loss: 0.2369
Epoch 8 Step 651 Train Loss: 0.2221
Epoch 8 Step 701 Train Loss: 0.2368
Epoch 8 Step 751 Train Loss: 0.2440
Epoch 8 Step 801 Train Loss: 0.2444
Epoch 8 Step 851 Train Loss: 0.2413
Epoch 8 Step 901 Train Loss: 0.2274
Epoch 8 Step 951 Train Loss: 0.2506
Epoch 8 Step 1001 Train Loss: 0.2504
Epoch 8 Step 1051 Train Loss: 0.2428
Epoch 8 Step 1101 Train Loss: 0.2360
Epoch 8 Step 1151 Train Loss: 0.2430
Epoch 8 Step 1201 Train Loss: 0.2330
Epoch 8 Step 1251 Train Loss: 0.2423
Epoch 8 Step 1301 Train Loss: 0.2210
Epoch 8 Step 1351 Train Loss: 0.2271
Epoch 8 Step 1401 Train Loss: 0.2539
Epoch 8 Step 1451 Train Loss: 0.2393
Epoch 8 Step 1501 Train Loss: 0.2423
Epoch 8 Step 1551 Train Loss: 0.2668
Epoch 8 Step 1601 Train Loss: 0.2246
Epoch 8 Step 1651 Train Loss: 0.2449
Epoch 8 Step 1701 Train Loss: 0.2362
Epoch 8 Step 1751 Train Loss: 0.2362
Epoch 8 Step 1801 Train Loss: 0.2245
Epoch 8 Step 1851 Train Loss: 0.2290
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Epoch 9 Step 1 Train Loss: 0.2424
Epoch 9 Step 51 Train Loss: 0.2694
Epoch 9 Step 101 Train Loss: 0.2385
Epoch 9 Step 151 Train Loss: 0.2350
Epoch 9 Step 201 Train Loss: 0.2413
Epoch 9 Step 251 Train Loss: 0.2337
Epoch 9 Step 301 Train Loss: 0.2392
Epoch 9 Step 351 Train Loss: 0.2386
Epoch 9 Step 401 Train Loss: 0.2438
Epoch 9 Step 451 Train Loss: 0.2452
Epoch 9 Step 501 Train Loss: 0.2313
Epoch 9 Step 551 Train Loss: 0.2290
Epoch 9 Step 601 Train Loss: 0.2472
Epoch 9 Step 651 Train Loss: 0.2447
Epoch 9 Step 701 Train Loss: 0.2609
Epoch 9 Step 751 Train Loss: 0.2568
Epoch 9 Step 801 Train Loss: 0.2361
Epoch 9 Step 851 Train Loss: 0.2441
Epoch 9 Step 901 Train Loss: 0.2382
Epoch 9 Step 951 Train Loss: 0.2408
Epoch 9 Step 1001 Train Loss: 0.2381
Epoch 9 Step 1051 Train Loss: 0.2359
Epoch 9 Step 1101 Train Loss: 0.2199
Epoch 9 Step 1151 Train Loss: 0.2146
Epoch 9 Step 1201 Train Loss: 0.2362
Epoch 9 Step 1251 Train Loss: 0.2238
Epoch 9 Step 1301 Train Loss: 0.2415
Epoch 9 Step 1351 Train Loss: 0.2428
Epoch 9 Step 1401 Train Loss: 0.2462
Epoch 9 Step 1451 Train Loss: 0.2374
Epoch 9 Step 1501 Train Loss: 0.2372
Epoch 9 Step 1551 Train Loss: 0.2441
Epoch 9 Step 1601 Train Loss: 0.2332
Epoch 9 Step 1651 Train Loss: 0.2417
Epoch 9 Step 1701 Train Loss: 0.2200
Epoch 9 Step 1751 Train Loss: 0.2553
Epoch 9 Step 1801 Train Loss: 0.2498
Epoch 9 Step 1851 Train Loss: 0.2441
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Epoch 10 Step 1 Train Loss: 0.2317
Epoch 10 Step 51 Train Loss: 0.2343
Epoch 10 Step 101 Train Loss: 0.2531
Epoch 10 Step 151 Train Loss: 0.2472
Epoch 10 Step 201 Train Loss: 0.2512
Epoch 10 Step 251 Train Loss: 0.2526
Epoch 10 Step 301 Train Loss: 0.2473
Epoch 10 Step 351 Train Loss: 0.2208
Epoch 10 Step 401 Train Loss: 0.2220
Epoch 10 Step 451 Train Loss: 0.2328
Epoch 10 Step 501 Train Loss: 0.2553
Epoch 10 Step 551 Train Loss: 0.2189
Epoch 10 Step 601 Train Loss: 0.2259
Epoch 10 Step 651 Train Loss: 0.2262
Epoch 10 Step 701 Train Loss: 0.2331
Epoch 10 Step 751 Train Loss: 0.2354
Epoch 10 Step 801 Train Loss: 0.2613
Epoch 10 Step 851 Train Loss: 0.2552
Epoch 10 Step 901 Train Loss: 0.2711
Epoch 10 Step 951 Train Loss: 0.2458
Epoch 10 Step 1001 Train Loss: 0.2684
Epoch 10 Step 1051 Train Loss: 0.2429
Epoch 10 Step 1101 Train Loss: 0.2442
Epoch 10 Step 1151 Train Loss: 0.2505
Epoch 10 Step 1201 Train Loss: 0.2224
Epoch 10 Step 1251 Train Loss: 0.2442
Epoch 10 Step 1301 Train Loss: 0.2341
Epoch 10 Step 1351 Train Loss: 0.2464
Epoch 10 Step 1401 Train Loss: 0.2337
Epoch 10 Step 1451 Train Loss: 0.2323
Epoch 10 Step 1501 Train Loss: 0.2328
Epoch 10 Step 1551 Train Loss: 0.2387
Epoch 10 Step 1601 Train Loss: 0.2422
Epoch 10 Step 1651 Train Loss: 0.2403
Epoch 10 Step 1701 Train Loss: 0.2412
Epoch 10 Step 1751 Train Loss: 0.2458
Epoch 10 Step 1801 Train Loss: 0.2386
Epoch 10 Step 1851 Train Loss: 0.2256
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Epoch 11 Step 1 Train Loss: 0.2440
Epoch 11 Step 51 Train Loss: 0.2198
Epoch 11 Step 101 Train Loss: 0.2397
Epoch 11 Step 151 Train Loss: 0.2563
Epoch 11 Step 201 Train Loss: 0.2335
Epoch 11 Step 251 Train Loss: 0.2447
Epoch 11 Step 301 Train Loss: 0.2261
Epoch 11 Step 351 Train Loss: 0.2457
Epoch 11 Step 401 Train Loss: 0.2299
Epoch 11 Step 451 Train Loss: 0.2370
Epoch 11 Step 501 Train Loss: 0.2507
Epoch 11 Step 551 Train Loss: 0.2484
Epoch 11 Step 601 Train Loss: 0.2517
Epoch 11 Step 651 Train Loss: 0.2352
Epoch 11 Step 701 Train Loss: 0.2380
Epoch 11 Step 751 Train Loss: 0.2347
Epoch 11 Step 801 Train Loss: 0.2215
Epoch 11 Step 851 Train Loss: 0.2441
Epoch 11 Step 901 Train Loss: 0.2365
Epoch 11 Step 951 Train Loss: 0.2333
Epoch 11 Step 1001 Train Loss: 0.2551
Epoch 11 Step 1051 Train Loss: 0.2301
Epoch 11 Step 1101 Train Loss: 0.2245
Epoch 11 Step 1151 Train Loss: 0.2339
Epoch 11 Step 1201 Train Loss: 0.2517
Epoch 11 Step 1251 Train Loss: 0.2463
Epoch 11 Step 1301 Train Loss: 0.2238
Epoch 11 Step 1351 Train Loss: 0.2400
Epoch 11 Step 1401 Train Loss: 0.2549
Epoch 11 Step 1451 Train Loss: 0.2642
Epoch 11 Step 1501 Train Loss: 0.2390
Epoch 11 Step 1551 Train Loss: 0.2296
Epoch 11 Step 1601 Train Loss: 0.2329
Epoch 11 Step 1651 Train Loss: 0.2475
Epoch 11 Step 1701 Train Loss: 0.2317
Epoch 11 Step 1751 Train Loss: 0.2365
Epoch 11 Step 1801 Train Loss: 0.2445
Epoch 11 Step 1851 Train Loss: 0.2417
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Epoch 12 Step 1 Train Loss: 0.2317
Epoch 12 Step 51 Train Loss: 0.2460
Epoch 12 Step 101 Train Loss: 0.2340
Epoch 12 Step 151 Train Loss: 0.2433
Epoch 12 Step 201 Train Loss: 0.2382
Epoch 12 Step 251 Train Loss: 0.2429
Epoch 12 Step 301 Train Loss: 0.2345
Epoch 12 Step 351 Train Loss: 0.2394
Epoch 12 Step 401 Train Loss: 0.2357
Epoch 12 Step 451 Train Loss: 0.2219
Epoch 12 Step 501 Train Loss: 0.2374
Epoch 12 Step 551 Train Loss: 0.2305
Epoch 12 Step 601 Train Loss: 0.2378
Epoch 12 Step 651 Train Loss: 0.2361
Epoch 12 Step 701 Train Loss: 0.2304
Epoch 12 Step 751 Train Loss: 0.2509
Epoch 12 Step 801 Train Loss: 0.2270
Epoch 12 Step 851 Train Loss: 0.2113
Epoch 12 Step 901 Train Loss: 0.2499
Epoch 12 Step 951 Train Loss: 0.2412
Epoch 12 Step 1001 Train Loss: 0.2429
Epoch 12 Step 1051 Train Loss: 0.2407
Epoch 12 Step 1101 Train Loss: 0.2428
Epoch 12 Step 1151 Train Loss: 0.2522
Epoch 12 Step 1201 Train Loss: 0.2398
Epoch 12 Step 1251 Train Loss: 0.2328
Epoch 12 Step 1301 Train Loss: 0.2360
Epoch 12 Step 1351 Train Loss: 0.2518
Epoch 12 Step 1401 Train Loss: 0.2368
Epoch 12 Step 1451 Train Loss: 0.2358
Epoch 12 Step 1501 Train Loss: 0.2384
Epoch 12 Step 1551 Train Loss: 0.2480
Epoch 12 Step 1601 Train Loss: 0.2429
Epoch 12 Step 1651 Train Loss: 0.2491
Epoch 12 Step 1701 Train Loss: 0.2482
Epoch 12 Step 1751 Train Loss: 0.2294
Epoch 12 Step 1801 Train Loss: 0.2350
Epoch 12 Step 1851 Train Loss: 0.2560
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 13 Step 1 Train Loss: 0.2199
Epoch 13 Step 51 Train Loss: 0.2441
Epoch 13 Step 101 Train Loss: 0.2432
Epoch 13 Step 151 Train Loss: 0.2420
Epoch 13 Step 201 Train Loss: 0.2393
Epoch 13 Step 251 Train Loss: 0.2340
Epoch 13 Step 301 Train Loss: 0.2498
Epoch 13 Step 351 Train Loss: 0.2384
Epoch 13 Step 401 Train Loss: 0.2247
Epoch 13 Step 451 Train Loss: 0.2162
Epoch 13 Step 501 Train Loss: 0.2322
Epoch 13 Step 551 Train Loss: 0.2077
Epoch 13 Step 601 Train Loss: 0.2352
Epoch 13 Step 651 Train Loss: 0.2888
Epoch 13 Step 701 Train Loss: 0.2353
Epoch 13 Step 751 Train Loss: 0.2406
Epoch 13 Step 801 Train Loss: 0.2338
Epoch 13 Step 851 Train Loss: 0.2357
Epoch 13 Step 901 Train Loss: 0.2384
Epoch 13 Step 951 Train Loss: 0.2351
Epoch 13 Step 1001 Train Loss: 0.2315
Epoch 13 Step 1051 Train Loss: 0.2528
Epoch 13 Step 1101 Train Loss: 0.2492
Epoch 13 Step 1151 Train Loss: 0.2531
Epoch 13 Step 1201 Train Loss: 0.2280
Epoch 13 Step 1251 Train Loss: 0.2661
Epoch 13 Step 1301 Train Loss: 0.2221
Epoch 13 Step 1351 Train Loss: 0.2322
Epoch 13 Step 1401 Train Loss: 0.2442
Epoch 13 Step 1451 Train Loss: 0.2317
Epoch 13 Step 1501 Train Loss: 0.2468
Epoch 13 Step 1551 Train Loss: 0.2500
Epoch 13 Step 1601 Train Loss: 0.2379
Epoch 13 Step 1651 Train Loss: 0.2504
Epoch 13 Step 1701 Train Loss: 0.2523
Epoch 13 Step 1751 Train Loss: 0.2371
Epoch 13 Step 1801 Train Loss: 0.2368
Epoch 13 Step 1851 Train Loss: 0.2312
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Epoch 14 Step 1 Train Loss: 0.2220
Epoch 14 Step 51 Train Loss: 0.2239
Epoch 14 Step 101 Train Loss: 0.2429
Epoch 14 Step 151 Train Loss: 0.2372
Epoch 14 Step 201 Train Loss: 0.2396
Epoch 14 Step 251 Train Loss: 0.2319
Epoch 14 Step 301 Train Loss: 0.2358
Epoch 14 Step 351 Train Loss: 0.2184
Epoch 14 Step 401 Train Loss: 0.2570
Epoch 14 Step 451 Train Loss: 0.2332
Epoch 14 Step 501 Train Loss: 0.2402
Epoch 14 Step 551 Train Loss: 0.2282
Epoch 14 Step 601 Train Loss: 0.2458
Epoch 14 Step 651 Train Loss: 0.2316
Epoch 14 Step 701 Train Loss: 0.2369
Epoch 14 Step 751 Train Loss: 0.2335
Epoch 14 Step 801 Train Loss: 0.2492
Epoch 14 Step 851 Train Loss: 0.2442
Epoch 14 Step 901 Train Loss: 0.2334
Epoch 14 Step 951 Train Loss: 0.2277
Epoch 14 Step 1001 Train Loss: 0.2487
Epoch 14 Step 1051 Train Loss: 0.2186
Epoch 14 Step 1101 Train Loss: 0.2352
Epoch 14 Step 1151 Train Loss: 0.2340
Epoch 14 Step 1201 Train Loss: 0.2646
Epoch 14 Step 1251 Train Loss: 0.2491
Epoch 14 Step 1301 Train Loss: 0.2294
Epoch 14 Step 1351 Train Loss: 0.2369
Epoch 14 Step 1401 Train Loss: 0.2417
Epoch 14 Step 1451 Train Loss: 0.2481
Epoch 14 Step 1501 Train Loss: 0.2290
Epoch 14 Step 1551 Train Loss: 0.2558
Epoch 14 Step 1601 Train Loss: 0.2448
Epoch 14 Step 1651 Train Loss: 0.2370
Epoch 14 Step 1701 Train Loss: 0.2413
Epoch 14 Step 1751 Train Loss: 0.2650
Epoch 14 Step 1801 Train Loss: 0.2413
Epoch 14 Step 1851 Train Loss: 0.2455
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0082. 
Epoch 15 Step 1 Train Loss: 0.2336
Epoch 15 Step 51 Train Loss: 0.2324
Epoch 15 Step 101 Train Loss: 0.2430
Epoch 15 Step 151 Train Loss: 0.2241
Epoch 15 Step 201 Train Loss: 0.2374
Epoch 15 Step 251 Train Loss: 0.2535
Epoch 15 Step 301 Train Loss: 0.2318
Epoch 15 Step 351 Train Loss: 0.2306
Epoch 15 Step 401 Train Loss: 0.2519
Epoch 15 Step 451 Train Loss: 0.2255
Epoch 15 Step 501 Train Loss: 0.2574
Epoch 15 Step 551 Train Loss: 0.2465
Epoch 15 Step 601 Train Loss: 0.2400
Epoch 15 Step 651 Train Loss: 0.2365
Epoch 15 Step 701 Train Loss: 0.2477
Epoch 15 Step 751 Train Loss: 0.2449
Epoch 15 Step 801 Train Loss: 0.2395
Epoch 15 Step 851 Train Loss: 0.2441
Epoch 15 Step 901 Train Loss: 0.2608
Epoch 15 Step 951 Train Loss: 0.2477
Epoch 15 Step 1001 Train Loss: 0.2260
Epoch 15 Step 1051 Train Loss: 0.2398
Epoch 15 Step 1101 Train Loss: 0.2332
Epoch 15 Step 1151 Train Loss: 0.2409
Epoch 15 Step 1201 Train Loss: 0.2253
Epoch 15 Step 1251 Train Loss: 0.2469
Epoch 15 Step 1301 Train Loss: 0.2447
Epoch 15 Step 1351 Train Loss: 0.2301
Epoch 15 Step 1401 Train Loss: 0.2453
Epoch 15 Step 1451 Train Loss: 0.2364
Epoch 15 Step 1501 Train Loss: 0.2278
Epoch 15 Step 1551 Train Loss: 0.2481
Epoch 15 Step 1601 Train Loss: 0.2389
Epoch 15 Step 1651 Train Loss: 0.2426
Epoch 15 Step 1701 Train Loss: 0.2449
Epoch 15 Step 1751 Train Loss: 0.2288
Epoch 15 Step 1801 Train Loss: 0.2374
Epoch 15 Step 1851 Train Loss: 0.2332
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0005. 
Train Top 20 DE MSE: 0.0055 Validation Top 20 DE MSE: 0.0082. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0045
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00033546105
test_unseen_single_pearson: 0.992764015846386
test_unseen_single_mse_de: 0.004466136
test_unseen_single_pearson_de: 0.8021957276605479
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1565405180233177
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3598039215686275
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9294117647058823
test_unseen_single_mse_top20_de_non_dropout: 0.0074662888716816285
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.037 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ
wandb:                                                   val_de_mse ‚ñà‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                                               val_de_pearson ‚ñÉ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00447
wandb:                                              test_de_pearson 0.8022
wandb:               test_frac_opposite_direction_top20_non_dropout 0.3598
wandb:                          test_frac_sigma_below_1_non_dropout 0.92941
wandb:                                                     test_mse 0.00034
wandb:                                test_mse_top20_de_non_dropout 0.00747
wandb:                                                 test_pearson 0.99276
wandb:                                           test_pearson_delta 0.15654
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.3598
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.92941
wandb:                                       test_unseen_single_mse 0.00034
wandb:                                    test_unseen_single_mse_de 0.00447
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00747
wandb:                                   test_unseen_single_pearson 0.99276
wandb:                                test_unseen_single_pearson_de 0.8022
wandb:                             test_unseen_single_pearson_delta 0.15654
wandb:                                                 train_de_mse 0.00545
wandb:                                             train_de_pearson 0.7911
wandb:                                                    train_mse 0.00033
wandb:                                                train_pearson 0.99282
wandb:                                                training_loss 0.25669
wandb:                                                   val_de_mse 0.00824
wandb:                                               val_de_pearson 0.74993
wandb:                                                      val_mse 0.00055
wandb:                                                  val_pearson 0.98864
wandb: 
wandb: üöÄ View run geneformer_XuCao2023_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/ebpo5ekv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_030331-ebpo5ekv/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_035353-cvy3uh5y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_XuCao2023_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/cvy3uh5y
wandb: WARNING Serializing object of type ndarray that is 21184640 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2517
Epoch 1 Step 51 Train Loss: 0.2690
Epoch 1 Step 101 Train Loss: 0.2393
Epoch 1 Step 151 Train Loss: 0.2869
Epoch 1 Step 201 Train Loss: 0.2627
Epoch 1 Step 251 Train Loss: 0.2400
Epoch 1 Step 301 Train Loss: 0.2520
Epoch 1 Step 351 Train Loss: 0.2635
Epoch 1 Step 401 Train Loss: 0.2405
Epoch 1 Step 451 Train Loss: 0.2477
Epoch 1 Step 501 Train Loss: 0.2545
Epoch 1 Step 551 Train Loss: 0.2576
Epoch 1 Step 601 Train Loss: 0.2711
Epoch 1 Step 651 Train Loss: 0.2299
Epoch 1 Step 701 Train Loss: 0.2256
Epoch 1 Step 751 Train Loss: 0.2188
Epoch 1 Step 801 Train Loss: 0.2350
Epoch 1 Step 851 Train Loss: 0.2376
Epoch 1 Step 901 Train Loss: 0.2413
Epoch 1 Step 951 Train Loss: 0.2400
Epoch 1 Step 1001 Train Loss: 0.2148
Epoch 1 Step 1051 Train Loss: 0.2262
Epoch 1 Step 1101 Train Loss: 0.2392
Epoch 1 Step 1151 Train Loss: 0.2175
Epoch 1 Step 1201 Train Loss: 0.2224
Epoch 1 Step 1251 Train Loss: 0.2281
Epoch 1 Step 1301 Train Loss: 0.2272
Epoch 1 Step 1351 Train Loss: 0.2222
Epoch 1 Step 1401 Train Loss: 0.2147
Epoch 1 Step 1451 Train Loss: 0.2517
Epoch 1 Step 1501 Train Loss: 0.2315
Epoch 1 Step 1551 Train Loss: 0.2253
Epoch 1 Step 1601 Train Loss: 0.2081
Epoch 1 Step 1651 Train Loss: 0.2116
Epoch 1 Step 1701 Train Loss: 0.2103
Epoch 1 Step 1751 Train Loss: 0.2235
Epoch 1 Step 1801 Train Loss: 0.2408
Epoch 1 Step 1851 Train Loss: 0.2046
Epoch 1 Step 1901 Train Loss: 0.2024
Epoch 1 Step 1951 Train Loss: 0.2134
Epoch 1 Step 2001 Train Loss: 0.1981
Epoch 1: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0038. 
Epoch 2 Step 1 Train Loss: 0.2328
Epoch 2 Step 51 Train Loss: 0.2472
Epoch 2 Step 101 Train Loss: 0.2108
Epoch 2 Step 151 Train Loss: 0.2223
Epoch 2 Step 201 Train Loss: 0.2432
Epoch 2 Step 251 Train Loss: 0.2636
Epoch 2 Step 301 Train Loss: 0.2205
Epoch 2 Step 351 Train Loss: 0.2136
Epoch 2 Step 401 Train Loss: 0.2226
Epoch 2 Step 451 Train Loss: 0.2377
Epoch 2 Step 501 Train Loss: 0.2154
Epoch 2 Step 551 Train Loss: 0.2453
Epoch 2 Step 601 Train Loss: 0.2082
Epoch 2 Step 651 Train Loss: 0.2188
Epoch 2 Step 701 Train Loss: 0.2224
Epoch 2 Step 751 Train Loss: 0.1978
Epoch 2 Step 801 Train Loss: 0.2338
Epoch 2 Step 851 Train Loss: 0.2053
Epoch 2 Step 901 Train Loss: 0.2162
Epoch 2 Step 951 Train Loss: 0.2216
Epoch 2 Step 1001 Train Loss: 0.2448
Epoch 2 Step 1051 Train Loss: 0.2259
Epoch 2 Step 1101 Train Loss: 0.2457
Epoch 2 Step 1151 Train Loss: 0.2019
Epoch 2 Step 1201 Train Loss: 0.2281
Epoch 2 Step 1251 Train Loss: 0.2251
Epoch 2 Step 1301 Train Loss: 0.2238
Epoch 2 Step 1351 Train Loss: 0.2284
Epoch 2 Step 1401 Train Loss: 0.2500
Epoch 2 Step 1451 Train Loss: 0.2280
Epoch 2 Step 1501 Train Loss: 0.2340
Epoch 2 Step 1551 Train Loss: 0.2352
Epoch 2 Step 1601 Train Loss: 0.2118
Epoch 2 Step 1651 Train Loss: 0.2563
Epoch 2 Step 1701 Train Loss: 0.2433
Epoch 2 Step 1751 Train Loss: 0.2073
Epoch 2 Step 1801 Train Loss: 0.2301
Epoch 2 Step 1851 Train Loss: 0.2224
Epoch 2 Step 1901 Train Loss: 0.2322
Epoch 2 Step 1951 Train Loss: 0.2038
Epoch 2 Step 2001 Train Loss: 0.2417
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 3 Step 1 Train Loss: 0.2323
Epoch 3 Step 51 Train Loss: 0.2286
Epoch 3 Step 101 Train Loss: 0.2016
Epoch 3 Step 151 Train Loss: 0.2303
Epoch 3 Step 201 Train Loss: 0.2285
Epoch 3 Step 251 Train Loss: 0.2054
Epoch 3 Step 301 Train Loss: 0.2163
Epoch 3 Step 351 Train Loss: 0.2072
Epoch 3 Step 401 Train Loss: 0.2339
Epoch 3 Step 451 Train Loss: 0.2287
Epoch 3 Step 501 Train Loss: 0.2619
Epoch 3 Step 551 Train Loss: 0.2551
Epoch 3 Step 601 Train Loss: 0.2263
Epoch 3 Step 651 Train Loss: 0.2364
Epoch 3 Step 701 Train Loss: 0.2205
Epoch 3 Step 751 Train Loss: 0.2164
Epoch 3 Step 801 Train Loss: 0.2163
Epoch 3 Step 851 Train Loss: 0.2146
Epoch 3 Step 901 Train Loss: 0.2485
Epoch 3 Step 951 Train Loss: 0.2362
Epoch 3 Step 1001 Train Loss: 0.2230
Epoch 3 Step 1051 Train Loss: 0.2295
Epoch 3 Step 1101 Train Loss: 0.2379
Epoch 3 Step 1151 Train Loss: 0.2144
Epoch 3 Step 1201 Train Loss: 0.2375
Epoch 3 Step 1251 Train Loss: 0.2275
Epoch 3 Step 1301 Train Loss: 0.2254
Epoch 3 Step 1351 Train Loss: 0.2486
Epoch 3 Step 1401 Train Loss: 0.2367
Epoch 3 Step 1451 Train Loss: 0.2274
Epoch 3 Step 1501 Train Loss: 0.2225
Epoch 3 Step 1551 Train Loss: 0.2353
Epoch 3 Step 1601 Train Loss: 0.2153
Epoch 3 Step 1651 Train Loss: 0.2404
Epoch 3 Step 1701 Train Loss: 0.2314
Epoch 3 Step 1751 Train Loss: 0.2223
Epoch 3 Step 1801 Train Loss: 0.2375
Epoch 3 Step 1851 Train Loss: 0.2430
Epoch 3 Step 1901 Train Loss: 0.2209
Epoch 3 Step 1951 Train Loss: 0.2378
Epoch 3 Step 2001 Train Loss: 0.2336
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 4 Step 1 Train Loss: 0.2119
Epoch 4 Step 51 Train Loss: 0.2133
Epoch 4 Step 101 Train Loss: 0.2315
Epoch 4 Step 151 Train Loss: 0.2356
Epoch 4 Step 201 Train Loss: 0.2430
Epoch 4 Step 251 Train Loss: 0.2230
Epoch 4 Step 301 Train Loss: 0.2597
Epoch 4 Step 351 Train Loss: 0.2283
Epoch 4 Step 401 Train Loss: 0.2604
Epoch 4 Step 451 Train Loss: 0.2196
Epoch 4 Step 501 Train Loss: 0.2559
Epoch 4 Step 551 Train Loss: 0.2362
Epoch 4 Step 601 Train Loss: 0.2431
Epoch 4 Step 651 Train Loss: 0.2379
Epoch 4 Step 701 Train Loss: 0.2605
Epoch 4 Step 751 Train Loss: 0.2467
Epoch 4 Step 801 Train Loss: 0.2584
Epoch 4 Step 851 Train Loss: 0.2665
Epoch 4 Step 901 Train Loss: 0.2312
Epoch 4 Step 951 Train Loss: 0.2816
Epoch 4 Step 1001 Train Loss: 0.2361
Epoch 4 Step 1051 Train Loss: 0.2380
Epoch 4 Step 1101 Train Loss: 0.2411
Epoch 4 Step 1151 Train Loss: 0.2451
Epoch 4 Step 1201 Train Loss: 0.2138
Epoch 4 Step 1251 Train Loss: 0.2347
Epoch 4 Step 1301 Train Loss: 0.2157
Epoch 4 Step 1351 Train Loss: 0.2029
Epoch 4 Step 1401 Train Loss: 0.2556
Epoch 4 Step 1451 Train Loss: 0.2273
Epoch 4 Step 1501 Train Loss: 0.2319
Epoch 4 Step 1551 Train Loss: 0.2310
Epoch 4 Step 1601 Train Loss: 0.2332
Epoch 4 Step 1651 Train Loss: 0.2333
Epoch 4 Step 1701 Train Loss: 0.2530
Epoch 4 Step 1751 Train Loss: 0.2334
Epoch 4 Step 1801 Train Loss: 0.2301
Epoch 4 Step 1851 Train Loss: 0.2557
Epoch 4 Step 1901 Train Loss: 0.2549
Epoch 4 Step 1951 Train Loss: 0.2234
Epoch 4 Step 2001 Train Loss: 0.2452
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 5 Step 1 Train Loss: 0.2316
Epoch 5 Step 51 Train Loss: 0.2500
Epoch 5 Step 101 Train Loss: 0.2312
Epoch 5 Step 151 Train Loss: 0.2299
Epoch 5 Step 201 Train Loss: 0.2429
Epoch 5 Step 251 Train Loss: 0.2522
Epoch 5 Step 301 Train Loss: 0.2286
Epoch 5 Step 351 Train Loss: 0.2489
Epoch 5 Step 401 Train Loss: 0.2297
Epoch 5 Step 451 Train Loss: 0.2471
Epoch 5 Step 501 Train Loss: 0.2431
Epoch 5 Step 551 Train Loss: 0.2489
Epoch 5 Step 601 Train Loss: 0.2222
Epoch 5 Step 651 Train Loss: 0.2249
Epoch 5 Step 701 Train Loss: 0.2429
Epoch 5 Step 751 Train Loss: 0.2192
Epoch 5 Step 801 Train Loss: 0.2223
Epoch 5 Step 851 Train Loss: 0.2172
Epoch 5 Step 901 Train Loss: 0.2288
Epoch 5 Step 951 Train Loss: 0.2314
Epoch 5 Step 1001 Train Loss: 0.2428
Epoch 5 Step 1051 Train Loss: 0.2183
Epoch 5 Step 1101 Train Loss: 0.2139
Epoch 5 Step 1151 Train Loss: 0.2449
Epoch 5 Step 1201 Train Loss: 0.2295
Epoch 5 Step 1251 Train Loss: 0.2391
Epoch 5 Step 1301 Train Loss: 0.2217
Epoch 5 Step 1351 Train Loss: 0.2385
Epoch 5 Step 1401 Train Loss: 0.2364
Epoch 5 Step 1451 Train Loss: 0.2570
Epoch 5 Step 1501 Train Loss: 0.2631
Epoch 5 Step 1551 Train Loss: 0.2267
Epoch 5 Step 1601 Train Loss: 0.2263
Epoch 5 Step 1651 Train Loss: 0.2356
Epoch 5 Step 1701 Train Loss: 0.2249
Epoch 5 Step 1751 Train Loss: 0.2325
Epoch 5 Step 1801 Train Loss: 0.2459
Epoch 5 Step 1851 Train Loss: 0.2470
Epoch 5 Step 1901 Train Loss: 0.2348
Epoch 5 Step 1951 Train Loss: 0.2442
Epoch 5 Step 2001 Train Loss: 0.2303
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 6 Step 1 Train Loss: 0.2435
Epoch 6 Step 51 Train Loss: 0.2344
Epoch 6 Step 101 Train Loss: 0.2585
Epoch 6 Step 151 Train Loss: 0.2347
Epoch 6 Step 201 Train Loss: 0.2320
Epoch 6 Step 251 Train Loss: 0.2347
Epoch 6 Step 301 Train Loss: 0.2238
Epoch 6 Step 351 Train Loss: 0.2288
Epoch 6 Step 401 Train Loss: 0.2234
Epoch 6 Step 451 Train Loss: 0.2611
Epoch 6 Step 501 Train Loss: 0.2450
Epoch 6 Step 551 Train Loss: 0.2306
Epoch 6 Step 601 Train Loss: 0.2261
Epoch 6 Step 651 Train Loss: 0.2281
Epoch 6 Step 701 Train Loss: 0.2423
Epoch 6 Step 751 Train Loss: 0.2517
Epoch 6 Step 801 Train Loss: 0.2434
Epoch 6 Step 851 Train Loss: 0.2336
Epoch 6 Step 901 Train Loss: 0.2380
Epoch 6 Step 951 Train Loss: 0.2379
Epoch 6 Step 1001 Train Loss: 0.2185
Epoch 6 Step 1051 Train Loss: 0.2410
Epoch 6 Step 1101 Train Loss: 0.2272
Epoch 6 Step 1151 Train Loss: 0.2447
Epoch 6 Step 1201 Train Loss: 0.2246
Epoch 6 Step 1251 Train Loss: 0.2491
Epoch 6 Step 1301 Train Loss: 0.2279
Epoch 6 Step 1351 Train Loss: 0.2211
Epoch 6 Step 1401 Train Loss: 0.2241
Epoch 6 Step 1451 Train Loss: 0.2546
Epoch 6 Step 1501 Train Loss: 0.2371
Epoch 6 Step 1551 Train Loss: 0.2595
Epoch 6 Step 1601 Train Loss: 0.2489
Epoch 6 Step 1651 Train Loss: 0.2523
Epoch 6 Step 1701 Train Loss: 0.2474
Epoch 6 Step 1751 Train Loss: 0.2683
Epoch 6 Step 1801 Train Loss: 0.2372
Epoch 6 Step 1851 Train Loss: 0.2374
Epoch 6 Step 1901 Train Loss: 0.2232
Epoch 6 Step 1951 Train Loss: 0.2188
Epoch 6 Step 2001 Train Loss: 0.2766
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 7 Step 1 Train Loss: 0.2408
Epoch 7 Step 51 Train Loss: 0.2629
Epoch 7 Step 101 Train Loss: 0.2270
Epoch 7 Step 151 Train Loss: 0.2415
Epoch 7 Step 201 Train Loss: 0.2211
Epoch 7 Step 251 Train Loss: 0.2459
Epoch 7 Step 301 Train Loss: 0.2251
Epoch 7 Step 351 Train Loss: 0.2299
Epoch 7 Step 401 Train Loss: 0.2474
Epoch 7 Step 451 Train Loss: 0.2320
Epoch 7 Step 501 Train Loss: 0.2325
Epoch 7 Step 551 Train Loss: 0.2314
Epoch 7 Step 601 Train Loss: 0.2314
Epoch 7 Step 651 Train Loss: 0.2418
Epoch 7 Step 701 Train Loss: 0.2629
Epoch 7 Step 751 Train Loss: 0.2428
Epoch 7 Step 801 Train Loss: 0.2306
Epoch 7 Step 851 Train Loss: 0.2398
Epoch 7 Step 901 Train Loss: 0.2511
Epoch 7 Step 951 Train Loss: 0.2271
Epoch 7 Step 1001 Train Loss: 0.2499
Epoch 7 Step 1051 Train Loss: 0.2329
Epoch 7 Step 1101 Train Loss: 0.2396
Epoch 7 Step 1151 Train Loss: 0.2326
Epoch 7 Step 1201 Train Loss: 0.2624
Epoch 7 Step 1251 Train Loss: 0.2479
Epoch 7 Step 1301 Train Loss: 0.2242
Epoch 7 Step 1351 Train Loss: 0.2645
Epoch 7 Step 1401 Train Loss: 0.2223
Epoch 7 Step 1451 Train Loss: 0.2294
Epoch 7 Step 1501 Train Loss: 0.2477
Epoch 7 Step 1551 Train Loss: 0.2263
Epoch 7 Step 1601 Train Loss: 0.2435
Epoch 7 Step 1651 Train Loss: 0.2269
Epoch 7 Step 1701 Train Loss: 0.2314
Epoch 7 Step 1751 Train Loss: 0.2413
Epoch 7 Step 1801 Train Loss: 0.2484
Epoch 7 Step 1851 Train Loss: 0.2597
Epoch 7 Step 1901 Train Loss: 0.2520
Epoch 7 Step 1951 Train Loss: 0.2474
Epoch 7 Step 2001 Train Loss: 0.2562
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 8 Step 1 Train Loss: 0.2367
Epoch 8 Step 51 Train Loss: 0.2508
Epoch 8 Step 101 Train Loss: 0.2315
Epoch 8 Step 151 Train Loss: 0.2594
Epoch 8 Step 201 Train Loss: 0.2326
Epoch 8 Step 251 Train Loss: 0.2416
Epoch 8 Step 301 Train Loss: 0.2402
Epoch 8 Step 351 Train Loss: 0.2393
Epoch 8 Step 401 Train Loss: 0.2502
Epoch 8 Step 451 Train Loss: 0.2349
Epoch 8 Step 501 Train Loss: 0.2247
Epoch 8 Step 551 Train Loss: 0.2356
Epoch 8 Step 601 Train Loss: 0.2308
Epoch 8 Step 651 Train Loss: 0.2344
Epoch 8 Step 701 Train Loss: 0.2361
Epoch 8 Step 751 Train Loss: 0.2520
Epoch 8 Step 801 Train Loss: 0.2268
Epoch 8 Step 851 Train Loss: 0.2338
Epoch 8 Step 901 Train Loss: 0.2282
Epoch 8 Step 951 Train Loss: 0.2485
Epoch 8 Step 1001 Train Loss: 0.2427
Epoch 8 Step 1051 Train Loss: 0.2518
Epoch 8 Step 1101 Train Loss: 0.2638
Epoch 8 Step 1151 Train Loss: 0.2352
Epoch 8 Step 1201 Train Loss: 0.2499
Epoch 8 Step 1251 Train Loss: 0.2432
Epoch 8 Step 1301 Train Loss: 0.2496
Epoch 8 Step 1351 Train Loss: 0.2427
Epoch 8 Step 1401 Train Loss: 0.2360
Epoch 8 Step 1451 Train Loss: 0.2458
Epoch 8 Step 1501 Train Loss: 0.2116
Epoch 8 Step 1551 Train Loss: 0.2336
Epoch 8 Step 1601 Train Loss: 0.2503
Epoch 8 Step 1651 Train Loss: 0.2401
Epoch 8 Step 1701 Train Loss: 0.2396
Epoch 8 Step 1751 Train Loss: 0.2440
Epoch 8 Step 1801 Train Loss: 0.2232
Epoch 8 Step 1851 Train Loss: 0.2427
Epoch 8 Step 1901 Train Loss: 0.2263
Epoch 8 Step 1951 Train Loss: 0.2567
Epoch 8 Step 2001 Train Loss: 0.2454
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 9 Step 1 Train Loss: 0.2493
Epoch 9 Step 51 Train Loss: 0.2452
Epoch 9 Step 101 Train Loss: 0.2352
Epoch 9 Step 151 Train Loss: 0.2382
Epoch 9 Step 201 Train Loss: 0.2191
Epoch 9 Step 251 Train Loss: 0.2466
Epoch 9 Step 301 Train Loss: 0.2429
Epoch 9 Step 351 Train Loss: 0.2417
Epoch 9 Step 401 Train Loss: 0.2299
Epoch 9 Step 451 Train Loss: 0.2396
Epoch 9 Step 501 Train Loss: 0.2450
Epoch 9 Step 551 Train Loss: 0.2326
Epoch 9 Step 601 Train Loss: 0.2219
Epoch 9 Step 651 Train Loss: 0.2466
Epoch 9 Step 701 Train Loss: 0.2469
Epoch 9 Step 751 Train Loss: 0.2322
Epoch 9 Step 801 Train Loss: 0.2275
Epoch 9 Step 851 Train Loss: 0.2541
Epoch 9 Step 901 Train Loss: 0.2361
Epoch 9 Step 951 Train Loss: 0.2420
Epoch 9 Step 1001 Train Loss: 0.2268
Epoch 9 Step 1051 Train Loss: 0.2267
Epoch 9 Step 1101 Train Loss: 0.2338
Epoch 9 Step 1151 Train Loss: 0.2509
Epoch 9 Step 1201 Train Loss: 0.2513
Epoch 9 Step 1251 Train Loss: 0.2209
Epoch 9 Step 1301 Train Loss: 0.2549
Epoch 9 Step 1351 Train Loss: 0.2319
Epoch 9 Step 1401 Train Loss: 0.2333
Epoch 9 Step 1451 Train Loss: 0.2329
Epoch 9 Step 1501 Train Loss: 0.2144
Epoch 9 Step 1551 Train Loss: 0.2710
Epoch 9 Step 1601 Train Loss: 0.2383
Epoch 9 Step 1651 Train Loss: 0.2433
Epoch 9 Step 1701 Train Loss: 0.2386
Epoch 9 Step 1751 Train Loss: 0.2379
Epoch 9 Step 1801 Train Loss: 0.2450
Epoch 9 Step 1851 Train Loss: 0.2265
Epoch 9 Step 1901 Train Loss: 0.2254
Epoch 9 Step 1951 Train Loss: 0.2608
Epoch 9 Step 2001 Train Loss: 0.2328
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 10 Step 1 Train Loss: 0.2441
Epoch 10 Step 51 Train Loss: 0.2369
Epoch 10 Step 101 Train Loss: 0.2228
Epoch 10 Step 151 Train Loss: 0.2542
Epoch 10 Step 201 Train Loss: 0.2439
Epoch 10 Step 251 Train Loss: 0.2560
Epoch 10 Step 301 Train Loss: 0.2410
Epoch 10 Step 351 Train Loss: 0.2446
Epoch 10 Step 401 Train Loss: 0.2324
Epoch 10 Step 451 Train Loss: 0.2335
Epoch 10 Step 501 Train Loss: 0.2284
Epoch 10 Step 551 Train Loss: 0.2282
Epoch 10 Step 601 Train Loss: 0.2228
Epoch 10 Step 651 Train Loss: 0.2190
Epoch 10 Step 701 Train Loss: 0.2201
Epoch 10 Step 751 Train Loss: 0.2358
Epoch 10 Step 801 Train Loss: 0.2177
Epoch 10 Step 851 Train Loss: 0.2405
Epoch 10 Step 901 Train Loss: 0.2455
Epoch 10 Step 951 Train Loss: 0.2289
Epoch 10 Step 1001 Train Loss: 0.2332
Epoch 10 Step 1051 Train Loss: 0.2294
Epoch 10 Step 1101 Train Loss: 0.2304
Epoch 10 Step 1151 Train Loss: 0.2340
Epoch 10 Step 1201 Train Loss: 0.2542
Epoch 10 Step 1251 Train Loss: 0.2350
Epoch 10 Step 1301 Train Loss: 0.2609
Epoch 10 Step 1351 Train Loss: 0.2417
Epoch 10 Step 1401 Train Loss: 0.2262
Epoch 10 Step 1451 Train Loss: 0.2415
Epoch 10 Step 1501 Train Loss: 0.2285
Epoch 10 Step 1551 Train Loss: 0.2199
Epoch 10 Step 1601 Train Loss: 0.2471
Epoch 10 Step 1651 Train Loss: 0.2220
Epoch 10 Step 1701 Train Loss: 0.2457
Epoch 10 Step 1751 Train Loss: 0.2361
Epoch 10 Step 1801 Train Loss: 0.2414
Epoch 10 Step 1851 Train Loss: 0.2412
Epoch 10 Step 1901 Train Loss: 0.2287
Epoch 10 Step 1951 Train Loss: 0.2307
Epoch 10 Step 2001 Train Loss: 0.2439
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 11 Step 1 Train Loss: 0.2562
Epoch 11 Step 51 Train Loss: 0.2490
Epoch 11 Step 101 Train Loss: 0.2353
Epoch 11 Step 151 Train Loss: 0.2329
Epoch 11 Step 201 Train Loss: 0.2358
Epoch 11 Step 251 Train Loss: 0.2479
Epoch 11 Step 301 Train Loss: 0.2278
Epoch 11 Step 351 Train Loss: 0.2331
Epoch 11 Step 401 Train Loss: 0.2108
Epoch 11 Step 451 Train Loss: 0.2198
Epoch 11 Step 501 Train Loss: 0.2395
Epoch 11 Step 551 Train Loss: 0.2362
Epoch 11 Step 601 Train Loss: 0.2413
Epoch 11 Step 651 Train Loss: 0.2454
Epoch 11 Step 701 Train Loss: 0.2661
Epoch 11 Step 751 Train Loss: 0.2383
Epoch 11 Step 801 Train Loss: 0.2418
Epoch 11 Step 851 Train Loss: 0.2590
Epoch 11 Step 901 Train Loss: 0.2446
Epoch 11 Step 951 Train Loss: 0.2432
Epoch 11 Step 1001 Train Loss: 0.2441
Epoch 11 Step 1051 Train Loss: 0.2311
Epoch 11 Step 1101 Train Loss: 0.2431
Epoch 11 Step 1151 Train Loss: 0.2393
Epoch 11 Step 1201 Train Loss: 0.2419
Epoch 11 Step 1251 Train Loss: 0.2278
Epoch 11 Step 1301 Train Loss: 0.2191
Epoch 11 Step 1351 Train Loss: 0.2388
Epoch 11 Step 1401 Train Loss: 0.2432
Epoch 11 Step 1451 Train Loss: 0.2368
Epoch 11 Step 1501 Train Loss: 0.2502
Epoch 11 Step 1551 Train Loss: 0.2496
Epoch 11 Step 1601 Train Loss: 0.2156
Epoch 11 Step 1651 Train Loss: 0.2608
Epoch 11 Step 1701 Train Loss: 0.2321
Epoch 11 Step 1751 Train Loss: 0.2281
Epoch 11 Step 1801 Train Loss: 0.2308
Epoch 11 Step 1851 Train Loss: 0.2400
Epoch 11 Step 1901 Train Loss: 0.2518
Epoch 11 Step 1951 Train Loss: 0.2347
Epoch 11 Step 2001 Train Loss: 0.2368
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 12 Step 1 Train Loss: 0.2392
Epoch 12 Step 51 Train Loss: 0.2304
Epoch 12 Step 101 Train Loss: 0.2226
Epoch 12 Step 151 Train Loss: 0.2375
Epoch 12 Step 201 Train Loss: 0.2282
Epoch 12 Step 251 Train Loss: 0.2224
Epoch 12 Step 301 Train Loss: 0.2259
Epoch 12 Step 351 Train Loss: 0.2319
Epoch 12 Step 401 Train Loss: 0.2598
Epoch 12 Step 451 Train Loss: 0.2494
Epoch 12 Step 501 Train Loss: 0.2422
Epoch 12 Step 551 Train Loss: 0.2312
Epoch 12 Step 601 Train Loss: 0.2407
Epoch 12 Step 651 Train Loss: 0.2334
Epoch 12 Step 701 Train Loss: 0.2323
Epoch 12 Step 751 Train Loss: 0.2326
Epoch 12 Step 801 Train Loss: 0.2227
Epoch 12 Step 851 Train Loss: 0.2228
Epoch 12 Step 901 Train Loss: 0.2376
Epoch 12 Step 951 Train Loss: 0.2314
Epoch 12 Step 1001 Train Loss: 0.2291
Epoch 12 Step 1051 Train Loss: 0.2583
Epoch 12 Step 1101 Train Loss: 0.2473
Epoch 12 Step 1151 Train Loss: 0.2493
Epoch 12 Step 1201 Train Loss: 0.2366
Epoch 12 Step 1251 Train Loss: 0.2387
Epoch 12 Step 1301 Train Loss: 0.2285
Epoch 12 Step 1351 Train Loss: 0.2518
Epoch 12 Step 1401 Train Loss: 0.2671
Epoch 12 Step 1451 Train Loss: 0.2313
Epoch 12 Step 1501 Train Loss: 0.2347
Epoch 12 Step 1551 Train Loss: 0.2477
Epoch 12 Step 1601 Train Loss: 0.2340
Epoch 12 Step 1651 Train Loss: 0.2362
Epoch 12 Step 1701 Train Loss: 0.2348
Epoch 12 Step 1751 Train Loss: 0.2282
Epoch 12 Step 1801 Train Loss: 0.2289
Epoch 12 Step 1851 Train Loss: 0.2482
Epoch 12 Step 1901 Train Loss: 0.2249
Epoch 12 Step 1951 Train Loss: 0.2631
Epoch 12 Step 2001 Train Loss: 0.2423
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 13 Step 1 Train Loss: 0.2327
Epoch 13 Step 51 Train Loss: 0.2151
Epoch 13 Step 101 Train Loss: 0.2580
Epoch 13 Step 151 Train Loss: 0.2346
Epoch 13 Step 201 Train Loss: 0.2599
Epoch 13 Step 251 Train Loss: 0.2360
Epoch 13 Step 301 Train Loss: 0.2292
Epoch 13 Step 351 Train Loss: 0.2416
Epoch 13 Step 401 Train Loss: 0.2452
Epoch 13 Step 451 Train Loss: 0.2631
Epoch 13 Step 501 Train Loss: 0.2353
Epoch 13 Step 551 Train Loss: 0.2408
Epoch 13 Step 601 Train Loss: 0.2389
Epoch 13 Step 651 Train Loss: 0.2346
Epoch 13 Step 701 Train Loss: 0.2341
Epoch 13 Step 751 Train Loss: 0.2336
Epoch 13 Step 801 Train Loss: 0.2571
Epoch 13 Step 851 Train Loss: 0.2446
Epoch 13 Step 901 Train Loss: 0.2509
Epoch 13 Step 951 Train Loss: 0.2442
Epoch 13 Step 1001 Train Loss: 0.2469
Epoch 13 Step 1051 Train Loss: 0.2225
Epoch 13 Step 1101 Train Loss: 0.2403
Epoch 13 Step 1151 Train Loss: 0.2502
Epoch 13 Step 1201 Train Loss: 0.2242
Epoch 13 Step 1251 Train Loss: 0.2564
Epoch 13 Step 1301 Train Loss: 0.2343
Epoch 13 Step 1351 Train Loss: 0.2168
Epoch 13 Step 1401 Train Loss: 0.2676
Epoch 13 Step 1451 Train Loss: 0.2167
Epoch 13 Step 1501 Train Loss: 0.2149
Epoch 13 Step 1551 Train Loss: 0.2522
Epoch 13 Step 1601 Train Loss: 0.2488
Epoch 13 Step 1651 Train Loss: 0.2324
Epoch 13 Step 1701 Train Loss: 0.2382
Epoch 13 Step 1751 Train Loss: 0.2272
Epoch 13 Step 1801 Train Loss: 0.2390
Epoch 13 Step 1851 Train Loss: 0.2288
Epoch 13 Step 1901 Train Loss: 0.2531
Epoch 13 Step 1951 Train Loss: 0.2240
Epoch 13 Step 2001 Train Loss: 0.2391
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 14 Step 1 Train Loss: 0.2444
Epoch 14 Step 51 Train Loss: 0.2252
Epoch 14 Step 101 Train Loss: 0.2445
Epoch 14 Step 151 Train Loss: 0.2332
Epoch 14 Step 201 Train Loss: 0.2318
Epoch 14 Step 251 Train Loss: 0.2523
Epoch 14 Step 301 Train Loss: 0.2190
Epoch 14 Step 351 Train Loss: 0.2393
Epoch 14 Step 401 Train Loss: 0.2170
Epoch 14 Step 451 Train Loss: 0.2361
Epoch 14 Step 501 Train Loss: 0.2494
Epoch 14 Step 551 Train Loss: 0.2403
Epoch 14 Step 601 Train Loss: 0.2288
Epoch 14 Step 651 Train Loss: 0.2454
Epoch 14 Step 701 Train Loss: 0.2371
Epoch 14 Step 751 Train Loss: 0.2505
Epoch 14 Step 801 Train Loss: 0.2546
Epoch 14 Step 851 Train Loss: 0.2373
Epoch 14 Step 901 Train Loss: 0.2228
Epoch 14 Step 951 Train Loss: 0.2356
Epoch 14 Step 1001 Train Loss: 0.2382
Epoch 14 Step 1051 Train Loss: 0.2366
Epoch 14 Step 1101 Train Loss: 0.2336
Epoch 14 Step 1151 Train Loss: 0.2231
Epoch 14 Step 1201 Train Loss: 0.2458
Epoch 14 Step 1251 Train Loss: 0.2448
Epoch 14 Step 1301 Train Loss: 0.2199
Epoch 14 Step 1351 Train Loss: 0.2485
Epoch 14 Step 1401 Train Loss: 0.2331
Epoch 14 Step 1451 Train Loss: 0.2463
Epoch 14 Step 1501 Train Loss: 0.2644
Epoch 14 Step 1551 Train Loss: 0.2346
Epoch 14 Step 1601 Train Loss: 0.2246
Epoch 14 Step 1651 Train Loss: 0.2386
Epoch 14 Step 1701 Train Loss: 0.2438
Epoch 14 Step 1751 Train Loss: 0.2272
Epoch 14 Step 1801 Train Loss: 0.2281
Epoch 14 Step 1851 Train Loss: 0.2642
Epoch 14 Step 1901 Train Loss: 0.2298
Epoch 14 Step 1951 Train Loss: 0.2609
Epoch 14 Step 2001 Train Loss: 0.2263
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Epoch 15 Step 1 Train Loss: 0.2393
Epoch 15 Step 51 Train Loss: 0.2380
Epoch 15 Step 101 Train Loss: 0.2310
Epoch 15 Step 151 Train Loss: 0.2279
Epoch 15 Step 201 Train Loss: 0.2392
Epoch 15 Step 251 Train Loss: 0.2365
Epoch 15 Step 301 Train Loss: 0.2295
Epoch 15 Step 351 Train Loss: 0.2484
Epoch 15 Step 401 Train Loss: 0.2319
Epoch 15 Step 451 Train Loss: 0.2425
Epoch 15 Step 501 Train Loss: 0.2339
Epoch 15 Step 551 Train Loss: 0.2653
Epoch 15 Step 601 Train Loss: 0.2326
Epoch 15 Step 651 Train Loss: 0.2331
Epoch 15 Step 701 Train Loss: 0.2438
Epoch 15 Step 751 Train Loss: 0.2410
Epoch 15 Step 801 Train Loss: 0.2348
Epoch 15 Step 851 Train Loss: 0.2548
Epoch 15 Step 901 Train Loss: 0.2519
Epoch 15 Step 951 Train Loss: 0.2495
Epoch 15 Step 1001 Train Loss: 0.2179
Epoch 15 Step 1051 Train Loss: 0.2449
Epoch 15 Step 1101 Train Loss: 0.2189
Epoch 15 Step 1151 Train Loss: 0.2313
Epoch 15 Step 1201 Train Loss: 0.2690
Epoch 15 Step 1251 Train Loss: 0.2264
Epoch 15 Step 1301 Train Loss: 0.2252
Epoch 15 Step 1351 Train Loss: 0.2550
Epoch 15 Step 1401 Train Loss: 0.2442
Epoch 15 Step 1451 Train Loss: 0.2356
Epoch 15 Step 1501 Train Loss: 0.2286
Epoch 15 Step 1551 Train Loss: 0.2128
Epoch 15 Step 1601 Train Loss: 0.2415
Epoch 15 Step 1651 Train Loss: 0.2364
Epoch 15 Step 1701 Train Loss: 0.2348
Epoch 15 Step 1751 Train Loss: 0.2460
Epoch 15 Step 1801 Train Loss: 0.2368
Epoch 15 Step 1851 Train Loss: 0.2331
Epoch 15 Step 1901 Train Loss: 0.2205
Epoch 15 Step 1951 Train Loss: 0.2653
Epoch 15 Step 2001 Train Loss: 0.2202
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0038. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0040
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00036253993
test_unseen_single_pearson: 0.9923235550168112
test_unseen_single_mse_de: 0.0040462203
test_unseen_single_pearson_de: 0.8040085781216362
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1522301990647424
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.34411764705882353
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9284313725490196
test_unseen_single_mse_top20_de_non_dropout: 0.00864232524972995
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.040 MB uploadedwandb: | 0.001 MB of 0.040 MB uploadedwandb: / 0.038 MB of 0.040 MB uploadedwandb: - 0.038 MB of 0.040 MB uploadedwandb: \ 0.040 MB of 0.040 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÉ
wandb:                                                   val_de_mse ‚ñÜ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00405
wandb:                                              test_de_pearson 0.80401
wandb:               test_frac_opposite_direction_top20_non_dropout 0.34412
wandb:                          test_frac_sigma_below_1_non_dropout 0.92843
wandb:                                                     test_mse 0.00036
wandb:                                test_mse_top20_de_non_dropout 0.00864
wandb:                                                 test_pearson 0.99232
wandb:                                           test_pearson_delta 0.15223
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.34412
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.92843
wandb:                                       test_unseen_single_mse 0.00036
wandb:                                    test_unseen_single_mse_de 0.00405
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00864
wandb:                                   test_unseen_single_pearson 0.99232
wandb:                                test_unseen_single_pearson_de 0.80401
wandb:                             test_unseen_single_pearson_delta 0.15223
wandb:                                                 train_de_mse 0.0061
wandb:                                             train_de_pearson 0.79554
wandb:                                                    train_mse 0.00032
wandb:                                                train_pearson 0.99306
wandb:                                                training_loss 0.23029
wandb:                                                   val_de_mse 0.00379
wandb:                                               val_de_pearson 0.70241
wandb:                                                      val_mse 0.00056
wandb:                                                  val_pearson 0.98822
wandb: 
wandb: üöÄ View run geneformer_XuCao2023_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/cvy3uh5y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_035353-cvy3uh5y/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_044636-zek8phm9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_XuCao2023_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zek8phm9
wandb: WARNING Serializing object of type ndarray that is 21184640 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2507
Epoch 1 Step 51 Train Loss: 0.2665
Epoch 1 Step 101 Train Loss: 0.2580
Epoch 1 Step 151 Train Loss: 0.2508
Epoch 1 Step 201 Train Loss: 0.2396
Epoch 1 Step 251 Train Loss: 0.2567
Epoch 1 Step 301 Train Loss: 0.2409
Epoch 1 Step 351 Train Loss: 0.2309
Epoch 1 Step 401 Train Loss: 0.2414
Epoch 1 Step 451 Train Loss: 0.2657
Epoch 1 Step 501 Train Loss: 0.2409
Epoch 1 Step 551 Train Loss: 0.2438
Epoch 1 Step 601 Train Loss: 0.2300
Epoch 1 Step 651 Train Loss: 0.2274
Epoch 1 Step 701 Train Loss: 0.2335
Epoch 1 Step 751 Train Loss: 0.2271
Epoch 1 Step 801 Train Loss: 0.2322
Epoch 1 Step 851 Train Loss: 0.2580
Epoch 1 Step 901 Train Loss: 0.2250
Epoch 1 Step 951 Train Loss: 0.2090
Epoch 1 Step 1001 Train Loss: 0.2027
Epoch 1 Step 1051 Train Loss: 0.2346
Epoch 1 Step 1101 Train Loss: 0.1938
Epoch 1 Step 1151 Train Loss: 0.2200
Epoch 1 Step 1201 Train Loss: 0.2278
Epoch 1 Step 1251 Train Loss: 0.2482
Epoch 1 Step 1301 Train Loss: 0.2305
Epoch 1 Step 1351 Train Loss: 0.2162
Epoch 1 Step 1401 Train Loss: 0.2403
Epoch 1 Step 1451 Train Loss: 0.2259
Epoch 1 Step 1501 Train Loss: 0.2291
Epoch 1 Step 1551 Train Loss: 0.2369
Epoch 1 Step 1601 Train Loss: 0.2175
Epoch 1 Step 1651 Train Loss: 0.2309
Epoch 1 Step 1701 Train Loss: 0.2241
Epoch 1 Step 1751 Train Loss: 0.2097
Epoch 1 Step 1801 Train Loss: 0.2511
Epoch 1 Step 1851 Train Loss: 0.2465
Epoch 1 Step 1901 Train Loss: 0.2054
Epoch 1: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0154. 
Epoch 2 Step 1 Train Loss: 0.2570
Epoch 2 Step 51 Train Loss: 0.2364
Epoch 2 Step 101 Train Loss: 0.2212
Epoch 2 Step 151 Train Loss: 0.2516
Epoch 2 Step 201 Train Loss: 0.2068
Epoch 2 Step 251 Train Loss: 0.2363
Epoch 2 Step 301 Train Loss: 0.2245
Epoch 2 Step 351 Train Loss: 0.2197
Epoch 2 Step 401 Train Loss: 0.2489
Epoch 2 Step 451 Train Loss: 0.2198
Epoch 2 Step 501 Train Loss: 0.2205
Epoch 2 Step 551 Train Loss: 0.2183
Epoch 2 Step 601 Train Loss: 0.2321
Epoch 2 Step 651 Train Loss: 0.2194
Epoch 2 Step 701 Train Loss: 0.2284
Epoch 2 Step 751 Train Loss: 0.2094
Epoch 2 Step 801 Train Loss: 0.2557
Epoch 2 Step 851 Train Loss: 0.2192
Epoch 2 Step 901 Train Loss: 0.2158
Epoch 2 Step 951 Train Loss: 0.2310
Epoch 2 Step 1001 Train Loss: 0.2404
Epoch 2 Step 1051 Train Loss: 0.2524
Epoch 2 Step 1101 Train Loss: 0.2331
Epoch 2 Step 1151 Train Loss: 0.2129
Epoch 2 Step 1201 Train Loss: 0.2437
Epoch 2 Step 1251 Train Loss: 0.2369
Epoch 2 Step 1301 Train Loss: 0.2081
Epoch 2 Step 1351 Train Loss: 0.2277
Epoch 2 Step 1401 Train Loss: 0.2210
Epoch 2 Step 1451 Train Loss: 0.2214
Epoch 2 Step 1501 Train Loss: 0.2130
Epoch 2 Step 1551 Train Loss: 0.2021
Epoch 2 Step 1601 Train Loss: 0.2215
Epoch 2 Step 1651 Train Loss: 0.2310
Epoch 2 Step 1701 Train Loss: 0.2283
Epoch 2 Step 1751 Train Loss: 0.2254
Epoch 2 Step 1801 Train Loss: 0.2204
Epoch 2 Step 1851 Train Loss: 0.2030
Epoch 2 Step 1901 Train Loss: 0.2316
Epoch 2: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0153. 
Epoch 3 Step 1 Train Loss: 0.2201
Epoch 3 Step 51 Train Loss: 0.2274
Epoch 3 Step 101 Train Loss: 0.2453
Epoch 3 Step 151 Train Loss: 0.2230
Epoch 3 Step 201 Train Loss: 0.2300
Epoch 3 Step 251 Train Loss: 0.2228
Epoch 3 Step 301 Train Loss: 0.2390
Epoch 3 Step 351 Train Loss: 0.2384
Epoch 3 Step 401 Train Loss: 0.2156
Epoch 3 Step 451 Train Loss: 0.2297
Epoch 3 Step 501 Train Loss: 0.2076
Epoch 3 Step 551 Train Loss: 0.2318
Epoch 3 Step 601 Train Loss: 0.2552
Epoch 3 Step 651 Train Loss: 0.2537
Epoch 3 Step 701 Train Loss: 0.2144
Epoch 3 Step 751 Train Loss: 0.2276
Epoch 3 Step 801 Train Loss: 0.2400
Epoch 3 Step 851 Train Loss: 0.2100
Epoch 3 Step 901 Train Loss: 0.2298
Epoch 3 Step 951 Train Loss: 0.2210
Epoch 3 Step 1001 Train Loss: 0.2302
Epoch 3 Step 1051 Train Loss: 0.2309
Epoch 3 Step 1101 Train Loss: 0.2322
Epoch 3 Step 1151 Train Loss: 0.2536
Epoch 3 Step 1201 Train Loss: 0.2218
Epoch 3 Step 1251 Train Loss: 0.2471
Epoch 3 Step 1301 Train Loss: 0.2125
Epoch 3 Step 1351 Train Loss: 0.2333
Epoch 3 Step 1401 Train Loss: 0.2231
Epoch 3 Step 1451 Train Loss: 0.2400
Epoch 3 Step 1501 Train Loss: 0.2321
Epoch 3 Step 1551 Train Loss: 0.2592
Epoch 3 Step 1601 Train Loss: 0.2474
Epoch 3 Step 1651 Train Loss: 0.2152
Epoch 3 Step 1701 Train Loss: 0.2442
Epoch 3 Step 1751 Train Loss: 0.2376
Epoch 3 Step 1801 Train Loss: 0.2273
Epoch 3 Step 1851 Train Loss: 0.2372
Epoch 3 Step 1901 Train Loss: 0.2242
Epoch 3: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 4 Step 1 Train Loss: 0.2214
Epoch 4 Step 51 Train Loss: 0.2269
Epoch 4 Step 101 Train Loss: 0.2378
Epoch 4 Step 151 Train Loss: 0.2376
Epoch 4 Step 201 Train Loss: 0.2101
Epoch 4 Step 251 Train Loss: 0.2342
Epoch 4 Step 301 Train Loss: 0.2437
Epoch 4 Step 351 Train Loss: 0.2299
Epoch 4 Step 401 Train Loss: 0.2253
Epoch 4 Step 451 Train Loss: 0.2409
Epoch 4 Step 501 Train Loss: 0.2240
Epoch 4 Step 551 Train Loss: 0.2331
Epoch 4 Step 601 Train Loss: 0.2397
Epoch 4 Step 651 Train Loss: 0.2361
Epoch 4 Step 701 Train Loss: 0.2264
Epoch 4 Step 751 Train Loss: 0.2189
Epoch 4 Step 801 Train Loss: 0.2321
Epoch 4 Step 851 Train Loss: 0.2355
Epoch 4 Step 901 Train Loss: 0.2208
Epoch 4 Step 951 Train Loss: 0.2435
Epoch 4 Step 1001 Train Loss: 0.2375
Epoch 4 Step 1051 Train Loss: 0.2409
Epoch 4 Step 1101 Train Loss: 0.2292
Epoch 4 Step 1151 Train Loss: 0.2378
Epoch 4 Step 1201 Train Loss: 0.2251
Epoch 4 Step 1251 Train Loss: 0.2173
Epoch 4 Step 1301 Train Loss: 0.2392
Epoch 4 Step 1351 Train Loss: 0.2481
Epoch 4 Step 1401 Train Loss: 0.2238
Epoch 4 Step 1451 Train Loss: 0.2312
Epoch 4 Step 1501 Train Loss: 0.2130
Epoch 4 Step 1551 Train Loss: 0.2554
Epoch 4 Step 1601 Train Loss: 0.2514
Epoch 4 Step 1651 Train Loss: 0.2426
Epoch 4 Step 1701 Train Loss: 0.2217
Epoch 4 Step 1751 Train Loss: 0.2422
Epoch 4 Step 1801 Train Loss: 0.2235
Epoch 4 Step 1851 Train Loss: 0.2698
Epoch 4 Step 1901 Train Loss: 0.2332
Epoch 4: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0153. 
Epoch 5 Step 1 Train Loss: 0.2537
Epoch 5 Step 51 Train Loss: 0.2272
Epoch 5 Step 101 Train Loss: 0.2480
Epoch 5 Step 151 Train Loss: 0.2284
Epoch 5 Step 201 Train Loss: 0.2294
Epoch 5 Step 251 Train Loss: 0.2275
Epoch 5 Step 301 Train Loss: 0.2546
Epoch 5 Step 351 Train Loss: 0.2323
Epoch 5 Step 401 Train Loss: 0.2295
Epoch 5 Step 451 Train Loss: 0.2338
Epoch 5 Step 501 Train Loss: 0.2294
Epoch 5 Step 551 Train Loss: 0.2344
Epoch 5 Step 601 Train Loss: 0.2390
Epoch 5 Step 651 Train Loss: 0.2260
Epoch 5 Step 701 Train Loss: 0.2496
Epoch 5 Step 751 Train Loss: 0.2767
Epoch 5 Step 801 Train Loss: 0.2477
Epoch 5 Step 851 Train Loss: 0.2439
Epoch 5 Step 901 Train Loss: 0.2550
Epoch 5 Step 951 Train Loss: 0.2403
Epoch 5 Step 1001 Train Loss: 0.2120
Epoch 5 Step 1051 Train Loss: 0.2504
Epoch 5 Step 1101 Train Loss: 0.2441
Epoch 5 Step 1151 Train Loss: 0.2452
Epoch 5 Step 1201 Train Loss: 0.2429
Epoch 5 Step 1251 Train Loss: 0.2365
Epoch 5 Step 1301 Train Loss: 0.2504
Epoch 5 Step 1351 Train Loss: 0.2331
Epoch 5 Step 1401 Train Loss: 0.2499
Epoch 5 Step 1451 Train Loss: 0.2160
Epoch 5 Step 1501 Train Loss: 0.2446
Epoch 5 Step 1551 Train Loss: 0.2328
Epoch 5 Step 1601 Train Loss: 0.2204
Epoch 5 Step 1651 Train Loss: 0.2246
Epoch 5 Step 1701 Train Loss: 0.2471
Epoch 5 Step 1751 Train Loss: 0.2468
Epoch 5 Step 1801 Train Loss: 0.2376
Epoch 5 Step 1851 Train Loss: 0.2335
Epoch 5 Step 1901 Train Loss: 0.2530
Epoch 5: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0153. 
Epoch 6 Step 1 Train Loss: 0.2440
Epoch 6 Step 51 Train Loss: 0.2558
Epoch 6 Step 101 Train Loss: 0.2551
Epoch 6 Step 151 Train Loss: 0.2273
Epoch 6 Step 201 Train Loss: 0.2242
Epoch 6 Step 251 Train Loss: 0.2421
Epoch 6 Step 301 Train Loss: 0.2351
Epoch 6 Step 351 Train Loss: 0.2400
Epoch 6 Step 401 Train Loss: 0.2254
Epoch 6 Step 451 Train Loss: 0.2446
Epoch 6 Step 501 Train Loss: 0.2538
Epoch 6 Step 551 Train Loss: 0.2394
Epoch 6 Step 601 Train Loss: 0.2675
Epoch 6 Step 651 Train Loss: 0.2207
Epoch 6 Step 701 Train Loss: 0.2300
Epoch 6 Step 751 Train Loss: 0.2281
Epoch 6 Step 801 Train Loss: 0.2281
Epoch 6 Step 851 Train Loss: 0.2484
Epoch 6 Step 901 Train Loss: 0.2306
Epoch 6 Step 951 Train Loss: 0.2497
Epoch 6 Step 1001 Train Loss: 0.2369
Epoch 6 Step 1051 Train Loss: 0.2287
Epoch 6 Step 1101 Train Loss: 0.2354
Epoch 6 Step 1151 Train Loss: 0.2380
Epoch 6 Step 1201 Train Loss: 0.2252
Epoch 6 Step 1251 Train Loss: 0.2193
Epoch 6 Step 1301 Train Loss: 0.2382
Epoch 6 Step 1351 Train Loss: 0.2413
Epoch 6 Step 1401 Train Loss: 0.2538
Epoch 6 Step 1451 Train Loss: 0.2426
Epoch 6 Step 1501 Train Loss: 0.2351
Epoch 6 Step 1551 Train Loss: 0.2394
Epoch 6 Step 1601 Train Loss: 0.2257
Epoch 6 Step 1651 Train Loss: 0.2719
Epoch 6 Step 1701 Train Loss: 0.2332
Epoch 6 Step 1751 Train Loss: 0.2563
Epoch 6 Step 1801 Train Loss: 0.2270
Epoch 6 Step 1851 Train Loss: 0.2422
Epoch 6 Step 1901 Train Loss: 0.2555
Epoch 6: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 7 Step 1 Train Loss: 0.2449
Epoch 7 Step 51 Train Loss: 0.2431
Epoch 7 Step 101 Train Loss: 0.2559
Epoch 7 Step 151 Train Loss: 0.2386
Epoch 7 Step 201 Train Loss: 0.2291
Epoch 7 Step 251 Train Loss: 0.2273
Epoch 7 Step 301 Train Loss: 0.2261
Epoch 7 Step 351 Train Loss: 0.2379
Epoch 7 Step 401 Train Loss: 0.2420
Epoch 7 Step 451 Train Loss: 0.2713
Epoch 7 Step 501 Train Loss: 0.2515
Epoch 7 Step 551 Train Loss: 0.2781
Epoch 7 Step 601 Train Loss: 0.2281
Epoch 7 Step 651 Train Loss: 0.2332
Epoch 7 Step 701 Train Loss: 0.2528
Epoch 7 Step 751 Train Loss: 0.2356
Epoch 7 Step 801 Train Loss: 0.2662
Epoch 7 Step 851 Train Loss: 0.2245
Epoch 7 Step 901 Train Loss: 0.2560
Epoch 7 Step 951 Train Loss: 0.2339
Epoch 7 Step 1001 Train Loss: 0.2362
Epoch 7 Step 1051 Train Loss: 0.2187
Epoch 7 Step 1101 Train Loss: 0.2199
Epoch 7 Step 1151 Train Loss: 0.2283
Epoch 7 Step 1201 Train Loss: 0.2297
Epoch 7 Step 1251 Train Loss: 0.2541
Epoch 7 Step 1301 Train Loss: 0.2272
Epoch 7 Step 1351 Train Loss: 0.2609
Epoch 7 Step 1401 Train Loss: 0.2410
Epoch 7 Step 1451 Train Loss: 0.2250
Epoch 7 Step 1501 Train Loss: 0.2365
Epoch 7 Step 1551 Train Loss: 0.2635
Epoch 7 Step 1601 Train Loss: 0.2366
Epoch 7 Step 1651 Train Loss: 0.2456
Epoch 7 Step 1701 Train Loss: 0.2388
Epoch 7 Step 1751 Train Loss: 0.2238
Epoch 7 Step 1801 Train Loss: 0.2439
Epoch 7 Step 1851 Train Loss: 0.2249
Epoch 7 Step 1901 Train Loss: 0.2392
Epoch 7: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 8 Step 1 Train Loss: 0.2414
Epoch 8 Step 51 Train Loss: 0.2232
Epoch 8 Step 101 Train Loss: 0.2280
Epoch 8 Step 151 Train Loss: 0.2439
Epoch 8 Step 201 Train Loss: 0.2316
Epoch 8 Step 251 Train Loss: 0.2611
Epoch 8 Step 301 Train Loss: 0.2473
Epoch 8 Step 351 Train Loss: 0.2345
Epoch 8 Step 401 Train Loss: 0.2388
Epoch 8 Step 451 Train Loss: 0.2327
Epoch 8 Step 501 Train Loss: 0.2541
Epoch 8 Step 551 Train Loss: 0.2385
Epoch 8 Step 601 Train Loss: 0.2703
Epoch 8 Step 651 Train Loss: 0.2424
Epoch 8 Step 701 Train Loss: 0.2438
Epoch 8 Step 751 Train Loss: 0.2515
Epoch 8 Step 801 Train Loss: 0.2339
Epoch 8 Step 851 Train Loss: 0.2420
Epoch 8 Step 901 Train Loss: 0.2186
Epoch 8 Step 951 Train Loss: 0.2290
Epoch 8 Step 1001 Train Loss: 0.2282
Epoch 8 Step 1051 Train Loss: 0.2146
Epoch 8 Step 1101 Train Loss: 0.2577
Epoch 8 Step 1151 Train Loss: 0.2334
Epoch 8 Step 1201 Train Loss: 0.2380
Epoch 8 Step 1251 Train Loss: 0.2439
Epoch 8 Step 1301 Train Loss: 0.2576
Epoch 8 Step 1351 Train Loss: 0.2581
Epoch 8 Step 1401 Train Loss: 0.2441
Epoch 8 Step 1451 Train Loss: 0.2480
Epoch 8 Step 1501 Train Loss: 0.2772
Epoch 8 Step 1551 Train Loss: 0.2675
Epoch 8 Step 1601 Train Loss: 0.2216
Epoch 8 Step 1651 Train Loss: 0.2289
Epoch 8 Step 1701 Train Loss: 0.2562
Epoch 8 Step 1751 Train Loss: 0.2389
Epoch 8 Step 1801 Train Loss: 0.2516
Epoch 8 Step 1851 Train Loss: 0.2346
Epoch 8 Step 1901 Train Loss: 0.2284
Epoch 8: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 9 Step 1 Train Loss: 0.2220
Epoch 9 Step 51 Train Loss: 0.2348
Epoch 9 Step 101 Train Loss: 0.2449
Epoch 9 Step 151 Train Loss: 0.2714
Epoch 9 Step 201 Train Loss: 0.2257
Epoch 9 Step 251 Train Loss: 0.2471
Epoch 9 Step 301 Train Loss: 0.2584
Epoch 9 Step 351 Train Loss: 0.2661
Epoch 9 Step 401 Train Loss: 0.2275
Epoch 9 Step 451 Train Loss: 0.2357
Epoch 9 Step 501 Train Loss: 0.2285
Epoch 9 Step 551 Train Loss: 0.2226
Epoch 9 Step 601 Train Loss: 0.2338
Epoch 9 Step 651 Train Loss: 0.2667
Epoch 9 Step 701 Train Loss: 0.2419
Epoch 9 Step 751 Train Loss: 0.2410
Epoch 9 Step 801 Train Loss: 0.2243
Epoch 9 Step 851 Train Loss: 0.2461
Epoch 9 Step 901 Train Loss: 0.2455
Epoch 9 Step 951 Train Loss: 0.2360
Epoch 9 Step 1001 Train Loss: 0.2454
Epoch 9 Step 1051 Train Loss: 0.2417
Epoch 9 Step 1101 Train Loss: 0.2494
Epoch 9 Step 1151 Train Loss: 0.2439
Epoch 9 Step 1201 Train Loss: 0.2540
Epoch 9 Step 1251 Train Loss: 0.2441
Epoch 9 Step 1301 Train Loss: 0.2539
Epoch 9 Step 1351 Train Loss: 0.2572
Epoch 9 Step 1401 Train Loss: 0.2471
Epoch 9 Step 1451 Train Loss: 0.2412
Epoch 9 Step 1501 Train Loss: 0.2442
Epoch 9 Step 1551 Train Loss: 0.2422
Epoch 9 Step 1601 Train Loss: 0.2374
Epoch 9 Step 1651 Train Loss: 0.2251
Epoch 9 Step 1701 Train Loss: 0.2288
Epoch 9 Step 1751 Train Loss: 0.2608
Epoch 9 Step 1801 Train Loss: 0.2233
Epoch 9 Step 1851 Train Loss: 0.2525
Epoch 9 Step 1901 Train Loss: 0.2346
Epoch 9: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 10 Step 1 Train Loss: 0.2360
Epoch 10 Step 51 Train Loss: 0.2386
Epoch 10 Step 101 Train Loss: 0.2315
Epoch 10 Step 151 Train Loss: 0.2384
Epoch 10 Step 201 Train Loss: 0.2369
Epoch 10 Step 251 Train Loss: 0.2570
Epoch 10 Step 301 Train Loss: 0.2278
Epoch 10 Step 351 Train Loss: 0.2184
Epoch 10 Step 401 Train Loss: 0.2409
Epoch 10 Step 451 Train Loss: 0.2418
Epoch 10 Step 501 Train Loss: 0.2288
Epoch 10 Step 551 Train Loss: 0.2493
Epoch 10 Step 601 Train Loss: 0.2331
Epoch 10 Step 651 Train Loss: 0.2215
Epoch 10 Step 701 Train Loss: 0.2361
Epoch 10 Step 751 Train Loss: 0.2480
Epoch 10 Step 801 Train Loss: 0.2514
Epoch 10 Step 851 Train Loss: 0.2282
Epoch 10 Step 901 Train Loss: 0.2433
Epoch 10 Step 951 Train Loss: 0.2439
Epoch 10 Step 1001 Train Loss: 0.2451
Epoch 10 Step 1051 Train Loss: 0.2158
Epoch 10 Step 1101 Train Loss: 0.2382
Epoch 10 Step 1151 Train Loss: 0.2454
Epoch 10 Step 1201 Train Loss: 0.2247
Epoch 10 Step 1251 Train Loss: 0.2473
Epoch 10 Step 1301 Train Loss: 0.2359
Epoch 10 Step 1351 Train Loss: 0.2533
Epoch 10 Step 1401 Train Loss: 0.2259
Epoch 10 Step 1451 Train Loss: 0.2558
Epoch 10 Step 1501 Train Loss: 0.2397
Epoch 10 Step 1551 Train Loss: 0.2486
Epoch 10 Step 1601 Train Loss: 0.2427
Epoch 10 Step 1651 Train Loss: 0.2445
Epoch 10 Step 1701 Train Loss: 0.2497
Epoch 10 Step 1751 Train Loss: 0.2548
Epoch 10 Step 1801 Train Loss: 0.2494
Epoch 10 Step 1851 Train Loss: 0.2345
Epoch 10 Step 1901 Train Loss: 0.2383
Epoch 10: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 11 Step 1 Train Loss: 0.2370
Epoch 11 Step 51 Train Loss: 0.2424
Epoch 11 Step 101 Train Loss: 0.2318
Epoch 11 Step 151 Train Loss: 0.2412
Epoch 11 Step 201 Train Loss: 0.2684
Epoch 11 Step 251 Train Loss: 0.2267
Epoch 11 Step 301 Train Loss: 0.2477
Epoch 11 Step 351 Train Loss: 0.2349
Epoch 11 Step 401 Train Loss: 0.2534
Epoch 11 Step 451 Train Loss: 0.2409
Epoch 11 Step 501 Train Loss: 0.2555
Epoch 11 Step 551 Train Loss: 0.2455
Epoch 11 Step 601 Train Loss: 0.2377
Epoch 11 Step 651 Train Loss: 0.2551
Epoch 11 Step 701 Train Loss: 0.2377
Epoch 11 Step 751 Train Loss: 0.2291
Epoch 11 Step 801 Train Loss: 0.2606
Epoch 11 Step 851 Train Loss: 0.2239
Epoch 11 Step 901 Train Loss: 0.2304
Epoch 11 Step 951 Train Loss: 0.2427
Epoch 11 Step 1001 Train Loss: 0.2344
Epoch 11 Step 1051 Train Loss: 0.2330
Epoch 11 Step 1101 Train Loss: 0.2568
Epoch 11 Step 1151 Train Loss: 0.2359
Epoch 11 Step 1201 Train Loss: 0.2596
Epoch 11 Step 1251 Train Loss: 0.2418
Epoch 11 Step 1301 Train Loss: 0.2499
Epoch 11 Step 1351 Train Loss: 0.2354
Epoch 11 Step 1401 Train Loss: 0.2216
Epoch 11 Step 1451 Train Loss: 0.2375
Epoch 11 Step 1501 Train Loss: 0.2486
Epoch 11 Step 1551 Train Loss: 0.2368
Epoch 11 Step 1601 Train Loss: 0.2371
Epoch 11 Step 1651 Train Loss: 0.2297
Epoch 11 Step 1701 Train Loss: 0.2395
Epoch 11 Step 1751 Train Loss: 0.2295
Epoch 11 Step 1801 Train Loss: 0.2557
Epoch 11 Step 1851 Train Loss: 0.2442
Epoch 11 Step 1901 Train Loss: 0.2392
Epoch 11: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 12 Step 1 Train Loss: 0.2528
Epoch 12 Step 51 Train Loss: 0.2486
Epoch 12 Step 101 Train Loss: 0.2248
Epoch 12 Step 151 Train Loss: 0.2440
Epoch 12 Step 201 Train Loss: 0.2358
Epoch 12 Step 251 Train Loss: 0.2413
Epoch 12 Step 301 Train Loss: 0.2486
Epoch 12 Step 351 Train Loss: 0.2395
Epoch 12 Step 401 Train Loss: 0.2679
Epoch 12 Step 451 Train Loss: 0.2336
Epoch 12 Step 501 Train Loss: 0.2305
Epoch 12 Step 551 Train Loss: 0.2250
Epoch 12 Step 601 Train Loss: 0.2386
Epoch 12 Step 651 Train Loss: 0.2251
Epoch 12 Step 701 Train Loss: 0.2396
Epoch 12 Step 751 Train Loss: 0.2319
Epoch 12 Step 801 Train Loss: 0.2262
Epoch 12 Step 851 Train Loss: 0.2511
Epoch 12 Step 901 Train Loss: 0.2511
Epoch 12 Step 951 Train Loss: 0.2429
Epoch 12 Step 1001 Train Loss: 0.2435
Epoch 12 Step 1051 Train Loss: 0.2360
Epoch 12 Step 1101 Train Loss: 0.2360
Epoch 12 Step 1151 Train Loss: 0.2510
Epoch 12 Step 1201 Train Loss: 0.2572
Epoch 12 Step 1251 Train Loss: 0.2483
Epoch 12 Step 1301 Train Loss: 0.2407
Epoch 12 Step 1351 Train Loss: 0.2317
Epoch 12 Step 1401 Train Loss: 0.2410
Epoch 12 Step 1451 Train Loss: 0.2307
Epoch 12 Step 1501 Train Loss: 0.2416
Epoch 12 Step 1551 Train Loss: 0.2371
Epoch 12 Step 1601 Train Loss: 0.2170
Epoch 12 Step 1651 Train Loss: 0.2526
Epoch 12 Step 1701 Train Loss: 0.2662
Epoch 12 Step 1751 Train Loss: 0.2287
Epoch 12 Step 1801 Train Loss: 0.2365
Epoch 12 Step 1851 Train Loss: 0.2291
Epoch 12 Step 1901 Train Loss: 0.2583
Epoch 12: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 13 Step 1 Train Loss: 0.2273
Epoch 13 Step 51 Train Loss: 0.2578
Epoch 13 Step 101 Train Loss: 0.2250
Epoch 13 Step 151 Train Loss: 0.2208
Epoch 13 Step 201 Train Loss: 0.2273
Epoch 13 Step 251 Train Loss: 0.2468
Epoch 13 Step 301 Train Loss: 0.2488
Epoch 13 Step 351 Train Loss: 0.2317
Epoch 13 Step 401 Train Loss: 0.2175
Epoch 13 Step 451 Train Loss: 0.2385
Epoch 13 Step 501 Train Loss: 0.2518
Epoch 13 Step 551 Train Loss: 0.2350
Epoch 13 Step 601 Train Loss: 0.2294
Epoch 13 Step 651 Train Loss: 0.2365
Epoch 13 Step 701 Train Loss: 0.2250
Epoch 13 Step 751 Train Loss: 0.2350
Epoch 13 Step 801 Train Loss: 0.2343
Epoch 13 Step 851 Train Loss: 0.2182
Epoch 13 Step 901 Train Loss: 0.2403
Epoch 13 Step 951 Train Loss: 0.2599
Epoch 13 Step 1001 Train Loss: 0.2553
Epoch 13 Step 1051 Train Loss: 0.2538
Epoch 13 Step 1101 Train Loss: 0.2322
Epoch 13 Step 1151 Train Loss: 0.2449
Epoch 13 Step 1201 Train Loss: 0.2472
Epoch 13 Step 1251 Train Loss: 0.2283
Epoch 13 Step 1301 Train Loss: 0.2253
Epoch 13 Step 1351 Train Loss: 0.2301
Epoch 13 Step 1401 Train Loss: 0.2301
Epoch 13 Step 1451 Train Loss: 0.2464
Epoch 13 Step 1501 Train Loss: 0.2502
Epoch 13 Step 1551 Train Loss: 0.2475
Epoch 13 Step 1601 Train Loss: 0.2512
Epoch 13 Step 1651 Train Loss: 0.2445
Epoch 13 Step 1701 Train Loss: 0.2396
Epoch 13 Step 1751 Train Loss: 0.2539
Epoch 13 Step 1801 Train Loss: 0.2379
Epoch 13 Step 1851 Train Loss: 0.2598
Epoch 13 Step 1901 Train Loss: 0.2469
Epoch 13: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 14 Step 1 Train Loss: 0.2576
Epoch 14 Step 51 Train Loss: 0.2286
Epoch 14 Step 101 Train Loss: 0.2475
Epoch 14 Step 151 Train Loss: 0.2450
Epoch 14 Step 201 Train Loss: 0.2350
Epoch 14 Step 251 Train Loss: 0.2461
Epoch 14 Step 301 Train Loss: 0.2389
Epoch 14 Step 351 Train Loss: 0.2514
Epoch 14 Step 401 Train Loss: 0.2230
Epoch 14 Step 451 Train Loss: 0.2297
Epoch 14 Step 501 Train Loss: 0.2467
Epoch 14 Step 551 Train Loss: 0.2283
Epoch 14 Step 601 Train Loss: 0.2337
Epoch 14 Step 651 Train Loss: 0.2274
Epoch 14 Step 701 Train Loss: 0.2297
Epoch 14 Step 751 Train Loss: 0.2380
Epoch 14 Step 801 Train Loss: 0.2424
Epoch 14 Step 851 Train Loss: 0.2479
Epoch 14 Step 901 Train Loss: 0.2507
Epoch 14 Step 951 Train Loss: 0.2616
Epoch 14 Step 1001 Train Loss: 0.2470
Epoch 14 Step 1051 Train Loss: 0.2279
Epoch 14 Step 1101 Train Loss: 0.2303
Epoch 14 Step 1151 Train Loss: 0.2358
Epoch 14 Step 1201 Train Loss: 0.2415
Epoch 14 Step 1251 Train Loss: 0.2541
Epoch 14 Step 1301 Train Loss: 0.2497
Epoch 14 Step 1351 Train Loss: 0.2332
Epoch 14 Step 1401 Train Loss: 0.2471
Epoch 14 Step 1451 Train Loss: 0.2224
Epoch 14 Step 1501 Train Loss: 0.2217
Epoch 14 Step 1551 Train Loss: 0.2223
Epoch 14 Step 1601 Train Loss: 0.2526
Epoch 14 Step 1651 Train Loss: 0.2368
Epoch 14 Step 1701 Train Loss: 0.2387
Epoch 14 Step 1751 Train Loss: 0.2260
Epoch 14 Step 1801 Train Loss: 0.2411
Epoch 14 Step 1851 Train Loss: 0.2416
Epoch 14 Step 1901 Train Loss: 0.2492
Epoch 14: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Epoch 15 Step 1 Train Loss: 0.2593
Epoch 15 Step 51 Train Loss: 0.2575
Epoch 15 Step 101 Train Loss: 0.2476
Epoch 15 Step 151 Train Loss: 0.2323
Epoch 15 Step 201 Train Loss: 0.2534
Epoch 15 Step 251 Train Loss: 0.2714
Epoch 15 Step 301 Train Loss: 0.2559
Epoch 15 Step 351 Train Loss: 0.2508
Epoch 15 Step 401 Train Loss: 0.2432
Epoch 15 Step 451 Train Loss: 0.2538
Epoch 15 Step 501 Train Loss: 0.2420
Epoch 15 Step 551 Train Loss: 0.2462
Epoch 15 Step 601 Train Loss: 0.2417
Epoch 15 Step 651 Train Loss: 0.2324
Epoch 15 Step 701 Train Loss: 0.2229
Epoch 15 Step 751 Train Loss: 0.2507
Epoch 15 Step 801 Train Loss: 0.2474
Epoch 15 Step 851 Train Loss: 0.2335
Epoch 15 Step 901 Train Loss: 0.2616
Epoch 15 Step 951 Train Loss: 0.2325
Epoch 15 Step 1001 Train Loss: 0.2442
Epoch 15 Step 1051 Train Loss: 0.2613
Epoch 15 Step 1101 Train Loss: 0.2403
Epoch 15 Step 1151 Train Loss: 0.2411
Epoch 15 Step 1201 Train Loss: 0.2592
Epoch 15 Step 1251 Train Loss: 0.2397
Epoch 15 Step 1301 Train Loss: 0.2376
Epoch 15 Step 1351 Train Loss: 0.2402
Epoch 15 Step 1401 Train Loss: 0.2489
Epoch 15 Step 1451 Train Loss: 0.2382
Epoch 15 Step 1501 Train Loss: 0.2377
Epoch 15 Step 1551 Train Loss: 0.2364
Epoch 15 Step 1601 Train Loss: 0.2380
Epoch 15 Step 1651 Train Loss: 0.2348
Epoch 15 Step 1701 Train Loss: 0.2483
Epoch 15 Step 1751 Train Loss: 0.2432
Epoch 15 Step 1801 Train Loss: 0.2463
Epoch 15 Step 1851 Train Loss: 0.2386
Epoch 15 Step 1901 Train Loss: 0.2462
Epoch 15: Train Overall MSE: 0.0003 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0152. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0040
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00041899417
test_unseen_single_pearson: 0.9910532469656695
test_unseen_single_mse_de: 0.003960943
test_unseen_single_pearson_de: 0.7162842026816605
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.13236602645439796
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3607843137254902
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9117647058823528
test_unseen_single_mse_top20_de_non_dropout: 0.009175948679787417
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.039 MB uploadedwandb: | 0.037 MB of 0.039 MB uploadedwandb: / 0.037 MB of 0.039 MB uploadedwandb: - 0.037 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÉ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÅ
wandb:                                                   val_de_mse ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00396
wandb:                                              test_de_pearson 0.71628
wandb:               test_frac_opposite_direction_top20_non_dropout 0.36078
wandb:                          test_frac_sigma_below_1_non_dropout 0.91176
wandb:                                                     test_mse 0.00042
wandb:                                test_mse_top20_de_non_dropout 0.00918
wandb:                                                 test_pearson 0.99105
wandb:                                           test_pearson_delta 0.13237
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.36078
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.91176
wandb:                                       test_unseen_single_mse 0.00042
wandb:                                    test_unseen_single_mse_de 0.00396
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00918
wandb:                                   test_unseen_single_pearson 0.99105
wandb:                                test_unseen_single_pearson_de 0.71628
wandb:                             test_unseen_single_pearson_delta 0.13237
wandb:                                                 train_de_mse 0.00479
wandb:                                             train_de_pearson 0.80332
wandb:                                                    train_mse 0.00033
wandb:                                                train_pearson 0.99289
wandb:                                                training_loss 0.23662
wandb:                                                   val_de_mse 0.01525
wandb:                                               val_de_pearson 0.92431
wandb:                                                      val_mse 0.0003
wandb:                                                  val_pearson 0.99348
wandb: 
wandb: üöÄ View run geneformer_XuCao2023_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/zek8phm9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_044636-zek8phm9/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_053824-g6r566sr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_XuCao2023_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/g6r566sr
wandb: WARNING Serializing object of type ndarray that is 21184640 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2670
Epoch 1 Step 51 Train Loss: 0.2628
Epoch 1 Step 101 Train Loss: 0.2462
Epoch 1 Step 151 Train Loss: 0.2531
Epoch 1 Step 201 Train Loss: 0.2401
Epoch 1 Step 251 Train Loss: 0.2481
Epoch 1 Step 301 Train Loss: 0.2458
Epoch 1 Step 351 Train Loss: 0.2248
Epoch 1 Step 401 Train Loss: 0.2496
Epoch 1 Step 451 Train Loss: 0.2434
Epoch 1 Step 501 Train Loss: 0.2466
Epoch 1 Step 551 Train Loss: 0.2386
Epoch 1 Step 601 Train Loss: 0.2546
Epoch 1 Step 651 Train Loss: 0.2339
Epoch 1 Step 701 Train Loss: 0.2441
Epoch 1 Step 751 Train Loss: 0.2365
Epoch 1 Step 801 Train Loss: 0.2320
Epoch 1 Step 851 Train Loss: 0.2344
Epoch 1 Step 901 Train Loss: 0.2409
Epoch 1 Step 951 Train Loss: 0.2158
Epoch 1 Step 1001 Train Loss: 0.2532
Epoch 1 Step 1051 Train Loss: 0.2290
Epoch 1 Step 1101 Train Loss: 0.2155
Epoch 1 Step 1151 Train Loss: 0.2203
Epoch 1 Step 1201 Train Loss: 0.2404
Epoch 1 Step 1251 Train Loss: 0.2357
Epoch 1 Step 1301 Train Loss: 0.2271
Epoch 1 Step 1351 Train Loss: 0.2291
Epoch 1 Step 1401 Train Loss: 0.2200
Epoch 1 Step 1451 Train Loss: 0.2168
Epoch 1 Step 1501 Train Loss: 0.2097
Epoch 1 Step 1551 Train Loss: 0.2223
Epoch 1 Step 1601 Train Loss: 0.2216
Epoch 1 Step 1651 Train Loss: 0.2064
Epoch 1 Step 1701 Train Loss: 0.2196
Epoch 1 Step 1751 Train Loss: 0.2171
Epoch 1 Step 1801 Train Loss: 0.2210
Epoch 1 Step 1851 Train Loss: 0.2059
Epoch 1 Step 1901 Train Loss: 0.2217
Epoch 1 Step 1951 Train Loss: 0.2021
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0054. 
Epoch 2 Step 1 Train Loss: 0.2106
Epoch 2 Step 51 Train Loss: 0.2206
Epoch 2 Step 101 Train Loss: 0.2536
Epoch 2 Step 151 Train Loss: 0.2205
Epoch 2 Step 201 Train Loss: 0.2234
Epoch 2 Step 251 Train Loss: 0.2343
Epoch 2 Step 301 Train Loss: 0.2176
Epoch 2 Step 351 Train Loss: 0.2342
Epoch 2 Step 401 Train Loss: 0.2209
Epoch 2 Step 451 Train Loss: 0.2161
Epoch 2 Step 501 Train Loss: 0.2035
Epoch 2 Step 551 Train Loss: 0.2248
Epoch 2 Step 601 Train Loss: 0.2133
Epoch 2 Step 651 Train Loss: 0.2204
Epoch 2 Step 701 Train Loss: 0.2156
Epoch 2 Step 751 Train Loss: 0.2343
Epoch 2 Step 801 Train Loss: 0.2390
Epoch 2 Step 851 Train Loss: 0.2152
Epoch 2 Step 901 Train Loss: 0.2166
Epoch 2 Step 951 Train Loss: 0.2201
Epoch 2 Step 1001 Train Loss: 0.1928
Epoch 2 Step 1051 Train Loss: 0.2438
Epoch 2 Step 1101 Train Loss: 0.2126
Epoch 2 Step 1151 Train Loss: 0.2049
Epoch 2 Step 1201 Train Loss: 0.2245
Epoch 2 Step 1251 Train Loss: 0.2571
Epoch 2 Step 1301 Train Loss: 0.2335
Epoch 2 Step 1351 Train Loss: 0.2130
Epoch 2 Step 1401 Train Loss: 0.2184
Epoch 2 Step 1451 Train Loss: 0.2267
Epoch 2 Step 1501 Train Loss: 0.2400
Epoch 2 Step 1551 Train Loss: 0.2181
Epoch 2 Step 1601 Train Loss: 0.2306
Epoch 2 Step 1651 Train Loss: 0.2071
Epoch 2 Step 1701 Train Loss: 0.2247
Epoch 2 Step 1751 Train Loss: 0.2114
Epoch 2 Step 1801 Train Loss: 0.2283
Epoch 2 Step 1851 Train Loss: 0.2133
Epoch 2 Step 1901 Train Loss: 0.2187
Epoch 2 Step 1951 Train Loss: 0.2362
Epoch 2: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 3 Step 1 Train Loss: 0.2287
Epoch 3 Step 51 Train Loss: 0.2752
Epoch 3 Step 101 Train Loss: 0.2254
Epoch 3 Step 151 Train Loss: 0.2438
Epoch 3 Step 201 Train Loss: 0.2342
Epoch 3 Step 251 Train Loss: 0.2384
Epoch 3 Step 301 Train Loss: 0.2323
Epoch 3 Step 351 Train Loss: 0.2461
Epoch 3 Step 401 Train Loss: 0.2326
Epoch 3 Step 451 Train Loss: 0.2375
Epoch 3 Step 501 Train Loss: 0.2348
Epoch 3 Step 551 Train Loss: 0.2247
Epoch 3 Step 601 Train Loss: 0.2148
Epoch 3 Step 651 Train Loss: 0.2439
Epoch 3 Step 701 Train Loss: 0.2115
Epoch 3 Step 751 Train Loss: 0.2214
Epoch 3 Step 801 Train Loss: 0.2258
Epoch 3 Step 851 Train Loss: 0.2368
Epoch 3 Step 901 Train Loss: 0.2124
Epoch 3 Step 951 Train Loss: 0.2462
Epoch 3 Step 1001 Train Loss: 0.2118
Epoch 3 Step 1051 Train Loss: 0.2409
Epoch 3 Step 1101 Train Loss: 0.2245
Epoch 3 Step 1151 Train Loss: 0.2256
Epoch 3 Step 1201 Train Loss: 0.2306
Epoch 3 Step 1251 Train Loss: 0.2128
Epoch 3 Step 1301 Train Loss: 0.2236
Epoch 3 Step 1351 Train Loss: 0.2279
Epoch 3 Step 1401 Train Loss: 0.2451
Epoch 3 Step 1451 Train Loss: 0.2576
Epoch 3 Step 1501 Train Loss: 0.2264
Epoch 3 Step 1551 Train Loss: 0.2260
Epoch 3 Step 1601 Train Loss: 0.2287
Epoch 3 Step 1651 Train Loss: 0.2148
Epoch 3 Step 1701 Train Loss: 0.2350
Epoch 3 Step 1751 Train Loss: 0.2270
Epoch 3 Step 1801 Train Loss: 0.2413
Epoch 3 Step 1851 Train Loss: 0.2178
Epoch 3 Step 1901 Train Loss: 0.2597
Epoch 3 Step 1951 Train Loss: 0.2468
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 4 Step 1 Train Loss: 0.2310
Epoch 4 Step 51 Train Loss: 0.2495
Epoch 4 Step 101 Train Loss: 0.2456
Epoch 4 Step 151 Train Loss: 0.2398
Epoch 4 Step 201 Train Loss: 0.2327
Epoch 4 Step 251 Train Loss: 0.2223
Epoch 4 Step 301 Train Loss: 0.2412
Epoch 4 Step 351 Train Loss: 0.2332
Epoch 4 Step 401 Train Loss: 0.2330
Epoch 4 Step 451 Train Loss: 0.2250
Epoch 4 Step 501 Train Loss: 0.2259
Epoch 4 Step 551 Train Loss: 0.2435
Epoch 4 Step 601 Train Loss: 0.2384
Epoch 4 Step 651 Train Loss: 0.2347
Epoch 4 Step 701 Train Loss: 0.2464
Epoch 4 Step 751 Train Loss: 0.2652
Epoch 4 Step 801 Train Loss: 0.2150
Epoch 4 Step 851 Train Loss: 0.2311
Epoch 4 Step 901 Train Loss: 0.2278
Epoch 4 Step 951 Train Loss: 0.2463
Epoch 4 Step 1001 Train Loss: 0.2438
Epoch 4 Step 1051 Train Loss: 0.2424
Epoch 4 Step 1101 Train Loss: 0.2107
Epoch 4 Step 1151 Train Loss: 0.2319
Epoch 4 Step 1201 Train Loss: 0.2451
Epoch 4 Step 1251 Train Loss: 0.2054
Epoch 4 Step 1301 Train Loss: 0.2232
Epoch 4 Step 1351 Train Loss: 0.2390
Epoch 4 Step 1401 Train Loss: 0.2206
Epoch 4 Step 1451 Train Loss: 0.2269
Epoch 4 Step 1501 Train Loss: 0.2435
Epoch 4 Step 1551 Train Loss: 0.2154
Epoch 4 Step 1601 Train Loss: 0.2497
Epoch 4 Step 1651 Train Loss: 0.2226
Epoch 4 Step 1701 Train Loss: 0.2520
Epoch 4 Step 1751 Train Loss: 0.2245
Epoch 4 Step 1801 Train Loss: 0.2372
Epoch 4 Step 1851 Train Loss: 0.2321
Epoch 4 Step 1901 Train Loss: 0.2404
Epoch 4 Step 1951 Train Loss: 0.2540
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 5 Step 1 Train Loss: 0.2232
Epoch 5 Step 51 Train Loss: 0.2261
Epoch 5 Step 101 Train Loss: 0.2259
Epoch 5 Step 151 Train Loss: 0.2412
Epoch 5 Step 201 Train Loss: 0.2319
Epoch 5 Step 251 Train Loss: 0.2168
Epoch 5 Step 301 Train Loss: 0.2435
Epoch 5 Step 351 Train Loss: 0.2373
Epoch 5 Step 401 Train Loss: 0.2151
Epoch 5 Step 451 Train Loss: 0.2358
Epoch 5 Step 501 Train Loss: 0.2325
Epoch 5 Step 551 Train Loss: 0.2375
Epoch 5 Step 601 Train Loss: 0.2212
Epoch 5 Step 651 Train Loss: 0.2158
Epoch 5 Step 701 Train Loss: 0.2283
Epoch 5 Step 751 Train Loss: 0.2299
Epoch 5 Step 801 Train Loss: 0.2418
Epoch 5 Step 851 Train Loss: 0.2566
Epoch 5 Step 901 Train Loss: 0.2373
Epoch 5 Step 951 Train Loss: 0.2387
Epoch 5 Step 1001 Train Loss: 0.2364
Epoch 5 Step 1051 Train Loss: 0.2358
Epoch 5 Step 1101 Train Loss: 0.2146
Epoch 5 Step 1151 Train Loss: 0.2255
Epoch 5 Step 1201 Train Loss: 0.2488
Epoch 5 Step 1251 Train Loss: 0.2178
Epoch 5 Step 1301 Train Loss: 0.2318
Epoch 5 Step 1351 Train Loss: 0.2287
Epoch 5 Step 1401 Train Loss: 0.2251
Epoch 5 Step 1451 Train Loss: 0.2494
Epoch 5 Step 1501 Train Loss: 0.2214
Epoch 5 Step 1551 Train Loss: 0.2383
Epoch 5 Step 1601 Train Loss: 0.2496
Epoch 5 Step 1651 Train Loss: 0.2308
Epoch 5 Step 1701 Train Loss: 0.2332
Epoch 5 Step 1751 Train Loss: 0.2258
Epoch 5 Step 1801 Train Loss: 0.2596
Epoch 5 Step 1851 Train Loss: 0.2639
Epoch 5 Step 1901 Train Loss: 0.2332
Epoch 5 Step 1951 Train Loss: 0.2333
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 6 Step 1 Train Loss: 0.2180
Epoch 6 Step 51 Train Loss: 0.2284
Epoch 6 Step 101 Train Loss: 0.2497
Epoch 6 Step 151 Train Loss: 0.2282
Epoch 6 Step 201 Train Loss: 0.2650
Epoch 6 Step 251 Train Loss: 0.2154
Epoch 6 Step 301 Train Loss: 0.2357
Epoch 6 Step 351 Train Loss: 0.2673
Epoch 6 Step 401 Train Loss: 0.2481
Epoch 6 Step 451 Train Loss: 0.2345
Epoch 6 Step 501 Train Loss: 0.2320
Epoch 6 Step 551 Train Loss: 0.2257
Epoch 6 Step 601 Train Loss: 0.2257
Epoch 6 Step 651 Train Loss: 0.2362
Epoch 6 Step 701 Train Loss: 0.2529
Epoch 6 Step 751 Train Loss: 0.2222
Epoch 6 Step 801 Train Loss: 0.2484
Epoch 6 Step 851 Train Loss: 0.2485
Epoch 6 Step 901 Train Loss: 0.2294
Epoch 6 Step 951 Train Loss: 0.2334
Epoch 6 Step 1001 Train Loss: 0.2297
Epoch 6 Step 1051 Train Loss: 0.2408
Epoch 6 Step 1101 Train Loss: 0.2435
Epoch 6 Step 1151 Train Loss: 0.2405
Epoch 6 Step 1201 Train Loss: 0.2196
Epoch 6 Step 1251 Train Loss: 0.2332
Epoch 6 Step 1301 Train Loss: 0.2265
Epoch 6 Step 1351 Train Loss: 0.2510
Epoch 6 Step 1401 Train Loss: 0.2406
Epoch 6 Step 1451 Train Loss: 0.2294
Epoch 6 Step 1501 Train Loss: 0.2324
Epoch 6 Step 1551 Train Loss: 0.2301
Epoch 6 Step 1601 Train Loss: 0.2499
Epoch 6 Step 1651 Train Loss: 0.2408
Epoch 6 Step 1701 Train Loss: 0.2428
Epoch 6 Step 1751 Train Loss: 0.2350
Epoch 6 Step 1801 Train Loss: 0.2627
Epoch 6 Step 1851 Train Loss: 0.2440
Epoch 6 Step 1901 Train Loss: 0.2311
Epoch 6 Step 1951 Train Loss: 0.2448
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 7 Step 1 Train Loss: 0.2342
Epoch 7 Step 51 Train Loss: 0.2505
Epoch 7 Step 101 Train Loss: 0.2207
Epoch 7 Step 151 Train Loss: 0.2095
Epoch 7 Step 201 Train Loss: 0.2479
Epoch 7 Step 251 Train Loss: 0.2421
Epoch 7 Step 301 Train Loss: 0.2499
Epoch 7 Step 351 Train Loss: 0.2600
Epoch 7 Step 401 Train Loss: 0.2432
Epoch 7 Step 451 Train Loss: 0.2257
Epoch 7 Step 501 Train Loss: 0.2275
Epoch 7 Step 551 Train Loss: 0.2501
Epoch 7 Step 601 Train Loss: 0.2335
Epoch 7 Step 651 Train Loss: 0.2368
Epoch 7 Step 701 Train Loss: 0.2334
Epoch 7 Step 751 Train Loss: 0.2420
Epoch 7 Step 801 Train Loss: 0.2431
Epoch 7 Step 851 Train Loss: 0.2439
Epoch 7 Step 901 Train Loss: 0.2403
Epoch 7 Step 951 Train Loss: 0.2392
Epoch 7 Step 1001 Train Loss: 0.2568
Epoch 7 Step 1051 Train Loss: 0.2174
Epoch 7 Step 1101 Train Loss: 0.2361
Epoch 7 Step 1151 Train Loss: 0.2294
Epoch 7 Step 1201 Train Loss: 0.2376
Epoch 7 Step 1251 Train Loss: 0.2391
Epoch 7 Step 1301 Train Loss: 0.2378
Epoch 7 Step 1351 Train Loss: 0.2539
Epoch 7 Step 1401 Train Loss: 0.2358
Epoch 7 Step 1451 Train Loss: 0.2281
Epoch 7 Step 1501 Train Loss: 0.2497
Epoch 7 Step 1551 Train Loss: 0.2286
Epoch 7 Step 1601 Train Loss: 0.2332
Epoch 7 Step 1651 Train Loss: 0.2314
Epoch 7 Step 1701 Train Loss: 0.2398
Epoch 7 Step 1751 Train Loss: 0.2379
Epoch 7 Step 1801 Train Loss: 0.2214
Epoch 7 Step 1851 Train Loss: 0.2596
Epoch 7 Step 1901 Train Loss: 0.2356
Epoch 7 Step 1951 Train Loss: 0.2635
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 8 Step 1 Train Loss: 0.2053
Epoch 8 Step 51 Train Loss: 0.2272
Epoch 8 Step 101 Train Loss: 0.2286
Epoch 8 Step 151 Train Loss: 0.2414
Epoch 8 Step 201 Train Loss: 0.2553
Epoch 8 Step 251 Train Loss: 0.2502
Epoch 8 Step 301 Train Loss: 0.2505
Epoch 8 Step 351 Train Loss: 0.2390
Epoch 8 Step 401 Train Loss: 0.2248
Epoch 8 Step 451 Train Loss: 0.2415
Epoch 8 Step 501 Train Loss: 0.2401
Epoch 8 Step 551 Train Loss: 0.2656
Epoch 8 Step 601 Train Loss: 0.2408
Epoch 8 Step 651 Train Loss: 0.2533
Epoch 8 Step 701 Train Loss: 0.2489
Epoch 8 Step 751 Train Loss: 0.2297
Epoch 8 Step 801 Train Loss: 0.2175
Epoch 8 Step 851 Train Loss: 0.2312
Epoch 8 Step 901 Train Loss: 0.2589
Epoch 8 Step 951 Train Loss: 0.2185
Epoch 8 Step 1001 Train Loss: 0.2483
Epoch 8 Step 1051 Train Loss: 0.2402
Epoch 8 Step 1101 Train Loss: 0.2495
Epoch 8 Step 1151 Train Loss: 0.2395
Epoch 8 Step 1201 Train Loss: 0.2404
Epoch 8 Step 1251 Train Loss: 0.2344
Epoch 8 Step 1301 Train Loss: 0.2207
Epoch 8 Step 1351 Train Loss: 0.2335
Epoch 8 Step 1401 Train Loss: 0.2739
Epoch 8 Step 1451 Train Loss: 0.2445
Epoch 8 Step 1501 Train Loss: 0.2184
Epoch 8 Step 1551 Train Loss: 0.2301
Epoch 8 Step 1601 Train Loss: 0.2276
Epoch 8 Step 1651 Train Loss: 0.2537
Epoch 8 Step 1701 Train Loss: 0.2580
Epoch 8 Step 1751 Train Loss: 0.2418
Epoch 8 Step 1801 Train Loss: 0.2319
Epoch 8 Step 1851 Train Loss: 0.2405
Epoch 8 Step 1901 Train Loss: 0.2394
Epoch 8 Step 1951 Train Loss: 0.2292
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 9 Step 1 Train Loss: 0.2481
Epoch 9 Step 51 Train Loss: 0.2210
Epoch 9 Step 101 Train Loss: 0.2320
Epoch 9 Step 151 Train Loss: 0.2386
Epoch 9 Step 201 Train Loss: 0.2308
Epoch 9 Step 251 Train Loss: 0.2374
Epoch 9 Step 301 Train Loss: 0.2369
Epoch 9 Step 351 Train Loss: 0.2303
Epoch 9 Step 401 Train Loss: 0.2359
Epoch 9 Step 451 Train Loss: 0.2386
Epoch 9 Step 501 Train Loss: 0.2341
Epoch 9 Step 551 Train Loss: 0.2330
Epoch 9 Step 601 Train Loss: 0.2330
Epoch 9 Step 651 Train Loss: 0.2356
Epoch 9 Step 701 Train Loss: 0.2314
Epoch 9 Step 751 Train Loss: 0.2518
Epoch 9 Step 801 Train Loss: 0.2318
Epoch 9 Step 851 Train Loss: 0.2283
Epoch 9 Step 901 Train Loss: 0.2366
Epoch 9 Step 951 Train Loss: 0.2316
Epoch 9 Step 1001 Train Loss: 0.2214
Epoch 9 Step 1051 Train Loss: 0.2520
Epoch 9 Step 1101 Train Loss: 0.2211
Epoch 9 Step 1151 Train Loss: 0.2276
Epoch 9 Step 1201 Train Loss: 0.2439
Epoch 9 Step 1251 Train Loss: 0.2121
Epoch 9 Step 1301 Train Loss: 0.2220
Epoch 9 Step 1351 Train Loss: 0.2514
Epoch 9 Step 1401 Train Loss: 0.2239
Epoch 9 Step 1451 Train Loss: 0.2379
Epoch 9 Step 1501 Train Loss: 0.2277
Epoch 9 Step 1551 Train Loss: 0.2510
Epoch 9 Step 1601 Train Loss: 0.2457
Epoch 9 Step 1651 Train Loss: 0.2335
Epoch 9 Step 1701 Train Loss: 0.2369
Epoch 9 Step 1751 Train Loss: 0.2390
Epoch 9 Step 1801 Train Loss: 0.2196
Epoch 9 Step 1851 Train Loss: 0.2207
Epoch 9 Step 1901 Train Loss: 0.2184
Epoch 9 Step 1951 Train Loss: 0.2335
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 10 Step 1 Train Loss: 0.2384
Epoch 10 Step 51 Train Loss: 0.2322
Epoch 10 Step 101 Train Loss: 0.2352
Epoch 10 Step 151 Train Loss: 0.2258
Epoch 10 Step 201 Train Loss: 0.2316
Epoch 10 Step 251 Train Loss: 0.2457
Epoch 10 Step 301 Train Loss: 0.2330
Epoch 10 Step 351 Train Loss: 0.2173
Epoch 10 Step 401 Train Loss: 0.2343
Epoch 10 Step 451 Train Loss: 0.2193
Epoch 10 Step 501 Train Loss: 0.2339
Epoch 10 Step 551 Train Loss: 0.2262
Epoch 10 Step 601 Train Loss: 0.2259
Epoch 10 Step 651 Train Loss: 0.2353
Epoch 10 Step 701 Train Loss: 0.2382
Epoch 10 Step 751 Train Loss: 0.2330
Epoch 10 Step 801 Train Loss: 0.2211
Epoch 10 Step 851 Train Loss: 0.2310
Epoch 10 Step 901 Train Loss: 0.2625
Epoch 10 Step 951 Train Loss: 0.2400
Epoch 10 Step 1001 Train Loss: 0.2453
Epoch 10 Step 1051 Train Loss: 0.2557
Epoch 10 Step 1101 Train Loss: 0.2297
Epoch 10 Step 1151 Train Loss: 0.2513
Epoch 10 Step 1201 Train Loss: 0.2430
Epoch 10 Step 1251 Train Loss: 0.2327
Epoch 10 Step 1301 Train Loss: 0.2463
Epoch 10 Step 1351 Train Loss: 0.2431
Epoch 10 Step 1401 Train Loss: 0.2300
Epoch 10 Step 1451 Train Loss: 0.2384
Epoch 10 Step 1501 Train Loss: 0.2395
Epoch 10 Step 1551 Train Loss: 0.2327
Epoch 10 Step 1601 Train Loss: 0.2343
Epoch 10 Step 1651 Train Loss: 0.2300
Epoch 10 Step 1701 Train Loss: 0.2545
Epoch 10 Step 1751 Train Loss: 0.2693
Epoch 10 Step 1801 Train Loss: 0.2344
Epoch 10 Step 1851 Train Loss: 0.2630
Epoch 10 Step 1901 Train Loss: 0.2493
Epoch 10 Step 1951 Train Loss: 0.2256
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 11 Step 1 Train Loss: 0.2352
Epoch 11 Step 51 Train Loss: 0.2402
Epoch 11 Step 101 Train Loss: 0.2219
Epoch 11 Step 151 Train Loss: 0.2300
Epoch 11 Step 201 Train Loss: 0.2384
Epoch 11 Step 251 Train Loss: 0.2368
Epoch 11 Step 301 Train Loss: 0.2558
Epoch 11 Step 351 Train Loss: 0.2495
Epoch 11 Step 401 Train Loss: 0.2203
Epoch 11 Step 451 Train Loss: 0.2391
Epoch 11 Step 501 Train Loss: 0.2295
Epoch 11 Step 551 Train Loss: 0.2346
Epoch 11 Step 601 Train Loss: 0.2380
Epoch 11 Step 651 Train Loss: 0.2220
Epoch 11 Step 701 Train Loss: 0.2200
Epoch 11 Step 751 Train Loss: 0.2485
Epoch 11 Step 801 Train Loss: 0.2296
Epoch 11 Step 851 Train Loss: 0.2490
Epoch 11 Step 901 Train Loss: 0.2431
Epoch 11 Step 951 Train Loss: 0.2472
Epoch 11 Step 1001 Train Loss: 0.2573
Epoch 11 Step 1051 Train Loss: 0.2366
Epoch 11 Step 1101 Train Loss: 0.2294
Epoch 11 Step 1151 Train Loss: 0.2304
Epoch 11 Step 1201 Train Loss: 0.2360
Epoch 11 Step 1251 Train Loss: 0.2475
Epoch 11 Step 1301 Train Loss: 0.2348
Epoch 11 Step 1351 Train Loss: 0.2359
Epoch 11 Step 1401 Train Loss: 0.2228
Epoch 11 Step 1451 Train Loss: 0.2381
Epoch 11 Step 1501 Train Loss: 0.2241
Epoch 11 Step 1551 Train Loss: 0.2425
Epoch 11 Step 1601 Train Loss: 0.2590
Epoch 11 Step 1651 Train Loss: 0.2500
Epoch 11 Step 1701 Train Loss: 0.2197
Epoch 11 Step 1751 Train Loss: 0.2258
Epoch 11 Step 1801 Train Loss: 0.2427
Epoch 11 Step 1851 Train Loss: 0.2398
Epoch 11 Step 1901 Train Loss: 0.2346
Epoch 11 Step 1951 Train Loss: 0.2364
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 12 Step 1 Train Loss: 0.2382
Epoch 12 Step 51 Train Loss: 0.2356
Epoch 12 Step 101 Train Loss: 0.2259
Epoch 12 Step 151 Train Loss: 0.2491
Epoch 12 Step 201 Train Loss: 0.2519
Epoch 12 Step 251 Train Loss: 0.2379
Epoch 12 Step 301 Train Loss: 0.2436
Epoch 12 Step 351 Train Loss: 0.2215
Epoch 12 Step 401 Train Loss: 0.2424
Epoch 12 Step 451 Train Loss: 0.2429
Epoch 12 Step 501 Train Loss: 0.2224
Epoch 12 Step 551 Train Loss: 0.2203
Epoch 12 Step 601 Train Loss: 0.2305
Epoch 12 Step 651 Train Loss: 0.2600
Epoch 12 Step 701 Train Loss: 0.2226
Epoch 12 Step 751 Train Loss: 0.2292
Epoch 12 Step 801 Train Loss: 0.2267
Epoch 12 Step 851 Train Loss: 0.2339
Epoch 12 Step 901 Train Loss: 0.2509
Epoch 12 Step 951 Train Loss: 0.2423
Epoch 12 Step 1001 Train Loss: 0.2230
Epoch 12 Step 1051 Train Loss: 0.2140
Epoch 12 Step 1101 Train Loss: 0.2357
Epoch 12 Step 1151 Train Loss: 0.2193
Epoch 12 Step 1201 Train Loss: 0.2524
Epoch 12 Step 1251 Train Loss: 0.2335
Epoch 12 Step 1301 Train Loss: 0.2336
Epoch 12 Step 1351 Train Loss: 0.2616
Epoch 12 Step 1401 Train Loss: 0.2594
Epoch 12 Step 1451 Train Loss: 0.2378
Epoch 12 Step 1501 Train Loss: 0.2289
Epoch 12 Step 1551 Train Loss: 0.2675
Epoch 12 Step 1601 Train Loss: 0.2407
Epoch 12 Step 1651 Train Loss: 0.2485
Epoch 12 Step 1701 Train Loss: 0.2383
Epoch 12 Step 1751 Train Loss: 0.2361
Epoch 12 Step 1801 Train Loss: 0.2471
Epoch 12 Step 1851 Train Loss: 0.2272
Epoch 12 Step 1901 Train Loss: 0.2492
Epoch 12 Step 1951 Train Loss: 0.2378
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 13 Step 1 Train Loss: 0.2315
Epoch 13 Step 51 Train Loss: 0.2335
Epoch 13 Step 101 Train Loss: 0.2335
Epoch 13 Step 151 Train Loss: 0.2296
Epoch 13 Step 201 Train Loss: 0.2430
Epoch 13 Step 251 Train Loss: 0.2301
Epoch 13 Step 301 Train Loss: 0.2252
Epoch 13 Step 351 Train Loss: 0.2208
Epoch 13 Step 401 Train Loss: 0.2388
Epoch 13 Step 451 Train Loss: 0.2395
Epoch 13 Step 501 Train Loss: 0.2310
Epoch 13 Step 551 Train Loss: 0.2318
Epoch 13 Step 601 Train Loss: 0.2258
Epoch 13 Step 651 Train Loss: 0.2361
Epoch 13 Step 701 Train Loss: 0.2434
Epoch 13 Step 751 Train Loss: 0.2573
Epoch 13 Step 801 Train Loss: 0.2520
Epoch 13 Step 851 Train Loss: 0.2380
Epoch 13 Step 901 Train Loss: 0.2466
Epoch 13 Step 951 Train Loss: 0.2509
Epoch 13 Step 1001 Train Loss: 0.2305
Epoch 13 Step 1051 Train Loss: 0.2493
Epoch 13 Step 1101 Train Loss: 0.2491
Epoch 13 Step 1151 Train Loss: 0.2405
Epoch 13 Step 1201 Train Loss: 0.2287
Epoch 13 Step 1251 Train Loss: 0.2210
Epoch 13 Step 1301 Train Loss: 0.2321
Epoch 13 Step 1351 Train Loss: 0.2455
Epoch 13 Step 1401 Train Loss: 0.2517
Epoch 13 Step 1451 Train Loss: 0.2607
Epoch 13 Step 1501 Train Loss: 0.2603
Epoch 13 Step 1551 Train Loss: 0.2380
Epoch 13 Step 1601 Train Loss: 0.2326
Epoch 13 Step 1651 Train Loss: 0.2301
Epoch 13 Step 1701 Train Loss: 0.2338
Epoch 13 Step 1751 Train Loss: 0.2194
Epoch 13 Step 1801 Train Loss: 0.2230
Epoch 13 Step 1851 Train Loss: 0.2454
Epoch 13 Step 1901 Train Loss: 0.2435
Epoch 13 Step 1951 Train Loss: 0.2452
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 14 Step 1 Train Loss: 0.2632
Epoch 14 Step 51 Train Loss: 0.2453
Epoch 14 Step 101 Train Loss: 0.2470
Epoch 14 Step 151 Train Loss: 0.2383
Epoch 14 Step 201 Train Loss: 0.2629
Epoch 14 Step 251 Train Loss: 0.2307
Epoch 14 Step 301 Train Loss: 0.2330
Epoch 14 Step 351 Train Loss: 0.2361
Epoch 14 Step 401 Train Loss: 0.2360
Epoch 14 Step 451 Train Loss: 0.2404
Epoch 14 Step 501 Train Loss: 0.2294
Epoch 14 Step 551 Train Loss: 0.2427
Epoch 14 Step 601 Train Loss: 0.2495
Epoch 14 Step 651 Train Loss: 0.2285
Epoch 14 Step 701 Train Loss: 0.2174
Epoch 14 Step 751 Train Loss: 0.2295
Epoch 14 Step 801 Train Loss: 0.2240
Epoch 14 Step 851 Train Loss: 0.2420
Epoch 14 Step 901 Train Loss: 0.2345
Epoch 14 Step 951 Train Loss: 0.2400
Epoch 14 Step 1001 Train Loss: 0.2451
Epoch 14 Step 1051 Train Loss: 0.2170
Epoch 14 Step 1101 Train Loss: 0.2276
Epoch 14 Step 1151 Train Loss: 0.2534
Epoch 14 Step 1201 Train Loss: 0.2306
Epoch 14 Step 1251 Train Loss: 0.2276
Epoch 14 Step 1301 Train Loss: 0.2574
Epoch 14 Step 1351 Train Loss: 0.2154
Epoch 14 Step 1401 Train Loss: 0.2738
Epoch 14 Step 1451 Train Loss: 0.2613
Epoch 14 Step 1501 Train Loss: 0.2322
Epoch 14 Step 1551 Train Loss: 0.2354
Epoch 14 Step 1601 Train Loss: 0.2484
Epoch 14 Step 1651 Train Loss: 0.2528
Epoch 14 Step 1701 Train Loss: 0.2262
Epoch 14 Step 1751 Train Loss: 0.2370
Epoch 14 Step 1801 Train Loss: 0.2359
Epoch 14 Step 1851 Train Loss: 0.2304
Epoch 14 Step 1901 Train Loss: 0.2485
Epoch 14 Step 1951 Train Loss: 0.2345
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Epoch 15 Step 1 Train Loss: 0.2355
Epoch 15 Step 51 Train Loss: 0.2222
Epoch 15 Step 101 Train Loss: 0.2500
Epoch 15 Step 151 Train Loss: 0.2266
Epoch 15 Step 201 Train Loss: 0.2121
Epoch 15 Step 251 Train Loss: 0.2574
Epoch 15 Step 301 Train Loss: 0.2623
Epoch 15 Step 351 Train Loss: 0.2247
Epoch 15 Step 401 Train Loss: 0.2469
Epoch 15 Step 451 Train Loss: 0.2357
Epoch 15 Step 501 Train Loss: 0.2436
Epoch 15 Step 551 Train Loss: 0.2323
Epoch 15 Step 601 Train Loss: 0.2413
Epoch 15 Step 651 Train Loss: 0.2401
Epoch 15 Step 701 Train Loss: 0.2455
Epoch 15 Step 751 Train Loss: 0.2418
Epoch 15 Step 801 Train Loss: 0.2618
Epoch 15 Step 851 Train Loss: 0.2336
Epoch 15 Step 901 Train Loss: 0.2345
Epoch 15 Step 951 Train Loss: 0.2337
Epoch 15 Step 1001 Train Loss: 0.2468
Epoch 15 Step 1051 Train Loss: 0.2466
Epoch 15 Step 1101 Train Loss: 0.2458
Epoch 15 Step 1151 Train Loss: 0.2301
Epoch 15 Step 1201 Train Loss: 0.2489
Epoch 15 Step 1251 Train Loss: 0.2454
Epoch 15 Step 1301 Train Loss: 0.2308
Epoch 15 Step 1351 Train Loss: 0.2364
Epoch 15 Step 1401 Train Loss: 0.2458
Epoch 15 Step 1451 Train Loss: 0.2265
Epoch 15 Step 1501 Train Loss: 0.2384
Epoch 15 Step 1551 Train Loss: 0.2332
Epoch 15 Step 1601 Train Loss: 0.2380
Epoch 15 Step 1651 Train Loss: 0.2301
Epoch 15 Step 1701 Train Loss: 0.2291
Epoch 15 Step 1751 Train Loss: 0.2446
Epoch 15 Step 1801 Train Loss: 0.2330
Epoch 15 Step 1851 Train Loss: 0.2161
Epoch 15 Step 1901 Train Loss: 0.2224
Epoch 15 Step 1951 Train Loss: 0.2304
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0004. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0053. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0033
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00033949513
test_unseen_single_pearson: 0.9926422158881568
test_unseen_single_mse_de: 0.0033396038
test_unseen_single_pearson_de: 0.7506649704890538
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.14202626798819756
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.36176470588235293
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9343137254901959
test_unseen_single_mse_top20_de_non_dropout: 0.007476805691697105
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.039 MB uploadedwandb: | 0.037 MB of 0.039 MB uploadedwandb: / 0.037 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñá‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÉ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00334
wandb:                                              test_de_pearson 0.75066
wandb:               test_frac_opposite_direction_top20_non_dropout 0.36176
wandb:                          test_frac_sigma_below_1_non_dropout 0.93431
wandb:                                                     test_mse 0.00034
wandb:                                test_mse_top20_de_non_dropout 0.00748
wandb:                                                 test_pearson 0.99264
wandb:                                           test_pearson_delta 0.14203
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.36176
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93431
wandb:                                       test_unseen_single_mse 0.00034
wandb:                                    test_unseen_single_mse_de 0.00334
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00748
wandb:                                   test_unseen_single_pearson 0.99264
wandb:                                test_unseen_single_pearson_de 0.75066
wandb:                             test_unseen_single_pearson_delta 0.14203
wandb:                                                 train_de_mse 0.00621
wandb:                                             train_de_pearson 0.80314
wandb:                                                    train_mse 0.00035
wandb:                                                train_pearson 0.99251
wandb:                                                training_loss 0.23559
wandb:                                                   val_de_mse 0.00529
wandb:                                               val_de_pearson 0.80919
wandb:                                                      val_mse 0.00039
wandb:                                                  val_pearson 0.99153
wandb: 
wandb: üöÄ View run geneformer_XuCao2023_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/g6r566sr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_053824-g6r566sr/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_063110-s6smghrk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_XuCao2023_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/s6smghrk
wandb: WARNING Serializing object of type ndarray that is 21184640 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2504
Epoch 1 Step 51 Train Loss: 0.2537
Epoch 1 Step 101 Train Loss: 0.2643
Epoch 1 Step 151 Train Loss: 0.2466
Epoch 1 Step 201 Train Loss: 0.2475
Epoch 1 Step 251 Train Loss: 0.2336
Epoch 1 Step 301 Train Loss: 0.2667
Epoch 1 Step 351 Train Loss: 0.2307
Epoch 1 Step 401 Train Loss: 0.2419
Epoch 1 Step 451 Train Loss: 0.2546
Epoch 1 Step 501 Train Loss: 0.2349
Epoch 1 Step 551 Train Loss: 0.2327
Epoch 1 Step 601 Train Loss: 0.2471
Epoch 1 Step 651 Train Loss: 0.2454
Epoch 1 Step 701 Train Loss: 0.2373
Epoch 1 Step 751 Train Loss: 0.2449
Epoch 1 Step 801 Train Loss: 0.2253
Epoch 1 Step 851 Train Loss: 0.2299
Epoch 1 Step 901 Train Loss: 0.2138
Epoch 1 Step 951 Train Loss: 0.2268
Epoch 1 Step 1001 Train Loss: 0.2237
Epoch 1 Step 1051 Train Loss: 0.2069
Epoch 1 Step 1101 Train Loss: 0.2536
Epoch 1 Step 1151 Train Loss: 0.2122
Epoch 1 Step 1201 Train Loss: 0.2307
Epoch 1 Step 1251 Train Loss: 0.2296
Epoch 1 Step 1301 Train Loss: 0.2348
Epoch 1 Step 1351 Train Loss: 0.2203
Epoch 1 Step 1401 Train Loss: 0.2168
Epoch 1 Step 1451 Train Loss: 0.1900
Epoch 1 Step 1501 Train Loss: 0.2242
Epoch 1 Step 1551 Train Loss: 0.2327
Epoch 1 Step 1601 Train Loss: 0.2111
Epoch 1 Step 1651 Train Loss: 0.2240
Epoch 1 Step 1701 Train Loss: 0.2409
Epoch 1 Step 1751 Train Loss: 0.2253
Epoch 1 Step 1801 Train Loss: 0.2217
Epoch 1: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0065. 
Epoch 2 Step 1 Train Loss: 0.2123
Epoch 2 Step 51 Train Loss: 0.2298
Epoch 2 Step 101 Train Loss: 0.2196
Epoch 2 Step 151 Train Loss: 0.2174
Epoch 2 Step 201 Train Loss: 0.2576
Epoch 2 Step 251 Train Loss: 0.2269
Epoch 2 Step 301 Train Loss: 0.2209
Epoch 2 Step 351 Train Loss: 0.2084
Epoch 2 Step 401 Train Loss: 0.2396
Epoch 2 Step 451 Train Loss: 0.2406
Epoch 2 Step 501 Train Loss: 0.2146
Epoch 2 Step 551 Train Loss: 0.2217
Epoch 2 Step 601 Train Loss: 0.2439
Epoch 2 Step 651 Train Loss: 0.2413
Epoch 2 Step 701 Train Loss: 0.2458
Epoch 2 Step 751 Train Loss: 0.2175
Epoch 2 Step 801 Train Loss: 0.2076
Epoch 2 Step 851 Train Loss: 0.2358
Epoch 2 Step 901 Train Loss: 0.2518
Epoch 2 Step 951 Train Loss: 0.2363
Epoch 2 Step 1001 Train Loss: 0.2442
Epoch 2 Step 1051 Train Loss: 0.2221
Epoch 2 Step 1101 Train Loss: 0.2322
Epoch 2 Step 1151 Train Loss: 0.2022
Epoch 2 Step 1201 Train Loss: 0.2176
Epoch 2 Step 1251 Train Loss: 0.2338
Epoch 2 Step 1301 Train Loss: 0.2107
Epoch 2 Step 1351 Train Loss: 0.2048
Epoch 2 Step 1401 Train Loss: 0.2253
Epoch 2 Step 1451 Train Loss: 0.2221
Epoch 2 Step 1501 Train Loss: 0.2229
Epoch 2 Step 1551 Train Loss: 0.2161
Epoch 2 Step 1601 Train Loss: 0.2247
Epoch 2 Step 1651 Train Loss: 0.2181
Epoch 2 Step 1701 Train Loss: 0.2238
Epoch 2 Step 1751 Train Loss: 0.2261
Epoch 2 Step 1801 Train Loss: 0.2260
Epoch 2: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 3 Step 1 Train Loss: 0.2240
Epoch 3 Step 51 Train Loss: 0.2130
Epoch 3 Step 101 Train Loss: 0.2426
Epoch 3 Step 151 Train Loss: 0.2320
Epoch 3 Step 201 Train Loss: 0.2504
Epoch 3 Step 251 Train Loss: 0.2276
Epoch 3 Step 301 Train Loss: 0.2322
Epoch 3 Step 351 Train Loss: 0.2420
Epoch 3 Step 401 Train Loss: 0.2190
Epoch 3 Step 451 Train Loss: 0.2205
Epoch 3 Step 501 Train Loss: 0.2496
Epoch 3 Step 551 Train Loss: 0.2218
Epoch 3 Step 601 Train Loss: 0.2412
Epoch 3 Step 651 Train Loss: 0.2200
Epoch 3 Step 701 Train Loss: 0.2440
Epoch 3 Step 751 Train Loss: 0.2615
Epoch 3 Step 801 Train Loss: 0.2228
Epoch 3 Step 851 Train Loss: 0.2294
Epoch 3 Step 901 Train Loss: 0.2304
Epoch 3 Step 951 Train Loss: 0.2317
Epoch 3 Step 1001 Train Loss: 0.2391
Epoch 3 Step 1051 Train Loss: 0.2254
Epoch 3 Step 1101 Train Loss: 0.2248
Epoch 3 Step 1151 Train Loss: 0.2244
Epoch 3 Step 1201 Train Loss: 0.2311
Epoch 3 Step 1251 Train Loss: 0.2407
Epoch 3 Step 1301 Train Loss: 0.2404
Epoch 3 Step 1351 Train Loss: 0.2400
Epoch 3 Step 1401 Train Loss: 0.2542
Epoch 3 Step 1451 Train Loss: 0.2385
Epoch 3 Step 1501 Train Loss: 0.2264
Epoch 3 Step 1551 Train Loss: 0.2516
Epoch 3 Step 1601 Train Loss: 0.2228
Epoch 3 Step 1651 Train Loss: 0.2557
Epoch 3 Step 1701 Train Loss: 0.2299
Epoch 3 Step 1751 Train Loss: 0.2223
Epoch 3 Step 1801 Train Loss: 0.2288
Epoch 3: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 4 Step 1 Train Loss: 0.2184
Epoch 4 Step 51 Train Loss: 0.2170
Epoch 4 Step 101 Train Loss: 0.2288
Epoch 4 Step 151 Train Loss: 0.2461
Epoch 4 Step 201 Train Loss: 0.2181
Epoch 4 Step 251 Train Loss: 0.2371
Epoch 4 Step 301 Train Loss: 0.2534
Epoch 4 Step 351 Train Loss: 0.2409
Epoch 4 Step 401 Train Loss: 0.2551
Epoch 4 Step 451 Train Loss: 0.2246
Epoch 4 Step 501 Train Loss: 0.2361
Epoch 4 Step 551 Train Loss: 0.2328
Epoch 4 Step 601 Train Loss: 0.2393
Epoch 4 Step 651 Train Loss: 0.2209
Epoch 4 Step 701 Train Loss: 0.2287
Epoch 4 Step 751 Train Loss: 0.2389
Epoch 4 Step 801 Train Loss: 0.2406
Epoch 4 Step 851 Train Loss: 0.2274
Epoch 4 Step 901 Train Loss: 0.2426
Epoch 4 Step 951 Train Loss: 0.2301
Epoch 4 Step 1001 Train Loss: 0.2324
Epoch 4 Step 1051 Train Loss: 0.2653
Epoch 4 Step 1101 Train Loss: 0.2219
Epoch 4 Step 1151 Train Loss: 0.2560
Epoch 4 Step 1201 Train Loss: 0.2368
Epoch 4 Step 1251 Train Loss: 0.2474
Epoch 4 Step 1301 Train Loss: 0.2454
Epoch 4 Step 1351 Train Loss: 0.2330
Epoch 4 Step 1401 Train Loss: 0.2299
Epoch 4 Step 1451 Train Loss: 0.2251
Epoch 4 Step 1501 Train Loss: 0.2241
Epoch 4 Step 1551 Train Loss: 0.2307
Epoch 4 Step 1601 Train Loss: 0.2078
Epoch 4 Step 1651 Train Loss: 0.2294
Epoch 4 Step 1701 Train Loss: 0.2201
Epoch 4 Step 1751 Train Loss: 0.2298
Epoch 4 Step 1801 Train Loss: 0.2213
Epoch 4: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 5 Step 1 Train Loss: 0.2376
Epoch 5 Step 51 Train Loss: 0.2442
Epoch 5 Step 101 Train Loss: 0.2435
Epoch 5 Step 151 Train Loss: 0.2371
Epoch 5 Step 201 Train Loss: 0.2334
Epoch 5 Step 251 Train Loss: 0.2232
Epoch 5 Step 301 Train Loss: 0.2177
Epoch 5 Step 351 Train Loss: 0.2359
Epoch 5 Step 401 Train Loss: 0.2385
Epoch 5 Step 451 Train Loss: 0.2392
Epoch 5 Step 501 Train Loss: 0.2334
Epoch 5 Step 551 Train Loss: 0.2389
Epoch 5 Step 601 Train Loss: 0.2439
Epoch 5 Step 651 Train Loss: 0.2398
Epoch 5 Step 701 Train Loss: 0.2425
Epoch 5 Step 751 Train Loss: 0.2163
Epoch 5 Step 801 Train Loss: 0.2187
Epoch 5 Step 851 Train Loss: 0.2366
Epoch 5 Step 901 Train Loss: 0.2448
Epoch 5 Step 951 Train Loss: 0.2401
Epoch 5 Step 1001 Train Loss: 0.2340
Epoch 5 Step 1051 Train Loss: 0.2430
Epoch 5 Step 1101 Train Loss: 0.2573
Epoch 5 Step 1151 Train Loss: 0.2547
Epoch 5 Step 1201 Train Loss: 0.2316
Epoch 5 Step 1251 Train Loss: 0.2287
Epoch 5 Step 1301 Train Loss: 0.2218
Epoch 5 Step 1351 Train Loss: 0.2595
Epoch 5 Step 1401 Train Loss: 0.2390
Epoch 5 Step 1451 Train Loss: 0.2410
Epoch 5 Step 1501 Train Loss: 0.2290
Epoch 5 Step 1551 Train Loss: 0.2602
Epoch 5 Step 1601 Train Loss: 0.2618
Epoch 5 Step 1651 Train Loss: 0.2275
Epoch 5 Step 1701 Train Loss: 0.2761
Epoch 5 Step 1751 Train Loss: 0.2335
Epoch 5 Step 1801 Train Loss: 0.2437
Epoch 5: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 6 Step 1 Train Loss: 0.2151
Epoch 6 Step 51 Train Loss: 0.2300
Epoch 6 Step 101 Train Loss: 0.2344
Epoch 6 Step 151 Train Loss: 0.2416
Epoch 6 Step 201 Train Loss: 0.2529
Epoch 6 Step 251 Train Loss: 0.2213
Epoch 6 Step 301 Train Loss: 0.2628
Epoch 6 Step 351 Train Loss: 0.2258
Epoch 6 Step 401 Train Loss: 0.2493
Epoch 6 Step 451 Train Loss: 0.2407
Epoch 6 Step 501 Train Loss: 0.2394
Epoch 6 Step 551 Train Loss: 0.2539
Epoch 6 Step 601 Train Loss: 0.2511
Epoch 6 Step 651 Train Loss: 0.2515
Epoch 6 Step 701 Train Loss: 0.2403
Epoch 6 Step 751 Train Loss: 0.2425
Epoch 6 Step 801 Train Loss: 0.2150
Epoch 6 Step 851 Train Loss: 0.2320
Epoch 6 Step 901 Train Loss: 0.2646
Epoch 6 Step 951 Train Loss: 0.2240
Epoch 6 Step 1001 Train Loss: 0.2479
Epoch 6 Step 1051 Train Loss: 0.2339
Epoch 6 Step 1101 Train Loss: 0.2270
Epoch 6 Step 1151 Train Loss: 0.2468
Epoch 6 Step 1201 Train Loss: 0.2388
Epoch 6 Step 1251 Train Loss: 0.2355
Epoch 6 Step 1301 Train Loss: 0.2360
Epoch 6 Step 1351 Train Loss: 0.2519
Epoch 6 Step 1401 Train Loss: 0.2357
Epoch 6 Step 1451 Train Loss: 0.2187
Epoch 6 Step 1501 Train Loss: 0.2543
Epoch 6 Step 1551 Train Loss: 0.2438
Epoch 6 Step 1601 Train Loss: 0.2605
Epoch 6 Step 1651 Train Loss: 0.2421
Epoch 6 Step 1701 Train Loss: 0.2555
Epoch 6 Step 1751 Train Loss: 0.2568
Epoch 6 Step 1801 Train Loss: 0.2443
Epoch 6: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 7 Step 1 Train Loss: 0.2527
Epoch 7 Step 51 Train Loss: 0.2421
Epoch 7 Step 101 Train Loss: 0.2335
Epoch 7 Step 151 Train Loss: 0.2511
Epoch 7 Step 201 Train Loss: 0.2333
Epoch 7 Step 251 Train Loss: 0.2205
Epoch 7 Step 301 Train Loss: 0.2281
Epoch 7 Step 351 Train Loss: 0.2535
Epoch 7 Step 401 Train Loss: 0.2195
Epoch 7 Step 451 Train Loss: 0.2568
Epoch 7 Step 501 Train Loss: 0.2321
Epoch 7 Step 551 Train Loss: 0.2364
Epoch 7 Step 601 Train Loss: 0.2269
Epoch 7 Step 651 Train Loss: 0.2701
Epoch 7 Step 701 Train Loss: 0.2395
Epoch 7 Step 751 Train Loss: 0.2384
Epoch 7 Step 801 Train Loss: 0.2474
Epoch 7 Step 851 Train Loss: 0.2356
Epoch 7 Step 901 Train Loss: 0.2497
Epoch 7 Step 951 Train Loss: 0.2416
Epoch 7 Step 1001 Train Loss: 0.2404
Epoch 7 Step 1051 Train Loss: 0.2536
Epoch 7 Step 1101 Train Loss: 0.2408
Epoch 7 Step 1151 Train Loss: 0.2299
Epoch 7 Step 1201 Train Loss: 0.2415
Epoch 7 Step 1251 Train Loss: 0.2186
Epoch 7 Step 1301 Train Loss: 0.2442
Epoch 7 Step 1351 Train Loss: 0.2283
Epoch 7 Step 1401 Train Loss: 0.2523
Epoch 7 Step 1451 Train Loss: 0.2332
Epoch 7 Step 1501 Train Loss: 0.2471
Epoch 7 Step 1551 Train Loss: 0.2375
Epoch 7 Step 1601 Train Loss: 0.2224
Epoch 7 Step 1651 Train Loss: 0.2980
Epoch 7 Step 1701 Train Loss: 0.2358
Epoch 7 Step 1751 Train Loss: 0.2310
Epoch 7 Step 1801 Train Loss: 0.2707
Epoch 7: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 8 Step 1 Train Loss: 0.2498
Epoch 8 Step 51 Train Loss: 0.2399
Epoch 8 Step 101 Train Loss: 0.2312
Epoch 8 Step 151 Train Loss: 0.2652
Epoch 8 Step 201 Train Loss: 0.2702
Epoch 8 Step 251 Train Loss: 0.2581
Epoch 8 Step 301 Train Loss: 0.2384
Epoch 8 Step 351 Train Loss: 0.2365
Epoch 8 Step 401 Train Loss: 0.2321
Epoch 8 Step 451 Train Loss: 0.2354
Epoch 8 Step 501 Train Loss: 0.2442
Epoch 8 Step 551 Train Loss: 0.2350
Epoch 8 Step 601 Train Loss: 0.2274
Epoch 8 Step 651 Train Loss: 0.2510
Epoch 8 Step 701 Train Loss: 0.2326
Epoch 8 Step 751 Train Loss: 0.2607
Epoch 8 Step 801 Train Loss: 0.2383
Epoch 8 Step 851 Train Loss: 0.2584
Epoch 8 Step 901 Train Loss: 0.2575
Epoch 8 Step 951 Train Loss: 0.2412
Epoch 8 Step 1001 Train Loss: 0.2604
Epoch 8 Step 1051 Train Loss: 0.2412
Epoch 8 Step 1101 Train Loss: 0.2439
Epoch 8 Step 1151 Train Loss: 0.2560
Epoch 8 Step 1201 Train Loss: 0.2441
Epoch 8 Step 1251 Train Loss: 0.2237
Epoch 8 Step 1301 Train Loss: 0.2498
Epoch 8 Step 1351 Train Loss: 0.2499
Epoch 8 Step 1401 Train Loss: 0.2354
Epoch 8 Step 1451 Train Loss: 0.2358
Epoch 8 Step 1501 Train Loss: 0.2275
Epoch 8 Step 1551 Train Loss: 0.2392
Epoch 8 Step 1601 Train Loss: 0.2373
Epoch 8 Step 1651 Train Loss: 0.2674
Epoch 8 Step 1701 Train Loss: 0.2487
Epoch 8 Step 1751 Train Loss: 0.2474
Epoch 8 Step 1801 Train Loss: 0.2492
Epoch 8: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 9 Step 1 Train Loss: 0.2589
Epoch 9 Step 51 Train Loss: 0.2403
Epoch 9 Step 101 Train Loss: 0.2586
Epoch 9 Step 151 Train Loss: 0.2300
Epoch 9 Step 201 Train Loss: 0.2458
Epoch 9 Step 251 Train Loss: 0.2381
Epoch 9 Step 301 Train Loss: 0.2435
Epoch 9 Step 351 Train Loss: 0.2394
Epoch 9 Step 401 Train Loss: 0.2415
Epoch 9 Step 451 Train Loss: 0.2250
Epoch 9 Step 501 Train Loss: 0.2349
Epoch 9 Step 551 Train Loss: 0.2626
Epoch 9 Step 601 Train Loss: 0.2400
Epoch 9 Step 651 Train Loss: 0.2519
Epoch 9 Step 701 Train Loss: 0.2546
Epoch 9 Step 751 Train Loss: 0.2580
Epoch 9 Step 801 Train Loss: 0.2401
Epoch 9 Step 851 Train Loss: 0.2231
Epoch 9 Step 901 Train Loss: 0.2439
Epoch 9 Step 951 Train Loss: 0.2431
Epoch 9 Step 1001 Train Loss: 0.2586
Epoch 9 Step 1051 Train Loss: 0.2484
Epoch 9 Step 1101 Train Loss: 0.2306
Epoch 9 Step 1151 Train Loss: 0.2394
Epoch 9 Step 1201 Train Loss: 0.2542
Epoch 9 Step 1251 Train Loss: 0.2602
Epoch 9 Step 1301 Train Loss: 0.2329
Epoch 9 Step 1351 Train Loss: 0.2412
Epoch 9 Step 1401 Train Loss: 0.2512
Epoch 9 Step 1451 Train Loss: 0.2383
Epoch 9 Step 1501 Train Loss: 0.2356
Epoch 9 Step 1551 Train Loss: 0.2416
Epoch 9 Step 1601 Train Loss: 0.2238
Epoch 9 Step 1651 Train Loss: 0.2523
Epoch 9 Step 1701 Train Loss: 0.2470
Epoch 9 Step 1751 Train Loss: 0.2529
Epoch 9 Step 1801 Train Loss: 0.2519
Epoch 9: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 10 Step 1 Train Loss: 0.2340
Epoch 10 Step 51 Train Loss: 0.2327
Epoch 10 Step 101 Train Loss: 0.2440
Epoch 10 Step 151 Train Loss: 0.2341
Epoch 10 Step 201 Train Loss: 0.2232
Epoch 10 Step 251 Train Loss: 0.2715
Epoch 10 Step 301 Train Loss: 0.2552
Epoch 10 Step 351 Train Loss: 0.2499
Epoch 10 Step 401 Train Loss: 0.2392
Epoch 10 Step 451 Train Loss: 0.2507
Epoch 10 Step 501 Train Loss: 0.2438
Epoch 10 Step 551 Train Loss: 0.2439
Epoch 10 Step 601 Train Loss: 0.2312
Epoch 10 Step 651 Train Loss: 0.2247
Epoch 10 Step 701 Train Loss: 0.2352
Epoch 10 Step 751 Train Loss: 0.2367
Epoch 10 Step 801 Train Loss: 0.2357
Epoch 10 Step 851 Train Loss: 0.2573
Epoch 10 Step 901 Train Loss: 0.2266
Epoch 10 Step 951 Train Loss: 0.2464
Epoch 10 Step 1001 Train Loss: 0.2229
Epoch 10 Step 1051 Train Loss: 0.2682
Epoch 10 Step 1101 Train Loss: 0.2320
Epoch 10 Step 1151 Train Loss: 0.2502
Epoch 10 Step 1201 Train Loss: 0.2465
Epoch 10 Step 1251 Train Loss: 0.2232
Epoch 10 Step 1301 Train Loss: 0.2130
Epoch 10 Step 1351 Train Loss: 0.2561
Epoch 10 Step 1401 Train Loss: 0.2300
Epoch 10 Step 1451 Train Loss: 0.2370
Epoch 10 Step 1501 Train Loss: 0.2412
Epoch 10 Step 1551 Train Loss: 0.2568
Epoch 10 Step 1601 Train Loss: 0.2425
Epoch 10 Step 1651 Train Loss: 0.2377
Epoch 10 Step 1701 Train Loss: 0.2447
Epoch 10 Step 1751 Train Loss: 0.2364
Epoch 10 Step 1801 Train Loss: 0.2259
Epoch 10: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 11 Step 1 Train Loss: 0.2499
Epoch 11 Step 51 Train Loss: 0.2155
Epoch 11 Step 101 Train Loss: 0.2344
Epoch 11 Step 151 Train Loss: 0.2483
Epoch 11 Step 201 Train Loss: 0.2365
Epoch 11 Step 251 Train Loss: 0.2337
Epoch 11 Step 301 Train Loss: 0.2380
Epoch 11 Step 351 Train Loss: 0.2349
Epoch 11 Step 401 Train Loss: 0.2376
Epoch 11 Step 451 Train Loss: 0.2417
Epoch 11 Step 501 Train Loss: 0.2423
Epoch 11 Step 551 Train Loss: 0.2359
Epoch 11 Step 601 Train Loss: 0.2417
Epoch 11 Step 651 Train Loss: 0.2506
Epoch 11 Step 701 Train Loss: 0.2256
Epoch 11 Step 751 Train Loss: 0.2580
Epoch 11 Step 801 Train Loss: 0.2512
Epoch 11 Step 851 Train Loss: 0.2170
Epoch 11 Step 901 Train Loss: 0.2326
Epoch 11 Step 951 Train Loss: 0.2170
Epoch 11 Step 1001 Train Loss: 0.2405
Epoch 11 Step 1051 Train Loss: 0.2361
Epoch 11 Step 1101 Train Loss: 0.2410
Epoch 11 Step 1151 Train Loss: 0.2250
Epoch 11 Step 1201 Train Loss: 0.2356
Epoch 11 Step 1251 Train Loss: 0.2527
Epoch 11 Step 1301 Train Loss: 0.2343
Epoch 11 Step 1351 Train Loss: 0.2491
Epoch 11 Step 1401 Train Loss: 0.2531
Epoch 11 Step 1451 Train Loss: 0.2345
Epoch 11 Step 1501 Train Loss: 0.2389
Epoch 11 Step 1551 Train Loss: 0.2659
Epoch 11 Step 1601 Train Loss: 0.2278
Epoch 11 Step 1651 Train Loss: 0.2482
Epoch 11 Step 1701 Train Loss: 0.2419
Epoch 11 Step 1751 Train Loss: 0.2554
Epoch 11 Step 1801 Train Loss: 0.2421
Epoch 11: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 12 Step 1 Train Loss: 0.2430
Epoch 12 Step 51 Train Loss: 0.2504
Epoch 12 Step 101 Train Loss: 0.2456
Epoch 12 Step 151 Train Loss: 0.2216
Epoch 12 Step 201 Train Loss: 0.2402
Epoch 12 Step 251 Train Loss: 0.2337
Epoch 12 Step 301 Train Loss: 0.2311
Epoch 12 Step 351 Train Loss: 0.2139
Epoch 12 Step 401 Train Loss: 0.2432
Epoch 12 Step 451 Train Loss: 0.2671
Epoch 12 Step 501 Train Loss: 0.2562
Epoch 12 Step 551 Train Loss: 0.2331
Epoch 12 Step 601 Train Loss: 0.2492
Epoch 12 Step 651 Train Loss: 0.2346
Epoch 12 Step 701 Train Loss: 0.2499
Epoch 12 Step 751 Train Loss: 0.2529
Epoch 12 Step 801 Train Loss: 0.2302
Epoch 12 Step 851 Train Loss: 0.2223
Epoch 12 Step 901 Train Loss: 0.2276
Epoch 12 Step 951 Train Loss: 0.2469
Epoch 12 Step 1001 Train Loss: 0.2617
Epoch 12 Step 1051 Train Loss: 0.2311
Epoch 12 Step 1101 Train Loss: 0.2251
Epoch 12 Step 1151 Train Loss: 0.2492
Epoch 12 Step 1201 Train Loss: 0.2632
Epoch 12 Step 1251 Train Loss: 0.2338
Epoch 12 Step 1301 Train Loss: 0.2415
Epoch 12 Step 1351 Train Loss: 0.2324
Epoch 12 Step 1401 Train Loss: 0.2317
Epoch 12 Step 1451 Train Loss: 0.2483
Epoch 12 Step 1501 Train Loss: 0.2278
Epoch 12 Step 1551 Train Loss: 0.2568
Epoch 12 Step 1601 Train Loss: 0.2362
Epoch 12 Step 1651 Train Loss: 0.2497
Epoch 12 Step 1701 Train Loss: 0.2543
Epoch 12 Step 1751 Train Loss: 0.2870
Epoch 12 Step 1801 Train Loss: 0.2640
Epoch 12: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 13 Step 1 Train Loss: 0.2559
Epoch 13 Step 51 Train Loss: 0.2465
Epoch 13 Step 101 Train Loss: 0.2473
Epoch 13 Step 151 Train Loss: 0.2498
Epoch 13 Step 201 Train Loss: 0.2417
Epoch 13 Step 251 Train Loss: 0.2454
Epoch 13 Step 301 Train Loss: 0.2320
Epoch 13 Step 351 Train Loss: 0.2298
Epoch 13 Step 401 Train Loss: 0.2493
Epoch 13 Step 451 Train Loss: 0.2319
Epoch 13 Step 501 Train Loss: 0.2497
Epoch 13 Step 551 Train Loss: 0.2351
Epoch 13 Step 601 Train Loss: 0.2299
Epoch 13 Step 651 Train Loss: 0.2369
Epoch 13 Step 701 Train Loss: 0.2361
Epoch 13 Step 751 Train Loss: 0.2292
Epoch 13 Step 801 Train Loss: 0.2515
Epoch 13 Step 851 Train Loss: 0.2334
Epoch 13 Step 901 Train Loss: 0.2478
Epoch 13 Step 951 Train Loss: 0.2344
Epoch 13 Step 1001 Train Loss: 0.2284
Epoch 13 Step 1051 Train Loss: 0.2577
Epoch 13 Step 1101 Train Loss: 0.2408
Epoch 13 Step 1151 Train Loss: 0.2342
Epoch 13 Step 1201 Train Loss: 0.2715
Epoch 13 Step 1251 Train Loss: 0.2414
Epoch 13 Step 1301 Train Loss: 0.2459
Epoch 13 Step 1351 Train Loss: 0.2342
Epoch 13 Step 1401 Train Loss: 0.2317
Epoch 13 Step 1451 Train Loss: 0.2593
Epoch 13 Step 1501 Train Loss: 0.2308
Epoch 13 Step 1551 Train Loss: 0.2599
Epoch 13 Step 1601 Train Loss: 0.2498
Epoch 13 Step 1651 Train Loss: 0.2313
Epoch 13 Step 1701 Train Loss: 0.2701
Epoch 13 Step 1751 Train Loss: 0.2304
Epoch 13 Step 1801 Train Loss: 0.2496
Epoch 13: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 14 Step 1 Train Loss: 0.2435
Epoch 14 Step 51 Train Loss: 0.2610
Epoch 14 Step 101 Train Loss: 0.2279
Epoch 14 Step 151 Train Loss: 0.2606
Epoch 14 Step 201 Train Loss: 0.2390
Epoch 14 Step 251 Train Loss: 0.2300
Epoch 14 Step 301 Train Loss: 0.2129
Epoch 14 Step 351 Train Loss: 0.2269
Epoch 14 Step 401 Train Loss: 0.2229
Epoch 14 Step 451 Train Loss: 0.2504
Epoch 14 Step 501 Train Loss: 0.2272
Epoch 14 Step 551 Train Loss: 0.2277
Epoch 14 Step 601 Train Loss: 0.2563
Epoch 14 Step 651 Train Loss: 0.2291
Epoch 14 Step 701 Train Loss: 0.2455
Epoch 14 Step 751 Train Loss: 0.2354
Epoch 14 Step 801 Train Loss: 0.2537
Epoch 14 Step 851 Train Loss: 0.2521
Epoch 14 Step 901 Train Loss: 0.2604
Epoch 14 Step 951 Train Loss: 0.2520
Epoch 14 Step 1001 Train Loss: 0.2372
Epoch 14 Step 1051 Train Loss: 0.2346
Epoch 14 Step 1101 Train Loss: 0.2319
Epoch 14 Step 1151 Train Loss: 0.2369
Epoch 14 Step 1201 Train Loss: 0.2373
Epoch 14 Step 1251 Train Loss: 0.2263
Epoch 14 Step 1301 Train Loss: 0.2366
Epoch 14 Step 1351 Train Loss: 0.2266
Epoch 14 Step 1401 Train Loss: 0.2280
Epoch 14 Step 1451 Train Loss: 0.2410
Epoch 14 Step 1501 Train Loss: 0.2320
Epoch 14 Step 1551 Train Loss: 0.2366
Epoch 14 Step 1601 Train Loss: 0.2412
Epoch 14 Step 1651 Train Loss: 0.2300
Epoch 14 Step 1701 Train Loss: 0.2406
Epoch 14 Step 1751 Train Loss: 0.2392
Epoch 14 Step 1801 Train Loss: 0.2666
Epoch 14: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Epoch 15 Step 1 Train Loss: 0.2181
Epoch 15 Step 51 Train Loss: 0.2470
Epoch 15 Step 101 Train Loss: 0.2411
Epoch 15 Step 151 Train Loss: 0.2459
Epoch 15 Step 201 Train Loss: 0.2334
Epoch 15 Step 251 Train Loss: 0.2239
Epoch 15 Step 301 Train Loss: 0.2631
Epoch 15 Step 351 Train Loss: 0.2569
Epoch 15 Step 401 Train Loss: 0.2206
Epoch 15 Step 451 Train Loss: 0.2499
Epoch 15 Step 501 Train Loss: 0.2350
Epoch 15 Step 551 Train Loss: 0.2270
Epoch 15 Step 601 Train Loss: 0.2397
Epoch 15 Step 651 Train Loss: 0.2493
Epoch 15 Step 701 Train Loss: 0.2359
Epoch 15 Step 751 Train Loss: 0.2345
Epoch 15 Step 801 Train Loss: 0.2352
Epoch 15 Step 851 Train Loss: 0.2433
Epoch 15 Step 901 Train Loss: 0.2368
Epoch 15 Step 951 Train Loss: 0.2443
Epoch 15 Step 1001 Train Loss: 0.2239
Epoch 15 Step 1051 Train Loss: 0.2396
Epoch 15 Step 1101 Train Loss: 0.2390
Epoch 15 Step 1151 Train Loss: 0.2299
Epoch 15 Step 1201 Train Loss: 0.2255
Epoch 15 Step 1251 Train Loss: 0.2783
Epoch 15 Step 1301 Train Loss: 0.2543
Epoch 15 Step 1351 Train Loss: 0.2541
Epoch 15 Step 1401 Train Loss: 0.2286
Epoch 15 Step 1451 Train Loss: 0.2403
Epoch 15 Step 1501 Train Loss: 0.2381
Epoch 15 Step 1551 Train Loss: 0.2541
Epoch 15 Step 1601 Train Loss: 0.2513
Epoch 15 Step 1651 Train Loss: 0.2414
Epoch 15 Step 1701 Train Loss: 0.2403
Epoch 15 Step 1751 Train Loss: 0.2354
Epoch 15 Step 1801 Train Loss: 0.2221
Epoch 15: Train Overall MSE: 0.0004 Validation Overall MSE: 0.0003. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0065. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0040
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00029887957
test_unseen_single_pearson: 0.9935580951019702
test_unseen_single_mse_de: 0.003971765
test_unseen_single_pearson_de: 0.8058443619727447
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.16126570298214052
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3705882352941176
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9323529411764705
test_unseen_single_mse_top20_de_non_dropout: 0.006693006973359943
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.036 MB of 0.038 MB uploadedwandb: / 0.036 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñá‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00397
wandb:                                              test_de_pearson 0.80584
wandb:               test_frac_opposite_direction_top20_non_dropout 0.37059
wandb:                          test_frac_sigma_below_1_non_dropout 0.93235
wandb:                                                     test_mse 0.0003
wandb:                                test_mse_top20_de_non_dropout 0.00669
wandb:                                                 test_pearson 0.99356
wandb:                                           test_pearson_delta 0.16127
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.37059
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93235
wandb:                                       test_unseen_single_mse 0.0003
wandb:                                    test_unseen_single_mse_de 0.00397
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00669
wandb:                                   test_unseen_single_pearson 0.99356
wandb:                                test_unseen_single_pearson_de 0.80584
wandb:                             test_unseen_single_pearson_delta 0.16127
wandb:                                                 train_de_mse 0.00577
wandb:                                             train_de_pearson 0.77955
wandb:                                                    train_mse 0.00038
wandb:                                                train_pearson 0.99192
wandb:                                                training_loss 0.22659
wandb:                                                   val_de_mse 0.00648
wandb:                                               val_de_pearson 0.83036
wandb:                                                      val_mse 0.00029
wandb:                                                  val_pearson 0.99376
wandb: 
wandb: üöÄ View run geneformer_XuCao2023_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/s6smghrk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_063110-s6smghrk/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_072135-c54kyfia
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_ShifrutMarson2018_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/c54kyfia
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
  0%|                                                  | 0/3286 [00:00<?, ?it/s]  0%|                                          | 8/3286 [00:00<00:49, 66.39it/s]  1%|‚ñè                                        | 18/3286 [00:00<00:40, 80.44it/s]  1%|‚ñé                                        | 28/3286 [00:00<00:39, 83.50it/s]  1%|‚ñç                                        | 39/3286 [00:00<00:35, 90.48it/s]  1%|‚ñå                                        | 49/3286 [00:00<00:35, 90.43it/s]  2%|‚ñã                                        | 59/3286 [00:00<00:35, 90.57it/s]  2%|‚ñä                                        | 69/3286 [00:00<00:35, 91.74it/s]  2%|‚ñâ                                        | 79/3286 [00:00<00:34, 92.21it/s]  3%|‚ñà                                        | 89/3286 [00:01<00:35, 90.12it/s]  3%|‚ñà‚ñè                                      | 100/3286 [00:01<00:34, 93.08it/s]  3%|‚ñà‚ñé                                      | 110/3286 [00:01<00:33, 93.76it/s]  4%|‚ñà‚ñç                                      | 120/3286 [00:01<00:33, 94.28it/s]  4%|‚ñà‚ñå                                      | 130/3286 [00:01<00:33, 93.92it/s]  4%|‚ñà‚ñã                                      | 140/3286 [00:01<00:33, 93.57it/s]  5%|‚ñà‚ñä                                      | 150/3286 [00:01<00:35, 88.00it/s]  5%|‚ñà‚ñâ                                      | 159/3286 [00:01<00:36, 84.63it/s]  5%|‚ñà‚ñà                                      | 168/3286 [00:01<00:37, 82.68it/s]  5%|‚ñà‚ñà‚ñè                                     | 177/3286 [00:02<00:38, 81.10it/s]  6%|‚ñà‚ñà‚ñé                                     | 186/3286 [00:02<00:39, 77.92it/s]  6%|‚ñà‚ñà‚ñé                                     | 195/3286 [00:02<00:39, 78.26it/s]  6%|‚ñà‚ñà‚ñç                                     | 203/3286 [00:02<00:39, 78.15it/s]  6%|‚ñà‚ñà‚ñå                                     | 211/3286 [00:02<00:39, 78.52it/s]  7%|‚ñà‚ñà‚ñã                                     | 221/3286 [00:02<00:38, 79.03it/s]  7%|‚ñà‚ñà‚ñä                                     | 230/3286 [00:02<00:37, 81.38it/s]  7%|‚ñà‚ñà‚ñâ                                     | 239/3286 [00:02<00:37, 80.74it/s]  8%|‚ñà‚ñà‚ñà                                     | 248/3286 [00:02<00:37, 80.34it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 257/3286 [00:03<00:37, 79.76it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 265/3286 [00:03<00:39, 77.41it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 274/3286 [00:03<00:37, 80.36it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 283/3286 [00:03<00:38, 77.52it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 292/3286 [00:03<00:38, 78.02it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 300/3286 [00:03<00:38, 78.09it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 308/3286 [00:03<00:37, 78.61it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 316/3286 [00:03<00:37, 78.85it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 325/3286 [00:03<00:36, 81.36it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 334/3286 [00:04<00:36, 79.99it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 343/3286 [00:04<00:37, 79.35it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 351/3286 [00:04<00:37, 78.86it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 359/3286 [00:04<00:36, 79.16it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 367/3286 [00:04<00:38, 76.57it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 376/3286 [00:04<00:36, 79.79it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 385/3286 [00:04<00:36, 80.01it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 394/3286 [00:04<00:37, 77.00it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 403/3286 [00:04<00:36, 80.07it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 412/3286 [00:04<00:35, 80.01it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 421/3286 [00:05<00:36, 79.47it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 429/3286 [00:05<00:36, 78.92it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 437/3286 [00:05<00:37, 76.44it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 446/3286 [00:05<00:35, 80.08it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 455/3286 [00:05<00:35, 79.76it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 464/3286 [00:05<00:35, 79.72it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 472/3286 [00:05<00:35, 79.28it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 480/3286 [00:05<00:35, 79.22it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 488/3286 [00:05<00:35, 78.95it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 497/3286 [00:06<00:35, 79.26it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 505/3286 [00:06<00:35, 79.21it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 513/3286 [00:06<00:35, 79.19it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 521/3286 [00:06<00:36, 76.71it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 531/3286 [00:06<00:35, 77.93it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 540/3286 [00:06<00:34, 80.52it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 549/3286 [00:06<00:34, 79.97it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 558/3286 [00:06<00:35, 77.38it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 567/3286 [00:06<00:34, 78.20it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 576/3286 [00:07<00:33, 81.39it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 585/3286 [00:07<00:33, 80.59it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 594/3286 [00:07<00:34, 77.76it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 603/3286 [00:07<00:33, 80.61it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 612/3286 [00:07<00:33, 80.35it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 621/3286 [00:07<00:33, 80.28it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 630/3286 [00:07<00:33, 79.97it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 639/3286 [00:07<00:33, 79.80it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 647/3286 [00:07<00:33, 79.70it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 655/3286 [00:08<00:33, 79.65it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 663/3286 [00:08<00:32, 79.57it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 671/3286 [00:08<00:32, 79.50it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 679/3286 [00:08<00:32, 79.09it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 687/3286 [00:08<00:34, 76.27it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 696/3286 [00:08<00:33, 77.18it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 705/3286 [00:08<00:32, 80.51it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 714/3286 [00:08<00:32, 79.99it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 723/3286 [00:08<00:32, 79.83it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 731/3286 [00:09<00:32, 79.54it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 739/3286 [00:09<00:32, 79.39it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 747/3286 [00:09<00:32, 79.22it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 755/3286 [00:09<00:31, 79.11it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 763/3286 [00:09<00:31, 79.07it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 771/3286 [00:09<00:31, 79.20it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 779/3286 [00:09<00:31, 79.24it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 787/3286 [00:09<00:31, 79.10it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 795/3286 [00:09<00:31, 79.28it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 803/3286 [00:09<00:32, 76.22it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 812/3286 [00:10<00:30, 79.91it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 821/3286 [00:10<00:30, 79.96it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 830/3286 [00:10<00:30, 79.58it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 838/3286 [00:10<00:30, 79.46it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 846/3286 [00:10<00:30, 79.28it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 854/3286 [00:10<00:30, 79.11it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 862/3286 [00:10<00:30, 78.85it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 870/3286 [00:10<00:30, 79.06it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 878/3286 [00:10<00:31, 75.91it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 886/3286 [00:10<00:31, 76.72it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 894/3286 [00:11<00:30, 77.53it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 903/3286 [00:11<00:30, 77.77it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 911/3286 [00:11<00:30, 78.02it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 919/3286 [00:11<00:30, 78.37it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 928/3286 [00:11<00:29, 78.63it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 936/3286 [00:11<00:29, 78.63it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 944/3286 [00:11<00:29, 78.75it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 953/3286 [00:11<00:29, 78.89it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 961/3286 [00:11<00:29, 78.70it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 969/3286 [00:12<00:29, 78.84it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 978/3286 [00:12<00:29, 78.87it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 986/3286 [00:12<00:29, 79.14it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 994/3286 [00:12<00:28, 79.22it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1003/3286 [00:12<00:28, 79.07it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1013/3286 [00:12<00:27, 82.38it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1022/3286 [00:12<00:27, 81.18it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1031/3286 [00:12<00:27, 80.58it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1040/3286 [00:12<00:27, 80.61it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1049/3286 [00:13<00:28, 79.82it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1058/3286 [00:13<00:28, 77.46it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1067/3286 [00:13<00:28, 77.90it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1076/3286 [00:13<00:27, 81.15it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1085/3286 [00:13<00:27, 80.88it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1094/3286 [00:13<00:28, 78.04it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1103/3286 [00:13<00:27, 77.99it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1113/3286 [00:13<00:26, 81.39it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1122/3286 [00:13<00:26, 80.80it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1131/3286 [00:14<00:26, 80.35it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1140/3286 [00:14<00:26, 80.36it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1149/3286 [00:14<00:26, 80.00it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1158/3286 [00:14<00:27, 77.50it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1167/3286 [00:14<00:26, 80.78it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1176/3286 [00:14<00:26, 80.42it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1185/3286 [00:14<00:26, 80.23it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1194/3286 [00:14<00:26, 77.51it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1203/3286 [00:14<00:26, 78.14it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1213/3286 [00:15<00:25, 81.36it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1222/3286 [00:15<00:25, 80.24it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1231/3286 [00:15<00:25, 80.18it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1240/3286 [00:15<00:25, 80.07it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1249/3286 [00:15<00:25, 79.77it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1257/3286 [00:15<00:25, 79.51it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1265/3286 [00:15<00:25, 79.28it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1273/3286 [00:15<00:25, 79.34it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1281/3286 [00:15<00:26, 76.75it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1290/3286 [00:16<00:24, 80.27it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1299/3286 [00:16<00:24, 80.08it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1308/3286 [00:16<00:25, 77.04it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1317/3286 [00:16<00:24, 80.52it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1326/3286 [00:16<00:24, 80.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1335/3286 [00:16<00:24, 79.76it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1344/3286 [00:16<00:25, 77.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1353/3286 [00:16<00:24, 77.80it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1363/3286 [00:16<00:23, 81.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1372/3286 [00:17<00:23, 80.86it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1381/3286 [00:17<00:23, 80.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1390/3286 [00:17<00:23, 79.89it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1398/3286 [00:17<00:23, 79.92it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1406/3286 [00:17<00:23, 79.43it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1414/3286 [00:17<00:23, 79.36it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1422/3286 [00:17<00:23, 79.08it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1430/3286 [00:17<00:23, 78.98it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1438/3286 [00:17<00:23, 79.00it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1446/3286 [00:18<00:23, 79.04it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1454/3286 [00:18<00:24, 76.14it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1463/3286 [00:18<00:23, 77.31it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1472/3286 [00:18<00:23, 78.41it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1481/3286 [00:18<00:22, 81.21it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1490/3286 [00:18<00:22, 80.60it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1499/3286 [00:18<00:22, 80.21it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1508/3286 [00:18<00:22, 77.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1517/3286 [00:18<00:21, 80.74it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1526/3286 [00:19<00:21, 80.22it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1535/3286 [00:19<00:21, 80.10it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1544/3286 [00:19<00:22, 77.43it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1553/3286 [00:19<00:21, 80.26it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1562/3286 [00:19<00:22, 77.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1571/3286 [00:19<00:21, 80.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1580/3286 [00:19<00:21, 80.07it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1589/3286 [00:19<00:21, 80.10it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1598/3286 [00:19<00:21, 79.74it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1607/3286 [00:20<00:21, 79.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1616/3286 [00:20<00:20, 79.77it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1624/3286 [00:20<00:20, 79.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1632/3286 [00:20<00:20, 79.64it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1640/3286 [00:20<00:20, 79.29it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1648/3286 [00:20<00:20, 79.20it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1657/3286 [00:20<00:21, 76.87it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1666/3286 [00:20<00:20, 80.40it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1675/3286 [00:20<00:20, 80.39it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1684/3286 [00:21<00:22, 71.80it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1692/3286 [00:21<00:24, 64.87it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1699/3286 [00:21<00:25, 62.98it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1706/3286 [00:21<00:24, 64.27it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1713/3286 [00:21<00:26, 59.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1728/3286 [00:21<00:19, 80.31it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1737/3286 [00:21<00:24, 63.04it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1754/3286 [00:22<00:18, 84.89it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1764/3286 [00:22<00:18, 83.22it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1774/3286 [00:22<00:17, 86.14it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1784/3286 [00:22<00:17, 84.59it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1794/3286 [00:22<00:16, 88.10it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1804/3286 [00:22<00:16, 89.27it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1814/3286 [00:22<00:16, 89.26it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1824/3286 [00:22<00:16, 89.08it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 1834/3286 [00:22<00:16, 86.29it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1844/3286 [00:23<00:16, 84.88it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1856/3286 [00:23<00:15, 91.52it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 1866/3286 [00:23<00:16, 88.28it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1877/3286 [00:23<00:15, 91.75it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 1887/3286 [00:23<00:15, 91.49it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1897/3286 [00:23<00:23, 58.67it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 1923/3286 [00:24<00:17, 80.02it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 1937/3286 [00:24<00:16, 80.61it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 1957/3286 [00:24<00:13, 99.24it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 1968/3286 [00:24<00:14, 89.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 1979/3286 [00:24<00:13, 93.74it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 1990/3286 [00:24<00:13, 95.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2001/3286 [00:24<00:14, 85.72it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2011/3286 [00:25<00:15, 84.86it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2023/3286 [00:25<00:13, 92.94it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2033/3286 [00:25<00:14, 89.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2043/3286 [00:25<00:13, 88.90it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2053/3286 [00:25<00:16, 73.30it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2061/3286 [00:25<00:17, 68.33it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2069/3286 [00:25<00:17, 67.63it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2085/3286 [00:25<00:13, 87.58it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2095/3286 [00:26<00:13, 88.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2105/3286 [00:26<00:15, 78.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2114/3286 [00:26<00:15, 77.56it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2123/3286 [00:26<00:16, 71.23it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2142/3286 [00:26<00:11, 97.31it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2153/3286 [00:26<00:11, 95.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2164/3286 [00:26<00:11, 94.75it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2174/3286 [00:26<00:11, 93.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2184/3286 [00:27<00:11, 92.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2194/3286 [00:27<00:12, 90.88it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2204/3286 [00:27<00:15, 68.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2218/3286 [00:27<00:12, 82.66it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2228/3286 [00:27<00:14, 72.22it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2241/3286 [00:27<00:12, 84.25it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2251/3286 [00:27<00:12, 83.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2261/3286 [00:28<00:11, 85.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2271/3286 [00:28<00:13, 74.87it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2280/3286 [00:28<00:13, 75.68it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2292/3286 [00:28<00:11, 85.84it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2302/3286 [00:28<00:11, 87.41it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2312/3286 [00:28<00:12, 78.89it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2321/3286 [00:28<00:14, 68.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2332/3286 [00:28<00:12, 76.71it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2343/3286 [00:29<00:13, 68.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2354/3286 [00:29<00:12, 76.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2366/3286 [00:29<00:10, 85.48it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2377/3286 [00:29<00:09, 91.36it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2389/3286 [00:29<00:09, 96.99it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2401/3286 [00:29<00:08, 101.07it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2412/3286 [00:29<00:08, 102.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2424/3286 [00:29<00:08, 104.85it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2436/3286 [00:30<00:08, 104.48it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2448/3286 [00:30<00:07, 107.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2460/3286 [00:30<00:07, 108.70it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2471/3286 [00:30<00:07, 108.22it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2483/3286 [00:30<00:07, 109.63it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2494/3286 [00:30<00:07, 106.54it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2505/3286 [00:30<00:07, 102.66it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2516/3286 [00:30<00:08, 94.83it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2526/3286 [00:30<00:08, 91.80it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2536/3286 [00:31<00:08, 89.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2545/3286 [00:31<00:08, 86.62it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2554/3286 [00:31<00:10, 72.73it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2567/3286 [00:31<00:08, 85.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2576/3286 [00:31<00:08, 82.02it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2585/3286 [00:31<00:08, 81.25it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2594/3286 [00:31<00:08, 80.27it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2603/3286 [00:31<00:08, 81.66it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2612/3286 [00:32<00:09, 70.22it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2623/3286 [00:32<00:08, 78.29it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2632/3286 [00:32<00:08, 73.31it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2640/3286 [00:32<00:09, 69.78it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2648/3286 [00:32<00:08, 70.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2656/3286 [00:32<00:09, 68.44it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2663/3286 [00:32<00:10, 60.82it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2673/3286 [00:32<00:08, 69.50it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2681/3286 [00:33<00:08, 68.26it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2689/3286 [00:33<00:08, 68.42it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2696/3286 [00:33<00:08, 67.76it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2703/3286 [00:33<00:08, 67.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2710/3286 [00:33<00:08, 66.75it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2718/3286 [00:33<00:08, 68.51it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2725/3286 [00:33<00:08, 67.99it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2732/3286 [00:33<00:08, 67.24it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2739/3286 [00:33<00:08, 66.30it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2747/3286 [00:34<00:08, 66.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2755/3286 [00:34<00:07, 68.94it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2762/3286 [00:34<00:07, 68.25it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2770/3286 [00:34<00:07, 69.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2777/3286 [00:34<00:07, 67.75it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2784/3286 [00:34<00:08, 61.87it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 2793/3286 [00:34<00:08, 54.90it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 2805/3286 [00:34<00:06, 69.09it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2813/3286 [00:35<00:06, 71.44it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2821/3286 [00:35<00:06, 70.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 2829/3286 [00:35<00:07, 64.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 2838/3286 [00:35<00:06, 70.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2846/3286 [00:35<00:06, 68.65it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2854/3286 [00:35<00:06, 67.75it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2861/3286 [00:35<00:06, 64.07it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 2868/3286 [00:35<00:07, 59.66it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2877/3286 [00:36<00:06, 66.01it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2884/3286 [00:36<00:06, 66.60it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2891/3286 [00:36<00:06, 60.24it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2898/3286 [00:36<00:06, 61.59it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2908/3286 [00:36<00:05, 70.93it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2916/3286 [00:36<00:05, 70.14it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2924/3286 [00:36<00:05, 68.67it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2931/3286 [00:36<00:05, 59.82it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2938/3286 [00:37<00:06, 52.60it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2956/3286 [00:37<00:04, 80.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2966/3286 [00:37<00:03, 83.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2975/3286 [00:37<00:03, 83.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2985/3286 [00:37<00:03, 86.58it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2996/3286 [00:37<00:03, 90.68it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3006/3286 [00:37<00:03, 89.30it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3017/3286 [00:37<00:02, 92.95it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3027/3286 [00:37<00:02, 90.77it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3037/3286 [00:38<00:02, 89.32it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3049/3286 [00:38<00:02, 95.62it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3059/3286 [00:38<00:02, 90.06it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3070/3286 [00:38<00:02, 90.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3080/3286 [00:38<00:02, 70.49it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3101/3286 [00:38<00:01, 100.22it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3113/3286 [00:38<00:01, 98.37it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3124/3286 [00:38<00:01, 91.49it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3135/3286 [00:39<00:01, 94.40it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3145/3286 [00:39<00:01, 94.27it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3156/3286 [00:39<00:01, 97.17it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3167/3286 [00:39<00:01, 94.61it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3177/3286 [00:39<00:01, 76.35it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3187/3286 [00:39<00:01, 80.80it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3202/3286 [00:39<00:00, 84.48it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3214/3286 [00:40<00:00, 77.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3237/3286 [00:40<00:00, 110.01it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3250/3286 [00:40<00:00, 105.93it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3262/3286 [00:40<00:00, 101.08it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3273/3286 [00:40<00:00, 93.97it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3283/3286 [00:40<00:00, 87.26it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3286/3286 [00:40<00:00, 80.62it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.2934
Epoch 1 Step 51 Train Loss: 0.3342
Epoch 1 Step 101 Train Loss: 0.2633
Epoch 1 Step 151 Train Loss: 0.3894
Epoch 1 Step 201 Train Loss: 0.2883
Epoch 1 Step 251 Train Loss: 0.2794
Epoch 1 Step 301 Train Loss: 0.3303
Epoch 1 Step 351 Train Loss: 0.3015
Epoch 1 Step 401 Train Loss: 0.2831
Epoch 1 Step 451 Train Loss: 0.3488
Epoch 1 Step 501 Train Loss: 0.2971
Epoch 1 Step 551 Train Loss: 0.2949
Epoch 1 Step 601 Train Loss: 0.3259
Epoch 1 Step 651 Train Loss: 0.2302
Epoch 1 Step 701 Train Loss: 0.3030
Epoch 1 Step 751 Train Loss: 0.2570
Epoch 1 Step 801 Train Loss: 0.2484
Epoch 1 Step 851 Train Loss: 0.2983
Epoch 1 Step 901 Train Loss: 0.2384
Epoch 1 Step 951 Train Loss: 0.3117
Epoch 1 Step 1001 Train Loss: 0.2660
Epoch 1 Step 1051 Train Loss: 0.2503
Epoch 1 Step 1101 Train Loss: 0.2836
Epoch 1 Step 1151 Train Loss: 0.2788
Epoch 1 Step 1201 Train Loss: 0.2441
Epoch 1 Step 1251 Train Loss: 0.2539
Epoch 1 Step 1301 Train Loss: 0.3206
Epoch 1 Step 1351 Train Loss: 0.3367
Epoch 1 Step 1401 Train Loss: 0.2147
Epoch 1: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0075. 
Epoch 2 Step 1 Train Loss: 0.2497
Epoch 2 Step 51 Train Loss: 0.2601
Epoch 2 Step 101 Train Loss: 0.2713
Epoch 2 Step 151 Train Loss: 0.2062
Epoch 2 Step 201 Train Loss: 0.2240
Epoch 2 Step 251 Train Loss: 0.3421
Epoch 2 Step 301 Train Loss: 0.2617
Epoch 2 Step 351 Train Loss: 0.2625
Epoch 2 Step 401 Train Loss: 0.3269
Epoch 2 Step 451 Train Loss: 0.3046
Epoch 2 Step 501 Train Loss: 0.2484
Epoch 2 Step 551 Train Loss: 0.3037
Epoch 2 Step 601 Train Loss: 0.2980
Epoch 2 Step 651 Train Loss: 0.3061
Epoch 2 Step 701 Train Loss: 0.2232
Epoch 2 Step 751 Train Loss: 0.3254
Epoch 2 Step 801 Train Loss: 0.2840
Epoch 2 Step 851 Train Loss: 0.2736
Epoch 2 Step 901 Train Loss: 0.2393
Epoch 2 Step 951 Train Loss: 0.2590
Epoch 2 Step 1001 Train Loss: 0.2258
Epoch 2 Step 1051 Train Loss: 0.2606
Epoch 2 Step 1101 Train Loss: 0.1915
Epoch 2 Step 1151 Train Loss: 0.2401
Epoch 2 Step 1201 Train Loss: 0.2215
Epoch 2 Step 1251 Train Loss: 0.3220
Epoch 2 Step 1301 Train Loss: 0.3211
Epoch 2 Step 1351 Train Loss: 0.1734
Epoch 2 Step 1401 Train Loss: 0.2977
Epoch 2: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0090. 
Epoch 3 Step 1 Train Loss: 0.2881
Epoch 3 Step 51 Train Loss: 0.2068
Epoch 3 Step 101 Train Loss: 0.2467
Epoch 3 Step 151 Train Loss: 0.2643
Epoch 3 Step 201 Train Loss: 0.3404
Epoch 3 Step 251 Train Loss: 0.2594
Epoch 3 Step 301 Train Loss: 0.2274
Epoch 3 Step 351 Train Loss: 0.3740
Epoch 3 Step 401 Train Loss: 0.2604
Epoch 3 Step 451 Train Loss: 0.1884
Epoch 3 Step 501 Train Loss: 0.2729
Epoch 3 Step 551 Train Loss: 0.2661
Epoch 3 Step 601 Train Loss: 0.3037
Epoch 3 Step 651 Train Loss: 0.2732
Epoch 3 Step 701 Train Loss: 0.2524
Epoch 3 Step 751 Train Loss: 0.2537
Epoch 3 Step 801 Train Loss: 0.2520
Epoch 3 Step 851 Train Loss: 0.2475
Epoch 3 Step 901 Train Loss: 0.2432
Epoch 3 Step 951 Train Loss: 0.2665
Epoch 3 Step 1001 Train Loss: 0.3467
Epoch 3 Step 1051 Train Loss: 0.2063
Epoch 3 Step 1101 Train Loss: 0.3268
Epoch 3 Step 1151 Train Loss: 0.2557
Epoch 3 Step 1201 Train Loss: 0.2461
Epoch 3 Step 1251 Train Loss: 0.2804
Epoch 3 Step 1301 Train Loss: 0.3115
Epoch 3 Step 1351 Train Loss: 0.2207
Epoch 3 Step 1401 Train Loss: 0.2699
Epoch 3: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0164 Validation Top 20 DE MSE: 0.0098. 
Epoch 4 Step 1 Train Loss: 0.1865
Epoch 4 Step 51 Train Loss: 0.2238
Epoch 4 Step 101 Train Loss: 0.3366
Epoch 4 Step 151 Train Loss: 0.2954
Epoch 4 Step 201 Train Loss: 0.2390
Epoch 4 Step 251 Train Loss: 0.3768
Epoch 4 Step 301 Train Loss: 0.2832
Epoch 4 Step 351 Train Loss: 0.2355
Epoch 4 Step 401 Train Loss: 0.2263
Epoch 4 Step 451 Train Loss: 0.1958
Epoch 4 Step 501 Train Loss: 0.2705
Epoch 4 Step 551 Train Loss: 0.2517
Epoch 4 Step 601 Train Loss: 0.3005
Epoch 4 Step 651 Train Loss: 0.3387
Epoch 4 Step 701 Train Loss: 0.2822
Epoch 4 Step 751 Train Loss: 0.2917
Epoch 4 Step 801 Train Loss: 0.2327
Epoch 4 Step 851 Train Loss: 0.1610
Epoch 4 Step 901 Train Loss: 0.3188
Epoch 4 Step 951 Train Loss: 0.2055
Epoch 4 Step 1001 Train Loss: 0.2696
Epoch 4 Step 1051 Train Loss: 0.2055
Epoch 4 Step 1101 Train Loss: 0.2478
Epoch 4 Step 1151 Train Loss: 0.3015
Epoch 4 Step 1201 Train Loss: 0.2549
Epoch 4 Step 1251 Train Loss: 0.2067
Epoch 4 Step 1301 Train Loss: 0.2661
Epoch 4 Step 1351 Train Loss: 0.2532
Epoch 4 Step 1401 Train Loss: 0.1647
Epoch 4: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0093 Validation Top 20 DE MSE: 0.0057. 
Epoch 5 Step 1 Train Loss: 0.2579
Epoch 5 Step 51 Train Loss: 0.2356
Epoch 5 Step 101 Train Loss: 0.2819
Epoch 5 Step 151 Train Loss: 0.2471
Epoch 5 Step 201 Train Loss: 0.2293
Epoch 5 Step 251 Train Loss: 0.2644
Epoch 5 Step 301 Train Loss: 0.2264
Epoch 5 Step 351 Train Loss: 0.2134
Epoch 5 Step 401 Train Loss: 0.2767
Epoch 5 Step 451 Train Loss: 0.2449
Epoch 5 Step 501 Train Loss: 0.2089
Epoch 5 Step 551 Train Loss: 0.2426
Epoch 5 Step 601 Train Loss: 0.2206
Epoch 5 Step 651 Train Loss: 0.1840
Epoch 5 Step 701 Train Loss: 0.2773
Epoch 5 Step 751 Train Loss: 0.2557
Epoch 5 Step 801 Train Loss: 0.2889
Epoch 5 Step 851 Train Loss: 0.2290
Epoch 5 Step 901 Train Loss: 0.3037
Epoch 5 Step 951 Train Loss: 0.2139
Epoch 5 Step 1001 Train Loss: 0.2337
Epoch 5 Step 1051 Train Loss: 0.2653
Epoch 5 Step 1101 Train Loss: 0.2788
Epoch 5 Step 1151 Train Loss: 0.2633
Epoch 5 Step 1201 Train Loss: 0.2385
Epoch 5 Step 1251 Train Loss: 0.2202
Epoch 5 Step 1301 Train Loss: 0.2596
Epoch 5 Step 1351 Train Loss: 0.2854
Epoch 5 Step 1401 Train Loss: 0.2522
Epoch 5: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.0057. 
Epoch 6 Step 1 Train Loss: 0.2308
Epoch 6 Step 51 Train Loss: 0.2630
Epoch 6 Step 101 Train Loss: 0.2953
Epoch 6 Step 151 Train Loss: 0.2481
Epoch 6 Step 201 Train Loss: 0.3023
Epoch 6 Step 251 Train Loss: 0.2175
Epoch 6 Step 301 Train Loss: 0.1854
Epoch 6 Step 351 Train Loss: 0.2363
Epoch 6 Step 401 Train Loss: 0.2534
Epoch 6 Step 451 Train Loss: 0.3020
Epoch 6 Step 501 Train Loss: 0.4051
Epoch 6 Step 551 Train Loss: 0.2313
Epoch 6 Step 601 Train Loss: 0.2844
Epoch 6 Step 651 Train Loss: 0.2949
Epoch 6 Step 701 Train Loss: 0.2281
Epoch 6 Step 751 Train Loss: 0.2445
Epoch 6 Step 801 Train Loss: 0.3150
Epoch 6 Step 851 Train Loss: 0.2625
Epoch 6 Step 901 Train Loss: 0.2811
Epoch 6 Step 951 Train Loss: 0.3544
Epoch 6 Step 1001 Train Loss: 0.2530
Epoch 6 Step 1051 Train Loss: 0.2667
Epoch 6 Step 1101 Train Loss: 0.2341
Epoch 6 Step 1151 Train Loss: 0.2273
Epoch 6 Step 1201 Train Loss: 0.2725
Epoch 6 Step 1251 Train Loss: 0.2794
Epoch 6 Step 1301 Train Loss: 0.3152
Epoch 6 Step 1351 Train Loss: 0.2801
Epoch 6 Step 1401 Train Loss: 0.3068
Epoch 6: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0059. 
Epoch 7 Step 1 Train Loss: 0.2888
Epoch 7 Step 51 Train Loss: 0.3370
Epoch 7 Step 101 Train Loss: 0.2751
Epoch 7 Step 151 Train Loss: 0.2330
Epoch 7 Step 201 Train Loss: 0.2379
Epoch 7 Step 251 Train Loss: 0.3142
Epoch 7 Step 301 Train Loss: 0.2086
Epoch 7 Step 351 Train Loss: 0.2245
Epoch 7 Step 401 Train Loss: 0.2939
Epoch 7 Step 451 Train Loss: 0.2658
Epoch 7 Step 501 Train Loss: 0.2613
Epoch 7 Step 551 Train Loss: 0.2205
Epoch 7 Step 601 Train Loss: 0.2237
Epoch 7 Step 651 Train Loss: 0.2417
Epoch 7 Step 701 Train Loss: 0.2196
Epoch 7 Step 751 Train Loss: 0.2528
Epoch 7 Step 801 Train Loss: 0.2273
Epoch 7 Step 851 Train Loss: 0.2351
Epoch 7 Step 901 Train Loss: 0.2563
Epoch 7 Step 951 Train Loss: 0.3079
Epoch 7 Step 1001 Train Loss: 0.1957
Epoch 7 Step 1051 Train Loss: 0.2540
Epoch 7 Step 1101 Train Loss: 0.2788
Epoch 7 Step 1151 Train Loss: 0.1715
Epoch 7 Step 1201 Train Loss: 0.2960
Epoch 7 Step 1251 Train Loss: 0.2794
Epoch 7 Step 1301 Train Loss: 0.2714
Epoch 7 Step 1351 Train Loss: 0.2596
Epoch 7 Step 1401 Train Loss: 0.2980
Epoch 7: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0065. 
Epoch 8 Step 1 Train Loss: 0.2817
Epoch 8 Step 51 Train Loss: 0.2838
Epoch 8 Step 101 Train Loss: 0.2714
Epoch 8 Step 151 Train Loss: 0.2513
Epoch 8 Step 201 Train Loss: 0.2486
Epoch 8 Step 251 Train Loss: 0.2167
Epoch 8 Step 301 Train Loss: 0.2159
Epoch 8 Step 351 Train Loss: 0.2382
Epoch 8 Step 401 Train Loss: 0.3271
Epoch 8 Step 451 Train Loss: 0.2608
Epoch 8 Step 501 Train Loss: 0.2680
Epoch 8 Step 551 Train Loss: 0.2631
Epoch 8 Step 601 Train Loss: 0.3979
Epoch 8 Step 651 Train Loss: 0.2876
Epoch 8 Step 701 Train Loss: 0.2712
Epoch 8 Step 751 Train Loss: 0.2115
Epoch 8 Step 801 Train Loss: 0.2028
Epoch 8 Step 851 Train Loss: 0.3031
Epoch 8 Step 901 Train Loss: 0.2653
Epoch 8 Step 951 Train Loss: 0.2093
Epoch 8 Step 1001 Train Loss: 0.2230
Epoch 8 Step 1051 Train Loss: 0.2323
Epoch 8 Step 1101 Train Loss: 0.2225
Epoch 8 Step 1151 Train Loss: 0.2682
Epoch 8 Step 1201 Train Loss: 0.3184
Epoch 8 Step 1251 Train Loss: 0.2732
Epoch 8 Step 1301 Train Loss: 0.2395
Epoch 8 Step 1351 Train Loss: 0.3584
Epoch 8 Step 1401 Train Loss: 0.3026
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0062. 
Epoch 9 Step 1 Train Loss: 0.2436
Epoch 9 Step 51 Train Loss: 0.2884
Epoch 9 Step 101 Train Loss: 0.2402
Epoch 9 Step 151 Train Loss: 0.2574
Epoch 9 Step 201 Train Loss: 0.1694
Epoch 9 Step 251 Train Loss: 0.2247
Epoch 9 Step 301 Train Loss: 0.2937
Epoch 9 Step 351 Train Loss: 0.2580
Epoch 9 Step 401 Train Loss: 0.2696
Epoch 9 Step 451 Train Loss: 0.3053
Epoch 9 Step 501 Train Loss: 0.3683
Epoch 9 Step 551 Train Loss: 0.2062
Epoch 9 Step 601 Train Loss: 0.3023
Epoch 9 Step 651 Train Loss: 0.1561
Epoch 9 Step 701 Train Loss: 0.2189
Epoch 9 Step 751 Train Loss: 0.2246
Epoch 9 Step 801 Train Loss: 0.2540
Epoch 9 Step 851 Train Loss: 0.2797
Epoch 9 Step 901 Train Loss: 0.3207
Epoch 9 Step 951 Train Loss: 0.2851
Epoch 9 Step 1001 Train Loss: 0.2496
Epoch 9 Step 1051 Train Loss: 0.2671
Epoch 9 Step 1101 Train Loss: 0.3338
Epoch 9 Step 1151 Train Loss: 0.2985
Epoch 9 Step 1201 Train Loss: 0.2684
Epoch 9 Step 1251 Train Loss: 0.2436
Epoch 9 Step 1301 Train Loss: 0.2542
Epoch 9 Step 1351 Train Loss: 0.2575
Epoch 9 Step 1401 Train Loss: 0.1721
Epoch 9: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0066. 
Epoch 10 Step 1 Train Loss: 0.3282
Epoch 10 Step 51 Train Loss: 0.3267
Epoch 10 Step 101 Train Loss: 0.2155
Epoch 10 Step 151 Train Loss: 0.2981
Epoch 10 Step 201 Train Loss: 0.3560
Epoch 10 Step 251 Train Loss: 0.2238
Epoch 10 Step 301 Train Loss: 0.2626
Epoch 10 Step 351 Train Loss: 0.2580
Epoch 10 Step 401 Train Loss: 0.2425
Epoch 10 Step 451 Train Loss: 0.2453
Epoch 10 Step 501 Train Loss: 0.2941
Epoch 10 Step 551 Train Loss: 0.2306
Epoch 10 Step 601 Train Loss: 0.2434
Epoch 10 Step 651 Train Loss: 0.2327
Epoch 10 Step 701 Train Loss: 0.2718
Epoch 10 Step 751 Train Loss: 0.2362
Epoch 10 Step 801 Train Loss: 0.2794
Epoch 10 Step 851 Train Loss: 0.2778
Epoch 10 Step 901 Train Loss: 0.2673
Epoch 10 Step 951 Train Loss: 0.3025
Epoch 10 Step 1001 Train Loss: 0.2606
Epoch 10 Step 1051 Train Loss: 0.2524
Epoch 10 Step 1101 Train Loss: 0.2175
Epoch 10 Step 1151 Train Loss: 0.2872
Epoch 10 Step 1201 Train Loss: 0.2330
Epoch 10 Step 1251 Train Loss: 0.2541
Epoch 10 Step 1301 Train Loss: 0.3231
Epoch 10 Step 1351 Train Loss: 0.2356
Epoch 10 Step 1401 Train Loss: 0.3023
Epoch 10: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0063. 
Epoch 11 Step 1 Train Loss: 0.2527
Epoch 11 Step 51 Train Loss: 0.2906
Epoch 11 Step 101 Train Loss: 0.2250
Epoch 11 Step 151 Train Loss: 0.2735
Epoch 11 Step 201 Train Loss: 0.2573
Epoch 11 Step 251 Train Loss: 0.2281
Epoch 11 Step 301 Train Loss: 0.3006
Epoch 11 Step 351 Train Loss: 0.2664
Epoch 11 Step 401 Train Loss: 0.2052
Epoch 11 Step 451 Train Loss: 0.2649
Epoch 11 Step 501 Train Loss: 0.2917
Epoch 11 Step 551 Train Loss: 0.3864
Epoch 11 Step 601 Train Loss: 0.2296
Epoch 11 Step 651 Train Loss: 0.2301
Epoch 11 Step 701 Train Loss: 0.2810
Epoch 11 Step 751 Train Loss: 0.2787
Epoch 11 Step 801 Train Loss: 0.2422
Epoch 11 Step 851 Train Loss: 0.2467
Epoch 11 Step 901 Train Loss: 0.2390
Epoch 11 Step 951 Train Loss: 0.1825
Epoch 11 Step 1001 Train Loss: 0.2423
Epoch 11 Step 1051 Train Loss: 0.3405
Epoch 11 Step 1101 Train Loss: 0.1912
Epoch 11 Step 1151 Train Loss: 0.2034
Epoch 11 Step 1201 Train Loss: 0.2775
Epoch 11 Step 1251 Train Loss: 0.2947
Epoch 11 Step 1301 Train Loss: 0.2493
Epoch 11 Step 1351 Train Loss: 0.2223
Epoch 11 Step 1401 Train Loss: 0.2584
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0063. 
Epoch 12 Step 1 Train Loss: 0.3057
Epoch 12 Step 51 Train Loss: 0.2851
Epoch 12 Step 101 Train Loss: 0.2621
Epoch 12 Step 151 Train Loss: 0.2032
Epoch 12 Step 201 Train Loss: 0.2696
Epoch 12 Step 251 Train Loss: 0.3379
Epoch 12 Step 301 Train Loss: 0.1918
Epoch 12 Step 351 Train Loss: 0.2643
Epoch 12 Step 401 Train Loss: 0.2861
Epoch 12 Step 451 Train Loss: 0.2757
Epoch 12 Step 501 Train Loss: 0.2317
Epoch 12 Step 551 Train Loss: 0.2854
Epoch 12 Step 601 Train Loss: 0.2225
Epoch 12 Step 651 Train Loss: 0.3105
Epoch 12 Step 701 Train Loss: 0.2697
Epoch 12 Step 751 Train Loss: 0.2727
Epoch 12 Step 801 Train Loss: 0.2303
Epoch 12 Step 851 Train Loss: 0.4812
Epoch 12 Step 901 Train Loss: 0.3146
Epoch 12 Step 951 Train Loss: 0.2132
Epoch 12 Step 1001 Train Loss: 0.2385
Epoch 12 Step 1051 Train Loss: 0.2178
Epoch 12 Step 1101 Train Loss: 0.2412
Epoch 12 Step 1151 Train Loss: 0.2503
Epoch 12 Step 1201 Train Loss: 0.2598
Epoch 12 Step 1251 Train Loss: 0.2320
Epoch 12 Step 1301 Train Loss: 0.2385
Epoch 12 Step 1351 Train Loss: 0.3954
Epoch 12 Step 1401 Train Loss: 0.3204
Epoch 12: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0064. 
Epoch 13 Step 1 Train Loss: 0.2740
Epoch 13 Step 51 Train Loss: 0.3167
Epoch 13 Step 101 Train Loss: 0.2169
Epoch 13 Step 151 Train Loss: 0.1897
Epoch 13 Step 201 Train Loss: 0.2474
Epoch 13 Step 251 Train Loss: 0.3014
Epoch 13 Step 301 Train Loss: 0.1410
Epoch 13 Step 351 Train Loss: 0.2466
Epoch 13 Step 401 Train Loss: 0.2704
Epoch 13 Step 451 Train Loss: 0.1893
Epoch 13 Step 501 Train Loss: 0.2746
Epoch 13 Step 551 Train Loss: 0.2472
Epoch 13 Step 601 Train Loss: 0.2989
Epoch 13 Step 651 Train Loss: 0.3068
Epoch 13 Step 701 Train Loss: 0.1686
Epoch 13 Step 751 Train Loss: 0.2369
Epoch 13 Step 801 Train Loss: 0.2288
Epoch 13 Step 851 Train Loss: 0.2675
Epoch 13 Step 901 Train Loss: 0.2309
Epoch 13 Step 951 Train Loss: 0.2847
Epoch 13 Step 1001 Train Loss: 0.2820
Epoch 13 Step 1051 Train Loss: 0.3352
Epoch 13 Step 1101 Train Loss: 0.2738
Epoch 13 Step 1151 Train Loss: 0.2582
Epoch 13 Step 1201 Train Loss: 0.2700
Epoch 13 Step 1251 Train Loss: 0.2988
Epoch 13 Step 1301 Train Loss: 0.3266
Epoch 13 Step 1351 Train Loss: 0.3394
Epoch 13 Step 1401 Train Loss: 0.2664
Epoch 13: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0091 Validation Top 20 DE MSE: 0.0067. 
Epoch 14 Step 1 Train Loss: 0.2915
Epoch 14 Step 51 Train Loss: 0.2392
Epoch 14 Step 101 Train Loss: 0.2075
Epoch 14 Step 151 Train Loss: 0.2408
Epoch 14 Step 201 Train Loss: 0.2334
Epoch 14 Step 251 Train Loss: 0.2276
Epoch 14 Step 301 Train Loss: 0.2105
Epoch 14 Step 351 Train Loss: 0.2752
Epoch 14 Step 401 Train Loss: 0.3532
Epoch 14 Step 451 Train Loss: 0.3507
Epoch 14 Step 501 Train Loss: 0.2322
Epoch 14 Step 551 Train Loss: 0.3096
Epoch 14 Step 601 Train Loss: 0.2194
Epoch 14 Step 651 Train Loss: 0.2551
Epoch 14 Step 701 Train Loss: 0.3087
Epoch 14 Step 751 Train Loss: 0.2480
Epoch 14 Step 801 Train Loss: 0.2336
Epoch 14 Step 851 Train Loss: 0.2423
Epoch 14 Step 901 Train Loss: 0.2236
Epoch 14 Step 951 Train Loss: 0.1808
Epoch 14 Step 1001 Train Loss: 0.2768
Epoch 14 Step 1051 Train Loss: 0.2584
Epoch 14 Step 1101 Train Loss: 0.2594
Epoch 14 Step 1151 Train Loss: 0.2769
Epoch 14 Step 1201 Train Loss: 0.3055
Epoch 14 Step 1251 Train Loss: 0.2143
Epoch 14 Step 1301 Train Loss: 0.2148
Epoch 14 Step 1351 Train Loss: 0.3078
Epoch 14 Step 1401 Train Loss: 0.2477
Epoch 14: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0064. 
Epoch 15 Step 1 Train Loss: 0.2170
Epoch 15 Step 51 Train Loss: 0.2508
Epoch 15 Step 101 Train Loss: 0.2468
Epoch 15 Step 151 Train Loss: 0.2224
Epoch 15 Step 201 Train Loss: 0.2008
Epoch 15 Step 251 Train Loss: 0.2370
Epoch 15 Step 301 Train Loss: 0.2460
Epoch 15 Step 351 Train Loss: 0.2247
Epoch 15 Step 401 Train Loss: 0.2892
Epoch 15 Step 451 Train Loss: 0.3314
Epoch 15 Step 501 Train Loss: 0.2643
Epoch 15 Step 551 Train Loss: 0.3617
Epoch 15 Step 601 Train Loss: 0.2102
Epoch 15 Step 651 Train Loss: 0.2654
Epoch 15 Step 701 Train Loss: 0.2951
Epoch 15 Step 751 Train Loss: 0.2455
Epoch 15 Step 801 Train Loss: 0.3030
Epoch 15 Step 851 Train Loss: 0.3762
Epoch 15 Step 901 Train Loss: 0.3082
Epoch 15 Step 951 Train Loss: 0.3160
Epoch 15 Step 1001 Train Loss: 0.2082
Epoch 15 Step 1051 Train Loss: 0.2565
Epoch 15 Step 1101 Train Loss: 0.3013
Epoch 15 Step 1151 Train Loss: 0.3158
Epoch 15 Step 1201 Train Loss: 0.2711
Epoch 15 Step 1251 Train Loss: 0.2285
Epoch 15 Step 1301 Train Loss: 0.1828
Epoch 15 Step 1351 Train Loss: 0.2640
Epoch 15 Step 1401 Train Loss: 0.2814
Epoch 15: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0092 Validation Top 20 DE MSE: 0.0063. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0079
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0011310823
test_unseen_single_pearson: 0.9947231778683664
test_unseen_single_mse_de: 0.007879181
test_unseen_single_pearson_de: 0.998302786180656
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.918089908043332
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.93
test_unseen_single_mse_top20_de_non_dropout: 0.009059766
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.034 MB uploadedwandb: | 0.009 MB of 0.034 MB uploadedwandb: / 0.009 MB of 0.034 MB uploadedwandb: - 0.034 MB of 0.034 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÖ‚ñá‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá
wandb:                                                    train_mse ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:                                                training_loss ‚ñá‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñà‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñá
wandb:                                                   val_de_mse ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÜ‚ñÅ‚ñà‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:                                                      val_mse ‚ñá‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:                                                  val_pearson ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00788
wandb:                                              test_de_pearson 0.9983
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.93
wandb:                                                     test_mse 0.00113
wandb:                                test_mse_top20_de_non_dropout 0.00906
wandb:                                                 test_pearson 0.99472
wandb:                                           test_pearson_delta 0.91809
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93
wandb:                                       test_unseen_single_mse 0.00113
wandb:                                    test_unseen_single_mse_de 0.00788
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00906
wandb:                                   test_unseen_single_pearson 0.99472
wandb:                                test_unseen_single_pearson_de 0.9983
wandb:                             test_unseen_single_pearson_delta 0.91809
wandb:                                                 train_de_mse 0.00922
wandb:                                             train_de_pearson 0.99793
wandb:                                                    train_mse 0.00118
wandb:                                                train_pearson 0.99469
wandb:                                                training_loss 0.21064
wandb:                                                   val_de_mse 0.00634
wandb:                                               val_de_pearson 0.82495
wandb:                                                      val_mse 0.0012
wandb:                                                  val_pearson 0.99452
wandb: 
wandb: üöÄ View run geneformer_ShifrutMarson2018_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/c54kyfia
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_072135-c54kyfia/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_074542-xru4v2gc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_ShifrutMarson2018_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/xru4v2gc
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3238
Epoch 1 Step 51 Train Loss: 0.3472
Epoch 1 Step 101 Train Loss: 0.4331
Epoch 1 Step 151 Train Loss: 0.3439
Epoch 1 Step 201 Train Loss: 0.2870
Epoch 1 Step 251 Train Loss: 0.2656
Epoch 1 Step 301 Train Loss: 0.3512
Epoch 1 Step 351 Train Loss: 0.3365
Epoch 1 Step 401 Train Loss: 0.3068
Epoch 1 Step 451 Train Loss: 0.2792
Epoch 1 Step 501 Train Loss: 0.3308
Epoch 1 Step 551 Train Loss: 0.2801
Epoch 1 Step 601 Train Loss: 0.2789
Epoch 1 Step 651 Train Loss: 0.3281
Epoch 1 Step 701 Train Loss: 0.2669
Epoch 1 Step 751 Train Loss: 0.3584
Epoch 1 Step 801 Train Loss: 0.3458
Epoch 1 Step 851 Train Loss: 0.2998
Epoch 1 Step 901 Train Loss: 0.2982
Epoch 1 Step 951 Train Loss: 0.2514
Epoch 1 Step 1001 Train Loss: 0.3019
Epoch 1 Step 1051 Train Loss: 0.2731
Epoch 1 Step 1101 Train Loss: 0.2611
Epoch 1 Step 1151 Train Loss: 0.2829
Epoch 1 Step 1201 Train Loss: 0.3189
Epoch 1 Step 1251 Train Loss: 0.3648
Epoch 1 Step 1301 Train Loss: 0.2790
Epoch 1 Step 1351 Train Loss: 0.3900
Epoch 1 Step 1401 Train Loss: 0.3401
Epoch 1: Train Overall MSE: 0.0047 Validation Overall MSE: 0.0049. 
Train Top 20 DE MSE: 0.0241 Validation Top 20 DE MSE: 0.0465. 
Epoch 2 Step 1 Train Loss: 0.2887
Epoch 2 Step 51 Train Loss: 0.2712
Epoch 2 Step 101 Train Loss: 0.2975
Epoch 2 Step 151 Train Loss: 0.2219
Epoch 2 Step 201 Train Loss: 0.2615
Epoch 2 Step 251 Train Loss: 0.3227
Epoch 2 Step 301 Train Loss: 0.3544
Epoch 2 Step 351 Train Loss: 0.2926
Epoch 2 Step 401 Train Loss: 0.2606
Epoch 2 Step 451 Train Loss: 0.3164
Epoch 2 Step 501 Train Loss: 0.2060
Epoch 2 Step 551 Train Loss: 0.2908
Epoch 2 Step 601 Train Loss: 0.2748
Epoch 2 Step 651 Train Loss: 0.3455
Epoch 2 Step 701 Train Loss: 0.2383
Epoch 2 Step 751 Train Loss: 0.3069
Epoch 2 Step 801 Train Loss: 0.3763
Epoch 2 Step 851 Train Loss: 0.2112
Epoch 2 Step 901 Train Loss: 0.2776
Epoch 2 Step 951 Train Loss: 0.2688
Epoch 2 Step 1001 Train Loss: 0.2560
Epoch 2 Step 1051 Train Loss: 0.2649
Epoch 2 Step 1101 Train Loss: 0.2340
Epoch 2 Step 1151 Train Loss: 0.2871
Epoch 2 Step 1201 Train Loss: 0.3069
Epoch 2 Step 1251 Train Loss: 0.2451
Epoch 2 Step 1301 Train Loss: 0.2045
Epoch 2 Step 1351 Train Loss: 0.2397
Epoch 2 Step 1401 Train Loss: 0.2228
Epoch 2: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0200 Validation Top 20 DE MSE: 0.0353. 
Epoch 3 Step 1 Train Loss: 0.2425
Epoch 3 Step 51 Train Loss: 0.2684
Epoch 3 Step 101 Train Loss: 0.2452
Epoch 3 Step 151 Train Loss: 0.2277
Epoch 3 Step 201 Train Loss: 0.2899
Epoch 3 Step 251 Train Loss: 0.1890
Epoch 3 Step 301 Train Loss: 0.3517
Epoch 3 Step 351 Train Loss: 0.4291
Epoch 3 Step 401 Train Loss: 0.2783
Epoch 3 Step 451 Train Loss: 0.2193
Epoch 3 Step 501 Train Loss: 0.2468
Epoch 3 Step 551 Train Loss: 0.2274
Epoch 3 Step 601 Train Loss: 0.2454
Epoch 3 Step 651 Train Loss: 0.2205
Epoch 3 Step 701 Train Loss: 0.1929
Epoch 3 Step 751 Train Loss: 0.2530
Epoch 3 Step 801 Train Loss: 0.2485
Epoch 3 Step 851 Train Loss: 0.2279
Epoch 3 Step 901 Train Loss: 0.2541
Epoch 3 Step 951 Train Loss: 0.2970
Epoch 3 Step 1001 Train Loss: 0.2166
Epoch 3 Step 1051 Train Loss: 0.2366
Epoch 3 Step 1101 Train Loss: 0.2590
Epoch 3 Step 1151 Train Loss: 0.2120
Epoch 3 Step 1201 Train Loss: 0.2796
Epoch 3 Step 1251 Train Loss: 0.2196
Epoch 3 Step 1301 Train Loss: 0.2161
Epoch 3 Step 1351 Train Loss: 0.2423
Epoch 3 Step 1401 Train Loss: 0.2597
Epoch 3: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.0256. 
Epoch 4 Step 1 Train Loss: 0.2885
Epoch 4 Step 51 Train Loss: 0.2109
Epoch 4 Step 101 Train Loss: 0.2930
Epoch 4 Step 151 Train Loss: 0.2411
Epoch 4 Step 201 Train Loss: 0.2817
Epoch 4 Step 251 Train Loss: 0.2393
Epoch 4 Step 301 Train Loss: 0.3064
Epoch 4 Step 351 Train Loss: 0.1427
Epoch 4 Step 401 Train Loss: 0.2255
Epoch 4 Step 451 Train Loss: 0.2434
Epoch 4 Step 501 Train Loss: 0.2597
Epoch 4 Step 551 Train Loss: 0.2724
Epoch 4 Step 601 Train Loss: 0.2501
Epoch 4 Step 651 Train Loss: 0.2126
Epoch 4 Step 701 Train Loss: 0.2428
Epoch 4 Step 751 Train Loss: 0.2816
Epoch 4 Step 801 Train Loss: 0.2345
Epoch 4 Step 851 Train Loss: 0.2836
Epoch 4 Step 901 Train Loss: 0.2974
Epoch 4 Step 951 Train Loss: 0.1933
Epoch 4 Step 1001 Train Loss: 0.2263
Epoch 4 Step 1051 Train Loss: 0.2420
Epoch 4 Step 1101 Train Loss: 0.2338
Epoch 4 Step 1151 Train Loss: 0.2125
Epoch 4 Step 1201 Train Loss: 0.2887
Epoch 4 Step 1251 Train Loss: 0.3316
Epoch 4 Step 1301 Train Loss: 0.3141
Epoch 4 Step 1351 Train Loss: 0.3026
Epoch 4 Step 1401 Train Loss: 0.2272
Epoch 4: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0228. 
Epoch 5 Step 1 Train Loss: 0.2847
Epoch 5 Step 51 Train Loss: 0.2903
Epoch 5 Step 101 Train Loss: 0.3025
Epoch 5 Step 151 Train Loss: 0.2489
Epoch 5 Step 201 Train Loss: 0.3253
Epoch 5 Step 251 Train Loss: 0.3050
Epoch 5 Step 301 Train Loss: 0.2766
Epoch 5 Step 351 Train Loss: 0.1760
Epoch 5 Step 401 Train Loss: 0.2398
Epoch 5 Step 451 Train Loss: 0.3097
Epoch 5 Step 501 Train Loss: 0.3206
Epoch 5 Step 551 Train Loss: 0.2187
Epoch 5 Step 601 Train Loss: 0.2408
Epoch 5 Step 651 Train Loss: 0.2936
Epoch 5 Step 701 Train Loss: 0.2743
Epoch 5 Step 751 Train Loss: 0.2558
Epoch 5 Step 801 Train Loss: 0.2828
Epoch 5 Step 851 Train Loss: 0.2686
Epoch 5 Step 901 Train Loss: 0.1419
Epoch 5 Step 951 Train Loss: 0.2378
Epoch 5 Step 1001 Train Loss: 0.2900
Epoch 5 Step 1051 Train Loss: 0.2152
Epoch 5 Step 1101 Train Loss: 0.2242
Epoch 5 Step 1151 Train Loss: 0.1610
Epoch 5 Step 1201 Train Loss: 0.1952
Epoch 5 Step 1251 Train Loss: 0.2528
Epoch 5 Step 1301 Train Loss: 0.2461
Epoch 5 Step 1351 Train Loss: 0.1816
Epoch 5 Step 1401 Train Loss: 0.2069
Epoch 5: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0117 Validation Top 20 DE MSE: 0.0258. 
Epoch 6 Step 1 Train Loss: 0.3544
Epoch 6 Step 51 Train Loss: 0.2409
Epoch 6 Step 101 Train Loss: 0.2801
Epoch 6 Step 151 Train Loss: 0.2135
Epoch 6 Step 201 Train Loss: 0.3045
Epoch 6 Step 251 Train Loss: 0.2116
Epoch 6 Step 301 Train Loss: 0.2970
Epoch 6 Step 351 Train Loss: 0.2379
Epoch 6 Step 401 Train Loss: 0.4130
Epoch 6 Step 451 Train Loss: 0.3092
Epoch 6 Step 501 Train Loss: 0.2633
Epoch 6 Step 551 Train Loss: 0.2421
Epoch 6 Step 601 Train Loss: 0.1789
Epoch 6 Step 651 Train Loss: 0.2571
Epoch 6 Step 701 Train Loss: 0.2116
Epoch 6 Step 751 Train Loss: 0.2574
Epoch 6 Step 801 Train Loss: 0.3224
Epoch 6 Step 851 Train Loss: 0.3552
Epoch 6 Step 901 Train Loss: 0.2978
Epoch 6 Step 951 Train Loss: 0.2464
Epoch 6 Step 1001 Train Loss: 0.2323
Epoch 6 Step 1051 Train Loss: 0.2222
Epoch 6 Step 1101 Train Loss: 0.2990
Epoch 6 Step 1151 Train Loss: 0.2422
Epoch 6 Step 1201 Train Loss: 0.2440
Epoch 6 Step 1251 Train Loss: 0.2105
Epoch 6 Step 1301 Train Loss: 0.3212
Epoch 6 Step 1351 Train Loss: 0.2416
Epoch 6 Step 1401 Train Loss: 0.2105
Epoch 6: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0218. 
Epoch 7 Step 1 Train Loss: 0.2286
Epoch 7 Step 51 Train Loss: 0.2875
Epoch 7 Step 101 Train Loss: 0.2335
Epoch 7 Step 151 Train Loss: 0.2769
Epoch 7 Step 201 Train Loss: 0.2087
Epoch 7 Step 251 Train Loss: 0.2385
Epoch 7 Step 301 Train Loss: 0.2529
Epoch 7 Step 351 Train Loss: 0.3016
Epoch 7 Step 401 Train Loss: 0.2885
Epoch 7 Step 451 Train Loss: 0.1880
Epoch 7 Step 501 Train Loss: 0.2521
Epoch 7 Step 551 Train Loss: 0.2296
Epoch 7 Step 601 Train Loss: 0.3075
Epoch 7 Step 651 Train Loss: 0.2469
Epoch 7 Step 701 Train Loss: 0.2381
Epoch 7 Step 751 Train Loss: 0.2532
Epoch 7 Step 801 Train Loss: 0.1642
Epoch 7 Step 851 Train Loss: 0.2726
Epoch 7 Step 901 Train Loss: 0.2807
Epoch 7 Step 951 Train Loss: 0.2560
Epoch 7 Step 1001 Train Loss: 0.2570
Epoch 7 Step 1051 Train Loss: 0.1965
Epoch 7 Step 1101 Train Loss: 0.2726
Epoch 7 Step 1151 Train Loss: 0.2676
Epoch 7 Step 1201 Train Loss: 0.2316
Epoch 7 Step 1251 Train Loss: 0.2452
Epoch 7 Step 1301 Train Loss: 0.2047
Epoch 7 Step 1351 Train Loss: 0.2927
Epoch 7 Step 1401 Train Loss: 0.3269
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0096 Validation Top 20 DE MSE: 0.0230. 
Epoch 8 Step 1 Train Loss: 0.2100
Epoch 8 Step 51 Train Loss: 0.2855
Epoch 8 Step 101 Train Loss: 0.2643
Epoch 8 Step 151 Train Loss: 0.3052
Epoch 8 Step 201 Train Loss: 0.2410
Epoch 8 Step 251 Train Loss: 0.2903
Epoch 8 Step 301 Train Loss: 0.2151
Epoch 8 Step 351 Train Loss: 0.2263
Epoch 8 Step 401 Train Loss: 0.2505
Epoch 8 Step 451 Train Loss: 0.3044
Epoch 8 Step 501 Train Loss: 0.2606
Epoch 8 Step 551 Train Loss: 0.2299
Epoch 8 Step 601 Train Loss: 0.2358
Epoch 8 Step 651 Train Loss: 0.2779
Epoch 8 Step 701 Train Loss: 0.2976
Epoch 8 Step 751 Train Loss: 0.3214
Epoch 8 Step 801 Train Loss: 0.2877
Epoch 8 Step 851 Train Loss: 0.2780
Epoch 8 Step 901 Train Loss: 0.2375
Epoch 8 Step 951 Train Loss: 0.3011
Epoch 8 Step 1001 Train Loss: 0.2201
Epoch 8 Step 1051 Train Loss: 0.2384
Epoch 8 Step 1101 Train Loss: 0.3027
Epoch 8 Step 1151 Train Loss: 0.2363
Epoch 8 Step 1201 Train Loss: 0.2234
Epoch 8 Step 1251 Train Loss: 0.2057
Epoch 8 Step 1301 Train Loss: 0.1452
Epoch 8 Step 1351 Train Loss: 0.2617
Epoch 8 Step 1401 Train Loss: 0.2145
Epoch 8: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0234. 
Epoch 9 Step 1 Train Loss: 0.2671
Epoch 9 Step 51 Train Loss: 0.2500
Epoch 9 Step 101 Train Loss: 0.3002
Epoch 9 Step 151 Train Loss: 0.2342
Epoch 9 Step 201 Train Loss: 0.3087
Epoch 9 Step 251 Train Loss: 0.2859
Epoch 9 Step 301 Train Loss: 0.2781
Epoch 9 Step 351 Train Loss: 0.2934
Epoch 9 Step 401 Train Loss: 0.2481
Epoch 9 Step 451 Train Loss: 0.2400
Epoch 9 Step 501 Train Loss: 0.2908
Epoch 9 Step 551 Train Loss: 0.2562
Epoch 9 Step 601 Train Loss: 0.3164
Epoch 9 Step 651 Train Loss: 0.3200
Epoch 9 Step 701 Train Loss: 0.2378
Epoch 9 Step 751 Train Loss: 0.3007
Epoch 9 Step 801 Train Loss: 0.3774
Epoch 9 Step 851 Train Loss: 0.2150
Epoch 9 Step 901 Train Loss: 0.2744
Epoch 9 Step 951 Train Loss: 0.3475
Epoch 9 Step 1001 Train Loss: 0.2407
Epoch 9 Step 1051 Train Loss: 0.2565
Epoch 9 Step 1101 Train Loss: 0.2179
Epoch 9 Step 1151 Train Loss: 0.1958
Epoch 9 Step 1201 Train Loss: 0.2389
Epoch 9 Step 1251 Train Loss: 0.2217
Epoch 9 Step 1301 Train Loss: 0.2572
Epoch 9 Step 1351 Train Loss: 0.3193
Epoch 9 Step 1401 Train Loss: 0.2563
Epoch 9: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0241. 
Epoch 10 Step 1 Train Loss: 0.2539
Epoch 10 Step 51 Train Loss: 0.3076
Epoch 10 Step 101 Train Loss: 0.2739
Epoch 10 Step 151 Train Loss: 0.2610
Epoch 10 Step 201 Train Loss: 0.2092
Epoch 10 Step 251 Train Loss: 0.2064
Epoch 10 Step 301 Train Loss: 0.2835
Epoch 10 Step 351 Train Loss: 0.2670
Epoch 10 Step 401 Train Loss: 0.2533
Epoch 10 Step 451 Train Loss: 0.2556
Epoch 10 Step 501 Train Loss: 0.2685
Epoch 10 Step 551 Train Loss: 0.2318
Epoch 10 Step 601 Train Loss: 0.2865
Epoch 10 Step 651 Train Loss: 0.3218
Epoch 10 Step 701 Train Loss: 0.2525
Epoch 10 Step 751 Train Loss: 0.1928
Epoch 10 Step 801 Train Loss: 0.2081
Epoch 10 Step 851 Train Loss: 0.2349
Epoch 10 Step 901 Train Loss: 0.2952
Epoch 10 Step 951 Train Loss: 0.3249
Epoch 10 Step 1001 Train Loss: 0.2437
Epoch 10 Step 1051 Train Loss: 0.2429
Epoch 10 Step 1101 Train Loss: 0.2428
Epoch 10 Step 1151 Train Loss: 0.2686
Epoch 10 Step 1201 Train Loss: 0.2984
Epoch 10 Step 1251 Train Loss: 0.1932
Epoch 10 Step 1301 Train Loss: 0.2583
Epoch 10 Step 1351 Train Loss: 0.3255
Epoch 10 Step 1401 Train Loss: 0.2468
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0084 Validation Top 20 DE MSE: 0.0239. 
Epoch 11 Step 1 Train Loss: 0.3343
Epoch 11 Step 51 Train Loss: 0.0471
Epoch 11 Step 101 Train Loss: 0.3200
Epoch 11 Step 151 Train Loss: 0.2198
Epoch 11 Step 201 Train Loss: 0.2759
Epoch 11 Step 251 Train Loss: 0.2498
Epoch 11 Step 301 Train Loss: 0.2422
Epoch 11 Step 351 Train Loss: 0.2434
Epoch 11 Step 401 Train Loss: 0.2964
Epoch 11 Step 451 Train Loss: 0.2309
Epoch 11 Step 501 Train Loss: 0.2798
Epoch 11 Step 551 Train Loss: 0.3220
Epoch 11 Step 601 Train Loss: 0.3128
Epoch 11 Step 651 Train Loss: 0.1920
Epoch 11 Step 701 Train Loss: 0.2536
Epoch 11 Step 751 Train Loss: 0.2253
Epoch 11 Step 801 Train Loss: 0.2812
Epoch 11 Step 851 Train Loss: 0.2729
Epoch 11 Step 901 Train Loss: 0.1878
Epoch 11 Step 951 Train Loss: 0.2568
Epoch 11 Step 1001 Train Loss: 0.3847
Epoch 11 Step 1051 Train Loss: 0.3003
Epoch 11 Step 1101 Train Loss: 0.2652
Epoch 11 Step 1151 Train Loss: 0.2818
Epoch 11 Step 1201 Train Loss: 0.2125
Epoch 11 Step 1251 Train Loss: 0.2270
Epoch 11 Step 1301 Train Loss: 0.1725
Epoch 11 Step 1351 Train Loss: 0.2220
Epoch 11 Step 1401 Train Loss: 0.2151
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0249. 
Epoch 12 Step 1 Train Loss: 0.2571
Epoch 12 Step 51 Train Loss: 0.1864
Epoch 12 Step 101 Train Loss: 0.2153
Epoch 12 Step 151 Train Loss: 0.2244
Epoch 12 Step 201 Train Loss: 0.2625
Epoch 12 Step 251 Train Loss: 0.2486
Epoch 12 Step 301 Train Loss: 0.2769
Epoch 12 Step 351 Train Loss: 0.2512
Epoch 12 Step 401 Train Loss: 0.1920
Epoch 12 Step 451 Train Loss: 0.2150
Epoch 12 Step 501 Train Loss: 0.2394
Epoch 12 Step 551 Train Loss: 0.3079
Epoch 12 Step 601 Train Loss: 0.2450
Epoch 12 Step 651 Train Loss: 0.2090
Epoch 12 Step 701 Train Loss: 0.2719
Epoch 12 Step 751 Train Loss: 0.2087
Epoch 12 Step 801 Train Loss: 0.2653
Epoch 12 Step 851 Train Loss: 0.2512
Epoch 12 Step 901 Train Loss: 0.1532
Epoch 12 Step 951 Train Loss: 0.3142
Epoch 12 Step 1001 Train Loss: 0.2024
Epoch 12 Step 1051 Train Loss: 0.1860
Epoch 12 Step 1101 Train Loss: 0.2122
Epoch 12 Step 1151 Train Loss: 0.2641
Epoch 12 Step 1201 Train Loss: 0.2353
Epoch 12 Step 1251 Train Loss: 0.2569
Epoch 12 Step 1301 Train Loss: 0.2676
Epoch 12 Step 1351 Train Loss: 0.2350
Epoch 12 Step 1401 Train Loss: 0.2364
Epoch 12: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0253. 
Epoch 13 Step 1 Train Loss: 0.2503
Epoch 13 Step 51 Train Loss: 0.2806
Epoch 13 Step 101 Train Loss: 0.2557
Epoch 13 Step 151 Train Loss: 0.2119
Epoch 13 Step 201 Train Loss: 0.2105
Epoch 13 Step 251 Train Loss: 0.1849
Epoch 13 Step 301 Train Loss: 0.2236
Epoch 13 Step 351 Train Loss: 0.2672
Epoch 13 Step 401 Train Loss: 0.3139
Epoch 13 Step 451 Train Loss: 0.2208
Epoch 13 Step 501 Train Loss: 0.3238
Epoch 13 Step 551 Train Loss: 0.2242
Epoch 13 Step 601 Train Loss: 0.2420
Epoch 13 Step 651 Train Loss: 0.3337
Epoch 13 Step 701 Train Loss: 0.2709
Epoch 13 Step 751 Train Loss: 0.2394
Epoch 13 Step 801 Train Loss: 0.2193
Epoch 13 Step 851 Train Loss: 0.2860
Epoch 13 Step 901 Train Loss: 0.2856
Epoch 13 Step 951 Train Loss: 0.3167
Epoch 13 Step 1001 Train Loss: 0.2727
Epoch 13 Step 1051 Train Loss: 0.2354
Epoch 13 Step 1101 Train Loss: 0.2126
Epoch 13 Step 1151 Train Loss: 0.3320
Epoch 13 Step 1201 Train Loss: 0.3163
Epoch 13 Step 1251 Train Loss: 0.2609
Epoch 13 Step 1301 Train Loss: 0.3090
Epoch 13 Step 1351 Train Loss: 0.2378
Epoch 13 Step 1401 Train Loss: 0.3030
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0087 Validation Top 20 DE MSE: 0.0241. 
Epoch 14 Step 1 Train Loss: 0.2043
Epoch 14 Step 51 Train Loss: 0.2517
Epoch 14 Step 101 Train Loss: 0.2680
Epoch 14 Step 151 Train Loss: 0.2335
Epoch 14 Step 201 Train Loss: 0.1925
Epoch 14 Step 251 Train Loss: 0.2089
Epoch 14 Step 301 Train Loss: 0.2349
Epoch 14 Step 351 Train Loss: 0.2745
Epoch 14 Step 401 Train Loss: 0.2698
Epoch 14 Step 451 Train Loss: 0.2827
Epoch 14 Step 501 Train Loss: 0.2109
Epoch 14 Step 551 Train Loss: 0.2151
Epoch 14 Step 601 Train Loss: 0.1769
Epoch 14 Step 651 Train Loss: 0.2137
Epoch 14 Step 701 Train Loss: 0.3307
Epoch 14 Step 751 Train Loss: 0.2550
Epoch 14 Step 801 Train Loss: 0.2656
Epoch 14 Step 851 Train Loss: 0.2287
Epoch 14 Step 901 Train Loss: 0.2593
Epoch 14 Step 951 Train Loss: 0.2654
Epoch 14 Step 1001 Train Loss: 0.2411
Epoch 14 Step 1051 Train Loss: 0.3361
Epoch 14 Step 1101 Train Loss: 0.2533
Epoch 14 Step 1151 Train Loss: 0.1487
Epoch 14 Step 1201 Train Loss: 0.2504
Epoch 14 Step 1251 Train Loss: 0.2544
Epoch 14 Step 1301 Train Loss: 0.3061
Epoch 14 Step 1351 Train Loss: 0.2953
Epoch 14 Step 1401 Train Loss: 0.2174
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.0244. 
Epoch 15 Step 1 Train Loss: 0.2451
Epoch 15 Step 51 Train Loss: 0.2209
Epoch 15 Step 101 Train Loss: 0.2866
Epoch 15 Step 151 Train Loss: 0.1971
Epoch 15 Step 201 Train Loss: 0.2773
Epoch 15 Step 251 Train Loss: 0.2962
Epoch 15 Step 301 Train Loss: 0.2003
Epoch 15 Step 351 Train Loss: 0.3113
Epoch 15 Step 401 Train Loss: 0.2057
Epoch 15 Step 451 Train Loss: 0.2227
Epoch 15 Step 501 Train Loss: 0.3184
Epoch 15 Step 551 Train Loss: 0.1966
Epoch 15 Step 601 Train Loss: 0.2310
Epoch 15 Step 651 Train Loss: 0.2246
Epoch 15 Step 701 Train Loss: 0.2299
Epoch 15 Step 751 Train Loss: 0.1967
Epoch 15 Step 801 Train Loss: 0.2400
Epoch 15 Step 851 Train Loss: 0.2791
Epoch 15 Step 901 Train Loss: 0.2398
Epoch 15 Step 951 Train Loss: 0.2795
Epoch 15 Step 1001 Train Loss: 0.2910
Epoch 15 Step 1051 Train Loss: 0.3039
Epoch 15 Step 1101 Train Loss: 0.2191
Epoch 15 Step 1151 Train Loss: 0.2837
Epoch 15 Step 1201 Train Loss: 0.1519
Epoch 15 Step 1251 Train Loss: 0.2530
Epoch 15 Step 1301 Train Loss: 0.2743
Epoch 15 Step 1351 Train Loss: 0.3087
Epoch 15 Step 1401 Train Loss: 0.3134
Epoch 15: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.0247. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0119
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0011944857
test_unseen_single_pearson: 0.9945080640772683
test_unseen_single_mse_de: 0.011889674
test_unseen_single_pearson_de: 0.9968837764744496
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8949389115258347
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.97
test_unseen_single_mse_top20_de_non_dropout: 0.017029885
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.001 MB of 0.033 MB uploadedwandb: / 0.028 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb: | 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÑ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01189
wandb:                                              test_de_pearson 0.99688
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.97
wandb:                                                     test_mse 0.00119
wandb:                                test_mse_top20_de_non_dropout 0.01703
wandb:                                                 test_pearson 0.99451
wandb:                                           test_pearson_delta 0.89494
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.97
wandb:                                       test_unseen_single_mse 0.00119
wandb:                                    test_unseen_single_mse_de 0.01189
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01703
wandb:                                   test_unseen_single_pearson 0.99451
wandb:                                test_unseen_single_pearson_de 0.99688
wandb:                             test_unseen_single_pearson_delta 0.89494
wandb:                                                 train_de_mse 0.00879
wandb:                                             train_de_pearson 0.9798
wandb:                                                    train_mse 0.00101
wandb:                                                train_pearson 0.99538
wandb:                                                training_loss 0.23452
wandb:                                                   val_de_mse 0.02469
wandb:                                               val_de_pearson 0.99511
wandb:                                                      val_mse 0.00103
wandb:                                                  val_pearson 0.99525
wandb: 
wandb: üöÄ View run geneformer_ShifrutMarson2018_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/xru4v2gc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_074542-xru4v2gc/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_080830-9xgoce4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_ShifrutMarson2018_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/9xgoce4z
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4084
Epoch 1 Step 51 Train Loss: 0.3392
Epoch 1 Step 101 Train Loss: 0.2619
Epoch 1 Step 151 Train Loss: 0.2884
Epoch 1 Step 201 Train Loss: 0.2875
Epoch 1 Step 251 Train Loss: 0.3374
Epoch 1 Step 301 Train Loss: 0.2745
Epoch 1 Step 351 Train Loss: 0.2759
Epoch 1 Step 401 Train Loss: 0.2745
Epoch 1 Step 451 Train Loss: 0.2647
Epoch 1 Step 501 Train Loss: 0.3210
Epoch 1 Step 551 Train Loss: 0.3732
Epoch 1 Step 601 Train Loss: 0.2228
Epoch 1 Step 651 Train Loss: 0.2395
Epoch 1 Step 701 Train Loss: 0.3847
Epoch 1 Step 751 Train Loss: 0.2958
Epoch 1 Step 801 Train Loss: 0.2789
Epoch 1 Step 851 Train Loss: 0.2638
Epoch 1 Step 901 Train Loss: 0.2831
Epoch 1 Step 951 Train Loss: 0.3137
Epoch 1 Step 1001 Train Loss: 0.2882
Epoch 1 Step 1051 Train Loss: 0.2872
Epoch 1 Step 1101 Train Loss: 0.3115
Epoch 1 Step 1151 Train Loss: 0.3149
Epoch 1 Step 1201 Train Loss: 0.3140
Epoch 1 Step 1251 Train Loss: 0.3024
Epoch 1 Step 1301 Train Loss: 0.2848
Epoch 1 Step 1351 Train Loss: 0.3125
Epoch 1: Train Overall MSE: 0.0035 Validation Overall MSE: 0.0045. 
Train Top 20 DE MSE: 0.0193 Validation Top 20 DE MSE: 0.0151. 
Epoch 2 Step 1 Train Loss: 0.2891
Epoch 2 Step 51 Train Loss: 0.2730
Epoch 2 Step 101 Train Loss: 0.2143
Epoch 2 Step 151 Train Loss: 0.2814
Epoch 2 Step 201 Train Loss: 0.3100
Epoch 2 Step 251 Train Loss: 0.2718
Epoch 2 Step 301 Train Loss: 0.2862
Epoch 2 Step 351 Train Loss: 0.2519
Epoch 2 Step 401 Train Loss: 0.3047
Epoch 2 Step 451 Train Loss: 0.2418
Epoch 2 Step 501 Train Loss: 0.2686
Epoch 2 Step 551 Train Loss: 0.2857
Epoch 2 Step 601 Train Loss: 0.1286
Epoch 2 Step 651 Train Loss: 0.2459
Epoch 2 Step 701 Train Loss: 0.3827
Epoch 2 Step 751 Train Loss: 0.3413
Epoch 2 Step 801 Train Loss: 0.2343
Epoch 2 Step 851 Train Loss: 0.2884
Epoch 2 Step 901 Train Loss: 0.3059
Epoch 2 Step 951 Train Loss: 0.2646
Epoch 2 Step 1001 Train Loss: 0.3009
Epoch 2 Step 1051 Train Loss: 0.2327
Epoch 2 Step 1101 Train Loss: 0.2978
Epoch 2 Step 1151 Train Loss: 0.2908
Epoch 2 Step 1201 Train Loss: 0.1781
Epoch 2 Step 1251 Train Loss: 0.2761
Epoch 2 Step 1301 Train Loss: 0.3115
Epoch 2 Step 1351 Train Loss: 0.3062
Epoch 2: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.0206. 
Epoch 3 Step 1 Train Loss: 0.3216
Epoch 3 Step 51 Train Loss: 0.2801
Epoch 3 Step 101 Train Loss: 0.2841
Epoch 3 Step 151 Train Loss: 0.2729
Epoch 3 Step 201 Train Loss: 0.2878
Epoch 3 Step 251 Train Loss: 0.2548
Epoch 3 Step 301 Train Loss: 0.2941
Epoch 3 Step 351 Train Loss: 0.2106
Epoch 3 Step 401 Train Loss: 0.3314
Epoch 3 Step 451 Train Loss: 0.4158
Epoch 3 Step 501 Train Loss: 0.2358
Epoch 3 Step 551 Train Loss: 0.2841
Epoch 3 Step 601 Train Loss: 0.2765
Epoch 3 Step 651 Train Loss: 0.2302
Epoch 3 Step 701 Train Loss: 0.2148
Epoch 3 Step 751 Train Loss: 0.2804
Epoch 3 Step 801 Train Loss: 0.2621
Epoch 3 Step 851 Train Loss: 0.2168
Epoch 3 Step 901 Train Loss: 0.2213
Epoch 3 Step 951 Train Loss: 0.3155
Epoch 3 Step 1001 Train Loss: 0.2997
Epoch 3 Step 1051 Train Loss: 0.2644
Epoch 3 Step 1101 Train Loss: 0.2460
Epoch 3 Step 1151 Train Loss: 0.2669
Epoch 3 Step 1201 Train Loss: 0.2491
Epoch 3 Step 1251 Train Loss: 0.2566
Epoch 3 Step 1301 Train Loss: 0.2709
Epoch 3 Step 1351 Train Loss: 0.2480
Epoch 3: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.0132. 
Epoch 4 Step 1 Train Loss: 0.3258
Epoch 4 Step 51 Train Loss: 0.2896
Epoch 4 Step 101 Train Loss: 0.2740
Epoch 4 Step 151 Train Loss: 0.2305
Epoch 4 Step 201 Train Loss: 0.2478
Epoch 4 Step 251 Train Loss: 0.2500
Epoch 4 Step 301 Train Loss: 0.2545
Epoch 4 Step 351 Train Loss: 0.2446
Epoch 4 Step 401 Train Loss: 0.2293
Epoch 4 Step 451 Train Loss: 0.3324
Epoch 4 Step 501 Train Loss: 0.2567
Epoch 4 Step 551 Train Loss: 0.2833
Epoch 4 Step 601 Train Loss: 0.2843
Epoch 4 Step 651 Train Loss: 0.2302
Epoch 4 Step 701 Train Loss: 0.2932
Epoch 4 Step 751 Train Loss: 0.2472
Epoch 4 Step 801 Train Loss: 0.2945
Epoch 4 Step 851 Train Loss: 0.2868
Epoch 4 Step 901 Train Loss: 0.2718
Epoch 4 Step 951 Train Loss: 0.2332
Epoch 4 Step 1001 Train Loss: 0.3045
Epoch 4 Step 1051 Train Loss: 0.2762
Epoch 4 Step 1101 Train Loss: 0.2869
Epoch 4 Step 1151 Train Loss: 0.2419
Epoch 4 Step 1201 Train Loss: 0.2786
Epoch 4 Step 1251 Train Loss: 0.1870
Epoch 4 Step 1301 Train Loss: 0.2284
Epoch 4 Step 1351 Train Loss: 0.2537
Epoch 4: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0148 Validation Top 20 DE MSE: 0.0145. 
Epoch 5 Step 1 Train Loss: 0.2718
Epoch 5 Step 51 Train Loss: 0.3927
Epoch 5 Step 101 Train Loss: 0.2814
Epoch 5 Step 151 Train Loss: 0.2523
Epoch 5 Step 201 Train Loss: 0.3354
Epoch 5 Step 251 Train Loss: 0.3700
Epoch 5 Step 301 Train Loss: 0.3001
Epoch 5 Step 351 Train Loss: 0.2746
Epoch 5 Step 401 Train Loss: 0.2398
Epoch 5 Step 451 Train Loss: 0.2345
Epoch 5 Step 501 Train Loss: 0.2034
Epoch 5 Step 551 Train Loss: 0.2519
Epoch 5 Step 601 Train Loss: 0.2495
Epoch 5 Step 651 Train Loss: 0.2406
Epoch 5 Step 701 Train Loss: 0.2219
Epoch 5 Step 751 Train Loss: 0.2786
Epoch 5 Step 801 Train Loss: 0.2801
Epoch 5 Step 851 Train Loss: 0.3134
Epoch 5 Step 901 Train Loss: 0.2423
Epoch 5 Step 951 Train Loss: 0.2271
Epoch 5 Step 1001 Train Loss: 0.2999
Epoch 5 Step 1051 Train Loss: 0.2642
Epoch 5 Step 1101 Train Loss: 0.2990
Epoch 5 Step 1151 Train Loss: 0.2800
Epoch 5 Step 1201 Train Loss: 0.2503
Epoch 5 Step 1251 Train Loss: 0.2787
Epoch 5 Step 1301 Train Loss: 0.2656
Epoch 5 Step 1351 Train Loss: 0.2226
Epoch 5: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0148. 
Epoch 6 Step 1 Train Loss: 0.2480
Epoch 6 Step 51 Train Loss: 0.2582
Epoch 6 Step 101 Train Loss: 0.2396
Epoch 6 Step 151 Train Loss: 0.3010
Epoch 6 Step 201 Train Loss: 0.2201
Epoch 6 Step 251 Train Loss: 0.1979
Epoch 6 Step 301 Train Loss: 0.3180
Epoch 6 Step 351 Train Loss: 0.2389
Epoch 6 Step 401 Train Loss: 0.2377
Epoch 6 Step 451 Train Loss: 0.2672
Epoch 6 Step 501 Train Loss: 0.2875
Epoch 6 Step 551 Train Loss: 0.2330
Epoch 6 Step 601 Train Loss: 0.2032
Epoch 6 Step 651 Train Loss: 0.3235
Epoch 6 Step 701 Train Loss: 0.2925
Epoch 6 Step 751 Train Loss: 0.2474
Epoch 6 Step 801 Train Loss: 0.2456
Epoch 6 Step 851 Train Loss: 0.2545
Epoch 6 Step 901 Train Loss: 0.4162
Epoch 6 Step 951 Train Loss: 0.2707
Epoch 6 Step 1001 Train Loss: 0.3131
Epoch 6 Step 1051 Train Loss: 0.3059
Epoch 6 Step 1101 Train Loss: 0.3441
Epoch 6 Step 1151 Train Loss: 0.1964
Epoch 6 Step 1201 Train Loss: 0.2153
Epoch 6 Step 1251 Train Loss: 0.2154
Epoch 6 Step 1301 Train Loss: 0.2699
Epoch 6 Step 1351 Train Loss: 0.2314
Epoch 6: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0121 Validation Top 20 DE MSE: 0.0142. 
Epoch 7 Step 1 Train Loss: 0.2352
Epoch 7 Step 51 Train Loss: 0.2806
Epoch 7 Step 101 Train Loss: 0.2009
Epoch 7 Step 151 Train Loss: 0.2140
Epoch 7 Step 201 Train Loss: 0.3030
Epoch 7 Step 251 Train Loss: 0.2644
Epoch 7 Step 301 Train Loss: 0.2098
Epoch 7 Step 351 Train Loss: 0.2601
Epoch 7 Step 401 Train Loss: 0.3060
Epoch 7 Step 451 Train Loss: 0.2408
Epoch 7 Step 501 Train Loss: 0.2816
Epoch 7 Step 551 Train Loss: 0.2803
Epoch 7 Step 601 Train Loss: 0.2429
Epoch 7 Step 651 Train Loss: 0.3514
Epoch 7 Step 701 Train Loss: 0.2466
Epoch 7 Step 751 Train Loss: 0.2966
Epoch 7 Step 801 Train Loss: 0.3020
Epoch 7 Step 851 Train Loss: 0.2409
Epoch 7 Step 901 Train Loss: 0.3007
Epoch 7 Step 951 Train Loss: 0.2342
Epoch 7 Step 1001 Train Loss: 0.2462
Epoch 7 Step 1051 Train Loss: 0.2446
Epoch 7 Step 1101 Train Loss: 0.3095
Epoch 7 Step 1151 Train Loss: 0.2623
Epoch 7 Step 1201 Train Loss: 0.2061
Epoch 7 Step 1251 Train Loss: 0.2034
Epoch 7 Step 1301 Train Loss: 0.2127
Epoch 7 Step 1351 Train Loss: 0.2269
Epoch 7: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0121. 
Epoch 8 Step 1 Train Loss: 0.2328
Epoch 8 Step 51 Train Loss: 0.2937
Epoch 8 Step 101 Train Loss: 0.1687
Epoch 8 Step 151 Train Loss: 0.2185
Epoch 8 Step 201 Train Loss: 0.2070
Epoch 8 Step 251 Train Loss: 0.2514
Epoch 8 Step 301 Train Loss: 0.2219
Epoch 8 Step 351 Train Loss: 0.2140
Epoch 8 Step 401 Train Loss: 0.2380
Epoch 8 Step 451 Train Loss: 0.2798
Epoch 8 Step 501 Train Loss: 0.2085
Epoch 8 Step 551 Train Loss: 0.2800
Epoch 8 Step 601 Train Loss: 0.2482
Epoch 8 Step 651 Train Loss: 0.2826
Epoch 8 Step 701 Train Loss: 0.2124
Epoch 8 Step 751 Train Loss: 0.1961
Epoch 8 Step 801 Train Loss: 0.2779
Epoch 8 Step 851 Train Loss: 0.1389
Epoch 8 Step 901 Train Loss: 0.1969
Epoch 8 Step 951 Train Loss: 0.2951
Epoch 8 Step 1001 Train Loss: 0.2393
Epoch 8 Step 1051 Train Loss: 0.2951
Epoch 8 Step 1101 Train Loss: 0.3275
Epoch 8 Step 1151 Train Loss: 0.2151
Epoch 8 Step 1201 Train Loss: 0.2805
Epoch 8 Step 1251 Train Loss: 0.2187
Epoch 8 Step 1301 Train Loss: 0.2265
Epoch 8 Step 1351 Train Loss: 0.2969
Epoch 8: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0106. 
Epoch 9 Step 1 Train Loss: 0.2890
Epoch 9 Step 51 Train Loss: 0.2144
Epoch 9 Step 101 Train Loss: 0.2157
Epoch 9 Step 151 Train Loss: 0.2510
Epoch 9 Step 201 Train Loss: 0.2009
Epoch 9 Step 251 Train Loss: 0.2062
Epoch 9 Step 301 Train Loss: 0.1976
Epoch 9 Step 351 Train Loss: 0.1968
Epoch 9 Step 401 Train Loss: 0.3180
Epoch 9 Step 451 Train Loss: 0.2696
Epoch 9 Step 501 Train Loss: 0.2670
Epoch 9 Step 551 Train Loss: 0.2908
Epoch 9 Step 601 Train Loss: 0.2858
Epoch 9 Step 651 Train Loss: 0.2617
Epoch 9 Step 701 Train Loss: 0.2644
Epoch 9 Step 751 Train Loss: 0.2553
Epoch 9 Step 801 Train Loss: 0.2775
Epoch 9 Step 851 Train Loss: 0.2035
Epoch 9 Step 901 Train Loss: 0.2500
Epoch 9 Step 951 Train Loss: 0.2440
Epoch 9 Step 1001 Train Loss: 0.3042
Epoch 9 Step 1051 Train Loss: 0.2634
Epoch 9 Step 1101 Train Loss: 0.2736
Epoch 9 Step 1151 Train Loss: 0.2800
Epoch 9 Step 1201 Train Loss: 0.2510
Epoch 9 Step 1251 Train Loss: 0.2008
Epoch 9 Step 1301 Train Loss: 0.2388
Epoch 9 Step 1351 Train Loss: 0.2419
Epoch 9: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0111. 
Epoch 10 Step 1 Train Loss: 0.3231
Epoch 10 Step 51 Train Loss: 0.2531
Epoch 10 Step 101 Train Loss: 0.2699
Epoch 10 Step 151 Train Loss: 0.2320
Epoch 10 Step 201 Train Loss: 0.2354
Epoch 10 Step 251 Train Loss: 0.2478
Epoch 10 Step 301 Train Loss: 0.2808
Epoch 10 Step 351 Train Loss: 0.2176
Epoch 10 Step 401 Train Loss: 0.2210
Epoch 10 Step 451 Train Loss: 0.2164
Epoch 10 Step 501 Train Loss: 0.2027
Epoch 10 Step 551 Train Loss: 0.2186
Epoch 10 Step 601 Train Loss: 0.2676
Epoch 10 Step 651 Train Loss: 0.2781
Epoch 10 Step 701 Train Loss: 0.2677
Epoch 10 Step 751 Train Loss: 0.2610
Epoch 10 Step 801 Train Loss: 0.2874
Epoch 10 Step 851 Train Loss: 0.2863
Epoch 10 Step 901 Train Loss: 0.2464
Epoch 10 Step 951 Train Loss: 0.2110
Epoch 10 Step 1001 Train Loss: 0.2997
Epoch 10 Step 1051 Train Loss: 0.2696
Epoch 10 Step 1101 Train Loss: 0.2418
Epoch 10 Step 1151 Train Loss: 0.2237
Epoch 10 Step 1201 Train Loss: 0.3133
Epoch 10 Step 1251 Train Loss: 0.3172
Epoch 10 Step 1301 Train Loss: 0.2371
Epoch 10 Step 1351 Train Loss: 0.3608
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.0121. 
Epoch 11 Step 1 Train Loss: 0.1646
Epoch 11 Step 51 Train Loss: 0.2524
Epoch 11 Step 101 Train Loss: 0.2573
Epoch 11 Step 151 Train Loss: 0.2662
Epoch 11 Step 201 Train Loss: 0.2645
Epoch 11 Step 251 Train Loss: 0.2738
Epoch 11 Step 301 Train Loss: 0.3292
Epoch 11 Step 351 Train Loss: 0.2452
Epoch 11 Step 401 Train Loss: 0.2421
Epoch 11 Step 451 Train Loss: 0.2173
Epoch 11 Step 501 Train Loss: 0.2727
Epoch 11 Step 551 Train Loss: 0.2750
Epoch 11 Step 601 Train Loss: 0.2756
Epoch 11 Step 651 Train Loss: 0.3212
Epoch 11 Step 701 Train Loss: 0.2934
Epoch 11 Step 751 Train Loss: 0.3628
Epoch 11 Step 801 Train Loss: 0.2556
Epoch 11 Step 851 Train Loss: 0.2641
Epoch 11 Step 901 Train Loss: 0.2539
Epoch 11 Step 951 Train Loss: 0.2804
Epoch 11 Step 1001 Train Loss: 0.2476
Epoch 11 Step 1051 Train Loss: 0.3307
Epoch 11 Step 1101 Train Loss: 0.2626
Epoch 11 Step 1151 Train Loss: 0.2521
Epoch 11 Step 1201 Train Loss: 0.1829
Epoch 11 Step 1251 Train Loss: 0.3239
Epoch 11 Step 1301 Train Loss: 0.1849
Epoch 11 Step 1351 Train Loss: 0.2669
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0132. 
Epoch 12 Step 1 Train Loss: 0.2517
Epoch 12 Step 51 Train Loss: 0.3200
Epoch 12 Step 101 Train Loss: 0.2272
Epoch 12 Step 151 Train Loss: 0.2127
Epoch 12 Step 201 Train Loss: 0.3255
Epoch 12 Step 251 Train Loss: 0.2001
Epoch 12 Step 301 Train Loss: 0.3017
Epoch 12 Step 351 Train Loss: 0.3129
Epoch 12 Step 401 Train Loss: 0.2526
Epoch 12 Step 451 Train Loss: 0.2997
Epoch 12 Step 501 Train Loss: 0.2748
Epoch 12 Step 551 Train Loss: 0.2597
Epoch 12 Step 601 Train Loss: 0.2341
Epoch 12 Step 651 Train Loss: 0.2463
Epoch 12 Step 701 Train Loss: 0.2789
Epoch 12 Step 751 Train Loss: 0.1778
Epoch 12 Step 801 Train Loss: 0.2755
Epoch 12 Step 851 Train Loss: 0.2728
Epoch 12 Step 901 Train Loss: 0.2988
Epoch 12 Step 951 Train Loss: 0.2019
Epoch 12 Step 1001 Train Loss: 0.4636
Epoch 12 Step 1051 Train Loss: 0.2729
Epoch 12 Step 1101 Train Loss: 0.2903
Epoch 12 Step 1151 Train Loss: 0.2488
Epoch 12 Step 1201 Train Loss: 0.2342
Epoch 12 Step 1251 Train Loss: 0.2487
Epoch 12 Step 1301 Train Loss: 0.2260
Epoch 12 Step 1351 Train Loss: 0.1968
Epoch 12: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0123. 
Epoch 13 Step 1 Train Loss: 0.3014
Epoch 13 Step 51 Train Loss: 0.2120
Epoch 13 Step 101 Train Loss: 0.1920
Epoch 13 Step 151 Train Loss: 0.2342
Epoch 13 Step 201 Train Loss: 0.2781
Epoch 13 Step 251 Train Loss: 0.2310
Epoch 13 Step 301 Train Loss: 0.2538
Epoch 13 Step 351 Train Loss: 0.2498
Epoch 13 Step 401 Train Loss: 0.3055
Epoch 13 Step 451 Train Loss: 0.2056
Epoch 13 Step 501 Train Loss: 0.2252
Epoch 13 Step 551 Train Loss: 0.2864
Epoch 13 Step 601 Train Loss: 0.2023
Epoch 13 Step 651 Train Loss: 0.2676
Epoch 13 Step 701 Train Loss: 0.1920
Epoch 13 Step 751 Train Loss: 0.3173
Epoch 13 Step 801 Train Loss: 0.2685
Epoch 13 Step 851 Train Loss: 0.2766
Epoch 13 Step 901 Train Loss: 0.2508
Epoch 13 Step 951 Train Loss: 0.2794
Epoch 13 Step 1001 Train Loss: 0.2813
Epoch 13 Step 1051 Train Loss: 0.2610
Epoch 13 Step 1101 Train Loss: 0.2327
Epoch 13 Step 1151 Train Loss: 0.2706
Epoch 13 Step 1201 Train Loss: 0.2167
Epoch 13 Step 1251 Train Loss: 0.2708
Epoch 13 Step 1301 Train Loss: 0.2408
Epoch 13 Step 1351 Train Loss: 0.2749
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0135. 
Epoch 14 Step 1 Train Loss: 0.2075
Epoch 14 Step 51 Train Loss: 0.2828
Epoch 14 Step 101 Train Loss: 0.2526
Epoch 14 Step 151 Train Loss: 0.2754
Epoch 14 Step 201 Train Loss: 0.2663
Epoch 14 Step 251 Train Loss: 0.2407
Epoch 14 Step 301 Train Loss: 0.2438
Epoch 14 Step 351 Train Loss: 0.2985
Epoch 14 Step 401 Train Loss: 0.2919
Epoch 14 Step 451 Train Loss: 0.2919
Epoch 14 Step 501 Train Loss: 0.2631
Epoch 14 Step 551 Train Loss: 0.3090
Epoch 14 Step 601 Train Loss: 0.2469
Epoch 14 Step 651 Train Loss: 0.3057
Epoch 14 Step 701 Train Loss: 0.2581
Epoch 14 Step 751 Train Loss: 0.2532
Epoch 14 Step 801 Train Loss: 0.2945
Epoch 14 Step 851 Train Loss: 0.2439
Epoch 14 Step 901 Train Loss: 0.2128
Epoch 14 Step 951 Train Loss: 0.2632
Epoch 14 Step 1001 Train Loss: 0.2657
Epoch 14 Step 1051 Train Loss: 0.2327
Epoch 14 Step 1101 Train Loss: 0.2089
Epoch 14 Step 1151 Train Loss: 0.2691
Epoch 14 Step 1201 Train Loss: 0.2468
Epoch 14 Step 1251 Train Loss: 0.2951
Epoch 14 Step 1301 Train Loss: 0.2486
Epoch 14 Step 1351 Train Loss: 0.2891
Epoch 14: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0116. 
Epoch 15 Step 1 Train Loss: 0.2708
Epoch 15 Step 51 Train Loss: 0.2301
Epoch 15 Step 101 Train Loss: 0.2511
Epoch 15 Step 151 Train Loss: 0.3070
Epoch 15 Step 201 Train Loss: 0.3438
Epoch 15 Step 251 Train Loss: 0.2567
Epoch 15 Step 301 Train Loss: 0.3249
Epoch 15 Step 351 Train Loss: 0.2856
Epoch 15 Step 401 Train Loss: 0.1877
Epoch 15 Step 451 Train Loss: 0.2571
Epoch 15 Step 501 Train Loss: 0.2414
Epoch 15 Step 551 Train Loss: 0.2507
Epoch 15 Step 601 Train Loss: 0.2635
Epoch 15 Step 651 Train Loss: 0.2855
Epoch 15 Step 701 Train Loss: 0.2729
Epoch 15 Step 751 Train Loss: 0.1864
Epoch 15 Step 801 Train Loss: 0.3035
Epoch 15 Step 851 Train Loss: 0.2286
Epoch 15 Step 901 Train Loss: 0.2578
Epoch 15 Step 951 Train Loss: 0.2612
Epoch 15 Step 1001 Train Loss: 0.3290
Epoch 15 Step 1051 Train Loss: 0.3082
Epoch 15 Step 1101 Train Loss: 0.1925
Epoch 15 Step 1151 Train Loss: 0.2968
Epoch 15 Step 1201 Train Loss: 0.2292
Epoch 15 Step 1251 Train Loss: 0.2629
Epoch 15 Step 1301 Train Loss: 0.3343
Epoch 15 Step 1351 Train Loss: 0.2032
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.0118. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0087
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0013027419
test_unseen_single_pearson: 0.9940650790015267
test_unseen_single_mse_de: 0.008652165
test_unseen_single_pearson_de: 0.9979809195171274
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8953585629936198
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.95
test_unseen_single_mse_top20_de_non_dropout: 0.010763679
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.003 MB of 0.033 MB uploadedwandb: / 0.033 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00865
wandb:                                              test_de_pearson 0.99798
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.95
wandb:                                                     test_mse 0.0013
wandb:                                test_mse_top20_de_non_dropout 0.01076
wandb:                                                 test_pearson 0.99407
wandb:                                           test_pearson_delta 0.89536
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.95
wandb:                                       test_unseen_single_mse 0.0013
wandb:                                    test_unseen_single_mse_de 0.00865
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01076
wandb:                                   test_unseen_single_pearson 0.99407
wandb:                                test_unseen_single_pearson_de 0.99798
wandb:                             test_unseen_single_pearson_delta 0.89536
wandb:                                                 train_de_mse 0.01037
wandb:                                             train_de_pearson 0.98346
wandb:                                                    train_mse 0.00113
wandb:                                                train_pearson 0.99486
wandb:                                                training_loss 0.26143
wandb:                                                   val_de_mse 0.0118
wandb:                                               val_de_pearson 0.99702
wandb:                                                      val_mse 0.00125
wandb:                                                  val_pearson 0.99446
wandb: 
wandb: üöÄ View run geneformer_ShifrutMarson2018_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/9xgoce4z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_080830-9xgoce4z/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_083104-khttli1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_ShifrutMarson2018_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/khttli1z
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4171
Epoch 1 Step 51 Train Loss: 0.3436
Epoch 1 Step 101 Train Loss: 0.3100
Epoch 1 Step 151 Train Loss: 0.3836
Epoch 1 Step 201 Train Loss: 0.3682
Epoch 1 Step 251 Train Loss: 0.2651
Epoch 1 Step 301 Train Loss: 0.2998
Epoch 1 Step 351 Train Loss: 0.2477
Epoch 1 Step 401 Train Loss: 0.3534
Epoch 1 Step 451 Train Loss: 0.2885
Epoch 1 Step 501 Train Loss: 0.3575
Epoch 1 Step 551 Train Loss: 0.4171
Epoch 1 Step 601 Train Loss: 0.3198
Epoch 1 Step 651 Train Loss: 0.3440
Epoch 1 Step 701 Train Loss: 0.2971
Epoch 1 Step 751 Train Loss: 0.3031
Epoch 1 Step 801 Train Loss: 0.3081
Epoch 1 Step 851 Train Loss: 0.3405
Epoch 1 Step 901 Train Loss: 0.2777
Epoch 1 Step 951 Train Loss: 0.2385
Epoch 1 Step 1001 Train Loss: 0.2554
Epoch 1 Step 1051 Train Loss: 0.3482
Epoch 1 Step 1101 Train Loss: 0.2641
Epoch 1 Step 1151 Train Loss: 0.2976
Epoch 1 Step 1201 Train Loss: 0.3122
Epoch 1 Step 1251 Train Loss: 0.2705
Epoch 1 Step 1301 Train Loss: 0.4300
Epoch 1 Step 1351 Train Loss: 0.3110
Epoch 1: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0126 Validation Top 20 DE MSE: 0.0105. 
Epoch 2 Step 1 Train Loss: 0.3060
Epoch 2 Step 51 Train Loss: 0.2155
Epoch 2 Step 101 Train Loss: 0.2583
Epoch 2 Step 151 Train Loss: 0.3262
Epoch 2 Step 201 Train Loss: 0.3378
Epoch 2 Step 251 Train Loss: 0.3361
Epoch 2 Step 301 Train Loss: 0.2279
Epoch 2 Step 351 Train Loss: 0.2498
Epoch 2 Step 401 Train Loss: 0.2928
Epoch 2 Step 451 Train Loss: 0.2823
Epoch 2 Step 501 Train Loss: 0.3005
Epoch 2 Step 551 Train Loss: 0.2341
Epoch 2 Step 601 Train Loss: 0.2288
Epoch 2 Step 651 Train Loss: 0.2524
Epoch 2 Step 701 Train Loss: 0.3368
Epoch 2 Step 751 Train Loss: 0.3402
Epoch 2 Step 801 Train Loss: 0.2503
Epoch 2 Step 851 Train Loss: 0.3257
Epoch 2 Step 901 Train Loss: 0.3006
Epoch 2 Step 951 Train Loss: 0.2687
Epoch 2 Step 1001 Train Loss: 0.2343
Epoch 2 Step 1051 Train Loss: 0.2204
Epoch 2 Step 1101 Train Loss: 0.2831
Epoch 2 Step 1151 Train Loss: 0.2590
Epoch 2 Step 1201 Train Loss: 0.2471
Epoch 2 Step 1251 Train Loss: 0.2825
Epoch 2 Step 1301 Train Loss: 0.2612
Epoch 2 Step 1351 Train Loss: 0.1990
Epoch 2: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0040. 
Train Top 20 DE MSE: 0.0126 Validation Top 20 DE MSE: 0.0133. 
Epoch 3 Step 1 Train Loss: 0.2919
Epoch 3 Step 51 Train Loss: 0.3286
Epoch 3 Step 101 Train Loss: 0.3512
Epoch 3 Step 151 Train Loss: 0.1809
Epoch 3 Step 201 Train Loss: 0.2853
Epoch 3 Step 251 Train Loss: 0.2250
Epoch 3 Step 301 Train Loss: 0.2390
Epoch 3 Step 351 Train Loss: 0.2473
Epoch 3 Step 401 Train Loss: 0.2598
Epoch 3 Step 451 Train Loss: 0.2977
Epoch 3 Step 501 Train Loss: 0.3354
Epoch 3 Step 551 Train Loss: 0.2541
Epoch 3 Step 601 Train Loss: 0.2827
Epoch 3 Step 651 Train Loss: 0.3161
Epoch 3 Step 701 Train Loss: 0.2749
Epoch 3 Step 751 Train Loss: 0.3065
Epoch 3 Step 801 Train Loss: 0.3448
Epoch 3 Step 851 Train Loss: 0.2145
Epoch 3 Step 901 Train Loss: 0.3677
Epoch 3 Step 951 Train Loss: 0.2045
Epoch 3 Step 1001 Train Loss: 0.2017
Epoch 3 Step 1051 Train Loss: 0.2452
Epoch 3 Step 1101 Train Loss: 0.2562
Epoch 3 Step 1151 Train Loss: 0.3560
Epoch 3 Step 1201 Train Loss: 0.2477
Epoch 3 Step 1251 Train Loss: 0.2829
Epoch 3 Step 1301 Train Loss: 0.3042
Epoch 3 Step 1351 Train Loss: 0.2496
Epoch 3: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0119 Validation Top 20 DE MSE: 0.0093. 
Epoch 4 Step 1 Train Loss: 0.2615
Epoch 4 Step 51 Train Loss: 0.2459
Epoch 4 Step 101 Train Loss: 0.2799
Epoch 4 Step 151 Train Loss: 0.2883
Epoch 4 Step 201 Train Loss: 0.2594
Epoch 4 Step 251 Train Loss: 0.2368
Epoch 4 Step 301 Train Loss: 0.2766
Epoch 4 Step 351 Train Loss: 0.3040
Epoch 4 Step 401 Train Loss: 0.3007
Epoch 4 Step 451 Train Loss: 0.2209
Epoch 4 Step 501 Train Loss: 0.2871
Epoch 4 Step 551 Train Loss: 0.3041
Epoch 4 Step 601 Train Loss: 0.2555
Epoch 4 Step 651 Train Loss: 0.2561
Epoch 4 Step 701 Train Loss: 0.3330
Epoch 4 Step 751 Train Loss: 0.1387
Epoch 4 Step 801 Train Loss: 0.2946
Epoch 4 Step 851 Train Loss: 0.2057
Epoch 4 Step 901 Train Loss: 0.3827
Epoch 4 Step 951 Train Loss: 0.2288
Epoch 4 Step 1001 Train Loss: 0.1990
Epoch 4 Step 1051 Train Loss: 0.2232
Epoch 4 Step 1101 Train Loss: 0.2288
Epoch 4 Step 1151 Train Loss: 0.2703
Epoch 4 Step 1201 Train Loss: 0.1419
Epoch 4 Step 1251 Train Loss: 0.2463
Epoch 4 Step 1301 Train Loss: 0.2357
Epoch 4 Step 1351 Train Loss: 0.2026
Epoch 4: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0090. 
Epoch 5 Step 1 Train Loss: 0.1960
Epoch 5 Step 51 Train Loss: 0.2408
Epoch 5 Step 101 Train Loss: 0.2707
Epoch 5 Step 151 Train Loss: 0.2453
Epoch 5 Step 201 Train Loss: 0.3046
Epoch 5 Step 251 Train Loss: 0.1882
Epoch 5 Step 301 Train Loss: 0.2608
Epoch 5 Step 351 Train Loss: 0.2221
Epoch 5 Step 401 Train Loss: 0.1251
Epoch 5 Step 451 Train Loss: 0.3109
Epoch 5 Step 501 Train Loss: 0.2867
Epoch 5 Step 551 Train Loss: 0.2757
Epoch 5 Step 601 Train Loss: 0.2763
Epoch 5 Step 651 Train Loss: 0.2329
Epoch 5 Step 701 Train Loss: 0.2597
Epoch 5 Step 751 Train Loss: 0.2502
Epoch 5 Step 801 Train Loss: 0.2129
Epoch 5 Step 851 Train Loss: 0.2831
Epoch 5 Step 901 Train Loss: 0.2055
Epoch 5 Step 951 Train Loss: 0.2324
Epoch 5 Step 1001 Train Loss: 0.2459
Epoch 5 Step 1051 Train Loss: 0.2885
Epoch 5 Step 1101 Train Loss: 0.3103
Epoch 5 Step 1151 Train Loss: 0.2717
Epoch 5 Step 1201 Train Loss: 0.3540
Epoch 5 Step 1251 Train Loss: 0.2359
Epoch 5 Step 1301 Train Loss: 0.3016
Epoch 5 Step 1351 Train Loss: 0.3135
Epoch 5: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.0088. 
Epoch 6 Step 1 Train Loss: 0.2172
Epoch 6 Step 51 Train Loss: 0.2941
Epoch 6 Step 101 Train Loss: 0.2852
Epoch 6 Step 151 Train Loss: 0.2499
Epoch 6 Step 201 Train Loss: 0.2827
Epoch 6 Step 251 Train Loss: 0.3290
Epoch 6 Step 301 Train Loss: 0.2498
Epoch 6 Step 351 Train Loss: 0.2783
Epoch 6 Step 401 Train Loss: 0.2462
Epoch 6 Step 451 Train Loss: 0.2020
Epoch 6 Step 501 Train Loss: 0.2384
Epoch 6 Step 551 Train Loss: 0.2926
Epoch 6 Step 601 Train Loss: 0.1816
Epoch 6 Step 651 Train Loss: 0.2798
Epoch 6 Step 701 Train Loss: 0.2093
Epoch 6 Step 751 Train Loss: 0.2979
Epoch 6 Step 801 Train Loss: 0.2455
Epoch 6 Step 851 Train Loss: 0.3003
Epoch 6 Step 901 Train Loss: 0.2401
Epoch 6 Step 951 Train Loss: 0.2411
Epoch 6 Step 1001 Train Loss: 0.2214
Epoch 6 Step 1051 Train Loss: 0.2421
Epoch 6 Step 1101 Train Loss: 0.2849
Epoch 6 Step 1151 Train Loss: 0.2626
Epoch 6 Step 1201 Train Loss: 0.2969
Epoch 6 Step 1251 Train Loss: 0.1948
Epoch 6 Step 1301 Train Loss: 0.2473
Epoch 6 Step 1351 Train Loss: 0.3594
Epoch 6: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0100. 
Epoch 7 Step 1 Train Loss: 0.2124
Epoch 7 Step 51 Train Loss: 0.2403
Epoch 7 Step 101 Train Loss: 0.2599
Epoch 7 Step 151 Train Loss: 0.2317
Epoch 7 Step 201 Train Loss: 0.2567
Epoch 7 Step 251 Train Loss: 0.2614
Epoch 7 Step 301 Train Loss: 0.3245
Epoch 7 Step 351 Train Loss: 0.2546
Epoch 7 Step 401 Train Loss: 0.2477
Epoch 7 Step 451 Train Loss: 0.2459
Epoch 7 Step 501 Train Loss: 0.1445
Epoch 7 Step 551 Train Loss: 0.2691
Epoch 7 Step 601 Train Loss: 0.2350
Epoch 7 Step 651 Train Loss: 0.2264
Epoch 7 Step 701 Train Loss: 0.2511
Epoch 7 Step 751 Train Loss: 0.2213
Epoch 7 Step 801 Train Loss: 0.2179
Epoch 7 Step 851 Train Loss: 0.2802
Epoch 7 Step 901 Train Loss: 0.2161
Epoch 7 Step 951 Train Loss: 0.2438
Epoch 7 Step 1001 Train Loss: 0.3277
Epoch 7 Step 1051 Train Loss: 0.2170
Epoch 7 Step 1101 Train Loss: 0.2075
Epoch 7 Step 1151 Train Loss: 0.2619
Epoch 7 Step 1201 Train Loss: 0.3218
Epoch 7 Step 1251 Train Loss: 0.2806
Epoch 7 Step 1301 Train Loss: 0.2520
Epoch 7 Step 1351 Train Loss: 0.2399
Epoch 7: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0115 Validation Top 20 DE MSE: 0.0078. 
Epoch 8 Step 1 Train Loss: 0.3025
Epoch 8 Step 51 Train Loss: 0.2786
Epoch 8 Step 101 Train Loss: 0.2452
Epoch 8 Step 151 Train Loss: 0.2293
Epoch 8 Step 201 Train Loss: 0.2508
Epoch 8 Step 251 Train Loss: 0.2622
Epoch 8 Step 301 Train Loss: 0.2546
Epoch 8 Step 351 Train Loss: 0.2726
Epoch 8 Step 401 Train Loss: 0.2562
Epoch 8 Step 451 Train Loss: 0.2614
Epoch 8 Step 501 Train Loss: 0.2356
Epoch 8 Step 551 Train Loss: 0.2378
Epoch 8 Step 601 Train Loss: 0.2030
Epoch 8 Step 651 Train Loss: 0.2793
Epoch 8 Step 701 Train Loss: 0.2168
Epoch 8 Step 751 Train Loss: 0.2446
Epoch 8 Step 801 Train Loss: 0.3071
Epoch 8 Step 851 Train Loss: 0.3090
Epoch 8 Step 901 Train Loss: 0.3093
Epoch 8 Step 951 Train Loss: 0.2628
Epoch 8 Step 1001 Train Loss: 0.2875
Epoch 8 Step 1051 Train Loss: 0.1476
Epoch 8 Step 1101 Train Loss: 0.2823
Epoch 8 Step 1151 Train Loss: 0.2612
Epoch 8 Step 1201 Train Loss: 0.2568
Epoch 8 Step 1251 Train Loss: 0.2876
Epoch 8 Step 1301 Train Loss: 0.2398
Epoch 8 Step 1351 Train Loss: 0.2942
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0113 Validation Top 20 DE MSE: 0.0084. 
Epoch 9 Step 1 Train Loss: 0.3113
Epoch 9 Step 51 Train Loss: 0.2688
Epoch 9 Step 101 Train Loss: 0.1743
Epoch 9 Step 151 Train Loss: 0.2833
Epoch 9 Step 201 Train Loss: 0.2410
Epoch 9 Step 251 Train Loss: 0.1876
Epoch 9 Step 301 Train Loss: 0.1605
Epoch 9 Step 351 Train Loss: 0.2264
Epoch 9 Step 401 Train Loss: 0.2038
Epoch 9 Step 451 Train Loss: 0.2286
Epoch 9 Step 501 Train Loss: 0.2444
Epoch 9 Step 551 Train Loss: 0.2501
Epoch 9 Step 601 Train Loss: 0.2446
Epoch 9 Step 651 Train Loss: 0.3026
Epoch 9 Step 701 Train Loss: 0.2314
Epoch 9 Step 751 Train Loss: 0.1923
Epoch 9 Step 801 Train Loss: 0.2683
Epoch 9 Step 851 Train Loss: 0.2323
Epoch 9 Step 901 Train Loss: 0.2671
Epoch 9 Step 951 Train Loss: 0.2537
Epoch 9 Step 1001 Train Loss: 0.3431
Epoch 9 Step 1051 Train Loss: 0.2502
Epoch 9 Step 1101 Train Loss: 0.2564
Epoch 9 Step 1151 Train Loss: 0.2399
Epoch 9 Step 1201 Train Loss: 0.1809
Epoch 9 Step 1251 Train Loss: 0.3285
Epoch 9 Step 1301 Train Loss: 0.2641
Epoch 9 Step 1351 Train Loss: 0.3050
Epoch 9: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0087. 
Epoch 10 Step 1 Train Loss: 0.3086
Epoch 10 Step 51 Train Loss: 0.3664
Epoch 10 Step 101 Train Loss: 0.2191
Epoch 10 Step 151 Train Loss: 0.2182
Epoch 10 Step 201 Train Loss: 0.2631
Epoch 10 Step 251 Train Loss: 0.3095
Epoch 10 Step 301 Train Loss: 0.2684
Epoch 10 Step 351 Train Loss: 0.2451
Epoch 10 Step 401 Train Loss: 0.3051
Epoch 10 Step 451 Train Loss: 0.1669
Epoch 10 Step 501 Train Loss: 0.2104
Epoch 10 Step 551 Train Loss: 0.3785
Epoch 10 Step 601 Train Loss: 0.5374
Epoch 10 Step 651 Train Loss: 0.2271
Epoch 10 Step 701 Train Loss: 0.2688
Epoch 10 Step 751 Train Loss: 0.3344
Epoch 10 Step 801 Train Loss: 0.2563
Epoch 10 Step 851 Train Loss: 0.2742
Epoch 10 Step 901 Train Loss: 0.2414
Epoch 10 Step 951 Train Loss: 0.2767
Epoch 10 Step 1001 Train Loss: 0.2644
Epoch 10 Step 1051 Train Loss: 0.2523
Epoch 10 Step 1101 Train Loss: 0.2824
Epoch 10 Step 1151 Train Loss: 0.1782
Epoch 10 Step 1201 Train Loss: 0.3197
Epoch 10 Step 1251 Train Loss: 0.1708
Epoch 10 Step 1301 Train Loss: 0.2816
Epoch 10 Step 1351 Train Loss: 0.2769
Epoch 10: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0083. 
Epoch 11 Step 1 Train Loss: 0.2889
Epoch 11 Step 51 Train Loss: 0.2882
Epoch 11 Step 101 Train Loss: 0.2081
Epoch 11 Step 151 Train Loss: 0.3035
Epoch 11 Step 201 Train Loss: 0.2448
Epoch 11 Step 251 Train Loss: 0.2946
Epoch 11 Step 301 Train Loss: 0.2648
Epoch 11 Step 351 Train Loss: 0.2662
Epoch 11 Step 401 Train Loss: 0.2743
Epoch 11 Step 451 Train Loss: 0.2723
Epoch 11 Step 501 Train Loss: 0.2143
Epoch 11 Step 551 Train Loss: 0.2770
Epoch 11 Step 601 Train Loss: 0.2136
Epoch 11 Step 651 Train Loss: 0.2676
Epoch 11 Step 701 Train Loss: 0.2119
Epoch 11 Step 751 Train Loss: 0.2192
Epoch 11 Step 801 Train Loss: 0.3536
Epoch 11 Step 851 Train Loss: 0.2144
Epoch 11 Step 901 Train Loss: 0.2866
Epoch 11 Step 951 Train Loss: 0.1895
Epoch 11 Step 1001 Train Loss: 0.1754
Epoch 11 Step 1051 Train Loss: 0.2406
Epoch 11 Step 1101 Train Loss: 0.2158
Epoch 11 Step 1151 Train Loss: 0.2064
Epoch 11 Step 1201 Train Loss: 0.3299
Epoch 11 Step 1251 Train Loss: 0.3227
Epoch 11 Step 1301 Train Loss: 0.2714
Epoch 11 Step 1351 Train Loss: 0.2494
Epoch 11: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0086. 
Epoch 12 Step 1 Train Loss: 0.2467
Epoch 12 Step 51 Train Loss: 0.2587
Epoch 12 Step 101 Train Loss: 0.1550
Epoch 12 Step 151 Train Loss: 0.3423
Epoch 12 Step 201 Train Loss: 0.2247
Epoch 12 Step 251 Train Loss: 0.2423
Epoch 12 Step 301 Train Loss: 0.2067
Epoch 12 Step 351 Train Loss: 0.2539
Epoch 12 Step 401 Train Loss: 0.3003
Epoch 12 Step 451 Train Loss: 0.2012
Epoch 12 Step 501 Train Loss: 0.2254
Epoch 12 Step 551 Train Loss: 0.2363
Epoch 12 Step 601 Train Loss: 0.3055
Epoch 12 Step 651 Train Loss: 0.2076
Epoch 12 Step 701 Train Loss: 0.2566
Epoch 12 Step 751 Train Loss: 0.3040
Epoch 12 Step 801 Train Loss: 0.2868
Epoch 12 Step 851 Train Loss: 0.2984
Epoch 12 Step 901 Train Loss: 0.2242
Epoch 12 Step 951 Train Loss: 0.1700
Epoch 12 Step 1001 Train Loss: 0.2973
Epoch 12 Step 1051 Train Loss: 0.2150
Epoch 12 Step 1101 Train Loss: 0.2773
Epoch 12 Step 1151 Train Loss: 0.2573
Epoch 12 Step 1201 Train Loss: 0.2455
Epoch 12 Step 1251 Train Loss: 0.2992
Epoch 12 Step 1301 Train Loss: 0.3470
Epoch 12 Step 1351 Train Loss: 0.3407
Epoch 12: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0089. 
Epoch 13 Step 1 Train Loss: 0.2892
Epoch 13 Step 51 Train Loss: 0.2654
Epoch 13 Step 101 Train Loss: 0.2046
Epoch 13 Step 151 Train Loss: 0.2184
Epoch 13 Step 201 Train Loss: 0.1800
Epoch 13 Step 251 Train Loss: 0.2958
Epoch 13 Step 301 Train Loss: 0.2612
Epoch 13 Step 351 Train Loss: 0.3807
Epoch 13 Step 401 Train Loss: 0.2773
Epoch 13 Step 451 Train Loss: 0.2452
Epoch 13 Step 501 Train Loss: 0.2414
Epoch 13 Step 551 Train Loss: 0.2296
Epoch 13 Step 601 Train Loss: 0.2700
Epoch 13 Step 651 Train Loss: 0.1883
Epoch 13 Step 701 Train Loss: 0.3212
Epoch 13 Step 751 Train Loss: 0.3168
Epoch 13 Step 801 Train Loss: 0.3003
Epoch 13 Step 851 Train Loss: 0.2949
Epoch 13 Step 901 Train Loss: 0.2606
Epoch 13 Step 951 Train Loss: 0.3076
Epoch 13 Step 1001 Train Loss: 0.3122
Epoch 13 Step 1051 Train Loss: 0.3012
Epoch 13 Step 1101 Train Loss: 0.2542
Epoch 13 Step 1151 Train Loss: 0.2620
Epoch 13 Step 1201 Train Loss: 0.2619
Epoch 13 Step 1251 Train Loss: 0.2927
Epoch 13 Step 1301 Train Loss: 0.3815
Epoch 13 Step 1351 Train Loss: 0.2501
Epoch 13: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0108 Validation Top 20 DE MSE: 0.0084. 
Epoch 14 Step 1 Train Loss: 0.2486
Epoch 14 Step 51 Train Loss: 0.3685
Epoch 14 Step 101 Train Loss: 0.2120
Epoch 14 Step 151 Train Loss: 0.2576
Epoch 14 Step 201 Train Loss: 0.2649
Epoch 14 Step 251 Train Loss: 0.2832
Epoch 14 Step 301 Train Loss: 0.2383
Epoch 14 Step 351 Train Loss: 0.2455
Epoch 14 Step 401 Train Loss: 0.3108
Epoch 14 Step 451 Train Loss: 0.2577
Epoch 14 Step 501 Train Loss: 0.3037
Epoch 14 Step 551 Train Loss: 0.2505
Epoch 14 Step 601 Train Loss: 0.2997
Epoch 14 Step 651 Train Loss: 0.2858
Epoch 14 Step 701 Train Loss: 0.1960
Epoch 14 Step 751 Train Loss: 0.3273
Epoch 14 Step 801 Train Loss: 0.2416
Epoch 14 Step 851 Train Loss: 0.2145
Epoch 14 Step 901 Train Loss: 0.1981
Epoch 14 Step 951 Train Loss: 0.2022
Epoch 14 Step 1001 Train Loss: 0.2660
Epoch 14 Step 1051 Train Loss: 0.2476
Epoch 14 Step 1101 Train Loss: 0.2806
Epoch 14 Step 1151 Train Loss: 0.2295
Epoch 14 Step 1201 Train Loss: 0.2558
Epoch 14 Step 1251 Train Loss: 0.2385
Epoch 14 Step 1301 Train Loss: 0.2122
Epoch 14 Step 1351 Train Loss: 0.2624
Epoch 14: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0083. 
Epoch 15 Step 1 Train Loss: 0.2211
Epoch 15 Step 51 Train Loss: 0.2718
Epoch 15 Step 101 Train Loss: 0.2827
Epoch 15 Step 151 Train Loss: 0.3033
Epoch 15 Step 201 Train Loss: 0.2405
Epoch 15 Step 251 Train Loss: 0.3139
Epoch 15 Step 301 Train Loss: 0.2268
Epoch 15 Step 351 Train Loss: 0.2176
Epoch 15 Step 401 Train Loss: 0.2534
Epoch 15 Step 451 Train Loss: 0.1938
Epoch 15 Step 501 Train Loss: 0.2839
Epoch 15 Step 551 Train Loss: 0.2920
Epoch 15 Step 601 Train Loss: 0.2217
Epoch 15 Step 651 Train Loss: 0.2473
Epoch 15 Step 701 Train Loss: 0.2283
Epoch 15 Step 751 Train Loss: 0.2104
Epoch 15 Step 801 Train Loss: 0.2031
Epoch 15 Step 851 Train Loss: 0.2578
Epoch 15 Step 901 Train Loss: 0.2331
Epoch 15 Step 951 Train Loss: 0.2819
Epoch 15 Step 1001 Train Loss: 0.2931
Epoch 15 Step 1051 Train Loss: 0.3749
Epoch 15 Step 1101 Train Loss: 0.2250
Epoch 15 Step 1151 Train Loss: 0.1930
Epoch 15 Step 1201 Train Loss: 0.3081
Epoch 15 Step 1251 Train Loss: 0.3110
Epoch 15 Step 1301 Train Loss: 0.2864
Epoch 15 Step 1351 Train Loss: 0.3103
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.0085. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0130
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0012419928
test_unseen_single_pearson: 0.9942860742594549
test_unseen_single_mse_de: 0.013019445
test_unseen_single_pearson_de: 0.9962679750449166
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8969763582232682
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.96
test_unseen_single_mse_top20_de_non_dropout: 0.013110864
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.003 MB of 0.033 MB uploadedwandb: / 0.003 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÇ
wandb:                                             train_de_pearson ‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                    train_mse ‚ñà‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ
wandb:                                                   val_de_mse ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÉ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01302
wandb:                                              test_de_pearson 0.99627
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.96
wandb:                                                     test_mse 0.00124
wandb:                                test_mse_top20_de_non_dropout 0.01311
wandb:                                                 test_pearson 0.99429
wandb:                                           test_pearson_delta 0.89698
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.96
wandb:                                       test_unseen_single_mse 0.00124
wandb:                                    test_unseen_single_mse_de 0.01302
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01311
wandb:                                   test_unseen_single_pearson 0.99429
wandb:                                test_unseen_single_pearson_de 0.99627
wandb:                             test_unseen_single_pearson_delta 0.89698
wandb:                                                 train_de_mse 0.01021
wandb:                                             train_de_pearson 0.97963
wandb:                                                    train_mse 0.00114
wandb:                                                train_pearson 0.99475
wandb:                                                training_loss 0.271
wandb:                                                   val_de_mse 0.00853
wandb:                                               val_de_pearson 0.99846
wandb:                                                      val_mse 0.00155
wandb:                                                  val_pearson 0.9929
wandb: 
wandb: üöÄ View run geneformer_ShifrutMarson2018_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/khttli1z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_083104-khttli1z/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_085258-iqc1j1uj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_ShifrutMarson2018_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/iqc1j1uj
wandb: WARNING Serializing object of type ndarray that is 20512896 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3664
Epoch 1 Step 51 Train Loss: 0.2891
Epoch 1 Step 101 Train Loss: 0.3408
Epoch 1 Step 151 Train Loss: 0.2712
Epoch 1 Step 201 Train Loss: 0.3336
Epoch 1 Step 251 Train Loss: 0.3665
Epoch 1 Step 301 Train Loss: 0.3557
Epoch 1 Step 351 Train Loss: 0.2754
Epoch 1 Step 401 Train Loss: 0.2486
Epoch 1 Step 451 Train Loss: 0.2803
Epoch 1 Step 501 Train Loss: 0.4417
Epoch 1 Step 551 Train Loss: 0.3781
Epoch 1 Step 601 Train Loss: 0.3178
Epoch 1 Step 651 Train Loss: 0.2922
Epoch 1 Step 701 Train Loss: 0.3879
Epoch 1 Step 751 Train Loss: 0.4592
Epoch 1 Step 801 Train Loss: 0.2834
Epoch 1 Step 851 Train Loss: 0.2556
Epoch 1 Step 901 Train Loss: 0.2963
Epoch 1 Step 951 Train Loss: 0.3055
Epoch 1 Step 1001 Train Loss: 0.3273
Epoch 1 Step 1051 Train Loss: 0.2580
Epoch 1 Step 1101 Train Loss: 0.2849
Epoch 1 Step 1151 Train Loss: 0.3356
Epoch 1 Step 1201 Train Loss: 0.1971
Epoch 1 Step 1251 Train Loss: 0.2875
Epoch 1 Step 1301 Train Loss: 0.3396
Epoch 1 Step 1351 Train Loss: 0.3393
Epoch 1 Step 1401 Train Loss: 0.2754
Epoch 1: Train Overall MSE: 0.0038 Validation Overall MSE: 0.0046. 
Train Top 20 DE MSE: 0.0170 Validation Top 20 DE MSE: 0.0114. 
Epoch 2 Step 1 Train Loss: 0.2557
Epoch 2 Step 51 Train Loss: 0.2524
Epoch 2 Step 101 Train Loss: 0.2124
Epoch 2 Step 151 Train Loss: 0.2635
Epoch 2 Step 201 Train Loss: 0.2363
Epoch 2 Step 251 Train Loss: 0.1742
Epoch 2 Step 301 Train Loss: 0.2603
Epoch 2 Step 351 Train Loss: 0.3180
Epoch 2 Step 401 Train Loss: 0.2996
Epoch 2 Step 451 Train Loss: 0.3208
Epoch 2 Step 501 Train Loss: 0.2971
Epoch 2 Step 551 Train Loss: 0.3039
Epoch 2 Step 601 Train Loss: 0.2613
Epoch 2 Step 651 Train Loss: 0.2286
Epoch 2 Step 701 Train Loss: 0.2624
Epoch 2 Step 751 Train Loss: 0.2916
Epoch 2 Step 801 Train Loss: 0.3217
Epoch 2 Step 851 Train Loss: 0.2212
Epoch 2 Step 901 Train Loss: 0.2981
Epoch 2 Step 951 Train Loss: 0.3075
Epoch 2 Step 1001 Train Loss: 0.2297
Epoch 2 Step 1051 Train Loss: 0.2287
Epoch 2 Step 1101 Train Loss: 0.2004
Epoch 2 Step 1151 Train Loss: 0.2651
Epoch 2 Step 1201 Train Loss: 0.3672
Epoch 2 Step 1251 Train Loss: 0.1833
Epoch 2 Step 1301 Train Loss: 0.2639
Epoch 2 Step 1351 Train Loss: 0.2398
Epoch 2 Step 1401 Train Loss: 0.2919
Epoch 2: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0176 Validation Top 20 DE MSE: 0.0091. 
Epoch 3 Step 1 Train Loss: 0.2765
Epoch 3 Step 51 Train Loss: 0.2196
Epoch 3 Step 101 Train Loss: 0.2986
Epoch 3 Step 151 Train Loss: 0.2496
Epoch 3 Step 201 Train Loss: 0.1802
Epoch 3 Step 251 Train Loss: 0.2872
Epoch 3 Step 301 Train Loss: 0.2192
Epoch 3 Step 351 Train Loss: 0.3026
Epoch 3 Step 401 Train Loss: 0.2169
Epoch 3 Step 451 Train Loss: 0.2148
Epoch 3 Step 501 Train Loss: 0.2639
Epoch 3 Step 551 Train Loss: 0.3082
Epoch 3 Step 601 Train Loss: 0.1625
Epoch 3 Step 651 Train Loss: 0.2815
Epoch 3 Step 701 Train Loss: 0.2145
Epoch 3 Step 751 Train Loss: 0.2202
Epoch 3 Step 801 Train Loss: 0.2490
Epoch 3 Step 851 Train Loss: 0.2505
Epoch 3 Step 901 Train Loss: 0.2457
Epoch 3 Step 951 Train Loss: 0.3004
Epoch 3 Step 1001 Train Loss: 0.2764
Epoch 3 Step 1051 Train Loss: 0.2896
Epoch 3 Step 1101 Train Loss: 0.2897
Epoch 3 Step 1151 Train Loss: 0.2395
Epoch 3 Step 1201 Train Loss: 0.2310
Epoch 3 Step 1251 Train Loss: 0.2733
Epoch 3 Step 1301 Train Loss: 0.2753
Epoch 3 Step 1351 Train Loss: 0.2060
Epoch 3 Step 1401 Train Loss: 0.2892
Epoch 3: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0117 Validation Top 20 DE MSE: 0.0070. 
Epoch 4 Step 1 Train Loss: 0.2258
Epoch 4 Step 51 Train Loss: 0.2631
Epoch 4 Step 101 Train Loss: 0.2699
Epoch 4 Step 151 Train Loss: 0.2066
Epoch 4 Step 201 Train Loss: 0.2789
Epoch 4 Step 251 Train Loss: 0.3088
Epoch 4 Step 301 Train Loss: 0.2824
Epoch 4 Step 351 Train Loss: 0.3979
Epoch 4 Step 401 Train Loss: 0.2150
Epoch 4 Step 451 Train Loss: 0.2170
Epoch 4 Step 501 Train Loss: 0.3098
Epoch 4 Step 551 Train Loss: 0.2126
Epoch 4 Step 601 Train Loss: 0.2167
Epoch 4 Step 651 Train Loss: 0.3130
Epoch 4 Step 701 Train Loss: 0.2384
Epoch 4 Step 751 Train Loss: 0.2491
Epoch 4 Step 801 Train Loss: 0.2351
Epoch 4 Step 851 Train Loss: 0.2570
Epoch 4 Step 901 Train Loss: 0.2460
Epoch 4 Step 951 Train Loss: 0.2851
Epoch 4 Step 1001 Train Loss: 0.2725
Epoch 4 Step 1051 Train Loss: 0.1991
Epoch 4 Step 1101 Train Loss: 0.2926
Epoch 4 Step 1151 Train Loss: 0.2673
Epoch 4 Step 1201 Train Loss: 0.2769
Epoch 4 Step 1251 Train Loss: 0.2451
Epoch 4 Step 1301 Train Loss: 0.2378
Epoch 4 Step 1351 Train Loss: 0.2121
Epoch 4 Step 1401 Train Loss: 0.2949
Epoch 4: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0180 Validation Top 20 DE MSE: 0.0104. 
Epoch 5 Step 1 Train Loss: 0.2120
Epoch 5 Step 51 Train Loss: 0.3625
Epoch 5 Step 101 Train Loss: 0.3068
Epoch 5 Step 151 Train Loss: 0.3616
Epoch 5 Step 201 Train Loss: 0.2310
Epoch 5 Step 251 Train Loss: 0.2710
Epoch 5 Step 301 Train Loss: 0.2312
Epoch 5 Step 351 Train Loss: 0.2291
Epoch 5 Step 401 Train Loss: 0.2763
Epoch 5 Step 451 Train Loss: 0.1830
Epoch 5 Step 501 Train Loss: 0.1882
Epoch 5 Step 551 Train Loss: 0.3499
Epoch 5 Step 601 Train Loss: 0.2969
Epoch 5 Step 651 Train Loss: 0.2879
Epoch 5 Step 701 Train Loss: 0.2312
Epoch 5 Step 751 Train Loss: 0.2974
Epoch 5 Step 801 Train Loss: 0.2722
Epoch 5 Step 851 Train Loss: 0.2773
Epoch 5 Step 901 Train Loss: 0.2486
Epoch 5 Step 951 Train Loss: 0.2552
Epoch 5 Step 1001 Train Loss: 0.2087
Epoch 5 Step 1051 Train Loss: 0.2406
Epoch 5 Step 1101 Train Loss: 0.2620
Epoch 5 Step 1151 Train Loss: 0.2990
Epoch 5 Step 1201 Train Loss: 0.2688
Epoch 5 Step 1251 Train Loss: 0.2623
Epoch 5 Step 1301 Train Loss: 0.1797
Epoch 5 Step 1351 Train Loss: 0.3248
Epoch 5 Step 1401 Train Loss: 0.2149
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0076. 
Epoch 6 Step 1 Train Loss: 0.2091
Epoch 6 Step 51 Train Loss: 0.2578
Epoch 6 Step 101 Train Loss: 0.1773
Epoch 6 Step 151 Train Loss: 0.2771
Epoch 6 Step 201 Train Loss: 0.2585
Epoch 6 Step 251 Train Loss: 0.3102
Epoch 6 Step 301 Train Loss: 0.3523
Epoch 6 Step 351 Train Loss: 0.2460
Epoch 6 Step 401 Train Loss: 0.2820
Epoch 6 Step 451 Train Loss: 0.3060
Epoch 6 Step 501 Train Loss: 0.1754
Epoch 6 Step 551 Train Loss: 0.1931
Epoch 6 Step 601 Train Loss: 0.2361
Epoch 6 Step 651 Train Loss: 0.1883
Epoch 6 Step 701 Train Loss: 0.2551
Epoch 6 Step 751 Train Loss: 0.2441
Epoch 6 Step 801 Train Loss: 0.2665
Epoch 6 Step 851 Train Loss: 0.2886
Epoch 6 Step 901 Train Loss: 0.2506
Epoch 6 Step 951 Train Loss: 0.2438
Epoch 6 Step 1001 Train Loss: 0.2307
Epoch 6 Step 1051 Train Loss: 0.2547
Epoch 6 Step 1101 Train Loss: 0.3376
Epoch 6 Step 1151 Train Loss: 0.2414
Epoch 6 Step 1201 Train Loss: 0.2867
Epoch 6 Step 1251 Train Loss: 0.2415
Epoch 6 Step 1301 Train Loss: 0.2370
Epoch 6 Step 1351 Train Loss: 0.2531
Epoch 6 Step 1401 Train Loss: 0.2788
Epoch 6: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.0061. 
Epoch 7 Step 1 Train Loss: 0.2892
Epoch 7 Step 51 Train Loss: 0.2262
Epoch 7 Step 101 Train Loss: 0.2021
Epoch 7 Step 151 Train Loss: 0.2766
Epoch 7 Step 201 Train Loss: 0.2139
Epoch 7 Step 251 Train Loss: 0.2096
Epoch 7 Step 301 Train Loss: 0.2740
Epoch 7 Step 351 Train Loss: 0.2104
Epoch 7 Step 401 Train Loss: 0.2982
Epoch 7 Step 451 Train Loss: 0.3045
Epoch 7 Step 501 Train Loss: 0.2454
Epoch 7 Step 551 Train Loss: 0.2754
Epoch 7 Step 601 Train Loss: 0.2668
Epoch 7 Step 651 Train Loss: 0.2801
Epoch 7 Step 701 Train Loss: 0.2624
Epoch 7 Step 751 Train Loss: 0.2571
Epoch 7 Step 801 Train Loss: 0.2315
Epoch 7 Step 851 Train Loss: 0.2030
Epoch 7 Step 901 Train Loss: 0.2783
Epoch 7 Step 951 Train Loss: 0.2402
Epoch 7 Step 1001 Train Loss: 0.2210
Epoch 7 Step 1051 Train Loss: 0.2590
Epoch 7 Step 1101 Train Loss: 0.2452
Epoch 7 Step 1151 Train Loss: 0.2541
Epoch 7 Step 1201 Train Loss: 0.3111
Epoch 7 Step 1251 Train Loss: 0.3053
Epoch 7 Step 1301 Train Loss: 0.2490
Epoch 7 Step 1351 Train Loss: 0.2736
Epoch 7 Step 1401 Train Loss: 0.2940
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.0067. 
Epoch 8 Step 1 Train Loss: 0.2144
Epoch 8 Step 51 Train Loss: 0.2299
Epoch 8 Step 101 Train Loss: 0.3202
Epoch 8 Step 151 Train Loss: 0.2143
Epoch 8 Step 201 Train Loss: 0.3009
Epoch 8 Step 251 Train Loss: 0.2510
Epoch 8 Step 301 Train Loss: 0.2032
Epoch 8 Step 351 Train Loss: 0.2992
Epoch 8 Step 401 Train Loss: 0.2940
Epoch 8 Step 451 Train Loss: 0.3275
Epoch 8 Step 501 Train Loss: 0.2924
Epoch 8 Step 551 Train Loss: 0.3304
Epoch 8 Step 601 Train Loss: 0.2321
Epoch 8 Step 651 Train Loss: 0.1590
Epoch 8 Step 701 Train Loss: 0.2257
Epoch 8 Step 751 Train Loss: 0.2969
Epoch 8 Step 801 Train Loss: 0.2654
Epoch 8 Step 851 Train Loss: 0.2451
Epoch 8 Step 901 Train Loss: 0.3343
Epoch 8 Step 951 Train Loss: 0.2527
Epoch 8 Step 1001 Train Loss: 0.2575
Epoch 8 Step 1051 Train Loss: 0.2154
Epoch 8 Step 1101 Train Loss: 0.2908
Epoch 8 Step 1151 Train Loss: 0.2491
Epoch 8 Step 1201 Train Loss: 0.1934
Epoch 8 Step 1251 Train Loss: 0.2374
Epoch 8 Step 1301 Train Loss: 0.2060
Epoch 8 Step 1351 Train Loss: 0.2169
Epoch 8 Step 1401 Train Loss: 0.2611
Epoch 8: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0100 Validation Top 20 DE MSE: 0.0068. 
Epoch 9 Step 1 Train Loss: 0.2675
Epoch 9 Step 51 Train Loss: 0.3477
Epoch 9 Step 101 Train Loss: 0.2934
Epoch 9 Step 151 Train Loss: 0.2252
Epoch 9 Step 201 Train Loss: 0.3065
Epoch 9 Step 251 Train Loss: 0.2672
Epoch 9 Step 301 Train Loss: 0.2376
Epoch 9 Step 351 Train Loss: 0.3356
Epoch 9 Step 401 Train Loss: 0.3063
Epoch 9 Step 451 Train Loss: 0.1933
Epoch 9 Step 501 Train Loss: 0.2531
Epoch 9 Step 551 Train Loss: 0.2070
Epoch 9 Step 601 Train Loss: 0.2821
Epoch 9 Step 651 Train Loss: 0.1731
Epoch 9 Step 701 Train Loss: 0.2313
Epoch 9 Step 751 Train Loss: 0.2524
Epoch 9 Step 801 Train Loss: 0.2571
Epoch 9 Step 851 Train Loss: 0.2774
Epoch 9 Step 901 Train Loss: 0.2202
Epoch 9 Step 951 Train Loss: 0.2452
Epoch 9 Step 1001 Train Loss: 0.2614
Epoch 9 Step 1051 Train Loss: 0.2996
Epoch 9 Step 1101 Train Loss: 0.2676
Epoch 9 Step 1151 Train Loss: 0.2405
Epoch 9 Step 1201 Train Loss: 0.2402
Epoch 9 Step 1251 Train Loss: 0.2242
Epoch 9 Step 1301 Train Loss: 0.2949
Epoch 9 Step 1351 Train Loss: 0.3377
Epoch 9 Step 1401 Train Loss: 0.2893
Epoch 9: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0068. 
Epoch 10 Step 1 Train Loss: 0.3341
Epoch 10 Step 51 Train Loss: 0.2717
Epoch 10 Step 101 Train Loss: 0.2513
Epoch 10 Step 151 Train Loss: 0.2835
Epoch 10 Step 201 Train Loss: 0.2789
Epoch 10 Step 251 Train Loss: 0.2741
Epoch 10 Step 301 Train Loss: 0.3800
Epoch 10 Step 351 Train Loss: 0.2532
Epoch 10 Step 401 Train Loss: 0.2739
Epoch 10 Step 451 Train Loss: 0.2805
Epoch 10 Step 501 Train Loss: 0.2157
Epoch 10 Step 551 Train Loss: 0.2566
Epoch 10 Step 601 Train Loss: 0.2890
Epoch 10 Step 651 Train Loss: 0.2641
Epoch 10 Step 701 Train Loss: 0.2621
Epoch 10 Step 751 Train Loss: 0.2568
Epoch 10 Step 801 Train Loss: 0.2974
Epoch 10 Step 851 Train Loss: 0.2142
Epoch 10 Step 901 Train Loss: 0.3582
Epoch 10 Step 951 Train Loss: 0.2151
Epoch 10 Step 1001 Train Loss: 0.2358
Epoch 10 Step 1051 Train Loss: 0.2321
Epoch 10 Step 1101 Train Loss: 0.2692
Epoch 10 Step 1151 Train Loss: 0.2681
Epoch 10 Step 1201 Train Loss: 0.3103
Epoch 10 Step 1251 Train Loss: 0.2161
Epoch 10 Step 1301 Train Loss: 0.2228
Epoch 10 Step 1351 Train Loss: 0.2771
Epoch 10 Step 1401 Train Loss: 0.2738
Epoch 10: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.0068. 
Epoch 11 Step 1 Train Loss: 0.3579
Epoch 11 Step 51 Train Loss: 0.2521
Epoch 11 Step 101 Train Loss: 0.3136
Epoch 11 Step 151 Train Loss: 0.3122
Epoch 11 Step 201 Train Loss: 0.2328
Epoch 11 Step 251 Train Loss: 0.2613
Epoch 11 Step 301 Train Loss: 0.2620
Epoch 11 Step 351 Train Loss: 0.2681
Epoch 11 Step 401 Train Loss: 0.2320
Epoch 11 Step 451 Train Loss: 0.2139
Epoch 11 Step 501 Train Loss: 0.2180
Epoch 11 Step 551 Train Loss: 0.2266
Epoch 11 Step 601 Train Loss: 0.2639
Epoch 11 Step 651 Train Loss: 0.2174
Epoch 11 Step 701 Train Loss: 0.2778
Epoch 11 Step 751 Train Loss: 0.2933
Epoch 11 Step 801 Train Loss: 0.2845
Epoch 11 Step 851 Train Loss: 0.2276
Epoch 11 Step 901 Train Loss: 0.2925
Epoch 11 Step 951 Train Loss: 0.2589
Epoch 11 Step 1001 Train Loss: 0.2147
Epoch 11 Step 1051 Train Loss: 0.2026
Epoch 11 Step 1101 Train Loss: 0.2139
Epoch 11 Step 1151 Train Loss: 0.2471
Epoch 11 Step 1201 Train Loss: 0.2582
Epoch 11 Step 1251 Train Loss: 0.2945
Epoch 11 Step 1301 Train Loss: 0.3041
Epoch 11 Step 1351 Train Loss: 0.1724
Epoch 11 Step 1401 Train Loss: 0.2082
Epoch 11: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0068. 
Epoch 12 Step 1 Train Loss: 0.2815
Epoch 12 Step 51 Train Loss: 0.2483
Epoch 12 Step 101 Train Loss: 0.2535
Epoch 12 Step 151 Train Loss: 0.2703
Epoch 12 Step 201 Train Loss: 0.2255
Epoch 12 Step 251 Train Loss: 0.2809
Epoch 12 Step 301 Train Loss: 0.2931
Epoch 12 Step 351 Train Loss: 0.2384
Epoch 12 Step 401 Train Loss: 0.2393
Epoch 12 Step 451 Train Loss: 0.2574
Epoch 12 Step 501 Train Loss: 0.2052
Epoch 12 Step 551 Train Loss: 0.2359
Epoch 12 Step 601 Train Loss: 0.2825
Epoch 12 Step 651 Train Loss: 0.2306
Epoch 12 Step 701 Train Loss: 0.3118
Epoch 12 Step 751 Train Loss: 0.2122
Epoch 12 Step 801 Train Loss: 0.2656
Epoch 12 Step 851 Train Loss: 0.2864
Epoch 12 Step 901 Train Loss: 0.2602
Epoch 12 Step 951 Train Loss: 0.2944
Epoch 12 Step 1001 Train Loss: 0.2866
Epoch 12 Step 1051 Train Loss: 0.2808
Epoch 12 Step 1101 Train Loss: 0.2390
Epoch 12 Step 1151 Train Loss: 0.2686
Epoch 12 Step 1201 Train Loss: 0.2644
Epoch 12 Step 1251 Train Loss: 0.2610
Epoch 12 Step 1301 Train Loss: 0.2046
Epoch 12 Step 1351 Train Loss: 0.2786
Epoch 12 Step 1401 Train Loss: 0.3185
Epoch 12: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0102 Validation Top 20 DE MSE: 0.0068. 
Epoch 13 Step 1 Train Loss: 0.1928
Epoch 13 Step 51 Train Loss: 0.3670
Epoch 13 Step 101 Train Loss: 0.2752
Epoch 13 Step 151 Train Loss: 0.3363
Epoch 13 Step 201 Train Loss: 0.2503
Epoch 13 Step 251 Train Loss: 0.2226
Epoch 13 Step 301 Train Loss: 0.2305
Epoch 13 Step 351 Train Loss: 0.1681
Epoch 13 Step 401 Train Loss: 0.2850
Epoch 13 Step 451 Train Loss: 0.2173
Epoch 13 Step 501 Train Loss: 0.2196
Epoch 13 Step 551 Train Loss: 0.2252
Epoch 13 Step 601 Train Loss: 0.2420
Epoch 13 Step 651 Train Loss: 0.2260
Epoch 13 Step 701 Train Loss: 0.2379
Epoch 13 Step 751 Train Loss: 0.2745
Epoch 13 Step 801 Train Loss: 0.2349
Epoch 13 Step 851 Train Loss: 0.2564
Epoch 13 Step 901 Train Loss: 0.2647
Epoch 13 Step 951 Train Loss: 0.2560
Epoch 13 Step 1001 Train Loss: 0.2861
Epoch 13 Step 1051 Train Loss: 0.2441
Epoch 13 Step 1101 Train Loss: 0.2603
Epoch 13 Step 1151 Train Loss: 0.2432
Epoch 13 Step 1201 Train Loss: 0.2679
Epoch 13 Step 1251 Train Loss: 0.1259
Epoch 13 Step 1301 Train Loss: 0.3218
Epoch 13 Step 1351 Train Loss: 0.2140
Epoch 13 Step 1401 Train Loss: 0.3003
Epoch 13: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0099 Validation Top 20 DE MSE: 0.0069. 
Epoch 14 Step 1 Train Loss: 0.2658
Epoch 14 Step 51 Train Loss: 0.2551
Epoch 14 Step 101 Train Loss: 0.3166
Epoch 14 Step 151 Train Loss: 0.2051
Epoch 14 Step 201 Train Loss: 0.2137
Epoch 14 Step 251 Train Loss: 0.2588
Epoch 14 Step 301 Train Loss: 0.2545
Epoch 14 Step 351 Train Loss: 0.2520
Epoch 14 Step 401 Train Loss: 0.3049
Epoch 14 Step 451 Train Loss: 0.3580
Epoch 14 Step 501 Train Loss: 0.2463
Epoch 14 Step 551 Train Loss: 0.2400
Epoch 14 Step 601 Train Loss: 0.2324
Epoch 14 Step 651 Train Loss: 0.3073
Epoch 14 Step 701 Train Loss: 0.3051
Epoch 14 Step 751 Train Loss: 0.2598
Epoch 14 Step 801 Train Loss: 0.3339
Epoch 14 Step 851 Train Loss: 0.2308
Epoch 14 Step 901 Train Loss: 0.3275
Epoch 14 Step 951 Train Loss: 0.1953
Epoch 14 Step 1001 Train Loss: 0.2806
Epoch 14 Step 1051 Train Loss: 0.2694
Epoch 14 Step 1101 Train Loss: 0.2586
Epoch 14 Step 1151 Train Loss: 0.2514
Epoch 14 Step 1201 Train Loss: 0.1805
Epoch 14 Step 1251 Train Loss: 0.2381
Epoch 14 Step 1301 Train Loss: 0.2974
Epoch 14 Step 1351 Train Loss: 0.2562
Epoch 14 Step 1401 Train Loss: 0.2262
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.0068. 
Epoch 15 Step 1 Train Loss: 0.2089
Epoch 15 Step 51 Train Loss: 0.2698
Epoch 15 Step 101 Train Loss: 0.3176
Epoch 15 Step 151 Train Loss: 0.3002
Epoch 15 Step 201 Train Loss: 0.3107
Epoch 15 Step 251 Train Loss: 0.2104
Epoch 15 Step 301 Train Loss: 0.2578
Epoch 15 Step 351 Train Loss: 0.3659
Epoch 15 Step 401 Train Loss: 0.2507
Epoch 15 Step 451 Train Loss: 0.2790
Epoch 15 Step 501 Train Loss: 0.2076
Epoch 15 Step 551 Train Loss: 0.2807
Epoch 15 Step 601 Train Loss: 0.3034
Epoch 15 Step 651 Train Loss: 0.2843
Epoch 15 Step 701 Train Loss: 0.2458
Epoch 15 Step 751 Train Loss: 0.1728
Epoch 15 Step 801 Train Loss: 0.2267
Epoch 15 Step 851 Train Loss: 0.2920
Epoch 15 Step 901 Train Loss: 0.2485
Epoch 15 Step 951 Train Loss: 0.2412
Epoch 15 Step 1001 Train Loss: 0.3292
Epoch 15 Step 1051 Train Loss: 0.2214
Epoch 15 Step 1101 Train Loss: 0.3011
Epoch 15 Step 1151 Train Loss: 0.2950
Epoch 15 Step 1201 Train Loss: 0.2768
Epoch 15 Step 1251 Train Loss: 0.2739
Epoch 15 Step 1301 Train Loss: 0.2443
Epoch 15 Step 1351 Train Loss: 0.2059
Epoch 15 Step 1401 Train Loss: 0.2962
Epoch 15: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.0069. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0115
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0013400584
test_unseen_single_pearson: 0.993893473433549
test_unseen_single_mse_de: 0.011476876
test_unseen_single_pearson_de: 0.9306341140576242
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.8959029076972268
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9400000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.014329444
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.009 MB of 0.033 MB uploadedwandb: / 0.033 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñà‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñà‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñá
wandb:                                                   val_de_mse ‚ñà‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01148
wandb:                                              test_de_pearson 0.93063
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 0.94
wandb:                                                     test_mse 0.00134
wandb:                                test_mse_top20_de_non_dropout 0.01433
wandb:                                                 test_pearson 0.99389
wandb:                                           test_pearson_delta 0.8959
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.94
wandb:                                       test_unseen_single_mse 0.00134
wandb:                                    test_unseen_single_mse_de 0.01148
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01433
wandb:                                   test_unseen_single_pearson 0.99389
wandb:                                test_unseen_single_pearson_de 0.93063
wandb:                             test_unseen_single_pearson_delta 0.8959
wandb:                                                 train_de_mse 0.01009
wandb:                                             train_de_pearson 0.9978
wandb:                                                    train_mse 0.00107
wandb:                                                train_pearson 0.99517
wandb:                                                training_loss 0.28619
wandb:                                                   val_de_mse 0.00686
wandb:                                               val_de_pearson 0.99774
wandb:                                                      val_mse 0.00126
wandb:                                                  val_pearson 0.99421
wandb: 
wandb: üöÄ View run geneformer_ShifrutMarson2018_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/iqc1j1uj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_085258-iqc1j1uj/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_091616-xbo4rbur
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/xbo4rbur
wandb: WARNING Serializing object of type ndarray that is 20480128 bytes
  0%|                                                  | 0/4744 [00:00<?, ?it/s]  0%|                                          | 5/4744 [00:00<01:44, 45.15it/s]  0%|                                         | 11/4744 [00:00<01:33, 50.55it/s]  0%|‚ñè                                        | 17/4744 [00:00<01:30, 52.48it/s]  1%|‚ñè                                        | 24/4744 [00:00<01:28, 53.17it/s]  1%|‚ñé                                        | 31/4744 [00:00<01:21, 57.68it/s]  1%|‚ñé                                        | 37/4744 [00:00<01:22, 56.94it/s]  1%|‚ñé                                        | 43/4744 [00:00<01:22, 56.89it/s]  1%|‚ñç                                        | 49/4744 [00:00<01:23, 56.56it/s]  1%|‚ñç                                        | 55/4744 [00:01<01:29, 52.32it/s]  1%|‚ñå                                        | 61/4744 [00:01<01:39, 46.90it/s]  1%|‚ñå                                        | 66/4744 [00:01<01:45, 44.16it/s]  1%|‚ñå                                        | 71/4744 [00:01<01:50, 42.24it/s]  2%|‚ñã                                        | 76/4744 [00:01<01:52, 41.66it/s]  2%|‚ñã                                        | 81/4744 [00:01<01:53, 41.00it/s]  2%|‚ñã                                        | 86/4744 [00:01<01:57, 39.76it/s]  2%|‚ñä                                        | 91/4744 [00:01<01:59, 38.89it/s]  2%|‚ñä                                        | 96/4744 [00:02<01:55, 40.40it/s]  2%|‚ñä                                       | 101/4744 [00:02<01:52, 41.39it/s]  2%|‚ñâ                                       | 108/4744 [00:02<01:37, 47.38it/s]  2%|‚ñâ                                       | 114/4744 [00:02<01:34, 49.04it/s]  3%|‚ñà                                       | 120/4744 [00:02<01:31, 50.31it/s]  3%|‚ñà                                       | 126/4744 [00:02<01:34, 48.89it/s]  3%|‚ñà                                       | 133/4744 [00:02<01:27, 52.93it/s]  3%|‚ñà‚ñè                                      | 139/4744 [00:02<01:27, 52.89it/s]  3%|‚ñà‚ñè                                      | 145/4744 [00:02<01:27, 52.56it/s]  3%|‚ñà‚ñé                                      | 151/4744 [00:03<01:31, 50.45it/s]  3%|‚ñà‚ñé                                      | 158/4744 [00:03<01:24, 54.34it/s]  3%|‚ñà‚ñç                                      | 164/4744 [00:03<01:24, 54.14it/s]  4%|‚ñà‚ñç                                      | 170/4744 [00:03<01:24, 54.18it/s]  4%|‚ñà‚ñç                                      | 176/4744 [00:03<01:28, 51.55it/s]  4%|‚ñà‚ñå                                      | 183/4744 [00:03<01:23, 54.90it/s]  4%|‚ñà‚ñå                                      | 189/4744 [00:03<01:24, 54.17it/s]  4%|‚ñà‚ñã                                      | 195/4744 [00:03<01:24, 54.02it/s]  4%|‚ñà‚ñã                                      | 201/4744 [00:04<01:28, 51.20it/s]  4%|‚ñà‚ñä                                      | 208/4744 [00:04<01:22, 54.69it/s]  5%|‚ñà‚ñä                                      | 214/4744 [00:04<01:23, 54.51it/s]  5%|‚ñà‚ñä                                      | 220/4744 [00:04<01:23, 54.43it/s]  5%|‚ñà‚ñâ                                      | 226/4744 [00:04<01:27, 51.71it/s]  5%|‚ñà‚ñâ                                      | 233/4744 [00:04<01:21, 55.05it/s]  5%|‚ñà‚ñà                                      | 239/4744 [00:04<01:22, 54.49it/s]  5%|‚ñà‚ñà                                      | 245/4744 [00:04<01:22, 54.27it/s]  5%|‚ñà‚ñà                                      | 251/4744 [00:04<01:27, 51.35it/s]  5%|‚ñà‚ñà‚ñè                                     | 258/4744 [00:05<01:21, 54.94it/s]  6%|‚ñà‚ñà‚ñè                                     | 264/4744 [00:05<01:21, 54.66it/s]  6%|‚ñà‚ñà‚ñé                                     | 270/4744 [00:05<01:21, 54.90it/s]  6%|‚ñà‚ñà‚ñé                                     | 276/4744 [00:05<01:25, 52.56it/s]  6%|‚ñà‚ñà‚ñç                                     | 283/4744 [00:05<01:19, 55.97it/s]  6%|‚ñà‚ñà‚ñç                                     | 289/4744 [00:05<01:19, 55.96it/s]  6%|‚ñà‚ñà‚ñç                                     | 295/4744 [00:05<01:19, 55.82it/s]  6%|‚ñà‚ñà‚ñå                                     | 301/4744 [00:05<01:23, 52.99it/s]  6%|‚ñà‚ñà‚ñå                                     | 308/4744 [00:05<01:17, 56.97it/s]  7%|‚ñà‚ñà‚ñã                                     | 314/4744 [00:06<01:18, 56.79it/s]  7%|‚ñà‚ñà‚ñã                                     | 320/4744 [00:06<01:18, 56.46it/s]  7%|‚ñà‚ñà‚ñã                                     | 326/4744 [00:06<01:22, 53.52it/s]  7%|‚ñà‚ñà‚ñä                                     | 333/4744 [00:06<01:17, 56.65it/s]  7%|‚ñà‚ñà‚ñä                                     | 339/4744 [00:06<01:17, 56.60it/s]  7%|‚ñà‚ñà‚ñâ                                     | 345/4744 [00:06<01:18, 56.24it/s]  7%|‚ñà‚ñà‚ñâ                                     | 351/4744 [00:06<01:18, 56.05it/s]  8%|‚ñà‚ñà‚ñà                                     | 357/4744 [00:06<01:18, 56.15it/s]  8%|‚ñà‚ñà‚ñà                                     | 363/4744 [00:06<01:18, 55.61it/s]  8%|‚ñà‚ñà‚ñà                                     | 369/4744 [00:07<01:18, 55.66it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 375/4744 [00:07<01:18, 55.79it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 381/4744 [00:07<01:18, 55.79it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 387/4744 [00:07<01:18, 55.32it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 393/4744 [00:07<01:18, 55.48it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 399/4744 [00:07<01:22, 52.83it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 405/4744 [00:07<01:20, 53.66it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 412/4744 [00:07<01:16, 56.72it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 418/4744 [00:07<01:16, 56.63it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 424/4744 [00:08<01:16, 56.44it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 430/4744 [00:08<01:16, 56.04it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 436/4744 [00:08<01:17, 55.67it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 442/4744 [00:08<01:17, 55.81it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 448/4744 [00:08<01:17, 55.42it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 454/4744 [00:08<01:17, 55.09it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 460/4744 [00:08<01:17, 55.25it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 466/4744 [00:08<01:17, 55.21it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 472/4744 [00:08<01:16, 56.06it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 478/4744 [00:09<01:17, 55.05it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 484/4744 [00:09<01:21, 52.45it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 490/4744 [00:09<01:23, 50.97it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 496/4744 [00:09<01:24, 50.05it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 502/4744 [00:09<01:25, 49.50it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 507/4744 [00:09<01:25, 49.28it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 513/4744 [00:09<01:23, 50.56it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 519/4744 [00:09<01:20, 52.59it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 525/4744 [00:09<01:17, 54.27it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 531/4744 [00:10<01:16, 54.95it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 537/4744 [00:10<01:15, 55.60it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 543/4744 [00:10<01:14, 56.09it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 549/4744 [00:10<01:14, 56.66it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 555/4744 [00:10<01:17, 54.16it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 562/4744 [00:10<01:12, 57.86it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 568/4744 [00:10<01:12, 57.96it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 574/4744 [00:10<01:11, 58.06it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 580/4744 [00:10<01:15, 54.97it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 587/4744 [00:11<01:11, 58.36it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 593/4744 [00:11<01:11, 57.85it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 599/4744 [00:11<01:11, 57.85it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 605/4744 [00:11<01:15, 55.02it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 612/4744 [00:11<01:10, 58.73it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 618/4744 [00:11<01:10, 58.48it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 624/4744 [00:11<01:10, 58.47it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 630/4744 [00:11<01:14, 55.32it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 637/4744 [00:11<01:09, 58.67it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 643/4744 [00:12<01:10, 58.27it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 649/4744 [00:12<01:16, 53.70it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 655/4744 [00:12<01:22, 49.86it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 662/4744 [00:12<01:16, 53.02it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 668/4744 [00:12<01:15, 53.81it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 674/4744 [00:12<01:22, 49.37it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 680/4744 [00:12<01:20, 50.19it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 686/4744 [00:12<01:19, 51.09it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 692/4744 [00:13<01:21, 49.87it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 698/4744 [00:13<01:23, 48.18it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 703/4744 [00:13<01:28, 45.80it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 708/4744 [00:13<01:26, 46.77it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 714/4744 [00:13<01:20, 50.14it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 720/4744 [00:13<01:22, 48.96it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 727/4744 [00:13<01:15, 52.87it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 734/4744 [00:13<01:12, 55.58it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 741/4744 [00:13<01:10, 56.99it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 747/4744 [00:14<01:16, 52.57it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 754/4744 [00:14<01:11, 56.12it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 761/4744 [00:14<01:08, 57.93it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 767/4744 [00:14<01:12, 54.97it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 773/4744 [00:14<01:13, 54.17it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 780/4744 [00:14<01:09, 57.42it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 787/4744 [00:14<01:07, 58.98it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 793/4744 [00:14<01:11, 55.14it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 799/4744 [00:15<01:12, 54.25it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 806/4744 [00:15<01:09, 56.26it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 812/4744 [00:15<01:12, 54.49it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 818/4744 [00:15<01:14, 52.69it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 824/4744 [00:15<01:16, 51.25it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 830/4744 [00:15<01:22, 47.26it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 835/4744 [00:15<01:21, 47.77it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 840/4744 [00:15<01:31, 42.52it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 845/4744 [00:16<01:29, 43.41it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 850/4744 [00:16<01:30, 42.98it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 855/4744 [00:16<01:32, 42.18it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 860/4744 [00:16<01:32, 41.84it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 865/4744 [00:16<01:29, 43.53it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 870/4744 [00:16<01:30, 42.63it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 875/4744 [00:16<01:29, 43.30it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 881/4744 [00:16<01:26, 44.71it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 886/4744 [00:16<01:30, 42.86it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 891/4744 [00:17<01:38, 39.13it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 896/4744 [00:17<01:35, 40.33it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 901/4744 [00:17<01:31, 41.96it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 907/4744 [00:17<01:26, 44.42it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 913/4744 [00:17<01:28, 43.08it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 919/4744 [00:17<01:24, 45.18it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 924/4744 [00:17<01:27, 43.75it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 929/4744 [00:17<01:24, 45.05it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 934/4744 [00:18<01:33, 40.69it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 939/4744 [00:18<01:31, 41.39it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 944/4744 [00:18<01:32, 41.23it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 949/4744 [00:18<01:33, 40.67it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 954/4744 [00:18<01:33, 40.40it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 959/4744 [00:18<01:34, 40.22it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 964/4744 [00:18<01:31, 41.30it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 969/4744 [00:18<01:30, 41.53it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 974/4744 [00:19<01:30, 41.49it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 979/4744 [00:19<01:32, 40.74it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 984/4744 [00:19<01:33, 40.39it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 989/4744 [00:19<01:29, 42.03it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 994/4744 [00:19<01:25, 43.71it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 1001/4744 [00:19<01:14, 50.18it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 1008/4744 [00:19<01:09, 54.02it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 1014/4744 [00:19<01:11, 52.27it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 1020/4744 [00:19<01:10, 53.14it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 1027/4744 [00:20<01:04, 57.75it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 1034/4744 [00:20<01:05, 56.62it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 1041/4744 [00:20<01:06, 55.96it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 1048/4744 [00:20<01:02, 59.01it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 1055/4744 [00:20<01:01, 60.24it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 1062/4744 [00:20<01:05, 56.45it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 1069/4744 [00:20<01:01, 59.66it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 1076/4744 [00:20<01:01, 59.36it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1083/4744 [00:21<01:00, 60.40it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1090/4744 [00:21<01:02, 58.23it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 1096/4744 [00:21<01:03, 57.84it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 1103/4744 [00:21<01:01, 59.03it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 1109/4744 [00:21<01:03, 56.88it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1115/4744 [00:21<01:04, 56.47it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1121/4744 [00:21<01:03, 56.68it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 1129/4744 [00:21<00:59, 60.85it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 1136/4744 [00:21<01:00, 60.02it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 1143/4744 [00:22<01:00, 59.31it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 1149/4744 [00:22<01:01, 58.50it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 1155/4744 [00:22<01:01, 58.31it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1161/4744 [00:22<01:01, 58.46it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1167/4744 [00:22<01:02, 57.20it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1173/4744 [00:22<01:02, 56.82it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1179/4744 [00:22<01:02, 57.23it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1186/4744 [00:22<01:04, 54.87it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1192/4744 [00:22<01:06, 53.46it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1198/4744 [00:23<01:12, 48.99it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1203/4744 [00:23<01:15, 46.95it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1208/4744 [00:23<01:14, 47.63it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1213/4744 [00:23<01:20, 43.76it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1218/4744 [00:23<01:20, 43.57it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1223/4744 [00:23<01:21, 43.06it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1228/4744 [00:23<01:22, 42.65it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1233/4744 [00:23<01:24, 41.55it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1238/4744 [00:24<01:23, 42.08it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1243/4744 [00:24<01:22, 42.35it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1249/4744 [00:24<01:15, 46.29it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1255/4744 [00:24<01:09, 49.89it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1261/4744 [00:24<01:09, 50.01it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1268/4744 [00:24<01:04, 53.91it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1274/4744 [00:24<01:02, 55.39it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1280/4744 [00:24<01:01, 56.60it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1286/4744 [00:24<01:03, 54.71it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1293/4744 [00:25<00:59, 57.77it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1299/4744 [00:25<00:59, 57.64it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1305/4744 [00:25<01:02, 55.25it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1311/4744 [00:25<01:03, 54.16it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1317/4744 [00:25<01:02, 54.89it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1324/4744 [00:25<00:57, 59.04it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1331/4744 [00:25<00:58, 58.75it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1337/4744 [00:25<01:05, 51.72it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1343/4744 [00:25<01:06, 50.87it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1349/4744 [00:26<01:14, 45.38it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1354/4744 [00:26<01:16, 44.14it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1359/4744 [00:26<01:18, 43.16it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1364/4744 [00:26<01:16, 44.29it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1369/4744 [00:26<01:23, 40.48it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1374/4744 [00:26<01:22, 40.64it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1379/4744 [00:26<01:19, 42.10it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1384/4744 [00:27<01:25, 39.38it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1389/4744 [00:27<01:25, 39.37it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1394/4744 [00:27<01:24, 39.50it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1399/4744 [00:27<01:23, 40.09it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1404/4744 [00:27<01:24, 39.72it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1409/4744 [00:27<01:23, 39.95it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1414/4744 [00:27<01:23, 39.91it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1419/4744 [00:27<01:22, 40.42it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1424/4744 [00:28<01:20, 41.16it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1429/4744 [00:28<01:21, 40.69it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1434/4744 [00:28<01:21, 40.59it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1439/4744 [00:28<01:22, 40.30it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1444/4744 [00:28<01:21, 40.45it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1450/4744 [00:28<01:21, 40.54it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1455/4744 [00:28<01:21, 40.54it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1460/4744 [00:28<01:20, 40.65it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1465/4744 [00:29<01:16, 42.72it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1470/4744 [00:29<01:21, 40.16it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1476/4744 [00:29<01:16, 42.61it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1481/4744 [00:29<01:22, 39.40it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1486/4744 [00:29<01:21, 39.90it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1491/4744 [00:29<01:21, 40.02it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1496/4744 [00:29<01:20, 40.58it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1501/4744 [00:29<01:20, 40.14it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1506/4744 [00:30<01:20, 40.18it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1511/4744 [00:30<01:19, 40.77it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1516/4744 [00:30<01:19, 40.62it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1521/4744 [00:30<01:18, 41.24it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1526/4744 [00:30<01:19, 40.66it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1531/4744 [00:30<01:19, 40.33it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1536/4744 [00:30<01:18, 40.73it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1541/4744 [00:30<01:18, 40.59it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1546/4744 [00:31<01:15, 42.37it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1554/4744 [00:31<01:03, 50.42it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1560/4744 [00:31<01:00, 52.85it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1566/4744 [00:31<00:58, 54.29it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1573/4744 [00:31<00:56, 56.45it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1580/4744 [00:31<00:54, 57.95it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1586/4744 [00:31<00:54, 57.78it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1593/4744 [00:31<00:53, 58.72it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1600/4744 [00:31<00:52, 59.83it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1607/4744 [00:32<00:52, 60.24it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1614/4744 [00:32<00:55, 56.47it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1622/4744 [00:32<00:51, 60.29it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1629/4744 [00:32<00:50, 61.08it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1636/4744 [00:32<00:51, 60.09it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1643/4744 [00:32<00:52, 59.27it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1650/4744 [00:32<00:51, 59.91it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1657/4744 [00:32<00:51, 60.20it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1664/4744 [00:32<00:54, 56.23it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1670/4744 [00:33<00:59, 51.40it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1676/4744 [00:33<01:04, 47.72it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1681/4744 [00:33<01:06, 45.79it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1686/4744 [00:33<01:07, 45.37it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1691/4744 [00:33<01:08, 44.84it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1696/4744 [00:33<01:09, 44.09it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1701/4744 [00:33<01:11, 42.58it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1706/4744 [00:34<01:12, 42.08it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1711/4744 [00:34<01:11, 42.50it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1716/4744 [00:34<01:10, 42.91it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1721/4744 [00:34<01:10, 42.63it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1726/4744 [00:34<01:12, 41.74it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1731/4744 [00:34<01:13, 41.20it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1736/4744 [00:34<01:11, 41.92it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1741/4744 [00:34<01:10, 42.67it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1746/4744 [00:34<01:10, 42.42it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1751/4744 [00:35<01:12, 41.39it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1756/4744 [00:35<01:12, 41.25it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1761/4744 [00:35<01:10, 42.27it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1766/4744 [00:35<01:09, 42.78it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1771/4744 [00:35<01:10, 42.35it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1776/4744 [00:35<01:08, 43.61it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1781/4744 [00:35<01:14, 40.01it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1786/4744 [00:35<01:11, 41.22it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1791/4744 [00:36<01:10, 42.14it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1796/4744 [00:36<01:09, 42.18it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1801/4744 [00:36<01:10, 41.48it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1806/4744 [00:36<01:11, 41.28it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1811/4744 [00:36<01:09, 42.27it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1816/4744 [00:36<01:08, 42.56it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1821/4744 [00:36<01:08, 42.87it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1826/4744 [00:36<01:09, 41.91it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1831/4744 [00:36<01:10, 41.48it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1836/4744 [00:37<01:08, 42.27it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1842/4744 [00:37<01:04, 45.17it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1848/4744 [00:37<00:59, 48.66it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1855/4744 [00:37<00:54, 52.63it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1862/4744 [00:37<00:52, 55.00it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1868/4744 [00:37<00:51, 56.00it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1874/4744 [00:37<00:50, 57.06it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1881/4744 [00:37<00:48, 58.77it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1887/4744 [00:37<00:48, 59.04it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1893/4744 [00:38<00:49, 58.10it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1899/4744 [00:38<00:52, 54.32it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1905/4744 [00:38<00:53, 53.44it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1911/4744 [00:38<00:55, 50.94it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1917/4744 [00:38<00:56, 49.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1923/4744 [00:38<00:58, 48.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1928/4744 [00:38<00:58, 48.45it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1934/4744 [00:38<00:57, 49.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1939/4744 [00:39<01:03, 44.30it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1945/4744 [00:39<01:02, 45.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1950/4744 [00:39<01:03, 44.15it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1955/4744 [00:39<01:05, 42.90it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1960/4744 [00:39<01:05, 42.73it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1965/4744 [00:39<01:03, 43.50it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1970/4744 [00:39<01:04, 42.76it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1975/4744 [00:39<01:04, 42.72it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1980/4744 [00:40<01:05, 42.14it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1985/4744 [00:40<01:02, 43.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1990/4744 [00:40<01:04, 42.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1995/4744 [00:40<01:05, 42.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2000/4744 [00:40<01:05, 42.13it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2005/4744 [00:40<01:05, 41.61it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 2010/4744 [00:40<01:05, 41.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 2015/4744 [00:40<01:03, 43.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 2020/4744 [00:40<01:04, 42.34it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2026/4744 [00:41<01:00, 44.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2031/4744 [00:41<01:06, 40.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2036/4744 [00:41<01:04, 42.03it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2041/4744 [00:41<01:03, 42.65it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2046/4744 [00:41<01:00, 44.39it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2051/4744 [00:41<01:06, 40.76it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2056/4744 [00:41<01:05, 40.83it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2061/4744 [00:41<01:03, 42.04it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2066/4744 [00:42<01:03, 42.30it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2071/4744 [00:42<01:04, 41.68it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2076/4744 [00:42<01:03, 41.84it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2081/4744 [00:42<01:04, 41.30it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 2086/4744 [00:42<01:02, 42.43it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 2091/4744 [00:42<01:02, 42.55it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 2096/4744 [00:42<01:03, 41.91it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2101/4744 [00:42<01:03, 41.55it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2106/4744 [00:43<01:03, 41.49it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2112/4744 [00:43<00:57, 46.00it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2118/4744 [00:43<00:53, 49.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2125/4744 [00:43<00:49, 52.87it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 2132/4744 [00:43<00:47, 55.48it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 2138/4744 [00:43<00:46, 55.99it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2144/4744 [00:43<00:46, 55.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2150/4744 [00:43<00:45, 56.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2157/4744 [00:43<00:44, 58.20it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2163/4744 [00:43<00:44, 58.14it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2169/4744 [00:44<00:43, 58.64it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2175/4744 [00:44<00:43, 58.61it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2182/4744 [00:44<00:42, 59.68it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2188/4744 [00:44<00:43, 59.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2195/4744 [00:44<00:42, 59.55it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2201/4744 [00:44<00:42, 59.47it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 2207/4744 [00:44<00:42, 59.55it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 2213/4744 [00:44<00:42, 59.22it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2220/4744 [00:44<00:42, 59.54it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2226/4744 [00:45<00:42, 59.52it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2233/4744 [00:45<00:41, 60.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 2240/4744 [00:45<00:42, 59.44it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 2246/4744 [00:45<00:41, 59.53it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2253/4744 [00:45<00:41, 60.22it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2260/4744 [00:45<00:41, 60.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2267/4744 [00:45<00:41, 59.56it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2274/4744 [00:45<00:41, 59.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2281/4744 [00:45<00:40, 60.56it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2288/4744 [00:46<00:40, 59.99it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2295/4744 [00:46<00:40, 60.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2302/4744 [00:46<00:40, 60.30it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2309/4744 [00:46<00:40, 60.61it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2316/4744 [00:46<00:40, 59.47it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2322/4744 [00:46<00:40, 59.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2329/4744 [00:46<00:40, 60.04it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2336/4744 [00:46<00:40, 59.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2342/4744 [00:46<00:41, 57.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2348/4744 [00:47<00:44, 53.32it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2354/4744 [00:47<00:46, 51.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2360/4744 [00:47<00:47, 50.25it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2366/4744 [00:47<00:48, 48.59it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2371/4744 [00:47<00:49, 48.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2376/4744 [00:47<00:49, 48.05it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2381/4744 [00:47<00:52, 45.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2386/4744 [00:47<00:52, 45.01it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2391/4744 [00:48<00:53, 43.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2396/4744 [00:48<00:55, 42.53it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2401/4744 [00:48<00:56, 41.68it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2406/4744 [00:48<00:56, 41.41it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2411/4744 [00:48<00:55, 42.38it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2416/4744 [00:48<00:54, 42.64it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2421/4744 [00:48<00:55, 42.10it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2426/4744 [00:48<00:55, 41.58it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2431/4744 [00:49<00:56, 41.27it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2436/4744 [00:49<00:54, 42.20it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2441/4744 [00:49<00:53, 42.68it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2446/4744 [00:49<00:54, 41.98it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2452/4744 [00:49<00:52, 44.06it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2457/4744 [00:49<00:53, 43.02it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2462/4744 [00:49<00:54, 41.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2467/4744 [00:49<00:54, 41.60it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2473/4744 [00:50<00:49, 45.44it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2480/4744 [00:50<00:45, 50.30it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2486/4744 [00:50<00:43, 52.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2492/4744 [00:50<00:41, 53.83it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2498/4744 [00:50<00:40, 55.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2504/4744 [00:50<00:39, 56.03it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2510/4744 [00:50<00:42, 52.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2516/4744 [00:50<00:41, 53.32it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2522/4744 [00:50<00:40, 55.13it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2529/4744 [00:50<00:38, 57.00it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2535/4744 [00:51<00:38, 57.18it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2541/4744 [00:51<00:38, 56.75it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2548/4744 [00:51<00:38, 57.12it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2554/4744 [00:51<00:39, 55.15it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2560/4744 [00:51<00:42, 51.75it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2566/4744 [00:51<00:45, 48.35it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2571/4744 [00:51<00:47, 45.86it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2577/4744 [00:51<00:46, 46.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2582/4744 [00:52<00:51, 41.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2587/4744 [00:52<00:49, 43.38it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2592/4744 [00:52<00:50, 42.82it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2597/4744 [00:52<01:18, 27.25it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2608/4744 [00:52<00:50, 42.30it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2614/4744 [00:52<00:48, 43.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2620/4744 [00:53<01:11, 29.55it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2632/4744 [00:53<00:49, 42.64it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2638/4744 [00:53<00:48, 43.55it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2644/4744 [00:53<01:08, 30.64it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2656/4744 [00:54<00:48, 43.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2663/4744 [00:54<00:47, 43.74it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2669/4744 [00:54<00:47, 43.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2675/4744 [00:54<00:46, 44.36it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2681/4744 [00:54<00:49, 41.33it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2686/4744 [00:54<00:48, 42.31it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2691/4744 [00:54<00:48, 42.52it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2696/4744 [00:54<00:48, 42.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2701/4744 [00:55<01:11, 28.65it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2712/4744 [00:55<00:46, 43.35it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2718/4744 [00:55<00:46, 43.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2724/4744 [00:55<00:43, 46.29it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2730/4744 [00:56<01:12, 27.66it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2756/4744 [00:56<00:31, 63.92it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2767/4744 [00:56<00:31, 62.41it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2777/4744 [00:56<00:31, 62.16it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2786/4744 [00:56<00:32, 60.92it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2794/4744 [00:56<00:32, 60.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2802/4744 [00:56<00:33, 57.28it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2809/4744 [00:57<00:34, 55.61it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2816/4744 [00:57<00:35, 54.09it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2822/4744 [00:57<00:36, 53.21it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2828/4744 [00:57<00:36, 52.65it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2834/4744 [00:57<00:36, 52.07it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2840/4744 [00:57<00:36, 51.47it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2846/4744 [00:57<00:37, 51.25it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2852/4744 [00:57<00:36, 51.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2858/4744 [00:58<00:36, 51.79it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2864/4744 [00:58<00:36, 51.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2870/4744 [00:58<00:36, 51.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2876/4744 [00:58<00:36, 51.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2882/4744 [00:58<00:36, 51.02it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2888/4744 [00:58<00:36, 50.44it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2894/4744 [00:58<00:36, 50.31it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2900/4744 [00:58<00:36, 50.65it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2906/4744 [00:59<00:36, 50.49it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2912/4744 [00:59<00:36, 50.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2918/4744 [00:59<00:36, 50.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2924/4744 [00:59<00:35, 50.58it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2930/4744 [00:59<00:35, 51.10it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2936/4744 [00:59<00:35, 50.85it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2942/4744 [00:59<00:35, 50.92it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2948/4744 [00:59<00:35, 51.10it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2954/4744 [00:59<00:34, 52.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2960/4744 [01:00<00:38, 46.33it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2965/4744 [01:00<00:37, 47.13it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2970/4744 [01:00<00:41, 42.27it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2975/4744 [01:00<01:02, 28.15it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2986/4744 [01:00<00:40, 42.99it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2992/4744 [01:00<00:40, 42.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2998/4744 [01:01<00:39, 43.80it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 3004/4744 [01:01<00:42, 40.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 3009/4744 [01:01<00:41, 41.92it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3014/4744 [01:01<00:40, 42.27it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3019/4744 [01:01<00:40, 42.11it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3024/4744 [01:01<00:39, 43.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 3029/4744 [01:01<00:43, 39.63it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 3034/4744 [01:01<00:42, 40.50it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 3039/4744 [01:02<00:41, 41.32it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 3044/4744 [01:02<00:41, 41.03it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 3051/4744 [01:02<00:35, 47.03it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3058/4744 [01:02<00:33, 50.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3064/4744 [01:02<00:31, 52.57it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3070/4744 [01:02<00:30, 54.11it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 3077/4744 [01:02<00:29, 56.68it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 3083/4744 [01:02<00:29, 56.97it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 3089/4744 [01:02<00:29, 57.05it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 3095/4744 [01:03<00:28, 57.41it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3102/4744 [01:03<00:27, 58.65it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3108/4744 [01:03<00:28, 58.34it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3114/4744 [01:03<00:28, 58.00it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3120/4744 [01:03<00:27, 58.18it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3127/4744 [01:03<00:27, 59.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 3133/4744 [01:03<00:27, 58.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 3139/4744 [01:03<00:27, 58.31it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 3145/4744 [01:03<00:27, 58.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 3152/4744 [01:04<00:26, 59.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 3158/4744 [01:04<00:26, 59.11it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 3164/4744 [01:04<00:26, 58.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 3170/4744 [01:04<00:26, 58.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 3177/4744 [01:04<00:26, 59.73it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3183/4744 [01:04<00:26, 58.89it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3189/4744 [01:04<00:26, 58.44it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 3195/4744 [01:04<00:26, 58.26it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 3202/4744 [01:04<00:25, 59.90it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 3208/4744 [01:04<00:25, 59.19it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3214/4744 [01:05<00:27, 55.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3220/4744 [01:05<00:30, 50.76it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 3226/4744 [01:05<00:30, 50.44it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 3232/4744 [01:05<00:30, 49.47it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 3237/4744 [01:05<00:31, 48.60it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3242/4744 [01:05<00:31, 47.63it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3248/4744 [01:05<00:29, 50.65it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 3254/4744 [01:05<00:28, 52.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 3260/4744 [01:06<00:27, 53.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 3266/4744 [01:06<00:27, 54.31it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3272/4744 [01:06<00:26, 55.91it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3278/4744 [01:06<00:48, 30.10it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 3285/4744 [01:06<00:44, 33.14it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 3294/4744 [01:06<00:35, 40.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3313/4744 [01:07<00:21, 67.53it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 3322/4744 [01:07<00:31, 45.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 3339/4744 [01:07<00:21, 64.25it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3349/4744 [01:07<00:22, 62.26it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3358/4744 [01:07<00:22, 60.88it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 3366/4744 [01:08<00:22, 60.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 3373/4744 [01:08<00:22, 59.92it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 3380/4744 [01:08<00:38, 35.68it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 3398/4744 [01:08<00:24, 56.05it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3407/4744 [01:08<00:25, 52.51it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3414/4744 [01:09<00:26, 49.38it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3421/4744 [01:09<00:27, 48.65it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 3427/4744 [01:09<00:27, 47.91it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 3433/4744 [01:09<00:29, 44.97it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 3438/4744 [01:09<00:29, 44.32it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 3443/4744 [01:09<00:29, 43.66it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 3448/4744 [01:09<00:30, 43.19it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3453/4744 [01:09<00:28, 44.58it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3459/4744 [01:10<00:27, 46.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3464/4744 [01:10<00:27, 47.11it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 3469/4744 [01:10<00:41, 30.48it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3488/4744 [01:10<00:20, 61.17it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3497/4744 [01:10<00:20, 60.64it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3505/4744 [01:10<00:20, 60.11it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3512/4744 [01:11<00:20, 59.74it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3519/4744 [01:11<00:20, 59.38it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3526/4744 [01:11<00:41, 29.47it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 3534/4744 [01:11<00:33, 35.63it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3556/4744 [01:11<00:18, 64.00it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 3566/4744 [01:12<00:21, 54.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3574/4744 [01:12<00:22, 52.60it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3581/4744 [01:12<00:24, 47.97it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3587/4744 [01:12<00:25, 46.17it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3593/4744 [01:12<00:25, 44.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3598/4744 [01:12<00:25, 44.68it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3603/4744 [01:13<00:26, 43.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3609/4744 [01:13<00:26, 42.97it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3614/4744 [01:13<00:26, 43.14it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3619/4744 [01:13<00:26, 42.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3624/4744 [01:13<00:25, 43.21it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3629/4744 [01:13<00:26, 42.78it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3635/4744 [01:14<00:46, 23.96it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3654/4744 [01:14<00:22, 48.71it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3662/4744 [01:14<00:23, 46.25it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3669/4744 [01:14<00:23, 45.16it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3675/4744 [01:14<00:23, 44.70it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3681/4744 [01:14<00:23, 45.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3687/4744 [01:15<00:25, 41.85it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3692/4744 [01:15<00:25, 41.52it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3697/4744 [01:15<00:24, 42.88it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3702/4744 [01:15<00:23, 43.49it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3707/4744 [01:15<00:24, 43.04it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3713/4744 [01:15<00:24, 42.24it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3718/4744 [01:15<00:24, 41.69it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3723/4744 [01:15<00:23, 43.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3728/4744 [01:16<00:23, 43.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3733/4744 [01:16<00:23, 42.71it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3738/4744 [01:16<00:23, 42.22it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3743/4744 [01:16<00:22, 44.15it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3748/4744 [01:16<00:23, 42.01it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3754/4744 [01:16<00:21, 45.74it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3760/4744 [01:16<00:19, 49.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3767/4744 [01:16<00:18, 53.05it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3774/4744 [01:16<00:17, 55.20it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3780/4744 [01:17<00:17, 56.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3786/4744 [01:17<00:17, 54.40it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3792/4744 [01:17<00:18, 51.53it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3798/4744 [01:17<00:18, 50.40it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3804/4744 [01:17<00:19, 47.93it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3809/4744 [01:17<00:19, 47.20it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3814/4744 [01:17<00:20, 44.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3819/4744 [01:17<00:21, 43.82it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3824/4744 [01:18<00:34, 26.34it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3835/4744 [01:18<00:32, 27.85it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3854/4744 [01:18<00:17, 50.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3862/4744 [01:18<00:17, 50.18it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3869/4744 [01:19<00:18, 46.42it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3875/4744 [01:19<00:19, 45.20it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3881/4744 [01:19<00:18, 46.16it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3887/4744 [01:19<00:18, 47.15it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3893/4744 [01:19<00:17, 47.84it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3899/4744 [01:19<00:17, 48.39it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3905/4744 [01:19<00:17, 48.76it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3911/4744 [01:20<00:16, 49.17it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3917/4744 [01:20<00:17, 47.25it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3922/4744 [01:20<00:17, 47.59it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3927/4744 [01:20<00:18, 45.10it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3932/4744 [01:20<00:18, 43.76it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3937/4744 [01:20<00:18, 44.01it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3943/4744 [01:20<00:16, 47.40it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3949/4744 [01:20<00:15, 50.61it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3955/4744 [01:20<00:15, 52.54it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3962/4744 [01:21<00:14, 55.20it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3968/4744 [01:21<00:13, 56.45it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3974/4744 [01:21<00:13, 56.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3980/4744 [01:21<00:13, 56.93it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3987/4744 [01:21<00:12, 58.33it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3993/4744 [01:21<00:12, 58.58it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3999/4744 [01:21<00:12, 58.19it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 4005/4744 [01:21<00:12, 57.60it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 4011/4744 [01:22<00:22, 32.60it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 4033/4744 [01:22<00:10, 67.44it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 4043/4744 [01:22<00:10, 64.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 4052/4744 [01:22<00:11, 62.60it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 4060/4744 [01:22<00:11, 61.10it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 4068/4744 [01:22<00:11, 60.54it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 4075/4744 [01:23<00:11, 59.63it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 4082/4744 [01:23<00:11, 59.47it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 4089/4744 [01:23<00:11, 59.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 4096/4744 [01:23<00:10, 58.93it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 4103/4744 [01:23<00:10, 58.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 4109/4744 [01:23<00:10, 58.44it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 4115/4744 [01:23<00:10, 58.54it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4121/4744 [01:23<00:10, 58.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4127/4744 [01:23<00:10, 57.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4133/4744 [01:24<00:19, 31.92it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 4150/4744 [01:24<00:10, 56.01it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4158/4744 [01:24<00:10, 53.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4165/4744 [01:24<00:11, 51.69it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4172/4744 [01:24<00:11, 49.88it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4178/4744 [01:25<00:11, 47.46it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4184/4744 [01:25<00:11, 47.29it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4190/4744 [01:25<00:11, 47.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4196/4744 [01:25<00:11, 46.44it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4201/4744 [01:25<00:11, 46.43it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4206/4744 [01:25<00:11, 47.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4211/4744 [01:25<00:11, 47.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4217/4744 [01:25<00:10, 48.62it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4222/4744 [01:25<00:10, 48.85it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4227/4744 [01:26<00:10, 48.83it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4232/4744 [01:26<00:10, 49.10it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4237/4744 [01:26<00:10, 49.06it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4243/4744 [01:26<00:10, 49.73it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4249/4744 [01:26<00:09, 50.27it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4255/4744 [01:26<00:09, 50.76it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4261/4744 [01:26<00:09, 51.39it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4267/4744 [01:26<00:09, 48.95it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4272/4744 [01:26<00:10, 46.80it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 4277/4744 [01:27<00:10, 43.41it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 4282/4744 [01:27<00:10, 44.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 4287/4744 [01:27<00:10, 42.23it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 4292/4744 [01:27<00:10, 41.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 4297/4744 [01:27<00:10, 40.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 4303/4744 [01:27<00:10, 42.29it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 4308/4744 [01:27<00:10, 42.05it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 4314/4744 [01:27<00:10, 42.13it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4319/4744 [01:28<00:09, 43.97it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4324/4744 [01:28<00:10, 41.61it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4329/4744 [01:28<00:09, 41.85it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4334/4744 [01:28<00:09, 42.65it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4339/4744 [01:28<00:09, 41.48it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4345/4744 [01:28<00:09, 41.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4350/4744 [01:28<00:09, 42.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4355/4744 [01:28<00:08, 44.21it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4360/4744 [01:29<00:09, 41.65it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 4365/4744 [01:29<00:09, 40.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 4370/4744 [01:29<00:08, 41.75it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 4375/4744 [01:29<00:08, 42.61it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4380/4744 [01:29<00:08, 44.04it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4387/4744 [01:29<00:07, 50.68it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4393/4744 [01:29<00:06, 52.82it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 4399/4744 [01:29<00:06, 54.28it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 4405/4744 [01:29<00:06, 55.30it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 4411/4744 [01:30<00:05, 55.96it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 4418/4744 [01:30<00:05, 57.28it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 4424/4744 [01:30<00:05, 56.81it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 4430/4744 [01:30<00:05, 57.28it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 4436/4744 [01:30<00:05, 57.65it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 4443/4744 [01:30<00:05, 58.59it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 4449/4744 [01:30<00:05, 58.08it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 4455/4744 [01:30<00:04, 58.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 4461/4744 [01:30<00:04, 58.88it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 4468/4744 [01:31<00:04, 59.63it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 4474/4744 [01:31<00:04, 58.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 4480/4744 [01:31<00:04, 59.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 4487/4744 [01:31<00:04, 59.56it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 4494/4744 [01:31<00:04, 59.89it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 4500/4744 [01:31<00:04, 58.93it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4507/4744 [01:31<00:03, 59.40it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4513/4744 [01:31<00:03, 59.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 4519/4744 [01:31<00:03, 59.39it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 4525/4744 [01:31<00:03, 58.33it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4532/4744 [01:32<00:03, 59.13it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4538/4744 [01:32<00:03, 59.15it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4544/4744 [01:32<00:03, 59.16it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4550/4744 [01:32<00:03, 58.25it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4557/4744 [01:32<00:03, 59.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4563/4744 [01:32<00:03, 59.25it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4569/4744 [01:32<00:02, 58.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4575/4744 [01:32<00:02, 57.94it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4581/4744 [01:32<00:02, 56.23it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4589/4744 [01:33<00:02, 57.78it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 4596/4744 [01:33<00:02, 60.49it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 4603/4744 [01:33<00:02, 59.26it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4609/4744 [01:33<00:02, 59.27it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4616/4744 [01:33<00:02, 59.69it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4622/4744 [01:33<00:02, 59.09it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4628/4744 [01:33<00:01, 58.39it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4634/4744 [01:33<00:01, 58.76it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4641/4744 [01:33<00:01, 59.57it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4647/4744 [01:34<00:01, 59.26it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4653/4744 [01:34<00:01, 58.57it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4659/4744 [01:34<00:01, 58.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4666/4744 [01:34<00:01, 59.20it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4672/4744 [01:34<00:01, 58.67it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4678/4744 [01:34<00:01, 58.16it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4684/4744 [01:34<00:01, 58.24it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4691/4744 [01:34<00:00, 59.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4697/4744 [01:34<00:00, 58.35it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 4703/4744 [01:35<00:00, 57.98it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 4709/4744 [01:35<00:00, 58.25it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4715/4744 [01:35<00:00, 56.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4721/4744 [01:35<00:00, 30.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 4740/4744 [01:35<00:00, 57.13it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4744/4744 [01:35<00:00, 49.51it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.3274
Epoch 1 Step 51 Train Loss: 0.3676
Epoch 1 Step 101 Train Loss: 0.3259
Epoch 1: Train Overall MSE: 0.0099 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0847 Validation Top 20 DE MSE: 0.1163. 
Epoch 2 Step 1 Train Loss: 0.3674
Epoch 2 Step 51 Train Loss: 0.3651
Epoch 2 Step 101 Train Loss: 0.3042
Epoch 2: Train Overall MSE: 0.0092 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0752 Validation Top 20 DE MSE: 0.0495. 
Epoch 3 Step 1 Train Loss: 0.2743
Epoch 3 Step 51 Train Loss: 0.3095
Epoch 3 Step 101 Train Loss: 0.3316
Epoch 3: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0418 Validation Top 20 DE MSE: 0.0353. 
Epoch 4 Step 1 Train Loss: 0.2810
Epoch 4 Step 51 Train Loss: 0.3172
Epoch 4 Step 101 Train Loss: 0.3075
Epoch 4: Train Overall MSE: 0.0082 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0486 Validation Top 20 DE MSE: 0.0557. 
Epoch 5 Step 1 Train Loss: 0.3494
Epoch 5 Step 51 Train Loss: 0.3366
Epoch 5 Step 101 Train Loss: 0.2831
Epoch 5: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0447 Validation Top 20 DE MSE: 0.0596. 
Epoch 6 Step 1 Train Loss: 0.3181
Epoch 6 Step 51 Train Loss: 0.3233
Epoch 6 Step 101 Train Loss: 0.3427
Epoch 6: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0422 Validation Top 20 DE MSE: 0.0457. 
Epoch 7 Step 1 Train Loss: 0.3206
Epoch 7 Step 51 Train Loss: 0.3150
Epoch 7 Step 101 Train Loss: 0.2718
Epoch 7: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0425 Validation Top 20 DE MSE: 0.0516. 
Epoch 8 Step 1 Train Loss: 0.2758
Epoch 8 Step 51 Train Loss: 0.3104
Epoch 8 Step 101 Train Loss: 0.3185
Epoch 8: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0401 Validation Top 20 DE MSE: 0.0403. 
Epoch 9 Step 1 Train Loss: 0.3180
Epoch 9 Step 51 Train Loss: 0.3194
Epoch 9 Step 101 Train Loss: 0.3047
Epoch 9: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0408 Validation Top 20 DE MSE: 0.0451. 
Epoch 10 Step 1 Train Loss: 0.2888
Epoch 10 Step 51 Train Loss: 0.3045
Epoch 10 Step 101 Train Loss: 0.3029
Epoch 10: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0395 Validation Top 20 DE MSE: 0.0442. 
Epoch 11 Step 1 Train Loss: 0.3153
Epoch 11 Step 51 Train Loss: 0.3044
Epoch 11 Step 101 Train Loss: 0.2884
Epoch 11: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0404 Validation Top 20 DE MSE: 0.0457. 
Epoch 12 Step 1 Train Loss: 0.3200
Epoch 12 Step 51 Train Loss: 0.3355
Epoch 12 Step 101 Train Loss: 0.3008
Epoch 12: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0398 Validation Top 20 DE MSE: 0.0422. 
Epoch 13 Step 1 Train Loss: 0.2969
Epoch 13 Step 51 Train Loss: 0.3143
Epoch 13 Step 101 Train Loss: 0.3092
Epoch 13: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0407 Validation Top 20 DE MSE: 0.0444. 
Epoch 14 Step 1 Train Loss: 0.3203
Epoch 14 Step 51 Train Loss: 0.3308
Epoch 14 Step 101 Train Loss: 0.2866
Epoch 14: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0391 Validation Top 20 DE MSE: 0.0385. 
Epoch 15 Step 1 Train Loss: 0.3127
Epoch 15 Step 51 Train Loss: 0.3305
Epoch 15 Step 101 Train Loss: 0.3223
Epoch 15: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0412 Validation Top 20 DE MSE: 0.0436. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.7738
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.009612592
test_unseen_single_pearson: 0.9664820972039637
test_unseen_single_mse_de: 0.77381647
test_unseen_single_pearson_de: 0.8795546581509095
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.2692060007616579
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5
test_unseen_single_mse_top20_de_non_dropout: 0.77381647
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.001 MB of 0.020 MB uploadedwandb: / 0.015 MB of 0.020 MB uploadedwandb: - 0.015 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.77382
wandb:                                              test_de_pearson 0.87955
wandb:               test_frac_opposite_direction_top20_non_dropout 0.25
wandb:                          test_frac_sigma_below_1_non_dropout 0.5
wandb:                                                     test_mse 0.00961
wandb:                                test_mse_top20_de_non_dropout 0.77382
wandb:                                                 test_pearson 0.96648
wandb:                                           test_pearson_delta 0.26921
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.5
wandb:                                       test_unseen_single_mse 0.00961
wandb:                                    test_unseen_single_mse_de 0.77382
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.77382
wandb:                                   test_unseen_single_pearson 0.96648
wandb:                                test_unseen_single_pearson_de 0.87955
wandb:                             test_unseen_single_pearson_delta 0.26921
wandb:                                                 train_de_mse 0.04115
wandb:                                             train_de_pearson 0.79784
wandb:                                                    train_mse 0.0081
wandb:                                                train_pearson 0.97414
wandb:                                                training_loss 0.29
wandb:                                                   val_de_mse 0.04356
wandb:                                               val_de_pearson 0.99182
wandb:                                                      val_mse 0.00245
wandb:                                                  val_pearson 0.99276
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/xbo4rbur
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_091616-xbo4rbur/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_092148-mwkdlwpn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/mwkdlwpn
wandb: WARNING Serializing object of type ndarray that is 20480128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3102
Epoch 1 Step 51 Train Loss: 0.3400
Epoch 1 Step 101 Train Loss: 0.3197
Epoch 1: Train Overall MSE: 0.0097 Validation Overall MSE: 0.0053. 
Train Top 20 DE MSE: 0.0873 Validation Top 20 DE MSE: 0.2650. 
Epoch 2 Step 1 Train Loss: 0.3240
Epoch 2 Step 51 Train Loss: 0.3575
Epoch 2 Step 101 Train Loss: 0.3237
Epoch 2: Train Overall MSE: 0.0100 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.1441 Validation Top 20 DE MSE: 0.2582. 
Epoch 3 Step 1 Train Loss: 0.3178
Epoch 3 Step 51 Train Loss: 0.3291
Epoch 3 Step 101 Train Loss: 0.3013
Epoch 3: Train Overall MSE: 0.0083 Validation Overall MSE: 0.0038. 
Train Top 20 DE MSE: 0.0447 Validation Top 20 DE MSE: 0.1814. 
Epoch 4 Step 1 Train Loss: 0.3199
Epoch 4 Step 51 Train Loss: 0.3483
Epoch 4 Step 101 Train Loss: 0.3345
Epoch 4: Train Overall MSE: 0.0078 Validation Overall MSE: 0.0040. 
Train Top 20 DE MSE: 0.0545 Validation Top 20 DE MSE: 0.1957. 
Epoch 5 Step 1 Train Loss: 0.2728
Epoch 5 Step 51 Train Loss: 0.3217
Epoch 5 Step 101 Train Loss: 0.3160
Epoch 5: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0518 Validation Top 20 DE MSE: 0.1409. 
Epoch 6 Step 1 Train Loss: 0.2845
Epoch 6 Step 51 Train Loss: 0.3206
Epoch 6 Step 101 Train Loss: 0.3269
Epoch 6: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0562 Validation Top 20 DE MSE: 0.1413. 
Epoch 7 Step 1 Train Loss: 0.2822
Epoch 7 Step 51 Train Loss: 0.3218
Epoch 7 Step 101 Train Loss: 0.3437
Epoch 7: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0469 Validation Top 20 DE MSE: 0.1335. 
Epoch 8 Step 1 Train Loss: 0.3485
Epoch 8 Step 51 Train Loss: 0.3309
Epoch 8 Step 101 Train Loss: 0.3331
Epoch 8: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0523 Validation Top 20 DE MSE: 0.1248. 
Epoch 9 Step 1 Train Loss: 0.3276
Epoch 9 Step 51 Train Loss: 0.3075
Epoch 9 Step 101 Train Loss: 0.3119
Epoch 9: Train Overall MSE: 0.0079 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0592 Validation Top 20 DE MSE: 0.1357. 
Epoch 10 Step 1 Train Loss: 0.2901
Epoch 10 Step 51 Train Loss: 0.3476
Epoch 10 Step 101 Train Loss: 0.3277
Epoch 10: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0495 Validation Top 20 DE MSE: 0.1154. 
Epoch 11 Step 1 Train Loss: 0.2955
Epoch 11 Step 51 Train Loss: 0.3119
Epoch 11 Step 101 Train Loss: 0.3136
Epoch 11: Train Overall MSE: 0.0076 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0484 Validation Top 20 DE MSE: 0.1228. 
Epoch 12 Step 1 Train Loss: 0.2878
Epoch 12 Step 51 Train Loss: 0.3034
Epoch 12 Step 101 Train Loss: 0.3161
Epoch 12: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0492 Validation Top 20 DE MSE: 0.1155. 
Epoch 13 Step 1 Train Loss: 0.3556
Epoch 13 Step 51 Train Loss: 0.2787
Epoch 13 Step 101 Train Loss: 0.2864
Epoch 13: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0497 Validation Top 20 DE MSE: 0.1222. 
Epoch 14 Step 1 Train Loss: 0.3064
Epoch 14 Step 51 Train Loss: 0.3185
Epoch 14 Step 101 Train Loss: 0.2974
Epoch 14: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0480 Validation Top 20 DE MSE: 0.1081. 
Epoch 15 Step 1 Train Loss: 0.2807
Epoch 15 Step 51 Train Loss: 0.2845
Epoch 15 Step 101 Train Loss: 0.3187
Epoch 15: Train Overall MSE: 0.0077 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0530 Validation Top 20 DE MSE: 0.1195. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0811
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0022783168
test_unseen_single_pearson: 0.9928231984598921
test_unseen_single_mse_de: 0.08108496
test_unseen_single_pearson_de: 0.9655944859922314
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.5674448367090233
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.075
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.08108495
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.009 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñá‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ
wandb:                                                   val_de_mse ‚ñà‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá
wandb:                                                      val_mse ‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.08108
wandb:                                              test_de_pearson 0.96559
wandb:               test_frac_opposite_direction_top20_non_dropout 0.075
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00228
wandb:                                test_mse_top20_de_non_dropout 0.08108
wandb:                                                 test_pearson 0.99282
wandb:                                           test_pearson_delta 0.56744
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.075
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00228
wandb:                                    test_unseen_single_mse_de 0.08108
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.08108
wandb:                                   test_unseen_single_pearson 0.99282
wandb:                                test_unseen_single_pearson_de 0.96559
wandb:                             test_unseen_single_pearson_delta 0.56744
wandb:                                                 train_de_mse 0.05301
wandb:                                             train_de_pearson 0.79726
wandb:                                                    train_mse 0.00772
wandb:                                                train_pearson 0.97466
wandb:                                                training_loss 0.3099
wandb:                                                   val_de_mse 0.11949
wandb:                                               val_de_pearson 0.95801
wandb:                                                      val_mse 0.00344
wandb:                                                  val_pearson 0.98866
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/mwkdlwpn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_092148-mwkdlwpn/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_092425-zk3qdtw9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zk3qdtw9
wandb: WARNING Serializing object of type ndarray that is 20480128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3640
Epoch 1 Step 51 Train Loss: 0.3205
Epoch 1 Step 101 Train Loss: 0.3673
Epoch 1: Train Overall MSE: 0.0036 Validation Overall MSE: 0.0417. 
Train Top 20 DE MSE: 0.1303 Validation Top 20 DE MSE: 0.1587. 
Epoch 2 Step 1 Train Loss: 0.3431
Epoch 2 Step 51 Train Loss: 0.3039
Epoch 2 Step 101 Train Loss: 0.3316
Epoch 2: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0453. 
Train Top 20 DE MSE: 0.0646 Validation Top 20 DE MSE: 0.1687. 
Epoch 3 Step 1 Train Loss: 0.3521
Epoch 3 Step 51 Train Loss: 0.3306
Epoch 3 Step 101 Train Loss: 0.3699
Epoch 3: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0430. 
Train Top 20 DE MSE: 0.0584 Validation Top 20 DE MSE: 0.1642. 
Epoch 4 Step 1 Train Loss: 0.3096
Epoch 4 Step 51 Train Loss: 0.3275
Epoch 4 Step 101 Train Loss: 0.3233
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0435. 
Train Top 20 DE MSE: 0.0423 Validation Top 20 DE MSE: 0.1682. 
Epoch 5 Step 1 Train Loss: 0.3428
Epoch 5 Step 51 Train Loss: 0.3355
Epoch 5 Step 101 Train Loss: 0.3041
Epoch 5: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0452. 
Train Top 20 DE MSE: 0.0425 Validation Top 20 DE MSE: 0.1780. 
Epoch 6 Step 1 Train Loss: 0.2857
Epoch 6 Step 51 Train Loss: 0.3011
Epoch 6 Step 101 Train Loss: 0.2970
Epoch 6: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0441. 
Train Top 20 DE MSE: 0.0367 Validation Top 20 DE MSE: 0.1705. 
Epoch 7 Step 1 Train Loss: 0.3617
Epoch 7 Step 51 Train Loss: 0.3033
Epoch 7 Step 101 Train Loss: 0.2901
Epoch 7: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0453. 
Train Top 20 DE MSE: 0.0304 Validation Top 20 DE MSE: 0.1668. 
Epoch 8 Step 1 Train Loss: 0.3350
Epoch 8 Step 51 Train Loss: 0.3057
Epoch 8 Step 101 Train Loss: 0.3264
Epoch 8: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0447. 
Train Top 20 DE MSE: 0.0308 Validation Top 20 DE MSE: 0.1681. 
Epoch 9 Step 1 Train Loss: 0.3250
Epoch 9 Step 51 Train Loss: 0.3297
Epoch 9 Step 101 Train Loss: 0.3539
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0451. 
Train Top 20 DE MSE: 0.0234 Validation Top 20 DE MSE: 0.1631. 
Epoch 10 Step 1 Train Loss: 0.3327
Epoch 10 Step 51 Train Loss: 0.3189
Epoch 10 Step 101 Train Loss: 0.3161
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0454. 
Train Top 20 DE MSE: 0.0225 Validation Top 20 DE MSE: 0.1626. 
Epoch 11 Step 1 Train Loss: 0.3095
Epoch 11 Step 51 Train Loss: 0.3024
Epoch 11 Step 101 Train Loss: 0.3194
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0447. 
Train Top 20 DE MSE: 0.0312 Validation Top 20 DE MSE: 0.1650. 
Epoch 12 Step 1 Train Loss: 0.2880
Epoch 12 Step 51 Train Loss: 0.3276
Epoch 12 Step 101 Train Loss: 0.3657
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0465. 
Train Top 20 DE MSE: 0.0229 Validation Top 20 DE MSE: 0.1623. 
Epoch 13 Step 1 Train Loss: 0.3263
Epoch 13 Step 51 Train Loss: 0.3391
Epoch 13 Step 101 Train Loss: 0.3391
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0464. 
Train Top 20 DE MSE: 0.0195 Validation Top 20 DE MSE: 0.1616. 
Epoch 14 Step 1 Train Loss: 0.3271
Epoch 14 Step 51 Train Loss: 0.3100
Epoch 14 Step 101 Train Loss: 0.3108
Epoch 14: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0450. 
Train Top 20 DE MSE: 0.0308 Validation Top 20 DE MSE: 0.1655. 
Epoch 15 Step 1 Train Loss: 0.3265
Epoch 15 Step 51 Train Loss: 0.3135
Epoch 15 Step 101 Train Loss: 0.3237
Epoch 15: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0445. 
Train Top 20 DE MSE: 0.0302 Validation Top 20 DE MSE: 0.1673. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0710
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0024150833
test_unseen_single_pearson: 0.9920049219541076
test_unseen_single_mse_de: 0.07102519
test_unseen_single_pearson_de: 0.9729672180539646
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.2239655074401273
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.07102519
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.003 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñÅ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÜ‚ñÖ
wandb:                                                  val_pearson ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.07103
wandb:                                              test_de_pearson 0.97297
wandb:               test_frac_opposite_direction_top20_non_dropout 0.4
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00242
wandb:                                test_mse_top20_de_non_dropout 0.07103
wandb:                                                 test_pearson 0.992
wandb:                                           test_pearson_delta 0.22397
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.4
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00242
wandb:                                    test_unseen_single_mse_de 0.07103
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.07103
wandb:                                   test_unseen_single_pearson 0.992
wandb:                                test_unseen_single_pearson_de 0.97297
wandb:                             test_unseen_single_pearson_delta 0.22397
wandb:                                                 train_de_mse 0.03024
wandb:                                             train_de_pearson 0.99455
wandb:                                                    train_mse 0.0018
wandb:                                                train_pearson 0.99467
wandb:                                                training_loss 0.31943
wandb:                                                   val_de_mse 0.1673
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.04453
wandb:                                                  val_pearson 0.84757
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/zk3qdtw9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_092425-zk3qdtw9/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_092656-660u0aiy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/660u0aiy
wandb: WARNING Serializing object of type ndarray that is 20480128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3080
Epoch 1 Step 51 Train Loss: 0.3242
Epoch 1: Train Overall MSE: 0.0097 Validation Overall MSE: 0.0151. 
Train Top 20 DE MSE: 0.0835 Validation Top 20 DE MSE: 1.3928. 
Epoch 2 Step 1 Train Loss: 0.3682
Epoch 2 Step 51 Train Loss: 0.3230
Epoch 2: Train Overall MSE: 0.0090 Validation Overall MSE: 0.0152. 
Train Top 20 DE MSE: 0.0762 Validation Top 20 DE MSE: 1.4165. 
Epoch 3 Step 1 Train Loss: 0.2866
Epoch 3 Step 51 Train Loss: 0.3500
Epoch 3: Train Overall MSE: 0.0089 Validation Overall MSE: 0.0152. 
Train Top 20 DE MSE: 0.0488 Validation Top 20 DE MSE: 1.3808. 
Epoch 4 Step 1 Train Loss: 0.3067
Epoch 4 Step 51 Train Loss: 0.3442
Epoch 4: Train Overall MSE: 0.0084 Validation Overall MSE: 0.0162. 
Train Top 20 DE MSE: 0.0420 Validation Top 20 DE MSE: 1.3593. 
Epoch 5 Step 1 Train Loss: 0.3358
Epoch 5 Step 51 Train Loss: 0.3209
Epoch 5: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0161. 
Train Top 20 DE MSE: 0.0406 Validation Top 20 DE MSE: 1.3452. 
Epoch 6 Step 1 Train Loss: 0.3486
Epoch 6 Step 51 Train Loss: 0.2968
Epoch 6: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0154. 
Train Top 20 DE MSE: 0.0406 Validation Top 20 DE MSE: 1.3531. 
Epoch 7 Step 1 Train Loss: 0.2834
Epoch 7 Step 51 Train Loss: 0.3253
Epoch 7: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0155. 
Train Top 20 DE MSE: 0.0405 Validation Top 20 DE MSE: 1.3506. 
Epoch 8 Step 1 Train Loss: 0.3207
Epoch 8 Step 51 Train Loss: 0.3869
Epoch 8: Train Overall MSE: 0.0102 Validation Overall MSE: 0.0156. 
Train Top 20 DE MSE: 0.0374 Validation Top 20 DE MSE: 1.3444. 
Epoch 9 Step 1 Train Loss: 0.3320
Epoch 9 Step 51 Train Loss: 0.3197
Epoch 9: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0154. 
Train Top 20 DE MSE: 0.0383 Validation Top 20 DE MSE: 1.3512. 
Epoch 10 Step 1 Train Loss: 0.3202
Epoch 10 Step 51 Train Loss: 0.3238
Epoch 10: Train Overall MSE: 0.0079 Validation Overall MSE: 0.0156. 
Train Top 20 DE MSE: 0.0385 Validation Top 20 DE MSE: 1.3446. 
Epoch 11 Step 1 Train Loss: 0.3057
Epoch 11 Step 51 Train Loss: 0.3627
Epoch 11: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0154. 
Train Top 20 DE MSE: 0.0392 Validation Top 20 DE MSE: 1.3499. 
Epoch 12 Step 1 Train Loss: 0.3082
Epoch 12 Step 51 Train Loss: 0.3042
Epoch 12: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0154. 
Train Top 20 DE MSE: 0.0391 Validation Top 20 DE MSE: 1.3504. 
Epoch 13 Step 1 Train Loss: 0.3157
Epoch 13 Step 51 Train Loss: 0.3186
Epoch 13: Train Overall MSE: 0.0078 Validation Overall MSE: 0.0157. 
Train Top 20 DE MSE: 0.0360 Validation Top 20 DE MSE: 1.3380. 
Epoch 14 Step 1 Train Loss: 0.3038
Epoch 14 Step 51 Train Loss: 0.3204
Epoch 14: Train Overall MSE: 0.0079 Validation Overall MSE: 0.0155. 
Train Top 20 DE MSE: 0.0375 Validation Top 20 DE MSE: 1.3444. 
Epoch 15 Step 1 Train Loss: 0.3092
Epoch 15 Step 51 Train Loss: 0.2956
Epoch 15: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0155. 
Train Top 20 DE MSE: 0.0389 Validation Top 20 DE MSE: 1.3482. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0144
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0022920014
test_unseen_single_pearson: 0.992608824710423
test_unseen_single_mse_de: 0.014381018
test_unseen_single_pearson_de: 0.9955747010176225
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.6643364127227193
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.0
test_unseen_single_frac_sigma_below_1_non_dropout: 1.0
test_unseen_single_mse_top20_de_non_dropout: 0.014381017
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.003 MB of 0.019 MB uploadedwandb: | 0.019 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñá‚ñÑ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÖ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñà‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ
wandb:                                                  val_pearson ‚ñá‚ñà‚ñá‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01438
wandb:                                              test_de_pearson 0.99557
wandb:               test_frac_opposite_direction_top20_non_dropout 0.0
wandb:                          test_frac_sigma_below_1_non_dropout 1.0
wandb:                                                     test_mse 0.00229
wandb:                                test_mse_top20_de_non_dropout 0.01438
wandb:                                                 test_pearson 0.99261
wandb:                                           test_pearson_delta 0.66434
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.0
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 1.0
wandb:                                       test_unseen_single_mse 0.00229
wandb:                                    test_unseen_single_mse_de 0.01438
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01438
wandb:                                   test_unseen_single_pearson 0.99261
wandb:                                test_unseen_single_pearson_de 0.99557
wandb:                             test_unseen_single_pearson_delta 0.66434
wandb:                                                 train_de_mse 0.03892
wandb:                                             train_de_pearson 0.7972
wandb:                                                    train_mse 0.00796
wandb:                                                train_pearson 0.97419
wandb:                                                training_loss 0.32221
wandb:                                                   val_de_mse 1.34816
wandb:                                               val_de_pearson 0.80421
wandb:                                                      val_mse 0.01547
wandb:                                                  val_pearson 0.94297
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/660u0aiy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_092656-660u0aiy/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_092914-ad417pbv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/ad417pbv
wandb: WARNING Serializing object of type ndarray that is 20480128 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3314
Epoch 1 Step 51 Train Loss: 0.3245
Epoch 1 Step 101 Train Loss: 0.3267
Epoch 1: Train Overall MSE: 0.0054 Validation Overall MSE: 0.0433. 
Train Top 20 DE MSE: 0.0620 Validation Top 20 DE MSE: 0.1949. 
Epoch 2 Step 1 Train Loss: 0.3367
Epoch 2 Step 51 Train Loss: 0.3423
Epoch 2 Step 101 Train Loss: 0.3465
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0417. 
Train Top 20 DE MSE: 0.0231 Validation Top 20 DE MSE: 0.1739. 
Epoch 3 Step 1 Train Loss: 0.3273
Epoch 3 Step 51 Train Loss: 0.3406
Epoch 3 Step 101 Train Loss: 0.3257
Epoch 3: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0419. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.1820. 
Epoch 4 Step 1 Train Loss: 0.3124
Epoch 4 Step 51 Train Loss: 0.3857
Epoch 4 Step 101 Train Loss: 0.3388
Epoch 4: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0424. 
Train Top 20 DE MSE: 0.0154 Validation Top 20 DE MSE: 0.1870. 
Epoch 5 Step 1 Train Loss: 0.3106
Epoch 5 Step 51 Train Loss: 0.3166
Epoch 5 Step 101 Train Loss: 0.3198
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0423. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.1858. 
Epoch 6 Step 1 Train Loss: 0.3296
Epoch 6 Step 51 Train Loss: 0.3136
Epoch 6 Step 101 Train Loss: 0.3219
Epoch 6: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0420. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.1841. 
Epoch 7 Step 1 Train Loss: 0.3174
Epoch 7 Step 51 Train Loss: 0.3397
Epoch 7 Step 101 Train Loss: 0.3009
Epoch 7: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0424. 
Train Top 20 DE MSE: 0.0105 Validation Top 20 DE MSE: 0.1860. 
Epoch 8 Step 1 Train Loss: 0.3260
Epoch 8 Step 51 Train Loss: 0.3520
Epoch 8 Step 101 Train Loss: 0.3133
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0418. 
Train Top 20 DE MSE: 0.0074 Validation Top 20 DE MSE: 0.1832. 
Epoch 9 Step 1 Train Loss: 0.3486
Epoch 9 Step 51 Train Loss: 0.3012
Epoch 9 Step 101 Train Loss: 0.3186
Epoch 9: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0424. 
Train Top 20 DE MSE: 0.0116 Validation Top 20 DE MSE: 0.1867. 
Epoch 10 Step 1 Train Loss: 0.3214
Epoch 10 Step 51 Train Loss: 0.3135
Epoch 10 Step 101 Train Loss: 0.3284
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0419. 
Train Top 20 DE MSE: 0.0071 Validation Top 20 DE MSE: 0.1836. 
Epoch 11 Step 1 Train Loss: 0.3168
Epoch 11 Step 51 Train Loss: 0.3217
Epoch 11 Step 101 Train Loss: 0.3107
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0425. 
Train Top 20 DE MSE: 0.0120 Validation Top 20 DE MSE: 0.1866. 
Epoch 12 Step 1 Train Loss: 0.3215
Epoch 12 Step 51 Train Loss: 0.3170
Epoch 12 Step 101 Train Loss: 0.3539
Epoch 12: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0420. 
Train Top 20 DE MSE: 0.0074 Validation Top 20 DE MSE: 0.1851. 
Epoch 13 Step 1 Train Loss: 0.3022
Epoch 13 Step 51 Train Loss: 0.2992
Epoch 13 Step 101 Train Loss: 0.3218
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0422. 
Train Top 20 DE MSE: 0.0089 Validation Top 20 DE MSE: 0.1857. 
Epoch 14 Step 1 Train Loss: 0.3250
Epoch 14 Step 51 Train Loss: 0.3212
Epoch 14 Step 101 Train Loss: 0.3064
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0421. 
Train Top 20 DE MSE: 0.0088 Validation Top 20 DE MSE: 0.1849. 
Epoch 15 Step 1 Train Loss: 0.2955
Epoch 15 Step 51 Train Loss: 0.3301
Epoch 15 Step 101 Train Loss: 0.3416
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0421. 
Train Top 20 DE MSE: 0.0078 Validation Top 20 DE MSE: 0.1853. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.7348
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00821236
test_unseen_single_pearson: 0.9706817836505048
test_unseen_single_mse_de: 0.7348015
test_unseen_single_pearson_de: 0.8893690447478926
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.36360024999129376
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5
test_unseen_single_mse_top20_de_non_dropout: 0.7348014
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.003 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.7348
wandb:                                              test_de_pearson 0.88937
wandb:               test_frac_opposite_direction_top20_non_dropout 0.25
wandb:                          test_frac_sigma_below_1_non_dropout 0.5
wandb:                                                     test_mse 0.00821
wandb:                                test_mse_top20_de_non_dropout 0.7348
wandb:                                                 test_pearson 0.97068
wandb:                                           test_pearson_delta 0.3636
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.5
wandb:                                       test_unseen_single_mse 0.00821
wandb:                                    test_unseen_single_mse_de 0.7348
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.7348
wandb:                                   test_unseen_single_pearson 0.97068
wandb:                                test_unseen_single_pearson_de 0.88937
wandb:                             test_unseen_single_pearson_delta 0.3636
wandb:                                                 train_de_mse 0.00775
wandb:                                             train_de_pearson 0.99706
wandb:                                                    train_mse 0.00158
wandb:                                                train_pearson 0.99537
wandb:                                                training_loss 0.37587
wandb:                                                   val_de_mse 0.18527
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.04213
wandb:                                                  val_pearson 0.86154
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_arrayed_RNA_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/ad417pbv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_092914-ad417pbv/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_093234-z214zxww
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_RNA_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/z214zxww
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
  0%|                                                  | 0/4752 [00:00<?, ?it/s]  0%|                                          | 4/4752 [00:00<02:03, 38.35it/s]  0%|                                          | 9/4752 [00:00<01:47, 44.08it/s]  0%|                                         | 14/4752 [00:00<01:43, 45.86it/s]  0%|‚ñè                                        | 19/4752 [00:00<01:40, 47.22it/s]  1%|‚ñè                                        | 24/4752 [00:00<01:39, 47.39it/s]  1%|‚ñé                                        | 31/4752 [00:00<01:30, 52.01it/s]  1%|‚ñé                                        | 37/4752 [00:00<01:30, 52.07it/s]  1%|‚ñé                                        | 43/4752 [00:00<01:30, 52.22it/s]  1%|‚ñç                                        | 49/4752 [00:00<01:30, 52.04it/s]  1%|‚ñç                                        | 55/4752 [00:01<01:30, 51.86it/s]  1%|‚ñå                                        | 61/4752 [00:01<01:32, 50.71it/s]  1%|‚ñå                                        | 67/4752 [00:01<01:40, 46.75it/s]  2%|‚ñå                                        | 72/4752 [00:01<01:44, 44.58it/s]  2%|‚ñã                                        | 77/4752 [00:01<01:50, 42.16it/s]  2%|‚ñã                                        | 82/4752 [00:01<01:53, 41.32it/s]  2%|‚ñä                                        | 87/4752 [00:01<01:52, 41.58it/s]  2%|‚ñä                                        | 92/4752 [00:02<01:53, 41.04it/s]  2%|‚ñä                                        | 97/4752 [00:02<01:53, 40.88it/s]  2%|‚ñä                                       | 102/4752 [00:02<01:54, 40.52it/s]  2%|‚ñâ                                       | 107/4752 [00:02<01:53, 40.81it/s]  2%|‚ñâ                                       | 112/4752 [00:02<01:54, 40.59it/s]  2%|‚ñâ                                       | 117/4752 [00:02<01:56, 39.94it/s]  3%|‚ñà                                       | 122/4752 [00:02<01:54, 40.49it/s]  3%|‚ñà                                       | 127/4752 [00:02<01:55, 40.08it/s]  3%|‚ñà                                       | 132/4752 [00:03<01:54, 40.20it/s]  3%|‚ñà‚ñè                                      | 137/4752 [00:03<01:53, 40.69it/s]  3%|‚ñà‚ñè                                      | 142/4752 [00:03<01:53, 40.54it/s]  3%|‚ñà‚ñè                                      | 147/4752 [00:03<01:51, 41.14it/s]  3%|‚ñà‚ñé                                      | 152/4752 [00:03<01:53, 40.62it/s]  3%|‚ñà‚ñé                                      | 157/4752 [00:03<01:54, 40.21it/s]  3%|‚ñà‚ñé                                      | 162/4752 [00:03<01:53, 40.33it/s]  4%|‚ñà‚ñç                                      | 167/4752 [00:03<01:53, 40.25it/s]  4%|‚ñà‚ñç                                      | 172/4752 [00:03<01:53, 40.51it/s]  4%|‚ñà‚ñç                                      | 177/4752 [00:04<01:54, 39.97it/s]  4%|‚ñà‚ñå                                      | 182/4752 [00:04<01:53, 40.15it/s]  4%|‚ñà‚ñå                                      | 187/4752 [00:04<01:52, 40.47it/s]  4%|‚ñà‚ñå                                      | 192/4752 [00:04<01:52, 40.49it/s]  4%|‚ñà‚ñã                                      | 197/4752 [00:04<01:52, 40.55it/s]  4%|‚ñà‚ñã                                      | 202/4752 [00:04<01:51, 40.65it/s]  4%|‚ñà‚ñã                                      | 207/4752 [00:04<01:52, 40.32it/s]  4%|‚ñà‚ñä                                      | 212/4752 [00:04<01:52, 40.33it/s]  5%|‚ñà‚ñä                                      | 217/4752 [00:05<01:52, 40.41it/s]  5%|‚ñà‚ñä                                      | 222/4752 [00:05<01:50, 40.83it/s]  5%|‚ñà‚ñâ                                      | 227/4752 [00:05<01:51, 40.56it/s]  5%|‚ñà‚ñâ                                      | 232/4752 [00:05<01:50, 41.06it/s]  5%|‚ñà‚ñâ                                      | 237/4752 [00:05<01:50, 40.96it/s]  5%|‚ñà‚ñà                                      | 242/4752 [00:05<01:51, 40.57it/s]  5%|‚ñà‚ñà                                      | 247/4752 [00:05<01:50, 40.78it/s]  5%|‚ñà‚ñà                                      | 252/4752 [00:05<01:49, 41.22it/s]  5%|‚ñà‚ñà‚ñè                                     | 257/4752 [00:06<01:50, 40.76it/s]  6%|‚ñà‚ñà‚ñè                                     | 262/4752 [00:06<01:51, 40.42it/s]  6%|‚ñà‚ñà‚ñè                                     | 267/4752 [00:06<01:51, 40.33it/s]  6%|‚ñà‚ñà‚ñé                                     | 272/4752 [00:06<01:50, 40.66it/s]  6%|‚ñà‚ñà‚ñé                                     | 277/4752 [00:06<01:51, 40.02it/s]  6%|‚ñà‚ñà‚ñé                                     | 282/4752 [00:06<01:50, 40.37it/s]  6%|‚ñà‚ñà‚ñç                                     | 287/4752 [00:06<01:49, 40.68it/s]  6%|‚ñà‚ñà‚ñç                                     | 292/4752 [00:06<01:50, 40.46it/s]  6%|‚ñà‚ñà‚ñå                                     | 297/4752 [00:07<01:48, 41.08it/s]  6%|‚ñà‚ñà‚ñå                                     | 302/4752 [00:07<01:49, 40.46it/s]  6%|‚ñà‚ñà‚ñå                                     | 307/4752 [00:07<01:50, 40.27it/s]  7%|‚ñà‚ñà‚ñã                                     | 312/4752 [00:07<01:50, 40.04it/s]  7%|‚ñà‚ñà‚ñã                                     | 317/4752 [00:07<01:51, 39.90it/s]  7%|‚ñà‚ñà‚ñã                                     | 322/4752 [00:07<01:49, 40.31it/s]  7%|‚ñà‚ñà‚ñä                                     | 327/4752 [00:07<01:51, 39.78it/s]  7%|‚ñà‚ñà‚ñä                                     | 332/4752 [00:07<01:49, 40.24it/s]  7%|‚ñà‚ñà‚ñä                                     | 337/4752 [00:08<01:48, 40.64it/s]  7%|‚ñà‚ñà‚ñâ                                     | 342/4752 [00:08<01:49, 40.30it/s]  7%|‚ñà‚ñà‚ñâ                                     | 347/4752 [00:08<01:47, 40.84it/s]  7%|‚ñà‚ñà‚ñâ                                     | 352/4752 [00:08<01:48, 40.72it/s]  8%|‚ñà‚ñà‚ñà                                     | 357/4752 [00:08<01:47, 41.05it/s]  8%|‚ñà‚ñà‚ñà                                     | 362/4752 [00:08<01:46, 41.22it/s]  8%|‚ñà‚ñà‚ñà                                     | 367/4752 [00:08<01:47, 40.72it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 372/4752 [00:08<01:46, 40.97it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 377/4752 [00:09<01:48, 40.37it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 382/4752 [00:09<01:48, 40.30it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 387/4752 [00:09<01:46, 40.82it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 392/4752 [00:09<01:47, 40.66it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 397/4752 [00:09<01:46, 41.04it/s]  8%|‚ñà‚ñà‚ñà‚ñç                                    | 402/4752 [00:09<01:47, 40.39it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 407/4752 [00:09<01:47, 40.35it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 412/4752 [00:09<01:46, 40.90it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 417/4752 [00:10<01:47, 40.38it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 422/4752 [00:10<01:46, 40.65it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 427/4752 [00:10<01:47, 40.20it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 432/4752 [00:10<01:45, 40.87it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 437/4752 [00:10<01:45, 41.07it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 442/4752 [00:10<01:46, 40.54it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 447/4752 [00:10<01:44, 41.15it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 452/4752 [00:10<01:45, 40.57it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 457/4752 [00:11<01:45, 40.60it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 462/4752 [00:11<01:46, 40.36it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 467/4752 [00:11<01:46, 40.08it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 472/4752 [00:11<01:45, 40.62it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 477/4752 [00:11<01:45, 40.41it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 482/4752 [00:11<01:45, 40.59it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 487/4752 [00:11<01:44, 40.73it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 492/4752 [00:11<01:45, 40.19it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 497/4752 [00:11<01:44, 40.86it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 502/4752 [00:12<01:45, 40.35it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 507/4752 [00:12<01:45, 40.27it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 512/4752 [00:12<01:44, 40.62it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 517/4752 [00:12<01:43, 40.72it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 522/4752 [00:12<01:43, 40.76it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 527/4752 [00:12<01:44, 40.24it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 532/4752 [00:12<01:44, 40.36it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 537/4752 [00:12<01:43, 40.66it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 542/4752 [00:13<01:44, 40.29it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 547/4752 [00:13<01:42, 41.04it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 552/4752 [00:13<01:43, 40.69it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 557/4752 [00:13<01:43, 40.67it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 562/4752 [00:13<01:42, 40.74it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 567/4752 [00:13<01:44, 40.15it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 572/4752 [00:13<01:43, 40.35it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 577/4752 [00:13<01:43, 40.18it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 582/4752 [00:14<01:43, 40.11it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 587/4752 [00:14<01:43, 40.34it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 592/4752 [00:14<01:43, 40.19it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 597/4752 [00:14<01:41, 40.86it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 602/4752 [00:14<01:40, 41.24it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 607/4752 [00:14<01:41, 40.75it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 612/4752 [00:14<01:41, 40.61it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 617/4752 [00:14<01:42, 40.21it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 622/4752 [00:15<01:41, 40.77it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 627/4752 [00:15<01:42, 40.28it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 632/4752 [00:15<01:41, 40.56it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 637/4752 [00:15<01:40, 40.95it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 642/4752 [00:15<01:40, 40.80it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 647/4752 [00:15<01:40, 40.89it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 652/4752 [00:15<01:40, 40.89it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 657/4752 [00:15<01:40, 40.67it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 662/4752 [00:16<01:40, 40.66it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 667/4752 [00:16<01:40, 40.46it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 672/4752 [00:16<01:39, 40.85it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 677/4752 [00:16<01:40, 40.55it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 682/4752 [00:16<01:40, 40.39it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 687/4752 [00:16<01:39, 40.72it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 692/4752 [00:16<01:40, 40.46it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 697/4752 [00:16<01:38, 41.07it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 702/4752 [00:17<01:38, 41.19it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 707/4752 [00:17<01:39, 40.74it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 712/4752 [00:17<01:39, 40.52it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 717/4752 [00:17<01:40, 40.30it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 722/4752 [00:17<01:39, 40.62it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 727/4752 [00:17<01:44, 38.53it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 731/4752 [00:17<01:44, 38.36it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 736/4752 [00:17<01:41, 39.40it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 740/4752 [00:18<01:41, 39.54it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 745/4752 [00:18<01:39, 40.11it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 750/4752 [00:18<01:37, 41.04it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 755/4752 [00:18<01:37, 40.90it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 760/4752 [00:18<01:43, 38.52it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 764/4752 [00:18<03:08, 21.11it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 769/4752 [00:19<02:37, 25.32it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 776/4752 [00:19<02:45, 23.97it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 790/4752 [00:19<01:47, 37.03it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 797/4752 [00:19<01:35, 41.58it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 802/4752 [00:19<01:38, 40.09it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 807/4752 [00:20<01:51, 35.53it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 811/4752 [00:20<01:54, 34.56it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 817/4752 [00:20<01:55, 34.04it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 821/4752 [00:20<02:18, 28.44it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 825/4752 [00:20<02:21, 27.76it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 829/4752 [00:20<02:38, 24.78it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 836/4752 [00:21<01:59, 32.70it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 840/4752 [00:21<02:07, 30.78it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 845/4752 [00:21<01:56, 33.68it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 849/4752 [00:21<02:04, 31.27it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 853/4752 [00:21<02:22, 27.45it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 857/4752 [00:21<02:35, 25.09it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 867/4752 [00:21<01:37, 40.00it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 872/4752 [00:22<01:34, 40.89it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 879/4752 [00:22<01:24, 46.03it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 885/4752 [00:22<01:20, 48.09it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 891/4752 [00:22<01:18, 49.37it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 897/4752 [00:22<01:17, 50.03it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 903/4752 [00:22<01:20, 48.11it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 909/4752 [00:22<01:17, 49.45it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 915/4752 [00:22<01:16, 50.00it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 922/4752 [00:23<01:15, 50.41it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 929/4752 [00:23<01:12, 53.07it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 935/4752 [00:23<01:11, 53.18it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 941/4752 [00:23<01:18, 48.34it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 946/4752 [00:23<01:18, 48.29it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 952/4752 [00:23<01:18, 48.67it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 957/4752 [00:23<01:27, 43.21it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 964/4752 [00:23<01:19, 47.95it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 970/4752 [00:23<01:15, 50.10it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 976/4752 [00:24<01:13, 51.48it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 982/4752 [00:24<01:11, 52.49it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 988/4752 [00:24<01:10, 53.77it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 994/4752 [00:24<01:08, 54.77it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 1000/4752 [00:24<01:07, 55.23it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 1006/4752 [00:24<01:07, 55.41it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 1012/4752 [00:24<01:10, 52.71it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 1019/4752 [00:24<01:06, 56.34it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 1025/4752 [00:24<01:09, 53.80it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 1032/4752 [00:25<01:05, 56.78it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 1038/4752 [00:25<01:05, 56.55it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 1044/4752 [00:25<01:05, 56.79it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 1050/4752 [00:25<01:21, 45.65it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 1055/4752 [00:25<01:20, 45.78it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 1062/4752 [00:25<01:15, 48.71it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 1069/4752 [00:25<01:09, 53.25it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 1075/4752 [00:25<01:11, 51.40it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1082/4752 [00:26<01:06, 54.99it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1088/4752 [00:26<01:05, 55.52it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 1094/4752 [00:26<01:05, 56.24it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 1100/4752 [00:26<01:04, 56.76it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 1106/4752 [00:26<01:05, 55.87it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1112/4752 [00:26<01:06, 54.87it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1118/4752 [00:26<01:06, 54.73it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 1124/4752 [00:26<01:06, 54.56it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 1130/4752 [00:26<01:06, 54.13it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 1136/4752 [00:27<01:06, 54.17it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 1142/4752 [00:27<01:06, 54.11it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 1148/4752 [00:27<01:07, 53.46it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 1154/4752 [00:27<01:07, 53.62it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1160/4752 [00:27<01:10, 51.14it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1167/4752 [00:27<01:09, 51.82it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1174/4752 [00:27<01:05, 54.64it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1180/4752 [00:27<01:05, 54.43it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1186/4752 [00:27<01:05, 54.36it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1192/4752 [00:28<01:05, 54.47it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1198/4752 [00:28<01:06, 53.82it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1204/4752 [00:28<01:05, 54.18it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1210/4752 [00:28<01:08, 51.45it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1217/4752 [00:28<01:07, 52.46it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1224/4752 [00:28<01:04, 54.87it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1230/4752 [00:28<01:04, 54.61it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1236/4752 [00:28<01:04, 54.54it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1242/4752 [00:29<01:04, 54.68it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1248/4752 [00:29<01:04, 53.94it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1254/4752 [00:29<01:04, 54.00it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1260/4752 [00:29<01:07, 51.59it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1267/4752 [00:29<01:06, 52.28it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1274/4752 [00:29<01:06, 52.41it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1281/4752 [00:29<01:02, 55.13it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1287/4752 [00:29<01:04, 53.89it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1293/4752 [00:30<01:10, 48.84it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1298/4752 [00:30<01:14, 46.28it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1303/4752 [00:30<01:17, 44.61it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1308/4752 [00:30<01:18, 43.65it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1313/4752 [00:30<01:21, 42.36it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1318/4752 [00:30<01:23, 41.24it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1323/4752 [00:30<01:23, 40.99it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1328/4752 [00:30<01:19, 42.89it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1333/4752 [00:31<01:23, 41.13it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1339/4752 [00:31<01:17, 44.17it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1345/4752 [00:31<01:12, 46.88it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1351/4752 [00:31<01:09, 48.64it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1356/4752 [00:31<01:23, 40.47it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1365/4752 [00:31<01:07, 50.26it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1371/4752 [00:31<01:06, 51.16it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1377/4752 [00:31<01:05, 51.75it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1383/4752 [00:31<01:03, 52.87it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1389/4752 [00:32<01:03, 52.86it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1395/4752 [00:32<01:05, 51.60it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1401/4752 [00:32<01:05, 51.13it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1407/4752 [00:32<01:04, 52.24it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1413/4752 [00:32<01:02, 53.10it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1419/4752 [00:32<01:01, 53.93it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1425/4752 [00:32<01:01, 54.20it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1431/4752 [00:32<01:01, 54.28it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1437/4752 [00:32<01:00, 54.84it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1443/4752 [00:33<01:00, 54.98it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1449/4752 [00:33<01:02, 52.75it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1456/4752 [00:33<00:58, 56.00it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1462/4752 [00:33<00:58, 55.85it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1468/4752 [00:33<00:59, 55.40it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1474/4752 [00:33<00:59, 55.47it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1480/4752 [00:33<01:14, 43.99it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1486/4752 [00:33<01:09, 46.98it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1492/4752 [00:34<01:05, 49.54it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1498/4752 [00:34<01:03, 51.38it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1504/4752 [00:34<01:02, 52.30it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1510/4752 [00:34<01:00, 53.17it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1516/4752 [00:34<01:00, 53.90it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1522/4752 [00:34<00:59, 54.57it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1528/4752 [00:34<01:05, 49.32it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1536/4752 [00:34<00:57, 56.28it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1542/4752 [00:34<00:56, 56.40it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1548/4752 [00:35<00:57, 55.76it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1554/4752 [00:35<00:57, 55.41it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1560/4752 [00:35<00:57, 55.56it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1566/4752 [00:35<00:57, 55.89it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1572/4752 [00:35<00:59, 53.08it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1579/4752 [00:35<00:56, 56.23it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1585/4752 [00:35<00:56, 55.97it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1591/4752 [00:35<00:56, 56.18it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1597/4752 [00:35<00:56, 56.03it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1603/4752 [00:36<00:56, 55.83it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1609/4752 [00:36<00:56, 55.89it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1615/4752 [00:36<00:55, 56.06it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1621/4752 [00:36<00:56, 55.44it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1627/4752 [00:36<00:56, 55.67it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1633/4752 [00:36<00:55, 56.08it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1639/4752 [00:36<00:56, 55.13it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1645/4752 [00:36<01:04, 48.36it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1654/4752 [00:37<00:53, 57.90it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1661/4752 [00:37<00:54, 56.89it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1667/4752 [00:37<00:54, 56.22it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1673/4752 [00:37<00:57, 53.56it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1680/4752 [00:37<00:54, 56.36it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1686/4752 [00:37<00:54, 56.00it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1692/4752 [00:37<00:54, 55.84it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1698/4752 [00:37<00:54, 56.08it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1704/4752 [00:37<00:54, 55.52it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1710/4752 [00:38<00:54, 55.45it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1716/4752 [00:38<00:57, 53.12it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1722/4752 [00:38<00:56, 53.79it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1729/4752 [00:38<00:56, 53.58it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1736/4752 [00:38<00:53, 56.73it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1742/4752 [00:38<00:52, 57.26it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1748/4752 [00:38<00:53, 56.44it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1754/4752 [00:38<00:53, 56.00it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1760/4752 [00:38<00:53, 55.58it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1766/4752 [00:39<00:57, 52.32it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1773/4752 [00:39<00:53, 55.31it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1779/4752 [00:39<00:53, 55.28it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1785/4752 [00:39<00:53, 55.30it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1791/4752 [00:39<00:53, 55.52it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1797/4752 [00:39<00:53, 55.61it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1803/4752 [00:39<00:53, 55.34it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1809/4752 [00:39<00:53, 54.92it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1815/4752 [00:39<00:55, 52.64it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1821/4752 [00:40<00:54, 53.62it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1828/4752 [00:40<00:52, 56.03it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1834/4752 [00:40<00:54, 53.22it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1841/4752 [00:40<00:51, 56.56it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1847/4752 [00:40<00:53, 54.62it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1853/4752 [00:40<00:53, 54.38it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1859/4752 [00:40<00:55, 52.24it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1866/4752 [00:40<00:51, 55.64it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1872/4752 [00:40<00:52, 55.05it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1878/4752 [00:41<00:52, 54.74it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1884/4752 [00:41<00:52, 55.08it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1890/4752 [00:41<00:51, 55.50it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1896/4752 [00:41<00:51, 55.05it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1902/4752 [00:41<00:52, 54.33it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1908/4752 [00:41<00:51, 54.72it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1914/4752 [00:41<00:51, 55.32it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1920/4752 [00:41<00:51, 54.63it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1926/4752 [00:41<00:51, 54.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1932/4752 [00:42<00:51, 54.41it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1938/4752 [00:42<00:55, 50.32it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1946/4752 [00:42<00:49, 56.32it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1952/4752 [00:42<00:50, 55.40it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1958/4752 [00:42<00:50, 55.28it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1964/4752 [00:42<00:54, 50.86it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1970/4752 [00:42<00:56, 49.56it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1976/4752 [00:42<00:55, 50.35it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1982/4752 [00:43<00:53, 51.68it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1988/4752 [00:43<00:52, 52.20it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1994/4752 [00:43<00:52, 52.26it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2000/4752 [00:43<00:55, 49.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 2007/4752 [00:43<00:51, 52.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 2013/4752 [00:43<00:51, 53.67it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 2019/4752 [00:43<00:53, 50.98it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2026/4752 [00:43<00:49, 54.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2032/4752 [00:43<00:49, 55.13it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 2038/4752 [00:44<00:48, 55.62it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2044/4752 [00:44<00:49, 54.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2050/4752 [00:44<00:49, 55.06it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 2056/4752 [00:44<00:49, 54.94it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2062/4752 [00:44<00:48, 55.18it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 2068/4752 [00:44<01:00, 44.13it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2078/4752 [00:44<00:47, 56.32it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 2085/4752 [00:44<00:47, 56.60it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 2091/4752 [00:45<00:47, 55.87it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 2097/4752 [00:45<00:48, 55.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2103/4752 [00:45<00:48, 55.10it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2109/4752 [00:45<00:47, 55.33it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 2115/4752 [00:45<00:50, 52.16it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2122/4752 [00:45<00:54, 48.21it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 2129/4752 [00:45<00:58, 45.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 2141/4752 [00:45<00:43, 60.37it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2148/4752 [00:46<00:44, 58.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 2155/4752 [00:46<00:53, 48.27it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2165/4752 [00:46<00:46, 55.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 2173/4752 [00:46<00:43, 59.87it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2180/4752 [00:46<00:43, 58.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 2187/4752 [00:46<00:51, 49.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2197/4752 [00:46<00:42, 59.93it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 2204/4752 [00:47<00:43, 59.03it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 2211/4752 [00:47<00:57, 43.94it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2225/4752 [00:47<00:40, 62.46it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 2233/4752 [00:47<00:41, 61.31it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 2241/4752 [00:47<00:42, 59.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 2248/4752 [00:47<00:42, 58.32it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2255/4752 [00:48<00:43, 57.78it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2262/4752 [00:48<00:43, 57.17it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 2268/4752 [00:48<00:44, 56.23it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2274/4752 [00:48<00:44, 55.60it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 2280/4752 [00:48<00:44, 55.57it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2286/4752 [00:48<00:46, 53.11it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2293/4752 [00:48<00:43, 55.92it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2299/4752 [00:48<00:44, 55.65it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2305/4752 [00:48<00:50, 48.35it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2313/4752 [00:49<00:44, 54.65it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2320/4752 [00:49<00:44, 54.78it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2326/4752 [00:49<00:52, 45.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2332/4752 [00:49<00:51, 46.59it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2339/4752 [00:49<00:48, 49.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2345/4752 [00:49<00:55, 43.01it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2353/4752 [00:50<00:55, 43.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2365/4752 [00:50<00:41, 57.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2372/4752 [00:50<00:45, 52.38it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2380/4752 [00:50<00:41, 57.30it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2387/4752 [00:50<00:43, 54.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2394/4752 [00:50<00:42, 55.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2400/4752 [00:50<00:41, 56.07it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2406/4752 [00:50<00:42, 55.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2412/4752 [00:51<00:44, 52.87it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2419/4752 [00:51<00:53, 43.91it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2430/4752 [00:51<00:40, 57.00it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2437/4752 [00:51<00:42, 54.30it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2444/4752 [00:51<00:40, 56.30it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2450/4752 [00:51<00:41, 55.81it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2456/4752 [00:51<00:41, 55.46it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2462/4752 [00:51<00:43, 52.30it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2469/4752 [00:52<00:47, 48.27it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2478/4752 [00:52<00:40, 56.41it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2484/4752 [00:52<00:40, 56.28it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2490/4752 [00:52<00:40, 55.41it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2496/4752 [00:52<00:40, 55.27it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2502/4752 [00:52<00:43, 52.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2509/4752 [00:52<00:40, 55.68it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2515/4752 [00:52<00:40, 54.93it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2521/4752 [00:53<00:40, 55.08it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2527/4752 [00:53<00:40, 55.09it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2533/4752 [00:53<00:40, 55.35it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2539/4752 [00:53<00:40, 55.07it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2545/4752 [00:53<00:40, 54.55it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2551/4752 [00:53<00:42, 52.07it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2558/4752 [00:53<00:41, 53.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2565/4752 [00:53<00:39, 56.02it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2571/4752 [00:53<00:39, 55.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2577/4752 [00:54<00:39, 55.01it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2583/4752 [00:54<00:39, 55.22it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2589/4752 [00:54<00:39, 55.16it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2595/4752 [00:54<00:41, 51.77it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2602/4752 [00:54<00:40, 52.57it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2608/4752 [00:54<00:40, 53.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2615/4752 [00:54<00:38, 56.01it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2621/4752 [00:54<00:39, 54.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2627/4752 [00:54<00:39, 54.25it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2633/4752 [00:55<00:38, 54.62it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2639/4752 [00:55<00:38, 54.59it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2645/4752 [00:55<00:39, 53.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2651/4752 [00:55<00:38, 53.94it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2657/4752 [00:55<00:38, 54.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2663/4752 [00:55<00:38, 53.88it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2669/4752 [00:55<00:39, 53.29it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2675/4752 [00:55<00:38, 53.51it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2681/4752 [00:56<00:42, 49.30it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2689/4752 [00:56<00:37, 55.50it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2695/4752 [00:56<00:37, 54.47it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2701/4752 [00:56<00:37, 54.54it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2707/4752 [00:56<00:39, 52.29it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2714/4752 [00:56<00:37, 55.03it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2720/4752 [00:56<00:37, 54.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2726/4752 [00:56<00:36, 54.88it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2732/4752 [00:56<00:36, 55.20it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2738/4752 [00:57<00:36, 54.80it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2744/4752 [00:57<00:36, 54.38it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2750/4752 [00:57<00:36, 54.55it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2756/4752 [00:57<00:36, 54.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2762/4752 [00:57<00:36, 54.85it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2768/4752 [00:57<00:46, 43.07it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2778/4752 [00:57<00:36, 54.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2784/4752 [00:57<00:35, 55.15it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2790/4752 [00:58<00:35, 54.66it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2796/4752 [00:58<00:38, 50.18it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2802/4752 [00:58<00:37, 51.34it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2809/4752 [00:58<00:35, 55.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2815/4752 [00:58<00:35, 54.45it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2821/4752 [00:58<00:35, 54.06it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2827/4752 [00:58<00:35, 54.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2833/4752 [00:58<00:35, 54.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2839/4752 [00:58<00:35, 53.93it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2845/4752 [00:59<00:35, 53.56it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2851/4752 [00:59<00:35, 53.93it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2857/4752 [00:59<00:36, 51.73it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2863/4752 [00:59<00:40, 47.04it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2872/4752 [00:59<00:34, 54.04it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2879/4752 [00:59<00:33, 56.43it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2885/4752 [00:59<00:33, 56.23it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2891/4752 [00:59<00:33, 55.39it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2897/4752 [01:00<00:33, 54.78it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2903/4752 [01:00<00:35, 51.94it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2910/4752 [01:00<00:33, 55.31it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2916/4752 [01:00<00:35, 52.37it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2923/4752 [01:00<00:33, 55.13it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2929/4752 [01:00<00:33, 54.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2935/4752 [01:00<00:33, 55.04it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2941/4752 [01:00<00:33, 54.36it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2947/4752 [01:00<00:33, 54.07it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2953/4752 [01:01<00:34, 51.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2960/4752 [01:01<00:32, 55.19it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2966/4752 [01:01<00:34, 51.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2973/4752 [01:01<00:32, 54.81it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2979/4752 [01:01<00:32, 54.28it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2985/4752 [01:01<00:32, 54.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2991/4752 [01:01<00:32, 54.26it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2997/4752 [01:01<00:32, 53.95it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 3003/4752 [01:02<00:34, 51.38it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 3010/4752 [01:02<00:31, 55.22it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3016/4752 [01:02<00:33, 51.87it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3022/4752 [01:02<00:32, 52.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 3029/4752 [01:02<00:31, 55.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 3035/4752 [01:02<00:30, 55.43it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 3041/4752 [01:02<00:31, 54.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 3047/4752 [01:02<00:31, 54.32it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 3053/4752 [01:02<00:33, 51.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 3060/4752 [01:03<00:30, 55.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3066/4752 [01:03<00:32, 52.04it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 3073/4752 [01:03<00:30, 55.42it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 3079/4752 [01:03<00:30, 54.79it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 3085/4752 [01:03<00:30, 55.34it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 3091/4752 [01:03<00:32, 51.35it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 3097/4752 [01:03<00:30, 53.57it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 3103/4752 [01:03<00:31, 52.66it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3109/4752 [01:03<00:31, 52.07it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3115/4752 [01:04<00:31, 51.63it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 3121/4752 [01:04<00:33, 48.60it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3126/4752 [01:04<00:35, 45.78it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3131/4752 [01:04<00:34, 46.58it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 3137/4752 [01:04<00:32, 49.67it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 3143/4752 [01:04<00:31, 51.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 3149/4752 [01:04<00:30, 52.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 3155/4752 [01:04<00:29, 53.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 3161/4752 [01:04<00:28, 55.02it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 3167/4752 [01:05<00:28, 55.32it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 3173/4752 [01:05<00:28, 54.54it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 3179/4752 [01:05<00:28, 54.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3185/4752 [01:05<00:30, 52.16it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3192/4752 [01:05<00:28, 54.97it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 3198/4752 [01:05<00:28, 54.55it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 3204/4752 [01:05<00:28, 54.62it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 3210/4752 [01:05<00:28, 54.67it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3216/4752 [01:06<00:28, 54.29it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3222/4752 [01:06<00:28, 53.93it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 3228/4752 [01:06<00:28, 53.99it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 3234/4752 [01:06<00:36, 42.03it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3245/4752 [01:06<00:26, 56.10it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3252/4752 [01:06<00:27, 55.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3258/4752 [01:06<00:26, 55.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 3264/4752 [01:06<00:27, 54.89it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 3270/4752 [01:07<00:27, 54.53it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3276/4752 [01:07<00:27, 54.19it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3282/4752 [01:07<00:26, 54.76it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 3288/4752 [01:07<00:26, 55.04it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 3294/4752 [01:07<00:26, 54.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 3300/4752 [01:07<00:26, 54.30it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3306/4752 [01:07<00:26, 54.66it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3312/4752 [01:07<00:26, 55.09it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 3318/4752 [01:07<00:26, 54.79it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 3324/4752 [01:08<00:26, 54.23it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 3330/4752 [01:08<00:25, 54.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 3336/4752 [01:08<00:25, 55.56it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 3342/4752 [01:08<01:09, 20.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 3347/4752 [01:09<01:01, 22.70it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3353/4752 [01:09<00:56, 24.84it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 3357/4752 [01:09<01:04, 21.75it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 3371/4752 [01:09<00:41, 33.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 3381/4752 [01:09<00:34, 39.74it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 3405/4752 [01:10<00:19, 70.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3414/4752 [01:10<00:19, 68.73it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 3423/4752 [01:10<00:20, 65.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 3431/4752 [01:10<00:21, 62.68it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 3438/4752 [01:10<00:21, 61.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 3445/4752 [01:10<00:21, 60.25it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 3452/4752 [01:10<00:23, 54.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3460/4752 [01:11<00:21, 59.77it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 3467/4752 [01:11<00:22, 56.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 3474/4752 [01:11<00:21, 59.06it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 3481/4752 [01:11<00:21, 59.15it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3488/4752 [01:11<00:21, 59.29it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3495/4752 [01:11<00:21, 58.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3501/4752 [01:11<00:21, 58.48it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3507/4752 [01:11<00:21, 58.69it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3513/4752 [01:11<00:21, 58.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3519/4752 [01:12<00:21, 58.61it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3525/4752 [01:12<00:21, 58.24it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3531/4752 [01:12<00:21, 58.03it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 3537/4752 [01:12<00:20, 58.01it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 3543/4752 [01:12<00:20, 57.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3549/4752 [01:12<00:21, 57.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3555/4752 [01:12<00:21, 54.50it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3562/4752 [01:12<00:20, 58.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 3568/4752 [01:12<00:20, 57.68it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 3574/4752 [01:12<00:20, 57.25it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3580/4752 [01:13<00:21, 54.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3587/4752 [01:13<00:19, 58.29it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3593/4752 [01:13<00:20, 57.93it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3599/4752 [01:13<00:20, 57.37it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3605/4752 [01:13<00:21, 54.61it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3612/4752 [01:13<00:19, 58.32it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3618/4752 [01:13<00:19, 57.48it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3624/4752 [01:13<00:19, 57.11it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3630/4752 [01:13<00:20, 54.88it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3636/4752 [01:14<00:20, 55.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3642/4752 [01:14<00:20, 54.79it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3648/4752 [01:14<00:23, 47.27it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3657/4752 [01:14<00:19, 57.14it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3663/4752 [01:14<00:19, 56.39it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3669/4752 [01:14<00:19, 55.71it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3675/4752 [01:14<00:19, 55.89it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3681/4752 [01:14<00:19, 56.02it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3687/4752 [01:15<00:19, 55.65it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3693/4752 [01:15<00:19, 55.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3699/4752 [01:15<00:18, 55.78it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3705/4752 [01:15<00:19, 54.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3712/4752 [01:15<00:17, 57.81it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3718/4752 [01:15<00:17, 57.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3724/4752 [01:15<00:17, 57.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3730/4752 [01:15<00:17, 57.61it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3736/4752 [01:15<00:17, 56.94it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3742/4752 [01:15<00:17, 56.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3748/4752 [01:16<00:17, 56.33it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3754/4752 [01:16<00:18, 53.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3761/4752 [01:16<00:17, 57.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3767/4752 [01:16<00:17, 56.89it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3773/4752 [01:16<00:17, 56.54it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3779/4752 [01:16<00:18, 53.96it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3786/4752 [01:16<00:17, 56.69it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3792/4752 [01:16<00:16, 56.62it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3798/4752 [01:16<00:16, 56.46it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3804/4752 [01:17<00:17, 53.49it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3811/4752 [01:17<00:16, 56.80it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3817/4752 [01:17<00:16, 56.69it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3823/4752 [01:17<00:16, 56.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3829/4752 [01:17<00:17, 53.36it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3836/4752 [01:17<00:16, 56.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3842/4752 [01:17<00:16, 56.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3848/4752 [01:17<00:16, 55.83it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3854/4752 [01:18<00:16, 53.30it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3861/4752 [01:18<00:15, 56.56it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3867/4752 [01:18<00:15, 56.55it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3873/4752 [01:18<00:15, 56.12it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3879/4752 [01:18<00:16, 53.88it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3886/4752 [01:18<00:15, 57.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3892/4752 [01:18<00:15, 57.29it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3898/4752 [01:18<00:15, 56.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3904/4752 [01:18<00:15, 54.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3911/4752 [01:19<00:14, 58.11it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3917/4752 [01:19<00:14, 57.52it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3923/4752 [01:19<00:14, 56.48it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3929/4752 [01:19<00:15, 53.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3936/4752 [01:19<00:14, 57.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3942/4752 [01:19<00:14, 56.74it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3948/4752 [01:19<00:14, 55.86it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3954/4752 [01:19<00:14, 53.61it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3961/4752 [01:19<00:13, 56.81it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3967/4752 [01:20<00:13, 56.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3973/4752 [01:20<00:14, 55.24it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3979/4752 [01:20<00:14, 53.06it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3985/4752 [01:20<00:14, 53.18it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3991/4752 [01:20<00:14, 51.41it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3997/4752 [01:20<00:14, 51.69it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 4003/4752 [01:20<00:14, 52.96it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 4009/4752 [01:20<00:13, 54.51it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 4015/4752 [01:20<00:13, 54.59it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 4021/4752 [01:21<00:13, 54.82it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 4027/4752 [01:21<00:13, 55.51it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 4033/4752 [01:21<00:12, 55.86it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 4039/4752 [01:21<00:12, 55.64it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 4045/4752 [01:21<00:16, 44.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 4056/4752 [01:21<00:11, 58.84it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 4063/4752 [01:21<00:11, 57.77it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 4070/4752 [01:21<00:12, 56.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 4076/4752 [01:22<00:12, 53.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 4082/4752 [01:22<00:12, 54.54it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 4089/4752 [01:22<00:11, 57.13it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 4095/4752 [01:22<00:12, 50.62it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 4103/4752 [01:22<00:11, 56.77it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 4109/4752 [01:22<00:11, 56.15it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 4115/4752 [01:22<00:11, 55.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 4121/4752 [01:22<00:11, 53.74it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 4127/4752 [01:22<00:11, 53.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4133/4752 [01:23<00:11, 54.60it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4139/4752 [01:23<00:11, 54.29it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 4145/4752 [01:23<00:11, 54.25it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 4151/4752 [01:23<00:11, 54.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 4157/4752 [01:23<00:10, 54.50it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4163/4752 [01:23<00:10, 54.20it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4169/4752 [01:23<00:10, 54.05it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4175/4752 [01:23<00:11, 51.62it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4182/4752 [01:23<00:10, 55.59it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4188/4752 [01:24<00:10, 55.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4194/4752 [01:24<00:10, 54.88it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4200/4752 [01:24<00:10, 55.18it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4206/4752 [01:24<00:09, 55.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4212/4752 [01:24<00:09, 55.19it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4218/4752 [01:24<00:09, 54.82it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4224/4752 [01:24<00:09, 52.95it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4231/4752 [01:24<00:09, 56.30it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4237/4752 [01:24<00:09, 56.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4243/4752 [01:25<00:09, 55.70it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4249/4752 [01:25<00:08, 55.94it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4255/4752 [01:25<00:08, 56.27it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4261/4752 [01:25<00:08, 56.36it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4267/4752 [01:25<00:08, 55.63it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4273/4752 [01:25<00:08, 55.85it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4279/4752 [01:25<00:08, 53.82it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 4286/4752 [01:25<00:08, 57.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 4292/4752 [01:25<00:08, 55.91it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 4298/4752 [01:26<00:08, 53.44it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 4305/4752 [01:26<00:07, 57.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 4311/4752 [01:26<00:07, 57.25it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 4317/4752 [01:26<00:08, 49.22it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4326/4752 [01:26<00:07, 58.66it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4333/4752 [01:26<00:07, 58.34it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4340/4752 [01:26<00:07, 56.09it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4346/4752 [01:26<00:07, 55.86it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4352/4752 [01:27<00:07, 56.18it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4358/4752 [01:27<00:07, 56.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4364/4752 [01:27<00:06, 56.13it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4370/4752 [01:27<00:06, 56.22it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 4376/4752 [01:27<00:06, 56.60it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 4382/4752 [01:27<00:06, 56.61it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4388/4752 [01:27<00:06, 56.77it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4394/4752 [01:27<00:06, 56.34it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4400/4752 [01:27<00:06, 53.89it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 4406/4752 [01:28<00:07, 49.18it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 4412/4752 [01:28<00:07, 46.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 4417/4752 [01:28<00:07, 44.80it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 4422/4752 [01:28<00:07, 42.61it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 4427/4752 [01:28<00:07, 41.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 4434/4752 [01:28<00:06, 47.51it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 4440/4752 [01:28<00:06, 49.34it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 4446/4752 [01:28<00:06, 50.27it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 4452/4752 [01:29<00:05, 50.85it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 4458/4752 [01:29<00:05, 52.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 4464/4752 [01:29<00:05, 52.75it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 4470/4752 [01:29<00:05, 52.84it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 4476/4752 [01:29<00:05, 50.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 4483/4752 [01:29<00:04, 54.49it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 4489/4752 [01:29<00:04, 54.30it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 4495/4752 [01:29<00:04, 53.58it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 4501/4752 [01:29<00:04, 53.60it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 4507/4752 [01:30<00:04, 53.77it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4513/4752 [01:30<00:04, 53.94it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4519/4752 [01:30<00:04, 53.75it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 4525/4752 [01:30<00:04, 53.69it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 4531/4752 [01:30<00:04, 53.62it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 4537/4752 [01:30<00:03, 53.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4543/4752 [01:30<00:04, 42.59it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4549/4752 [01:30<00:04, 45.32it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4555/4752 [01:31<00:04, 47.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4561/4752 [01:31<00:04, 47.16it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4568/4752 [01:31<00:03, 50.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4574/4752 [01:31<00:03, 50.74it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 4580/4752 [01:31<00:03, 50.81it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4586/4752 [01:31<00:03, 50.82it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4592/4752 [01:31<00:03, 50.68it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 4598/4752 [01:31<00:03, 50.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 4604/4752 [01:32<00:02, 49.63it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 4610/4752 [01:32<00:02, 49.92it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4616/4752 [01:32<00:02, 49.93it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4622/4752 [01:32<00:02, 50.09it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4628/4752 [01:32<00:02, 50.80it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4634/4752 [01:32<00:02, 51.49it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4640/4752 [01:32<00:02, 51.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4646/4752 [01:32<00:02, 51.76it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4652/4752 [01:32<00:01, 52.07it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4658/4752 [01:33<00:01, 52.77it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4664/4752 [01:33<00:01, 53.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4670/4752 [01:33<00:01, 53.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4676/4752 [01:33<00:01, 53.53it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4682/4752 [01:33<00:01, 53.84it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4688/4752 [01:33<00:01, 52.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4694/4752 [01:33<00:01, 52.12it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4700/4752 [01:33<00:00, 52.08it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4706/4752 [01:33<00:00, 53.27it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 4712/4752 [01:34<00:00, 53.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 4718/4752 [01:34<00:00, 53.83it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4724/4752 [01:34<00:00, 53.73it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4730/4752 [01:34<00:00, 54.24it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4736/4752 [01:34<00:00, 54.49it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 4742/4752 [01:34<00:00, 54.14it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 4748/4752 [01:34<00:00, 54.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4752/4752 [01:34<00:00, 50.13it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.7722
Epoch 1 Step 51 Train Loss: 0.7621
Epoch 1 Step 101 Train Loss: 0.7404
Epoch 1 Step 151 Train Loss: 0.8371
Epoch 1 Step 201 Train Loss: 0.7604
Epoch 1 Step 251 Train Loss: 0.8568
Epoch 1 Step 301 Train Loss: 0.7775
Epoch 1 Step 351 Train Loss: 0.8004
Epoch 1: Train Overall MSE: 0.0054 Validation Overall MSE: 0.0055. 
Train Top 20 DE MSE: 0.0857 Validation Top 20 DE MSE: 0.2301. 
Epoch 2 Step 1 Train Loss: 0.8265
Epoch 2 Step 51 Train Loss: 0.7477
Epoch 2 Step 101 Train Loss: 0.7733
Epoch 2 Step 151 Train Loss: 0.9888
Epoch 2 Step 201 Train Loss: 0.7558
Epoch 2 Step 251 Train Loss: 0.8153
Epoch 2 Step 301 Train Loss: 0.8045
Epoch 2 Step 351 Train Loss: 0.8133
Epoch 2: Train Overall MSE: 0.0029 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0673 Validation Top 20 DE MSE: 0.2557. 
Epoch 3 Step 1 Train Loss: 0.9096
Epoch 3 Step 51 Train Loss: 0.7909
Epoch 3 Step 101 Train Loss: 0.8206
Epoch 3 Step 151 Train Loss: 0.8624
Epoch 3 Step 201 Train Loss: 0.8590
Epoch 3 Step 251 Train Loss: 0.8752
Epoch 3 Step 301 Train Loss: 0.7285
Epoch 3 Step 351 Train Loss: 0.7075
Epoch 3: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0466 Validation Top 20 DE MSE: 0.2576. 
Epoch 4 Step 1 Train Loss: 0.9601
Epoch 4 Step 51 Train Loss: 0.7449
Epoch 4 Step 101 Train Loss: 0.7135
Epoch 4 Step 151 Train Loss: 0.6778
Epoch 4 Step 201 Train Loss: 0.7168
Epoch 4 Step 251 Train Loss: 0.8349
Epoch 4 Step 301 Train Loss: 0.6883
Epoch 4 Step 351 Train Loss: 0.8405
Epoch 4: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0433 Validation Top 20 DE MSE: 0.2458. 
Epoch 5 Step 1 Train Loss: 0.8053
Epoch 5 Step 51 Train Loss: 0.8637
Epoch 5 Step 101 Train Loss: 0.7834
Epoch 5 Step 151 Train Loss: 0.8367
Epoch 5 Step 201 Train Loss: 0.8102
Epoch 5 Step 251 Train Loss: 0.7522
Epoch 5 Step 301 Train Loss: 0.8193
Epoch 5 Step 351 Train Loss: 0.7973
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0230 Validation Top 20 DE MSE: 0.2512. 
Epoch 6 Step 1 Train Loss: 0.8204
Epoch 6 Step 51 Train Loss: 0.8049
Epoch 6 Step 101 Train Loss: 0.7723
Epoch 6 Step 151 Train Loss: 0.7786
Epoch 6 Step 201 Train Loss: 0.7687
Epoch 6 Step 251 Train Loss: 0.7145
Epoch 6 Step 301 Train Loss: 0.7994
Epoch 6 Step 351 Train Loss: 0.7010
Epoch 6: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0225 Validation Top 20 DE MSE: 0.2489. 
Epoch 7 Step 1 Train Loss: 0.7016
Epoch 7 Step 51 Train Loss: 0.7295
Epoch 7 Step 101 Train Loss: 0.7464
Epoch 7 Step 151 Train Loss: 0.8118
Epoch 7 Step 201 Train Loss: 0.7307
Epoch 7 Step 251 Train Loss: 0.8865
Epoch 7 Step 301 Train Loss: 0.6934
Epoch 7 Step 351 Train Loss: 0.7378
Epoch 7: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0199 Validation Top 20 DE MSE: 0.2522. 
Epoch 8 Step 1 Train Loss: 0.7643
Epoch 8 Step 51 Train Loss: 0.8868
Epoch 8 Step 101 Train Loss: 0.7509
Epoch 8 Step 151 Train Loss: 0.8783
Epoch 8 Step 201 Train Loss: 0.7838
Epoch 8 Step 251 Train Loss: 0.7837
Epoch 8 Step 301 Train Loss: 0.8465
Epoch 8 Step 351 Train Loss: 0.7582
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0259 Validation Top 20 DE MSE: 0.2521. 
Epoch 9 Step 1 Train Loss: 0.8228
Epoch 9 Step 51 Train Loss: 0.7992
Epoch 9 Step 101 Train Loss: 0.7456
Epoch 9 Step 151 Train Loss: 0.7578
Epoch 9 Step 201 Train Loss: 0.8236
Epoch 9 Step 251 Train Loss: 0.7299
Epoch 9 Step 301 Train Loss: 0.7584
Epoch 9 Step 351 Train Loss: 0.7781
Epoch 9: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0247 Validation Top 20 DE MSE: 0.2523. 
Epoch 10 Step 1 Train Loss: 0.6987
Epoch 10 Step 51 Train Loss: 0.6544
Epoch 10 Step 101 Train Loss: 0.7935
Epoch 10 Step 151 Train Loss: 0.8760
Epoch 10 Step 201 Train Loss: 0.7893
Epoch 10 Step 251 Train Loss: 0.7190
Epoch 10 Step 301 Train Loss: 0.7044
Epoch 10 Step 351 Train Loss: 0.7326
Epoch 10: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.2512. 
Epoch 11 Step 1 Train Loss: 0.6910
Epoch 11 Step 51 Train Loss: 0.8491
Epoch 11 Step 101 Train Loss: 0.6874
Epoch 11 Step 151 Train Loss: 0.8484
Epoch 11 Step 201 Train Loss: 0.8120
Epoch 11 Step 251 Train Loss: 0.7295
Epoch 11 Step 301 Train Loss: 0.9048
Epoch 11 Step 351 Train Loss: 0.6803
Epoch 11: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0228 Validation Top 20 DE MSE: 0.2514. 
Epoch 12 Step 1 Train Loss: 0.7064
Epoch 12 Step 51 Train Loss: 0.6614
Epoch 12 Step 101 Train Loss: 0.8115
Epoch 12 Step 151 Train Loss: 0.7659
Epoch 12 Step 201 Train Loss: 0.9864
Epoch 12 Step 251 Train Loss: 0.8263
Epoch 12 Step 301 Train Loss: 0.7911
Epoch 12 Step 351 Train Loss: 0.8091
Epoch 12: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0221 Validation Top 20 DE MSE: 0.2511. 
Epoch 13 Step 1 Train Loss: 0.7960
Epoch 13 Step 51 Train Loss: 0.8293
Epoch 13 Step 101 Train Loss: 0.7138
Epoch 13 Step 151 Train Loss: 0.7804
Epoch 13 Step 201 Train Loss: 0.8518
Epoch 13 Step 251 Train Loss: 0.9116
Epoch 13 Step 301 Train Loss: 0.7746
Epoch 13 Step 351 Train Loss: 0.8071
Epoch 13: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0214 Validation Top 20 DE MSE: 0.2513. 
Epoch 14 Step 1 Train Loss: 0.7696
Epoch 14 Step 51 Train Loss: 0.7086
Epoch 14 Step 101 Train Loss: 0.7111
Epoch 14 Step 151 Train Loss: 0.7970
Epoch 14 Step 201 Train Loss: 0.7963
Epoch 14 Step 251 Train Loss: 0.7526
Epoch 14 Step 301 Train Loss: 0.6843
Epoch 14 Step 351 Train Loss: 0.7881
Epoch 14: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0219 Validation Top 20 DE MSE: 0.2510. 
Epoch 15 Step 1 Train Loss: 0.7012
Epoch 15 Step 51 Train Loss: 0.6627
Epoch 15 Step 101 Train Loss: 0.7683
Epoch 15 Step 151 Train Loss: 0.8115
Epoch 15 Step 201 Train Loss: 0.7592
Epoch 15 Step 251 Train Loss: 0.7595
Epoch 15 Step 301 Train Loss: 0.8488
Epoch 15 Step 351 Train Loss: 0.8111
Epoch 15: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0219 Validation Top 20 DE MSE: 0.2507. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.3541
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.008337313
test_unseen_single_pearson: 0.9798447750025022
test_unseen_single_mse_de: 0.35405704
test_unseen_single_pearson_de: 0.8576823990943169
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1706359640006717
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2416666666666667
test_unseen_single_frac_sigma_below_1_non_dropout: 0.7916666666666666
test_unseen_single_mse_top20_de_non_dropout: 0.36487684
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.023 MB uploadedwandb: | 0.003 MB of 0.023 MB uploadedwandb: / 0.003 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ
wandb:                                                   val_de_mse ‚ñÅ‚ñà‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                               val_de_pearson ‚ñà‚ñÅ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.35406
wandb:                                              test_de_pearson 0.85768
wandb:               test_frac_opposite_direction_top20_non_dropout 0.24167
wandb:                          test_frac_sigma_below_1_non_dropout 0.79167
wandb:                                                     test_mse 0.00834
wandb:                                test_mse_top20_de_non_dropout 0.36488
wandb:                                                 test_pearson 0.97984
wandb:                                           test_pearson_delta 0.17064
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.24167
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.79167
wandb:                                       test_unseen_single_mse 0.00834
wandb:                                    test_unseen_single_mse_de 0.35406
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.36488
wandb:                                   test_unseen_single_pearson 0.97984
wandb:                                test_unseen_single_pearson_de 0.85768
wandb:                             test_unseen_single_pearson_delta 0.17064
wandb:                                                 train_de_mse 0.02185
wandb:                                             train_de_pearson 0.93065
wandb:                                                    train_mse 0.00147
wandb:                                                train_pearson 0.99649
wandb:                                                training_loss 0.73354
wandb:                                                   val_de_mse 0.25066
wandb:                                               val_de_pearson 0.95898
wandb:                                                      val_mse 0.00478
wandb:                                                  val_pearson 0.98864
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_RNA_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/z214zxww
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_093234-z214zxww/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_094435-r9ds6npy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_RNA_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/r9ds6npy
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.8251
Epoch 1 Step 51 Train Loss: 0.8616
Epoch 1 Step 101 Train Loss: 0.8513
Epoch 1 Step 151 Train Loss: 0.8444
Epoch 1 Step 201 Train Loss: 0.9381
Epoch 1 Step 251 Train Loss: 0.8738
Epoch 1 Step 301 Train Loss: 0.8576
Epoch 1 Step 351 Train Loss: 0.7335
Epoch 1: Train Overall MSE: 0.0032 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0760 Validation Top 20 DE MSE: 0.1692. 
Epoch 2 Step 1 Train Loss: 0.8712
Epoch 2 Step 51 Train Loss: 0.7802
Epoch 2 Step 101 Train Loss: 0.8081
Epoch 2 Step 151 Train Loss: 0.7565
Epoch 2 Step 201 Train Loss: 0.8644
Epoch 2 Step 251 Train Loss: 0.7676
Epoch 2 Step 301 Train Loss: 0.7546
Epoch 2 Step 351 Train Loss: 0.6836
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0428 Validation Top 20 DE MSE: 0.1773. 
Epoch 3 Step 1 Train Loss: 0.7971
Epoch 3 Step 51 Train Loss: 0.7186
Epoch 3 Step 101 Train Loss: 0.7470
Epoch 3 Step 151 Train Loss: 0.8079
Epoch 3 Step 201 Train Loss: 0.6875
Epoch 3 Step 251 Train Loss: 0.8270
Epoch 3 Step 301 Train Loss: 0.7614
Epoch 3 Step 351 Train Loss: 0.7612
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0506 Validation Top 20 DE MSE: 0.1701. 
Epoch 4 Step 1 Train Loss: 0.7259
Epoch 4 Step 51 Train Loss: 0.8040
Epoch 4 Step 101 Train Loss: 0.7357
Epoch 4 Step 151 Train Loss: 0.7972
Epoch 4 Step 201 Train Loss: 0.7695
Epoch 4 Step 251 Train Loss: 0.8334
Epoch 4 Step 301 Train Loss: 0.8744
Epoch 4 Step 351 Train Loss: 0.7909
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0439 Validation Top 20 DE MSE: 0.1695. 
Epoch 5 Step 1 Train Loss: 0.8445
Epoch 5 Step 51 Train Loss: 0.7582
Epoch 5 Step 101 Train Loss: 0.7550
Epoch 5 Step 151 Train Loss: 0.8149
Epoch 5 Step 201 Train Loss: 0.8061
Epoch 5 Step 251 Train Loss: 0.7937
Epoch 5 Step 301 Train Loss: 0.7991
Epoch 5 Step 351 Train Loss: 0.7917
Epoch 5: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0386 Validation Top 20 DE MSE: 0.1699. 
Epoch 6 Step 1 Train Loss: 0.7470
Epoch 6 Step 51 Train Loss: 0.7611
Epoch 6 Step 101 Train Loss: 0.8698
Epoch 6 Step 151 Train Loss: 0.7233
Epoch 6 Step 201 Train Loss: 0.6763
Epoch 6 Step 251 Train Loss: 0.7563
Epoch 6 Step 301 Train Loss: 0.7710
Epoch 6 Step 351 Train Loss: 0.8901
Epoch 6: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0328 Validation Top 20 DE MSE: 0.1677. 
Epoch 7 Step 1 Train Loss: 0.8298
Epoch 7 Step 51 Train Loss: 0.6937
Epoch 7 Step 101 Train Loss: 0.7173
Epoch 7 Step 151 Train Loss: 0.7450
Epoch 7 Step 201 Train Loss: 0.7930
Epoch 7 Step 251 Train Loss: 0.7155
Epoch 7 Step 301 Train Loss: 0.8775
Epoch 7 Step 351 Train Loss: 0.6977
Epoch 7: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0341 Validation Top 20 DE MSE: 0.1691. 
Epoch 8 Step 1 Train Loss: 0.8626
Epoch 8 Step 51 Train Loss: 0.8548
Epoch 8 Step 101 Train Loss: 0.6623
Epoch 8 Step 151 Train Loss: 0.7615
Epoch 8 Step 201 Train Loss: 0.7720
Epoch 8 Step 251 Train Loss: 0.8126
Epoch 8 Step 301 Train Loss: 0.7003
Epoch 8 Step 351 Train Loss: 0.8420
Epoch 8: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0336 Validation Top 20 DE MSE: 0.1696. 
Epoch 9 Step 1 Train Loss: 0.8893
Epoch 9 Step 51 Train Loss: 0.7581
Epoch 9 Step 101 Train Loss: 0.7460
Epoch 9 Step 151 Train Loss: 0.7275
Epoch 9 Step 201 Train Loss: 0.7751
Epoch 9 Step 251 Train Loss: 0.8249
Epoch 9 Step 301 Train Loss: 0.8189
Epoch 9 Step 351 Train Loss: 0.8176
Epoch 9: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0298 Validation Top 20 DE MSE: 0.1698. 
Epoch 10 Step 1 Train Loss: 0.8268
Epoch 10 Step 51 Train Loss: 0.7565
Epoch 10 Step 101 Train Loss: 0.7434
Epoch 10 Step 151 Train Loss: 0.7787
Epoch 10 Step 201 Train Loss: 0.7137
Epoch 10 Step 251 Train Loss: 0.8376
Epoch 10 Step 301 Train Loss: 0.7500
Epoch 10 Step 351 Train Loss: 0.7157
Epoch 10: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0313 Validation Top 20 DE MSE: 0.1689. 
Epoch 11 Step 1 Train Loss: 0.7081
Epoch 11 Step 51 Train Loss: 0.8852
Epoch 11 Step 101 Train Loss: 0.8274
Epoch 11 Step 151 Train Loss: 0.8388
Epoch 11 Step 201 Train Loss: 0.8505
Epoch 11 Step 251 Train Loss: 0.7307
Epoch 11 Step 301 Train Loss: 0.6866
Epoch 11 Step 351 Train Loss: 0.8165
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0276 Validation Top 20 DE MSE: 0.1695. 
Epoch 12 Step 1 Train Loss: 0.6977
Epoch 12 Step 51 Train Loss: 0.7905
Epoch 12 Step 101 Train Loss: 0.8051
Epoch 12 Step 151 Train Loss: 0.7121
Epoch 12 Step 201 Train Loss: 0.8326
Epoch 12 Step 251 Train Loss: 0.7031
Epoch 12 Step 301 Train Loss: 0.7752
Epoch 12 Step 351 Train Loss: 0.8227
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0321 Validation Top 20 DE MSE: 0.1691. 
Epoch 13 Step 1 Train Loss: 0.7831
Epoch 13 Step 51 Train Loss: 0.7712
Epoch 13 Step 101 Train Loss: 0.7949
Epoch 13 Step 151 Train Loss: 0.6876
Epoch 13 Step 201 Train Loss: 0.7353
Epoch 13 Step 251 Train Loss: 0.8499
Epoch 13 Step 301 Train Loss: 0.8972
Epoch 13 Step 351 Train Loss: 0.8613
Epoch 13: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0326 Validation Top 20 DE MSE: 0.1694. 
Epoch 14 Step 1 Train Loss: 0.7579
Epoch 14 Step 51 Train Loss: 0.7423
Epoch 14 Step 101 Train Loss: 0.8116
Epoch 14 Step 151 Train Loss: 0.7168
Epoch 14 Step 201 Train Loss: 0.9612
Epoch 14 Step 251 Train Loss: 0.8420
Epoch 14 Step 301 Train Loss: 0.6921
Epoch 14 Step 351 Train Loss: 0.7712
Epoch 14: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0285 Validation Top 20 DE MSE: 0.1699. 
Epoch 15 Step 1 Train Loss: 0.7174
Epoch 15 Step 51 Train Loss: 0.9076
Epoch 15 Step 101 Train Loss: 0.7151
Epoch 15 Step 151 Train Loss: 0.6939
Epoch 15 Step 201 Train Loss: 0.7235
Epoch 15 Step 251 Train Loss: 0.8572
Epoch 15 Step 301 Train Loss: 0.8284
Epoch 15 Step 351 Train Loss: 0.8229
Epoch 15: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0354 Validation Top 20 DE MSE: 0.1697. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.6728
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.008562098
test_unseen_single_pearson: 0.9794308173130092
test_unseen_single_mse_de: 0.67281324
test_unseen_single_pearson_de: 0.966963420447366
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.16782670315085493
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4416666666666667
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6166666666666667
test_unseen_single_mse_top20_de_non_dropout: 0.67330337
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.005 MB of 0.022 MB uploadedwandb: / 0.019 MB of 0.022 MB uploadedwandb: - 0.019 MB of 0.022 MB uploadedwandb: \ 0.019 MB of 0.022 MB uploadedwandb: | 0.019 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                             train_de_pearson ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                    train_mse ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÜ
wandb:                                                   val_de_mse ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                      val_mse ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÑ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.67281
wandb:                                              test_de_pearson 0.96696
wandb:               test_frac_opposite_direction_top20_non_dropout 0.44167
wandb:                          test_frac_sigma_below_1_non_dropout 0.61667
wandb:                                                     test_mse 0.00856
wandb:                                test_mse_top20_de_non_dropout 0.6733
wandb:                                                 test_pearson 0.97943
wandb:                                           test_pearson_delta 0.16783
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.44167
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.61667
wandb:                                       test_unseen_single_mse 0.00856
wandb:                                    test_unseen_single_mse_de 0.67281
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.6733
wandb:                                   test_unseen_single_pearson 0.97943
wandb:                                test_unseen_single_pearson_de 0.96696
wandb:                             test_unseen_single_pearson_delta 0.16783
wandb:                                                 train_de_mse 0.03542
wandb:                                             train_de_pearson 0.87126
wandb:                                                    train_mse 0.0018
wandb:                                                train_pearson 0.99574
wandb:                                                training_loss 0.78794
wandb:                                                   val_de_mse 0.16969
wandb:                                               val_de_pearson 0.95236
wandb:                                                      val_mse 0.00273
wandb:                                                  val_pearson 0.99358
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_RNA_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/r9ds6npy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_094435-r9ds6npy/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_095340-slb3210y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_RNA_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/slb3210y
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.9401
Epoch 1 Step 51 Train Loss: 0.9226
Epoch 1 Step 101 Train Loss: 0.8025
Epoch 1 Step 151 Train Loss: 0.9428
Epoch 1 Step 201 Train Loss: 0.7663
Epoch 1 Step 251 Train Loss: 0.8372
Epoch 1 Step 301 Train Loss: 0.8639
Epoch 1 Step 351 Train Loss: 0.7003
Epoch 1: Train Overall MSE: 0.0050 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0927 Validation Top 20 DE MSE: 0.0366. 
Epoch 2 Step 1 Train Loss: 0.7743
Epoch 2 Step 51 Train Loss: 0.7579
Epoch 2 Step 101 Train Loss: 0.9790
Epoch 2 Step 151 Train Loss: 0.7143
Epoch 2 Step 201 Train Loss: 0.8781
Epoch 2 Step 251 Train Loss: 0.9163
Epoch 2 Step 301 Train Loss: 0.8134
Epoch 2 Step 351 Train Loss: 0.7470
Epoch 2: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0681 Validation Top 20 DE MSE: 0.0333. 
Epoch 3 Step 1 Train Loss: 0.8573
Epoch 3 Step 51 Train Loss: 0.8698
Epoch 3 Step 101 Train Loss: 0.9001
Epoch 3 Step 151 Train Loss: 0.7489
Epoch 3 Step 201 Train Loss: 0.7414
Epoch 3 Step 251 Train Loss: 0.7056
Epoch 3 Step 301 Train Loss: 0.7946
Epoch 3 Step 351 Train Loss: 0.7302
Epoch 3: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0493 Validation Top 20 DE MSE: 0.0367. 
Epoch 4 Step 1 Train Loss: 0.7217
Epoch 4 Step 51 Train Loss: 0.7470
Epoch 4 Step 101 Train Loss: 0.7505
Epoch 4 Step 151 Train Loss: 0.7611
Epoch 4 Step 201 Train Loss: 0.7772
Epoch 4 Step 251 Train Loss: 0.7959
Epoch 4 Step 301 Train Loss: 0.8807
Epoch 4 Step 351 Train Loss: 0.9263
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0340 Validation Top 20 DE MSE: 0.0355. 
Epoch 5 Step 1 Train Loss: 0.7386
Epoch 5 Step 51 Train Loss: 0.8179
Epoch 5 Step 101 Train Loss: 0.8640
Epoch 5 Step 151 Train Loss: 0.7627
Epoch 5 Step 201 Train Loss: 0.7850
Epoch 5 Step 251 Train Loss: 0.8115
Epoch 5 Step 301 Train Loss: 0.7448
Epoch 5 Step 351 Train Loss: 0.7678
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0330 Validation Top 20 DE MSE: 0.0350. 
Epoch 6 Step 1 Train Loss: 0.7734
Epoch 6 Step 51 Train Loss: 0.7045
Epoch 6 Step 101 Train Loss: 0.6395
Epoch 6 Step 151 Train Loss: 0.6885
Epoch 6 Step 201 Train Loss: 0.7451
Epoch 6 Step 251 Train Loss: 0.7823
Epoch 6 Step 301 Train Loss: 0.6963
Epoch 6 Step 351 Train Loss: 0.7564
Epoch 6: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0367 Validation Top 20 DE MSE: 0.0354. 
Epoch 7 Step 1 Train Loss: 0.8474
Epoch 7 Step 51 Train Loss: 0.7598
Epoch 7 Step 101 Train Loss: 0.8360
Epoch 7 Step 151 Train Loss: 0.7600
Epoch 7 Step 201 Train Loss: 0.9143
Epoch 7 Step 251 Train Loss: 0.7196
Epoch 7 Step 301 Train Loss: 0.7071
Epoch 7 Step 351 Train Loss: 0.8236
Epoch 7: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0263 Validation Top 20 DE MSE: 0.0354. 
Epoch 8 Step 1 Train Loss: 0.8611
Epoch 8 Step 51 Train Loss: 0.8541
Epoch 8 Step 101 Train Loss: 0.7205
Epoch 8 Step 151 Train Loss: 0.7257
Epoch 8 Step 201 Train Loss: 0.7714
Epoch 8 Step 251 Train Loss: 0.7448
Epoch 8 Step 301 Train Loss: 0.8371
Epoch 8 Step 351 Train Loss: 0.7539
Epoch 8: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0324 Validation Top 20 DE MSE: 0.0356. 
Epoch 9 Step 1 Train Loss: 0.7494
Epoch 9 Step 51 Train Loss: 0.8037
Epoch 9 Step 101 Train Loss: 0.7105
Epoch 9 Step 151 Train Loss: 0.8796
Epoch 9 Step 201 Train Loss: 0.7533
Epoch 9 Step 251 Train Loss: 0.7968
Epoch 9 Step 301 Train Loss: 0.7961
Epoch 9 Step 351 Train Loss: 0.7708
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0301 Validation Top 20 DE MSE: 0.0358. 
Epoch 10 Step 1 Train Loss: 0.8154
Epoch 10 Step 51 Train Loss: 0.7360
Epoch 10 Step 101 Train Loss: 0.8337
Epoch 10 Step 151 Train Loss: 0.8541
Epoch 10 Step 201 Train Loss: 0.8230
Epoch 10 Step 251 Train Loss: 0.9515
Epoch 10 Step 301 Train Loss: 0.8516
Epoch 10 Step 351 Train Loss: 0.7538
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0297 Validation Top 20 DE MSE: 0.0354. 
Epoch 11 Step 1 Train Loss: 0.7259
Epoch 11 Step 51 Train Loss: 0.8247
Epoch 11 Step 101 Train Loss: 0.8120
Epoch 11 Step 151 Train Loss: 0.7107
Epoch 11 Step 201 Train Loss: 0.7950
Epoch 11 Step 251 Train Loss: 0.7855
Epoch 11 Step 301 Train Loss: 0.7557
Epoch 11 Step 351 Train Loss: 0.8448
Epoch 11: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0294 Validation Top 20 DE MSE: 0.0357. 
Epoch 12 Step 1 Train Loss: 0.7692
Epoch 12 Step 51 Train Loss: 0.8481
Epoch 12 Step 101 Train Loss: 0.6923
Epoch 12 Step 151 Train Loss: 0.7782
Epoch 12 Step 201 Train Loss: 0.7045
Epoch 12 Step 251 Train Loss: 0.7858
Epoch 12 Step 301 Train Loss: 0.8224
Epoch 12 Step 351 Train Loss: 0.8182
Epoch 12: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0241 Validation Top 20 DE MSE: 0.0354. 
Epoch 13 Step 1 Train Loss: 0.6770
Epoch 13 Step 51 Train Loss: 0.8159
Epoch 13 Step 101 Train Loss: 0.7414
Epoch 13 Step 151 Train Loss: 0.8679
Epoch 13 Step 201 Train Loss: 0.9682
Epoch 13 Step 251 Train Loss: 0.7455
Epoch 13 Step 301 Train Loss: 0.7410
Epoch 13 Step 351 Train Loss: 0.7262
Epoch 13: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0305 Validation Top 20 DE MSE: 0.0356. 
Epoch 14 Step 1 Train Loss: 0.6945
Epoch 14 Step 51 Train Loss: 0.7772
Epoch 14 Step 101 Train Loss: 0.8505
Epoch 14 Step 151 Train Loss: 0.7812
Epoch 14 Step 201 Train Loss: 0.7613
Epoch 14 Step 251 Train Loss: 0.8281
Epoch 14 Step 301 Train Loss: 0.7338
Epoch 14 Step 351 Train Loss: 0.8835
Epoch 14: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0355 Validation Top 20 DE MSE: 0.0352. 
Epoch 15 Step 1 Train Loss: 0.9019
Epoch 15 Step 51 Train Loss: 0.8368
Epoch 15 Step 101 Train Loss: 0.6978
Epoch 15 Step 151 Train Loss: 0.7040
Epoch 15 Step 201 Train Loss: 0.7518
Epoch 15 Step 251 Train Loss: 0.8190
Epoch 15 Step 301 Train Loss: 0.8079
Epoch 15 Step 351 Train Loss: 0.7080
Epoch 15: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0310 Validation Top 20 DE MSE: 0.0352. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1780
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0024538003
test_unseen_single_pearson: 0.9940481626166999
test_unseen_single_mse_de: 0.1780007
test_unseen_single_pearson_de: 0.9815258575572058
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.283784335101021
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.30833333333333335
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8416666666666667
test_unseen_single_mse_top20_de_non_dropout: 0.17822795
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.015 MB of 0.022 MB uploadedwandb: / 0.015 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.178
wandb:                                              test_de_pearson 0.98153
wandb:               test_frac_opposite_direction_top20_non_dropout 0.30833
wandb:                          test_frac_sigma_below_1_non_dropout 0.84167
wandb:                                                     test_mse 0.00245
wandb:                                test_mse_top20_de_non_dropout 0.17823
wandb:                                                 test_pearson 0.99405
wandb:                                           test_pearson_delta 0.28378
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.30833
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.84167
wandb:                                       test_unseen_single_mse 0.00245
wandb:                                    test_unseen_single_mse_de 0.178
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.17823
wandb:                                   test_unseen_single_pearson 0.99405
wandb:                                test_unseen_single_pearson_de 0.98153
wandb:                             test_unseen_single_pearson_delta 0.28378
wandb:                                                 train_de_mse 0.03103
wandb:                                             train_de_pearson 0.89352
wandb:                                                    train_mse 0.00207
wandb:                                                train_pearson 0.99507
wandb:                                                training_loss 0.8215
wandb:                                                   val_de_mse 0.0352
wandb:                                               val_de_pearson 0.99247
wandb:                                                      val_mse 0.00194
wandb:                                                  val_pearson 0.99543
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_RNA_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/slb3210y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_095340-slb3210y/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_100252-1mf8smh9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_RNA_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/1mf8smh9
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.9156
Epoch 1 Step 51 Train Loss: 0.8403
Epoch 1 Step 101 Train Loss: 0.9105
Epoch 1 Step 151 Train Loss: 0.8318
Epoch 1 Step 201 Train Loss: 0.8027
Epoch 1 Step 251 Train Loss: 0.7784
Epoch 1 Step 301 Train Loss: 0.8075
Epoch 1 Step 351 Train Loss: 0.7820
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0088. 
Train Top 20 DE MSE: 0.0464 Validation Top 20 DE MSE: 0.8463. 
Epoch 2 Step 1 Train Loss: 0.8817
Epoch 2 Step 51 Train Loss: 0.7824
Epoch 2 Step 101 Train Loss: 0.7734
Epoch 2 Step 151 Train Loss: 0.7875
Epoch 2 Step 201 Train Loss: 0.7795
Epoch 2 Step 251 Train Loss: 0.8623
Epoch 2 Step 301 Train Loss: 0.7993
Epoch 2 Step 351 Train Loss: 0.7701
Epoch 2: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0443 Validation Top 20 DE MSE: 0.8153. 
Epoch 3 Step 1 Train Loss: 0.8131
Epoch 3 Step 51 Train Loss: 0.7915
Epoch 3 Step 101 Train Loss: 0.7925
Epoch 3 Step 151 Train Loss: 0.7918
Epoch 3 Step 201 Train Loss: 0.8246
Epoch 3 Step 251 Train Loss: 0.8220
Epoch 3 Step 301 Train Loss: 0.7984
Epoch 3 Step 351 Train Loss: 0.8915
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0183 Validation Top 20 DE MSE: 0.7558. 
Epoch 4 Step 1 Train Loss: 0.6834
Epoch 4 Step 51 Train Loss: 0.7107
Epoch 4 Step 101 Train Loss: 0.8692
Epoch 4 Step 151 Train Loss: 0.7473
Epoch 4 Step 201 Train Loss: 0.7275
Epoch 4 Step 251 Train Loss: 0.7801
Epoch 4 Step 301 Train Loss: 0.7180
Epoch 4 Step 351 Train Loss: 0.8019
Epoch 4: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0265 Validation Top 20 DE MSE: 0.8355. 
Epoch 5 Step 1 Train Loss: 0.7719
Epoch 5 Step 51 Train Loss: 0.9000
Epoch 5 Step 101 Train Loss: 0.7144
Epoch 5 Step 151 Train Loss: 0.7884
Epoch 5 Step 201 Train Loss: 0.8910
Epoch 5 Step 251 Train Loss: 0.6895
Epoch 5 Step 301 Train Loss: 0.7051
Epoch 5 Step 351 Train Loss: 0.8097
Epoch 5: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0213 Validation Top 20 DE MSE: 0.8429. 
Epoch 6 Step 1 Train Loss: 0.8044
Epoch 6 Step 51 Train Loss: 0.7409
Epoch 6 Step 101 Train Loss: 0.7766
Epoch 6 Step 151 Train Loss: 0.7931
Epoch 6 Step 201 Train Loss: 0.7771
Epoch 6 Step 251 Train Loss: 0.7210
Epoch 6 Step 301 Train Loss: 0.7878
Epoch 6 Step 351 Train Loss: 0.8039
Epoch 6: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0085. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.8472. 
Epoch 7 Step 1 Train Loss: 0.6981
Epoch 7 Step 51 Train Loss: 0.8734
Epoch 7 Step 101 Train Loss: 0.8519
Epoch 7 Step 151 Train Loss: 0.7432
Epoch 7 Step 201 Train Loss: 0.7375
Epoch 7 Step 251 Train Loss: 0.7134
Epoch 7 Step 301 Train Loss: 0.6784
Epoch 7 Step 351 Train Loss: 0.7500
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.8429. 
Epoch 8 Step 1 Train Loss: 0.7958
Epoch 8 Step 51 Train Loss: 0.7747
Epoch 8 Step 101 Train Loss: 0.7608
Epoch 8 Step 151 Train Loss: 0.8140
Epoch 8 Step 201 Train Loss: 0.8437
Epoch 8 Step 251 Train Loss: 0.7690
Epoch 8 Step 301 Train Loss: 0.8324
Epoch 8 Step 351 Train Loss: 0.7685
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0115 Validation Top 20 DE MSE: 0.8305. 
Epoch 9 Step 1 Train Loss: 0.7431
Epoch 9 Step 51 Train Loss: 0.8419
Epoch 9 Step 101 Train Loss: 0.7701
Epoch 9 Step 151 Train Loss: 0.7471
Epoch 9 Step 201 Train Loss: 0.7892
Epoch 9 Step 251 Train Loss: 0.7569
Epoch 9 Step 301 Train Loss: 0.7297
Epoch 9 Step 351 Train Loss: 0.8805
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0176 Validation Top 20 DE MSE: 0.8373. 
Epoch 10 Step 1 Train Loss: 0.8243
Epoch 10 Step 51 Train Loss: 0.8704
Epoch 10 Step 101 Train Loss: 0.7811
Epoch 10 Step 151 Train Loss: 0.8407
Epoch 10 Step 201 Train Loss: 0.8514
Epoch 10 Step 251 Train Loss: 0.7574
Epoch 10 Step 301 Train Loss: 0.7151
Epoch 10 Step 351 Train Loss: 0.9029
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0085. 
Train Top 20 DE MSE: 0.0152 Validation Top 20 DE MSE: 0.8459. 
Epoch 11 Step 1 Train Loss: 0.7635
Epoch 11 Step 51 Train Loss: 0.8515
Epoch 11 Step 101 Train Loss: 0.7133
Epoch 11 Step 151 Train Loss: 0.7783
Epoch 11 Step 201 Train Loss: 0.7802
Epoch 11 Step 251 Train Loss: 0.8728
Epoch 11 Step 301 Train Loss: 0.7586
Epoch 11 Step 351 Train Loss: 0.8079
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0165 Validation Top 20 DE MSE: 0.8292. 
Epoch 12 Step 1 Train Loss: 0.7763
Epoch 12 Step 51 Train Loss: 0.7845
Epoch 12 Step 101 Train Loss: 0.7441
Epoch 12 Step 151 Train Loss: 0.7280
Epoch 12 Step 201 Train Loss: 0.7780
Epoch 12 Step 251 Train Loss: 0.9343
Epoch 12 Step 301 Train Loss: 0.7413
Epoch 12 Step 351 Train Loss: 0.7038
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0085. 
Train Top 20 DE MSE: 0.0181 Validation Top 20 DE MSE: 0.8419. 
Epoch 13 Step 1 Train Loss: 0.7373
Epoch 13 Step 51 Train Loss: 0.7501
Epoch 13 Step 101 Train Loss: 0.8904
Epoch 13 Step 151 Train Loss: 0.7554
Epoch 13 Step 201 Train Loss: 0.7395
Epoch 13 Step 251 Train Loss: 0.8340
Epoch 13 Step 301 Train Loss: 0.7917
Epoch 13 Step 351 Train Loss: 0.8262
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0120 Validation Top 20 DE MSE: 0.8357. 
Epoch 14 Step 1 Train Loss: 0.7639
Epoch 14 Step 51 Train Loss: 0.7387
Epoch 14 Step 101 Train Loss: 0.7712
Epoch 14 Step 151 Train Loss: 0.8316
Epoch 14 Step 201 Train Loss: 0.7547
Epoch 14 Step 251 Train Loss: 0.7211
Epoch 14 Step 301 Train Loss: 0.7428
Epoch 14 Step 351 Train Loss: 0.8391
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0085. 
Train Top 20 DE MSE: 0.0189 Validation Top 20 DE MSE: 0.8446. 
Epoch 15 Step 1 Train Loss: 0.7838
Epoch 15 Step 51 Train Loss: 0.7605
Epoch 15 Step 101 Train Loss: 0.8388
Epoch 15 Step 151 Train Loss: 0.7824
Epoch 15 Step 201 Train Loss: 0.7825
Epoch 15 Step 251 Train Loss: 0.7365
Epoch 15 Step 301 Train Loss: 0.6669
Epoch 15 Step 351 Train Loss: 0.8120
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0084. 
Train Top 20 DE MSE: 0.0114 Validation Top 20 DE MSE: 0.8342. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.5278
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.007242985
test_unseen_single_pearson: 0.9826341262660799
test_unseen_single_mse_de: 0.5277884
test_unseen_single_pearson_de: 0.9764514292002806
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1248278885120841
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.34166666666666673
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6833333333333332
test_unseen_single_mse_top20_de_non_dropout: 0.5303209
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.005 MB of 0.022 MB uploadedwandb: - 0.015 MB of 0.022 MB uploadedwandb: \ 0.015 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÜ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ
wandb:                                                      val_mse ‚ñà‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ
wandb:                                                  val_pearson ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.52779
wandb:                                              test_de_pearson 0.97645
wandb:               test_frac_opposite_direction_top20_non_dropout 0.34167
wandb:                          test_frac_sigma_below_1_non_dropout 0.68333
wandb:                                                     test_mse 0.00724
wandb:                                test_mse_top20_de_non_dropout 0.53032
wandb:                                                 test_pearson 0.98263
wandb:                                           test_pearson_delta 0.12483
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.34167
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.68333
wandb:                                       test_unseen_single_mse 0.00724
wandb:                                    test_unseen_single_mse_de 0.52779
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.53032
wandb:                                   test_unseen_single_pearson 0.98263
wandb:                                test_unseen_single_pearson_de 0.97645
wandb:                             test_unseen_single_pearson_delta 0.12483
wandb:                                                 train_de_mse 0.01141
wandb:                                             train_de_pearson 0.88144
wandb:                                                    train_mse 0.00156
wandb:                                                train_pearson 0.99633
wandb:                                                training_loss 0.60016
wandb:                                                   val_de_mse 0.83422
wandb:                                               val_de_pearson 0.9415
wandb:                                                      val_mse 0.00838
wandb:                                                  val_pearson 0.9803
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_RNA_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/1mf8smh9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_100252-1mf8smh9/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_101242-taem2m85
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_PapalexiSatija2021_eccite_RNA_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/taem2m85
wandb: WARNING Serializing object of type ndarray that is 20508800 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.8739
Epoch 1 Step 51 Train Loss: 0.8322
Epoch 1 Step 101 Train Loss: 0.9056
Epoch 1 Step 151 Train Loss: 0.7095
Epoch 1 Step 201 Train Loss: 0.8019
Epoch 1 Step 251 Train Loss: 0.8948
Epoch 1 Step 301 Train Loss: 0.7489
Epoch 1 Step 351 Train Loss: 0.7032
Epoch 1: Train Overall MSE: 0.0038 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0610 Validation Top 20 DE MSE: 0.0055. 
Epoch 2 Step 1 Train Loss: 0.8168
Epoch 2 Step 51 Train Loss: 0.7603
Epoch 2 Step 101 Train Loss: 0.7314
Epoch 2 Step 151 Train Loss: 0.7897
Epoch 2 Step 201 Train Loss: 0.8695
Epoch 2 Step 251 Train Loss: 0.7954
Epoch 2 Step 301 Train Loss: 0.7149
Epoch 2 Step 351 Train Loss: 0.7900
Epoch 2: Train Overall MSE: 0.0034 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0597 Validation Top 20 DE MSE: 0.0054. 
Epoch 3 Step 1 Train Loss: 0.9269
Epoch 3 Step 51 Train Loss: 0.7208
Epoch 3 Step 101 Train Loss: 0.9201
Epoch 3 Step 151 Train Loss: 0.7053
Epoch 3 Step 201 Train Loss: 0.8089
Epoch 3 Step 251 Train Loss: 0.9009
Epoch 3 Step 301 Train Loss: 0.6888
Epoch 3 Step 351 Train Loss: 0.7413
Epoch 3: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0652 Validation Top 20 DE MSE: 0.0052. 
Epoch 4 Step 1 Train Loss: 0.8740
Epoch 4 Step 51 Train Loss: 0.8891
Epoch 4 Step 101 Train Loss: 0.8112
Epoch 4 Step 151 Train Loss: 0.7946
Epoch 4 Step 201 Train Loss: 0.7141
Epoch 4 Step 251 Train Loss: 0.8064
Epoch 4 Step 301 Train Loss: 0.7478
Epoch 4 Step 351 Train Loss: 0.8828
Epoch 4: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0565 Validation Top 20 DE MSE: 0.0054. 
Epoch 5 Step 1 Train Loss: 0.8097
Epoch 5 Step 51 Train Loss: 0.8160
Epoch 5 Step 101 Train Loss: 0.6820
Epoch 5 Step 151 Train Loss: 0.7361
Epoch 5 Step 201 Train Loss: 0.8578
Epoch 5 Step 251 Train Loss: 0.6753
Epoch 5 Step 301 Train Loss: 1.0374
Epoch 5 Step 351 Train Loss: 0.9208
Epoch 5: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0617 Validation Top 20 DE MSE: 0.0058. 
Epoch 6 Step 1 Train Loss: 0.9310
Epoch 6 Step 51 Train Loss: 0.7930
Epoch 6 Step 101 Train Loss: 0.7773
Epoch 6 Step 151 Train Loss: 0.7746
Epoch 6 Step 201 Train Loss: 0.8145
Epoch 6 Step 251 Train Loss: 0.7199
Epoch 6 Step 301 Train Loss: 0.8208
Epoch 6 Step 351 Train Loss: 0.6486
Epoch 6: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0560 Validation Top 20 DE MSE: 0.0054. 
Epoch 7 Step 1 Train Loss: 0.7197
Epoch 7 Step 51 Train Loss: 0.8214
Epoch 7 Step 101 Train Loss: 0.9664
Epoch 7 Step 151 Train Loss: 0.6996
Epoch 7 Step 201 Train Loss: 0.6897
Epoch 7 Step 251 Train Loss: 0.7217
Epoch 7 Step 301 Train Loss: 0.7848
Epoch 7 Step 351 Train Loss: 0.7998
Epoch 7: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0548 Validation Top 20 DE MSE: 0.0054. 
Epoch 8 Step 1 Train Loss: 0.9536
Epoch 8 Step 51 Train Loss: 0.8505
Epoch 8 Step 101 Train Loss: 0.7835
Epoch 8 Step 151 Train Loss: 0.7449
Epoch 8 Step 201 Train Loss: 0.8531
Epoch 8 Step 251 Train Loss: 0.9039
Epoch 8 Step 301 Train Loss: 0.7864
Epoch 8 Step 351 Train Loss: 0.8018
Epoch 8: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0569 Validation Top 20 DE MSE: 0.0053. 
Epoch 9 Step 1 Train Loss: 0.9021
Epoch 9 Step 51 Train Loss: 0.7958
Epoch 9 Step 101 Train Loss: 0.8334
Epoch 9 Step 151 Train Loss: 0.8759
Epoch 9 Step 201 Train Loss: 0.6868
Epoch 9 Step 251 Train Loss: 0.7683
Epoch 9 Step 301 Train Loss: 0.7598
Epoch 9 Step 351 Train Loss: 1.0879
Epoch 9: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0501 Validation Top 20 DE MSE: 0.0053. 
Epoch 10 Step 1 Train Loss: 0.8361
Epoch 10 Step 51 Train Loss: 0.9011
Epoch 10 Step 101 Train Loss: 0.6792
Epoch 10 Step 151 Train Loss: 0.7436
Epoch 10 Step 201 Train Loss: 0.7214
Epoch 10 Step 251 Train Loss: 0.8822
Epoch 10 Step 301 Train Loss: 0.7906
Epoch 10 Step 351 Train Loss: 0.6877
Epoch 10: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0500 Validation Top 20 DE MSE: 0.0053. 
Epoch 11 Step 1 Train Loss: 0.7200
Epoch 11 Step 51 Train Loss: 0.8706
Epoch 11 Step 101 Train Loss: 0.7503
Epoch 11 Step 151 Train Loss: 0.7678
Epoch 11 Step 201 Train Loss: 0.7818
Epoch 11 Step 251 Train Loss: 0.8360
Epoch 11 Step 301 Train Loss: 0.7651
Epoch 11 Step 351 Train Loss: 0.7920
Epoch 11: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0523 Validation Top 20 DE MSE: 0.0053. 
Epoch 12 Step 1 Train Loss: 0.7567
Epoch 12 Step 51 Train Loss: 0.7564
Epoch 12 Step 101 Train Loss: 0.8066
Epoch 12 Step 151 Train Loss: 0.8302
Epoch 12 Step 201 Train Loss: 0.7408
Epoch 12 Step 251 Train Loss: 0.7997
Epoch 12 Step 301 Train Loss: 0.8450
Epoch 12 Step 351 Train Loss: 0.7118
Epoch 12: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0531 Validation Top 20 DE MSE: 0.0053. 
Epoch 13 Step 1 Train Loss: 0.7488
Epoch 13 Step 51 Train Loss: 0.6492
Epoch 13 Step 101 Train Loss: 0.8458
Epoch 13 Step 151 Train Loss: 0.7616
Epoch 13 Step 201 Train Loss: 0.6629
Epoch 13 Step 251 Train Loss: 0.8000
Epoch 13 Step 301 Train Loss: 0.7481
Epoch 13 Step 351 Train Loss: 0.8384
Epoch 13: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0492 Validation Top 20 DE MSE: 0.0052. 
Epoch 14 Step 1 Train Loss: 0.8658
Epoch 14 Step 51 Train Loss: 0.7810
Epoch 14 Step 101 Train Loss: 0.7201
Epoch 14 Step 151 Train Loss: 0.7829
Epoch 14 Step 201 Train Loss: 0.7376
Epoch 14 Step 251 Train Loss: 0.8465
Epoch 14 Step 301 Train Loss: 0.7970
Epoch 14 Step 351 Train Loss: 0.7482
Epoch 14: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0526 Validation Top 20 DE MSE: 0.0054. 
Epoch 15 Step 1 Train Loss: 0.8100
Epoch 15 Step 51 Train Loss: 0.7683
Epoch 15 Step 101 Train Loss: 0.7316
Epoch 15 Step 151 Train Loss: 0.7913
Epoch 15 Step 201 Train Loss: 0.7872
Epoch 15 Step 251 Train Loss: 0.7379
Epoch 15 Step 301 Train Loss: 0.7787
Epoch 15 Step 351 Train Loss: 0.7477
Epoch 15: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0561 Validation Top 20 DE MSE: 0.0054. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1332
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0027277095
test_unseen_single_pearson: 0.9934670460042914
test_unseen_single_mse_de: 0.13322063
test_unseen_single_pearson_de: 0.9685515114754543
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.20979813210590578
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.325
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9333333333333332
test_unseen_single_mse_top20_de_non_dropout: 0.1336058
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.022 MB uploadedwandb: - 0.001 MB of 0.022 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ
wandb:                                             train_de_pearson ‚ñà‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                    train_mse ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÉ‚ñà‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÜ
wandb:                                                   val_de_mse ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ
wandb:                                               val_de_pearson ‚ñÖ‚ñÅ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÇ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.13322
wandb:                                              test_de_pearson 0.96855
wandb:               test_frac_opposite_direction_top20_non_dropout 0.325
wandb:                          test_frac_sigma_below_1_non_dropout 0.93333
wandb:                                                     test_mse 0.00273
wandb:                                test_mse_top20_de_non_dropout 0.13361
wandb:                                                 test_pearson 0.99347
wandb:                                           test_pearson_delta 0.2098
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.325
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93333
wandb:                                       test_unseen_single_mse 0.00273
wandb:                                    test_unseen_single_mse_de 0.13322
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.13361
wandb:                                   test_unseen_single_pearson 0.99347
wandb:                                test_unseen_single_pearson_de 0.96855
wandb:                             test_unseen_single_pearson_delta 0.2098
wandb:                                                 train_de_mse 0.05613
wandb:                                             train_de_pearson 0.86629
wandb:                                                    train_mse 0.00225
wandb:                                                train_pearson 0.99464
wandb:                                                training_loss 0.81432
wandb:                                                   val_de_mse 0.00536
wandb:                                               val_de_pearson 0.99713
wandb:                                                      val_mse 0.00078
wandb:                                                  val_pearson 0.99817
wandb: 
wandb: üöÄ View run geneformer_PapalexiSatija2021_eccite_RNA_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/taem2m85
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_101242-taem2m85/logs
