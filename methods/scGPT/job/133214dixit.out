环境设置完成！
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/scgpt/split1
scGPT - INFO - Running on 2024-07-29 04:14:58
scGPT - INFO - match 4587/5015 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1027 batches | lr 0.0001 | ms/batch 415.93 | loss  0.20 | mse  0.20 |
scGPT - INFO - | epoch   1 | 200/1027 batches | lr 0.0001 | ms/batch 394.54 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1027 batches | lr 0.0001 | ms/batch 394.56 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/1027 batches | lr 0.0001 | ms/batch 394.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/1027 batches | lr 0.0001 | ms/batch 394.46 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 600/1027 batches | lr 0.0001 | ms/batch 394.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/1027 batches | lr 0.0001 | ms/batch 394.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/1027 batches | lr 0.0001 | ms/batch 394.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1027 batches | lr 0.0001 | ms/batch 394.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1027 batches | lr 0.0001 | ms/batch 394.83 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 427.51s | valid loss/mse 0.1025 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1025
scGPT - INFO - | epoch   2 | 100/1027 batches | lr 0.0001 | ms/batch 399.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1027 batches | lr 0.0001 | ms/batch 394.87 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1027 batches | lr 0.0001 | ms/batch 394.82 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1027 batches | lr 0.0001 | ms/batch 394.71 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1027 batches | lr 0.0001 | ms/batch 394.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1027 batches | lr 0.0001 | ms/batch 394.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1027 batches | lr 0.0001 | ms/batch 394.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1027 batches | lr 0.0001 | ms/batch 394.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1027 batches | lr 0.0001 | ms/batch 394.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1027 batches | lr 0.0001 | ms/batch 394.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 426.04s | valid loss/mse 0.0986 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0986
scGPT - INFO - | epoch   3 | 100/1027 batches | lr 0.0001 | ms/batch 398.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1027 batches | lr 0.0001 | ms/batch 394.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1027 batches | lr 0.0001 | ms/batch 394.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1027 batches | lr 0.0001 | ms/batch 395.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1027 batches | lr 0.0001 | ms/batch 395.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1027 batches | lr 0.0001 | ms/batch 394.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1027 batches | lr 0.0001 | ms/batch 394.76 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1027 batches | lr 0.0001 | ms/batch 394.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1027 batches | lr 0.0001 | ms/batch 394.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1027 batches | lr 0.0001 | ms/batch 395.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 426.21s | valid loss/mse 0.1006 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1027 batches | lr 0.0001 | ms/batch 399.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1027 batches | lr 0.0001 | ms/batch 395.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1027 batches | lr 0.0001 | ms/batch 394.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1027 batches | lr 0.0001 | ms/batch 394.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1027 batches | lr 0.0001 | ms/batch 394.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1027 batches | lr 0.0001 | ms/batch 395.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1027 batches | lr 0.0001 | ms/batch 394.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1027 batches | lr 0.0001 | ms/batch 394.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1027 batches | lr 0.0001 | ms/batch 395.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1027 batches | lr 0.0001 | ms/batch 395.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 426.22s | valid loss/mse 0.1031 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1027 batches | lr 0.0001 | ms/batch 399.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1027 batches | lr 0.0001 | ms/batch 395.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1027 batches | lr 0.0001 | ms/batch 394.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1027 batches | lr 0.0001 | ms/batch 394.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1027 batches | lr 0.0001 | ms/batch 394.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1027 batches | lr 0.0001 | ms/batch 394.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1027 batches | lr 0.0001 | ms/batch 393.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1027 batches | lr 0.0001 | ms/batch 393.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1027 batches | lr 0.0001 | ms/batch 393.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1027 batches | lr 0.0001 | ms/batch 393.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 425.45s | valid loss/mse 0.1012 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1027 batches | lr 0.0001 | ms/batch 397.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1027 batches | lr 0.0001 | ms/batch 393.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1027 batches | lr 0.0001 | ms/batch 393.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1027 batches | lr 0.0001 | ms/batch 393.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1027 batches | lr 0.0001 | ms/batch 393.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1027 batches | lr 0.0001 | ms/batch 393.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1027 batches | lr 0.0001 | ms/batch 393.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1027 batches | lr 0.0001 | ms/batch 393.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1027 batches | lr 0.0001 | ms/batch 393.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1027 batches | lr 0.0001 | ms/batch 393.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 424.36s | valid loss/mse 0.1022 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1027 batches | lr 0.0001 | ms/batch 397.07 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1027 batches | lr 0.0001 | ms/batch 393.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1027 batches | lr 0.0001 | ms/batch 392.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1027 batches | lr 0.0001 | ms/batch 392.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1027 batches | lr 0.0001 | ms/batch 393.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1027 batches | lr 0.0001 | ms/batch 393.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1027 batches | lr 0.0001 | ms/batch 393.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1027 batches | lr 0.0001 | ms/batch 393.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1027 batches | lr 0.0001 | ms/batch 393.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1027 batches | lr 0.0001 | ms/batch 393.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 424.25s | valid loss/mse 0.1032 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 7
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/scgpt/split2
scGPT - INFO - Running on 2024-07-29 05:30:09
scGPT - INFO - match 4587/5015 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/942 batches | lr 0.0001 | ms/batch 399.58 | loss  0.21 | mse  0.21 |
scGPT - INFO - | epoch   1 | 200/942 batches | lr 0.0001 | ms/batch 395.48 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/942 batches | lr 0.0001 | ms/batch 395.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/942 batches | lr 0.0001 | ms/batch 396.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/942 batches | lr 0.0001 | ms/batch 395.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/942 batches | lr 0.0001 | ms/batch 395.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/942 batches | lr 0.0001 | ms/batch 395.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/942 batches | lr 0.0001 | ms/batch 395.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/942 batches | lr 0.0001 | ms/batch 395.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 393.17s | valid loss/mse 0.1042 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1042
scGPT - INFO - | epoch   2 | 100/942 batches | lr 0.0001 | ms/batch 399.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/942 batches | lr 0.0001 | ms/batch 395.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/942 batches | lr 0.0001 | ms/batch 394.82 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/942 batches | lr 0.0001 | ms/batch 395.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/942 batches | lr 0.0001 | ms/batch 395.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/942 batches | lr 0.0001 | ms/batch 395.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/942 batches | lr 0.0001 | ms/batch 395.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/942 batches | lr 0.0001 | ms/batch 395.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/942 batches | lr 0.0001 | ms/batch 395.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 392.87s | valid loss/mse 0.1060 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/942 batches | lr 0.0001 | ms/batch 399.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/942 batches | lr 0.0001 | ms/batch 395.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/942 batches | lr 0.0001 | ms/batch 394.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/942 batches | lr 0.0001 | ms/batch 395.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/942 batches | lr 0.0001 | ms/batch 394.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/942 batches | lr 0.0001 | ms/batch 394.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/942 batches | lr 0.0001 | ms/batch 395.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/942 batches | lr 0.0001 | ms/batch 394.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/942 batches | lr 0.0001 | ms/batch 394.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 392.58s | valid loss/mse 0.1046 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/942 batches | lr 0.0001 | ms/batch 398.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/942 batches | lr 0.0001 | ms/batch 393.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/942 batches | lr 0.0001 | ms/batch 394.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/942 batches | lr 0.0001 | ms/batch 394.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/942 batches | lr 0.0001 | ms/batch 395.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/942 batches | lr 0.0001 | ms/batch 395.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/942 batches | lr 0.0001 | ms/batch 394.92 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/942 batches | lr 0.0001 | ms/batch 394.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/942 batches | lr 0.0001 | ms/batch 395.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 392.43s | valid loss/mse 0.1038 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1038
scGPT - INFO - | epoch   5 | 100/942 batches | lr 0.0001 | ms/batch 399.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/942 batches | lr 0.0001 | ms/batch 395.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/942 batches | lr 0.0001 | ms/batch 401.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/942 batches | lr 0.0001 | ms/batch 395.83 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/942 batches | lr 0.0001 | ms/batch 395.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/942 batches | lr 0.0001 | ms/batch 395.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/942 batches | lr 0.0001 | ms/batch 395.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/942 batches | lr 0.0001 | ms/batch 393.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/942 batches | lr 0.0001 | ms/batch 393.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 393.01s | valid loss/mse 0.1038 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/942 batches | lr 0.0001 | ms/batch 397.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/942 batches | lr 0.0001 | ms/batch 393.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/942 batches | lr 0.0001 | ms/batch 393.63 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/942 batches | lr 0.0001 | ms/batch 393.75 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/942 batches | lr 0.0001 | ms/batch 393.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/942 batches | lr 0.0001 | ms/batch 393.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/942 batches | lr 0.0001 | ms/batch 393.68 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/942 batches | lr 0.0001 | ms/batch 393.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/942 batches | lr 0.0001 | ms/batch 393.65 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 391.24s | valid loss/mse 0.1027 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1027
scGPT - INFO - | epoch   7 | 100/942 batches | lr 0.0001 | ms/batch 397.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/942 batches | lr 0.0001 | ms/batch 393.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/942 batches | lr 0.0001 | ms/batch 393.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/942 batches | lr 0.0001 | ms/batch 393.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/942 batches | lr 0.0001 | ms/batch 393.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/942 batches | lr 0.0001 | ms/batch 393.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/942 batches | lr 0.0001 | ms/batch 393.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/942 batches | lr 0.0001 | ms/batch 393.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/942 batches | lr 0.0001 | ms/batch 393.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 391.12s | valid loss/mse 0.1040 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/942 batches | lr 0.0000 | ms/batch 397.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/942 batches | lr 0.0000 | ms/batch 393.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/942 batches | lr 0.0000 | ms/batch 393.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/942 batches | lr 0.0000 | ms/batch 393.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/942 batches | lr 0.0000 | ms/batch 393.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/942 batches | lr 0.0000 | ms/batch 393.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/942 batches | lr 0.0000 | ms/batch 393.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/942 batches | lr 0.0000 | ms/batch 393.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/942 batches | lr 0.0000 | ms/batch 393.71 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 391.16s | valid loss/mse 0.1057 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/942 batches | lr 0.0000 | ms/batch 398.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/942 batches | lr 0.0000 | ms/batch 393.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/942 batches | lr 0.0000 | ms/batch 393.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/942 batches | lr 0.0000 | ms/batch 393.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/942 batches | lr 0.0000 | ms/batch 393.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/942 batches | lr 0.0000 | ms/batch 393.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/942 batches | lr 0.0000 | ms/batch 393.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/942 batches | lr 0.0000 | ms/batch 393.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/942 batches | lr 0.0000 | ms/batch 393.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 391.15s | valid loss/mse 0.1017 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1017
scGPT - INFO - | epoch  10 | 100/942 batches | lr 0.0000 | ms/batch 397.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 200/942 batches | lr 0.0000 | ms/batch 393.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 300/942 batches | lr 0.0000 | ms/batch 393.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 400/942 batches | lr 0.0000 | ms/batch 393.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 500/942 batches | lr 0.0000 | ms/batch 393.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 600/942 batches | lr 0.0000 | ms/batch 393.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 700/942 batches | lr 0.0000 | ms/batch 393.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 800/942 batches | lr 0.0000 | ms/batch 393.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 900/942 batches | lr 0.0000 | ms/batch 393.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 391.02s | valid loss/mse 0.1055 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/942 batches | lr 0.0000 | ms/batch 397.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 200/942 batches | lr 0.0000 | ms/batch 393.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 300/942 batches | lr 0.0000 | ms/batch 393.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 400/942 batches | lr 0.0000 | ms/batch 393.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 500/942 batches | lr 0.0000 | ms/batch 393.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 600/942 batches | lr 0.0000 | ms/batch 393.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 700/942 batches | lr 0.0000 | ms/batch 393.72 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 800/942 batches | lr 0.0000 | ms/batch 393.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 900/942 batches | lr 0.0000 | ms/batch 393.76 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 391.23s | valid loss/mse 0.1037 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/942 batches | lr 0.0000 | ms/batch 397.75 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 200/942 batches | lr 0.0000 | ms/batch 393.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 300/942 batches | lr 0.0000 | ms/batch 393.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 400/942 batches | lr 0.0000 | ms/batch 393.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 500/942 batches | lr 0.0000 | ms/batch 393.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 600/942 batches | lr 0.0000 | ms/batch 393.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 700/942 batches | lr 0.0000 | ms/batch 392.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 800/942 batches | lr 0.0000 | ms/batch 394.65 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 900/942 batches | lr 0.0000 | ms/batch 393.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 391.22s | valid loss/mse 0.1044 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/942 batches | lr 0.0000 | ms/batch 397.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 200/942 batches | lr 0.0000 | ms/batch 393.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 300/942 batches | lr 0.0000 | ms/batch 393.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 400/942 batches | lr 0.0000 | ms/batch 393.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 500/942 batches | lr 0.0000 | ms/batch 392.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 600/942 batches | lr 0.0000 | ms/batch 393.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 700/942 batches | lr 0.0000 | ms/batch 393.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 800/942 batches | lr 0.0000 | ms/batch 393.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  13 | 900/942 batches | lr 0.0000 | ms/batch 393.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 391.01s | valid loss/mse 0.1035 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/942 batches | lr 0.0000 | ms/batch 397.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 200/942 batches | lr 0.0000 | ms/batch 393.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 300/942 batches | lr 0.0000 | ms/batch 393.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 400/942 batches | lr 0.0000 | ms/batch 393.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 500/942 batches | lr 0.0000 | ms/batch 393.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 600/942 batches | lr 0.0000 | ms/batch 393.82 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 700/942 batches | lr 0.0000 | ms/batch 393.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 800/942 batches | lr 0.0000 | ms/batch 393.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  14 | 900/942 batches | lr 0.0000 | ms/batch 393.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 391.16s | valid loss/mse 0.1057 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 14
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/scgpt/split3
scGPT - INFO - Running on 2024-07-29 07:33:16
scGPT - INFO - match 4587/5015 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/989 batches | lr 0.0001 | ms/batch 400.93 | loss  0.20 | mse  0.20 |
scGPT - INFO - | epoch   1 | 200/989 batches | lr 0.0001 | ms/batch 395.70 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/989 batches | lr 0.0001 | ms/batch 395.72 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/989 batches | lr 0.0001 | ms/batch 395.96 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 500/989 batches | lr 0.0001 | ms/batch 395.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/989 batches | lr 0.0001 | ms/batch 395.84 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/989 batches | lr 0.0001 | ms/batch 395.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/989 batches | lr 0.0001 | ms/batch 395.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/989 batches | lr 0.0001 | ms/batch 396.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 423.76s | valid loss/mse 0.1084 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1084
scGPT - INFO - | epoch   2 | 100/989 batches | lr 0.0001 | ms/batch 400.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/989 batches | lr 0.0001 | ms/batch 395.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/989 batches | lr 0.0001 | ms/batch 395.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/989 batches | lr 0.0001 | ms/batch 395.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/989 batches | lr 0.0001 | ms/batch 395.91 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/989 batches | lr 0.0001 | ms/batch 395.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/989 batches | lr 0.0001 | ms/batch 395.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/989 batches | lr 0.0001 | ms/batch 395.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/989 batches | lr 0.0001 | ms/batch 395.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 423.22s | valid loss/mse 0.1110 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/989 batches | lr 0.0001 | ms/batch 399.83 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/989 batches | lr 0.0001 | ms/batch 395.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/989 batches | lr 0.0001 | ms/batch 395.63 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/989 batches | lr 0.0001 | ms/batch 395.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/989 batches | lr 0.0001 | ms/batch 395.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/989 batches | lr 0.0001 | ms/batch 395.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/989 batches | lr 0.0001 | ms/batch 395.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/989 batches | lr 0.0001 | ms/batch 395.75 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/989 batches | lr 0.0001 | ms/batch 395.81 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 423.20s | valid loss/mse 0.1089 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/989 batches | lr 0.0001 | ms/batch 399.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/989 batches | lr 0.0001 | ms/batch 395.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/989 batches | lr 0.0001 | ms/batch 395.74 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/989 batches | lr 0.0001 | ms/batch 395.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/989 batches | lr 0.0001 | ms/batch 395.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/989 batches | lr 0.0001 | ms/batch 395.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/989 batches | lr 0.0001 | ms/batch 395.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/989 batches | lr 0.0001 | ms/batch 395.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/989 batches | lr 0.0001 | ms/batch 395.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 422.86s | valid loss/mse 0.1068 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1068
scGPT - INFO - | epoch   5 | 100/989 batches | lr 0.0001 | ms/batch 399.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/989 batches | lr 0.0001 | ms/batch 395.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/989 batches | lr 0.0001 | ms/batch 395.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/989 batches | lr 0.0001 | ms/batch 395.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/989 batches | lr 0.0001 | ms/batch 395.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/989 batches | lr 0.0001 | ms/batch 395.63 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/989 batches | lr 0.0001 | ms/batch 395.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/989 batches | lr 0.0001 | ms/batch 395.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/989 batches | lr 0.0001 | ms/batch 395.76 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 423.07s | valid loss/mse 0.1113 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/989 batches | lr 0.0001 | ms/batch 399.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/989 batches | lr 0.0001 | ms/batch 395.75 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/989 batches | lr 0.0001 | ms/batch 395.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/989 batches | lr 0.0001 | ms/batch 395.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/989 batches | lr 0.0001 | ms/batch 395.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/989 batches | lr 0.0001 | ms/batch 395.63 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/989 batches | lr 0.0001 | ms/batch 395.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/989 batches | lr 0.0001 | ms/batch 395.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/989 batches | lr 0.0001 | ms/batch 395.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 422.91s | valid loss/mse 0.1087 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/989 batches | lr 0.0001 | ms/batch 399.81 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/989 batches | lr 0.0001 | ms/batch 395.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/989 batches | lr 0.0001 | ms/batch 395.76 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/989 batches | lr 0.0001 | ms/batch 395.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/989 batches | lr 0.0001 | ms/batch 395.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/989 batches | lr 0.0001 | ms/batch 395.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/989 batches | lr 0.0001 | ms/batch 395.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/989 batches | lr 0.0001 | ms/batch 395.75 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/989 batches | lr 0.0001 | ms/batch 395.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 423.04s | valid loss/mse 0.1095 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/989 batches | lr 0.0000 | ms/batch 400.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/989 batches | lr 0.0000 | ms/batch 395.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/989 batches | lr 0.0000 | ms/batch 395.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/989 batches | lr 0.0000 | ms/batch 396.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/989 batches | lr 0.0000 | ms/batch 395.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/989 batches | lr 0.0000 | ms/batch 395.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/989 batches | lr 0.0000 | ms/batch 395.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/989 batches | lr 0.0000 | ms/batch 395.98 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/989 batches | lr 0.0000 | ms/batch 396.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 423.38s | valid loss/mse 0.1107 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/989 batches | lr 0.0000 | ms/batch 399.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/989 batches | lr 0.0000 | ms/batch 395.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/989 batches | lr 0.0000 | ms/batch 395.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/989 batches | lr 0.0000 | ms/batch 395.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/989 batches | lr 0.0000 | ms/batch 395.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/989 batches | lr 0.0000 | ms/batch 395.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/989 batches | lr 0.0000 | ms/batch 395.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/989 batches | lr 0.0000 | ms/batch 395.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/989 batches | lr 0.0000 | ms/batch 395.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 422.86s | valid loss/mse 0.1084 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 9
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/scgpt/split4
scGPT - INFO - Running on 2024-07-29 09:00:14
scGPT - INFO - match 4587/5015 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1130 batches | lr 0.0001 | ms/batch 401.61 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 200/1130 batches | lr 0.0001 | ms/batch 395.83 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1130 batches | lr 0.0001 | ms/batch 396.57 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/1130 batches | lr 0.0001 | ms/batch 396.18 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 500/1130 batches | lr 0.0001 | ms/batch 396.16 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 600/1130 batches | lr 0.0001 | ms/batch 396.47 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 700/1130 batches | lr 0.0001 | ms/batch 396.49 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 800/1130 batches | lr 0.0001 | ms/batch 396.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1130 batches | lr 0.0001 | ms/batch 396.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1130 batches | lr 0.0001 | ms/batch 396.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1100/1130 batches | lr 0.0001 | ms/batch 396.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 458.29s | valid loss/mse 0.1044 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1044
scGPT - INFO - | epoch   2 | 100/1130 batches | lr 0.0001 | ms/batch 400.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1130 batches | lr 0.0001 | ms/batch 396.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1130 batches | lr 0.0001 | ms/batch 396.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1130 batches | lr 0.0001 | ms/batch 396.83 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1130 batches | lr 0.0001 | ms/batch 395.68 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1130 batches | lr 0.0001 | ms/batch 395.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1130 batches | lr 0.0001 | ms/batch 396.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1130 batches | lr 0.0001 | ms/batch 395.71 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1130 batches | lr 0.0001 | ms/batch 395.68 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1130 batches | lr 0.0001 | ms/batch 395.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1130 batches | lr 0.0001 | ms/batch 395.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 457.62s | valid loss/mse 0.1084 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1130 batches | lr 0.0001 | ms/batch 399.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1130 batches | lr 0.0001 | ms/batch 395.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1130 batches | lr 0.0001 | ms/batch 395.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1130 batches | lr 0.0001 | ms/batch 395.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1130 batches | lr 0.0001 | ms/batch 395.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1130 batches | lr 0.0001 | ms/batch 395.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1130 batches | lr 0.0001 | ms/batch 395.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1130 batches | lr 0.0001 | ms/batch 395.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1130 batches | lr 0.0001 | ms/batch 395.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1130 batches | lr 0.0001 | ms/batch 395.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1130 batches | lr 0.0001 | ms/batch 395.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 457.22s | valid loss/mse 0.1060 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1130 batches | lr 0.0001 | ms/batch 399.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1130 batches | lr 0.0001 | ms/batch 395.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1130 batches | lr 0.0001 | ms/batch 395.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1130 batches | lr 0.0001 | ms/batch 395.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1130 batches | lr 0.0001 | ms/batch 395.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1130 batches | lr 0.0001 | ms/batch 395.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1130 batches | lr 0.0001 | ms/batch 395.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1130 batches | lr 0.0001 | ms/batch 395.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1130 batches | lr 0.0001 | ms/batch 395.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1130 batches | lr 0.0001 | ms/batch 395.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1130 batches | lr 0.0001 | ms/batch 395.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 457.06s | valid loss/mse 0.1025 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1025
scGPT - INFO - | epoch   5 | 100/1130 batches | lr 0.0001 | ms/batch 399.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1130 batches | lr 0.0001 | ms/batch 395.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1130 batches | lr 0.0001 | ms/batch 395.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1130 batches | lr 0.0001 | ms/batch 395.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1130 batches | lr 0.0001 | ms/batch 395.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1130 batches | lr 0.0001 | ms/batch 395.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1130 batches | lr 0.0001 | ms/batch 395.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1130 batches | lr 0.0001 | ms/batch 395.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1130 batches | lr 0.0001 | ms/batch 395.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1130 batches | lr 0.0001 | ms/batch 395.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1130 batches | lr 0.0001 | ms/batch 395.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 456.88s | valid loss/mse 0.1032 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1130 batches | lr 0.0001 | ms/batch 399.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1130 batches | lr 0.0001 | ms/batch 395.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1130 batches | lr 0.0001 | ms/batch 395.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1130 batches | lr 0.0001 | ms/batch 395.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1130 batches | lr 0.0001 | ms/batch 395.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1130 batches | lr 0.0001 | ms/batch 395.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1130 batches | lr 0.0001 | ms/batch 395.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1130 batches | lr 0.0001 | ms/batch 395.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1130 batches | lr 0.0001 | ms/batch 395.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1130 batches | lr 0.0001 | ms/batch 395.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1130 batches | lr 0.0001 | ms/batch 395.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 456.98s | valid loss/mse 0.1050 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1130 batches | lr 0.0001 | ms/batch 400.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1130 batches | lr 0.0001 | ms/batch 395.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1130 batches | lr 0.0001 | ms/batch 395.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1130 batches | lr 0.0001 | ms/batch 395.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1130 batches | lr 0.0001 | ms/batch 395.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1130 batches | lr 0.0001 | ms/batch 398.07 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1130 batches | lr 0.0001 | ms/batch 398.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1130 batches | lr 0.0001 | ms/batch 396.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1130 batches | lr 0.0001 | ms/batch 395.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1130 batches | lr 0.0001 | ms/batch 395.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1130 batches | lr 0.0001 | ms/batch 395.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 457.82s | valid loss/mse 0.1032 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1130 batches | lr 0.0000 | ms/batch 399.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/1130 batches | lr 0.0000 | ms/batch 395.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/1130 batches | lr 0.0000 | ms/batch 395.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/1130 batches | lr 0.0000 | ms/batch 395.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/1130 batches | lr 0.0000 | ms/batch 395.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/1130 batches | lr 0.0000 | ms/batch 395.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/1130 batches | lr 0.0000 | ms/batch 395.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/1130 batches | lr 0.0000 | ms/batch 395.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/1130 batches | lr 0.0000 | ms/batch 395.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1000/1130 batches | lr 0.0000 | ms/batch 395.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1100/1130 batches | lr 0.0000 | ms/batch 395.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 456.83s | valid loss/mse 0.1043 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1130 batches | lr 0.0000 | ms/batch 401.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/1130 batches | lr 0.0000 | ms/batch 395.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/1130 batches | lr 0.0000 | ms/batch 395.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/1130 batches | lr 0.0000 | ms/batch 395.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/1130 batches | lr 0.0000 | ms/batch 395.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/1130 batches | lr 0.0000 | ms/batch 395.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/1130 batches | lr 0.0000 | ms/batch 395.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/1130 batches | lr 0.0000 | ms/batch 395.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/1130 batches | lr 0.0000 | ms/batch 395.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1000/1130 batches | lr 0.0000 | ms/batch 395.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1100/1130 batches | lr 0.0000 | ms/batch 395.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 457.01s | valid loss/mse 0.1030 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 9
---Creating test_res
test_res saved successfully----
Split4 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_combined/scgpt/split5
scGPT - INFO - Running on 2024-07-29 10:31:12
scGPT - INFO - match 4587/5015 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1110 batches | lr 0.0001 | ms/batch 400.50 | loss  0.23 | mse  0.23 |
scGPT - INFO - | epoch   1 | 200/1110 batches | lr 0.0001 | ms/batch 395.22 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1110 batches | lr 0.0001 | ms/batch 395.27 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/1110 batches | lr 0.0001 | ms/batch 398.00 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 500/1110 batches | lr 0.0001 | ms/batch 397.21 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 600/1110 batches | lr 0.0001 | ms/batch 395.29 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 700/1110 batches | lr 0.0001 | ms/batch 395.64 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 800/1110 batches | lr 0.0001 | ms/batch 395.59 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 900/1110 batches | lr 0.0001 | ms/batch 395.19 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 1000/1110 batches | lr 0.0001 | ms/batch 395.42 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 1100/1110 batches | lr 0.0001 | ms/batch 395.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 451.82s | valid loss/mse 0.0990 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0990
scGPT - INFO - | epoch   2 | 100/1110 batches | lr 0.0001 | ms/batch 397.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1110 batches | lr 0.0001 | ms/batch 393.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1110 batches | lr 0.0001 | ms/batch 395.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1110 batches | lr 0.0001 | ms/batch 393.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1110 batches | lr 0.0001 | ms/batch 393.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1110 batches | lr 0.0001 | ms/batch 393.76 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1110 batches | lr 0.0001 | ms/batch 393.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1110 batches | lr 0.0001 | ms/batch 393.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1110 batches | lr 0.0001 | ms/batch 393.68 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1110 batches | lr 0.0001 | ms/batch 393.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1110 batches | lr 0.0001 | ms/batch 393.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 449.34s | valid loss/mse 0.1008 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1110 batches | lr 0.0001 | ms/batch 398.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1110 batches | lr 0.0001 | ms/batch 393.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1110 batches | lr 0.0001 | ms/batch 393.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1110 batches | lr 0.0001 | ms/batch 393.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1110 batches | lr 0.0001 | ms/batch 393.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1110 batches | lr 0.0001 | ms/batch 393.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1110 batches | lr 0.0001 | ms/batch 393.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1110 batches | lr 0.0001 | ms/batch 393.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1110 batches | lr 0.0001 | ms/batch 393.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1110 batches | lr 0.0001 | ms/batch 393.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1110 batches | lr 0.0001 | ms/batch 394.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 448.88s | valid loss/mse 0.1004 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1110 batches | lr 0.0001 | ms/batch 398.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1110 batches | lr 0.0001 | ms/batch 393.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1110 batches | lr 0.0001 | ms/batch 393.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1110 batches | lr 0.0001 | ms/batch 393.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1110 batches | lr 0.0001 | ms/batch 393.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1110 batches | lr 0.0001 | ms/batch 393.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1110 batches | lr 0.0001 | ms/batch 393.65 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1110 batches | lr 0.0001 | ms/batch 393.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1110 batches | lr 0.0001 | ms/batch 393.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1110 batches | lr 0.0001 | ms/batch 393.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1110 batches | lr 0.0001 | ms/batch 393.84 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 449.22s | valid loss/mse 0.1008 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1110 batches | lr 0.0001 | ms/batch 397.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1110 batches | lr 0.0001 | ms/batch 393.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1110 batches | lr 0.0001 | ms/batch 394.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1110 batches | lr 0.0001 | ms/batch 394.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1110 batches | lr 0.0001 | ms/batch 394.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1110 batches | lr 0.0001 | ms/batch 394.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1110 batches | lr 0.0001 | ms/batch 394.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1110 batches | lr 0.0001 | ms/batch 394.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1110 batches | lr 0.0001 | ms/batch 394.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1110 batches | lr 0.0001 | ms/batch 393.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1110 batches | lr 0.0001 | ms/batch 393.92 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 449.53s | valid loss/mse 0.1008 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1110 batches | lr 0.0001 | ms/batch 397.87 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1110 batches | lr 0.0001 | ms/batch 394.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1110 batches | lr 0.0001 | ms/batch 394.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1110 batches | lr 0.0001 | ms/batch 393.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1110 batches | lr 0.0001 | ms/batch 393.65 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1110 batches | lr 0.0001 | ms/batch 394.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1110 batches | lr 0.0001 | ms/batch 393.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1110 batches | lr 0.0001 | ms/batch 393.92 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1110 batches | lr 0.0001 | ms/batch 394.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1110 batches | lr 0.0001 | ms/batch 393.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1110 batches | lr 0.0001 | ms/batch 393.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 449.43s | valid loss/mse 0.0981 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0981
scGPT - INFO - | epoch   7 | 100/1110 batches | lr 0.0001 | ms/batch 398.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1110 batches | lr 0.0001 | ms/batch 394.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1110 batches | lr 0.0001 | ms/batch 393.83 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1110 batches | lr 0.0001 | ms/batch 394.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1110 batches | lr 0.0001 | ms/batch 393.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1110 batches | lr 0.0001 | ms/batch 393.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1110 batches | lr 0.0001 | ms/batch 394.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1110 batches | lr 0.0001 | ms/batch 394.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1110 batches | lr 0.0001 | ms/batch 393.76 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1110 batches | lr 0.0001 | ms/batch 393.87 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1110 batches | lr 0.0001 | ms/batch 393.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 449.61s | valid loss/mse 0.1019 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1110 batches | lr 0.0000 | ms/batch 398.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/1110 batches | lr 0.0000 | ms/batch 393.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/1110 batches | lr 0.0000 | ms/batch 393.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/1110 batches | lr 0.0000 | ms/batch 393.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/1110 batches | lr 0.0000 | ms/batch 393.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/1110 batches | lr 0.0000 | ms/batch 393.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/1110 batches | lr 0.0000 | ms/batch 394.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/1110 batches | lr 0.0000 | ms/batch 394.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/1110 batches | lr 0.0000 | ms/batch 394.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1000/1110 batches | lr 0.0000 | ms/batch 393.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1100/1110 batches | lr 0.0000 | ms/batch 393.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 449.47s | valid loss/mse 0.0998 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1110 batches | lr 0.0000 | ms/batch 397.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/1110 batches | lr 0.0000 | ms/batch 393.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/1110 batches | lr 0.0000 | ms/batch 393.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/1110 batches | lr 0.0000 | ms/batch 393.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/1110 batches | lr 0.0000 | ms/batch 393.92 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/1110 batches | lr 0.0000 | ms/batch 393.91 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/1110 batches | lr 0.0000 | ms/batch 394.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/1110 batches | lr 0.0000 | ms/batch 393.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/1110 batches | lr 0.0000 | ms/batch 393.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1000/1110 batches | lr 0.0000 | ms/batch 393.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1100/1110 batches | lr 0.0000 | ms/batch 393.65 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 449.35s | valid loss/mse 0.1026 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/1110 batches | lr 0.0000 | ms/batch 397.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 200/1110 batches | lr 0.0000 | ms/batch 394.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 300/1110 batches | lr 0.0000 | ms/batch 393.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 400/1110 batches | lr 0.0000 | ms/batch 393.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 500/1110 batches | lr 0.0000 | ms/batch 393.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 600/1110 batches | lr 0.0000 | ms/batch 394.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 700/1110 batches | lr 0.0000 | ms/batch 394.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 800/1110 batches | lr 0.0000 | ms/batch 394.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 900/1110 batches | lr 0.0000 | ms/batch 394.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1000/1110 batches | lr 0.0000 | ms/batch 393.74 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1100/1110 batches | lr 0.0000 | ms/batch 393.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 449.47s | valid loss/mse 0.1016 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/1110 batches | lr 0.0000 | ms/batch 398.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 200/1110 batches | lr 0.0000 | ms/batch 393.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 300/1110 batches | lr 0.0000 | ms/batch 394.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 400/1110 batches | lr 0.0000 | ms/batch 393.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 500/1110 batches | lr 0.0000 | ms/batch 394.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 600/1110 batches | lr 0.0000 | ms/batch 394.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 700/1110 batches | lr 0.0000 | ms/batch 394.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 800/1110 batches | lr 0.0000 | ms/batch 394.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 900/1110 batches | lr 0.0000 | ms/batch 393.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1000/1110 batches | lr 0.0000 | ms/batch 393.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1100/1110 batches | lr 0.0000 | ms/batch 394.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 449.54s | valid loss/mse 0.1014 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 11
---Creating test_res
test_res saved successfully----
Split5 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396858/scgpt/split1
scGPT - INFO - Running on 2024-07-29 12:40:11
scGPT - INFO - match 4579/5008 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/671 batches | lr 0.0001 | ms/batch 407.53 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 200/671 batches | lr 0.0001 | ms/batch 394.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 300/671 batches | lr 0.0001 | ms/batch 392.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/671 batches | lr 0.0001 | ms/batch 392.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/671 batches | lr 0.0001 | ms/batch 392.85 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 600/671 batches | lr 0.0001 | ms/batch 392.83 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 272.35s | valid loss/mse 0.0954 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0954
scGPT - INFO - | epoch   2 | 100/671 batches | lr 0.0001 | ms/batch 397.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 200/671 batches | lr 0.0001 | ms/batch 394.35 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 300/671 batches | lr 0.0001 | ms/batch 393.14 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 400/671 batches | lr 0.0001 | ms/batch 393.14 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 500/671 batches | lr 0.0001 | ms/batch 393.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 600/671 batches | lr 0.0001 | ms/batch 393.09 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 271.42s | valid loss/mse 0.0987 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/671 batches | lr 0.0001 | ms/batch 396.94 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 200/671 batches | lr 0.0001 | ms/batch 392.99 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 300/671 batches | lr 0.0001 | ms/batch 393.00 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 400/671 batches | lr 0.0001 | ms/batch 392.78 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 500/671 batches | lr 0.0001 | ms/batch 393.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 600/671 batches | lr 0.0001 | ms/batch 392.86 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 271.18s | valid loss/mse 0.0956 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/671 batches | lr 0.0001 | ms/batch 396.97 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 200/671 batches | lr 0.0001 | ms/batch 393.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 300/671 batches | lr 0.0001 | ms/batch 392.96 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 400/671 batches | lr 0.0001 | ms/batch 393.13 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 500/671 batches | lr 0.0001 | ms/batch 394.35 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 600/671 batches | lr 0.0001 | ms/batch 393.15 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 271.42s | valid loss/mse 0.0962 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/671 batches | lr 0.0001 | ms/batch 398.99 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 200/671 batches | lr 0.0001 | ms/batch 393.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 300/671 batches | lr 0.0001 | ms/batch 393.29 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 400/671 batches | lr 0.0001 | ms/batch 393.38 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 500/671 batches | lr 0.0001 | ms/batch 392.92 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 600/671 batches | lr 0.0001 | ms/batch 393.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 271.61s | valid loss/mse 0.0942 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0942
scGPT - INFO - | epoch   6 | 100/671 batches | lr 0.0001 | ms/batch 398.11 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 200/671 batches | lr 0.0001 | ms/batch 395.87 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 300/671 batches | lr 0.0001 | ms/batch 394.61 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 400/671 batches | lr 0.0001 | ms/batch 392.97 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 500/671 batches | lr 0.0001 | ms/batch 393.05 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 600/671 batches | lr 0.0001 | ms/batch 392.97 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 271.79s | valid loss/mse 0.0951 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/671 batches | lr 0.0001 | ms/batch 397.04 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 200/671 batches | lr 0.0001 | ms/batch 393.00 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 300/671 batches | lr 0.0001 | ms/batch 393.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 400/671 batches | lr 0.0001 | ms/batch 392.99 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 500/671 batches | lr 0.0001 | ms/batch 393.03 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 600/671 batches | lr 0.0001 | ms/batch 393.04 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 271.29s | valid loss/mse 0.0987 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/671 batches | lr 0.0000 | ms/batch 397.15 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 200/671 batches | lr 0.0000 | ms/batch 393.28 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 300/671 batches | lr 0.0000 | ms/batch 393.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 400/671 batches | lr 0.0000 | ms/batch 393.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 500/671 batches | lr 0.0000 | ms/batch 393.10 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 600/671 batches | lr 0.0000 | ms/batch 393.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 271.35s | valid loss/mse 0.0981 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/671 batches | lr 0.0000 | ms/batch 397.31 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 200/671 batches | lr 0.0000 | ms/batch 393.17 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 300/671 batches | lr 0.0000 | ms/batch 393.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 400/671 batches | lr 0.0000 | ms/batch 393.00 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 500/671 batches | lr 0.0000 | ms/batch 393.10 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 600/671 batches | lr 0.0000 | ms/batch 393.19 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 271.37s | valid loss/mse 0.0957 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/671 batches | lr 0.0000 | ms/batch 397.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 200/671 batches | lr 0.0000 | ms/batch 394.04 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 300/671 batches | lr 0.0000 | ms/batch 393.38 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 400/671 batches | lr 0.0000 | ms/batch 393.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 500/671 batches | lr 0.0000 | ms/batch 393.14 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 600/671 batches | lr 0.0000 | ms/batch 393.15 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 271.43s | valid loss/mse 0.0929 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0929
scGPT - INFO - | epoch  11 | 100/671 batches | lr 0.0000 | ms/batch 397.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 200/671 batches | lr 0.0000 | ms/batch 393.20 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 300/671 batches | lr 0.0000 | ms/batch 393.13 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 400/671 batches | lr 0.0000 | ms/batch 393.19 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 500/671 batches | lr 0.0000 | ms/batch 393.31 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 600/671 batches | lr 0.0000 | ms/batch 393.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 271.44s | valid loss/mse 0.0963 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/671 batches | lr 0.0000 | ms/batch 397.44 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 200/671 batches | lr 0.0000 | ms/batch 393.65 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 300/671 batches | lr 0.0000 | ms/batch 395.75 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 400/671 batches | lr 0.0000 | ms/batch 393.73 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 500/671 batches | lr 0.0000 | ms/batch 393.36 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 600/671 batches | lr 0.0000 | ms/batch 393.49 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 271.81s | valid loss/mse 0.0979 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/671 batches | lr 0.0000 | ms/batch 397.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 200/671 batches | lr 0.0000 | ms/batch 393.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 300/671 batches | lr 0.0000 | ms/batch 393.37 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 400/671 batches | lr 0.0000 | ms/batch 393.24 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 500/671 batches | lr 0.0000 | ms/batch 393.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 600/671 batches | lr 0.0000 | ms/batch 393.30 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 271.49s | valid loss/mse 0.0967 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/671 batches | lr 0.0000 | ms/batch 397.85 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 200/671 batches | lr 0.0000 | ms/batch 393.26 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 300/671 batches | lr 0.0000 | ms/batch 394.12 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 400/671 batches | lr 0.0000 | ms/batch 394.12 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 500/671 batches | lr 0.0000 | ms/batch 393.21 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 600/671 batches | lr 0.0000 | ms/batch 393.46 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 271.69s | valid loss/mse 0.0966 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  15 | 100/671 batches | lr 0.0000 | ms/batch 397.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  15 | 200/671 batches | lr 0.0000 | ms/batch 393.18 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  15 | 300/671 batches | lr 0.0000 | ms/batch 393.43 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  15 | 400/671 batches | lr 0.0000 | ms/batch 393.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  15 | 500/671 batches | lr 0.0000 | ms/batch 393.53 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  15 | 600/671 batches | lr 0.0000 | ms/batch 393.36 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 271.54s | valid loss/mse 0.0974 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 15
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396858/scgpt/split2
scGPT - INFO - Running on 2024-07-29 13:51:59
scGPT - INFO - match 4579/5008 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/633 batches | lr 0.0001 | ms/batch 401.19 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 200/633 batches | lr 0.0001 | ms/batch 396.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 300/633 batches | lr 0.0001 | ms/batch 395.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/633 batches | lr 0.0001 | ms/batch 395.28 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 500/633 batches | lr 0.0001 | ms/batch 395.21 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 600/633 batches | lr 0.0001 | ms/batch 395.33 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 261.85s | valid loss/mse 0.1024 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1024
scGPT - INFO - | epoch   2 | 100/633 batches | lr 0.0001 | ms/batch 398.03 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 200/633 batches | lr 0.0001 | ms/batch 394.03 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 300/633 batches | lr 0.0001 | ms/batch 394.12 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 400/633 batches | lr 0.0001 | ms/batch 394.10 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 500/633 batches | lr 0.0001 | ms/batch 394.21 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 600/633 batches | lr 0.0001 | ms/batch 394.13 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 260.76s | valid loss/mse 0.0932 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0932
scGPT - INFO - | epoch   3 | 100/633 batches | lr 0.0001 | ms/batch 397.96 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 200/633 batches | lr 0.0001 | ms/batch 393.75 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 300/633 batches | lr 0.0001 | ms/batch 393.67 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 400/633 batches | lr 0.0001 | ms/batch 393.82 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 500/633 batches | lr 0.0001 | ms/batch 393.92 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 600/633 batches | lr 0.0001 | ms/batch 393.85 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 260.55s | valid loss/mse 0.0960 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/633 batches | lr 0.0001 | ms/batch 397.76 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 200/633 batches | lr 0.0001 | ms/batch 393.70 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 300/633 batches | lr 0.0001 | ms/batch 393.84 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 400/633 batches | lr 0.0001 | ms/batch 393.91 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 500/633 batches | lr 0.0001 | ms/batch 393.77 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 600/633 batches | lr 0.0001 | ms/batch 393.80 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 260.53s | valid loss/mse 0.0962 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/633 batches | lr 0.0001 | ms/batch 398.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 200/633 batches | lr 0.0001 | ms/batch 394.03 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 300/633 batches | lr 0.0001 | ms/batch 393.76 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 400/633 batches | lr 0.0001 | ms/batch 393.66 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 500/633 batches | lr 0.0001 | ms/batch 393.76 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 600/633 batches | lr 0.0001 | ms/batch 393.76 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 260.55s | valid loss/mse 0.0957 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/633 batches | lr 0.0001 | ms/batch 397.43 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 200/633 batches | lr 0.0001 | ms/batch 393.50 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 300/633 batches | lr 0.0001 | ms/batch 393.33 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 400/633 batches | lr 0.0001 | ms/batch 393.53 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 500/633 batches | lr 0.0001 | ms/batch 393.49 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 600/633 batches | lr 0.0001 | ms/batch 393.35 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 260.31s | valid loss/mse 0.0975 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/633 batches | lr 0.0001 | ms/batch 397.53 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 200/633 batches | lr 0.0001 | ms/batch 393.69 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 300/633 batches | lr 0.0001 | ms/batch 393.56 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 400/633 batches | lr 0.0001 | ms/batch 393.71 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 500/633 batches | lr 0.0001 | ms/batch 394.12 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 600/633 batches | lr 0.0001 | ms/batch 393.80 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 260.49s | valid loss/mse 0.0950 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 7
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396858/scgpt/split3
scGPT - INFO - Running on 2024-07-29 14:26:36
scGPT - INFO - match 4579/5008 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/507 batches | lr 0.0001 | ms/batch 399.91 | loss  0.20 | mse  0.20 |
scGPT - INFO - | epoch   1 | 200/507 batches | lr 0.0001 | ms/batch 394.58 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 300/507 batches | lr 0.0001 | ms/batch 393.89 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 400/507 batches | lr 0.0001 | ms/batch 393.72 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 500/507 batches | lr 0.0001 | ms/batch 393.77 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 220.16s | valid loss/mse 0.1036 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1036
scGPT - INFO - | epoch   2 | 100/507 batches | lr 0.0001 | ms/batch 398.40 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 200/507 batches | lr 0.0001 | ms/batch 393.88 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 300/507 batches | lr 0.0001 | ms/batch 393.97 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 400/507 batches | lr 0.0001 | ms/batch 393.79 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 500/507 batches | lr 0.0001 | ms/batch 394.00 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 219.87s | valid loss/mse 0.1008 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1008
scGPT - INFO - | epoch   3 | 100/507 batches | lr 0.0001 | ms/batch 398.04 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   3 | 200/507 batches | lr 0.0001 | ms/batch 394.34 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   3 | 300/507 batches | lr 0.0001 | ms/batch 394.31 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 400/507 batches | lr 0.0001 | ms/batch 394.14 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   3 | 500/507 batches | lr 0.0001 | ms/batch 394.47 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 220.01s | valid loss/mse 0.1053 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/507 batches | lr 0.0001 | ms/batch 398.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 200/507 batches | lr 0.0001 | ms/batch 394.10 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   4 | 300/507 batches | lr 0.0001 | ms/batch 394.02 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   4 | 400/507 batches | lr 0.0001 | ms/batch 394.16 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   4 | 500/507 batches | lr 0.0001 | ms/batch 394.14 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 219.96s | valid loss/mse 0.1002 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1002
scGPT - INFO - | epoch   5 | 100/507 batches | lr 0.0001 | ms/batch 398.00 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   5 | 200/507 batches | lr 0.0001 | ms/batch 394.66 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   5 | 300/507 batches | lr 0.0001 | ms/batch 395.89 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   5 | 400/507 batches | lr 0.0001 | ms/batch 395.74 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   5 | 500/507 batches | lr 0.0001 | ms/batch 395.49 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 220.64s | valid loss/mse 0.0983 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0983
scGPT - INFO - | epoch   6 | 100/507 batches | lr 0.0001 | ms/batch 400.58 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 200/507 batches | lr 0.0001 | ms/batch 396.63 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 300/507 batches | lr 0.0001 | ms/batch 395.42 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 400/507 batches | lr 0.0001 | ms/batch 395.48 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 500/507 batches | lr 0.0001 | ms/batch 395.85 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 221.13s | valid loss/mse 0.0992 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/507 batches | lr 0.0001 | ms/batch 400.49 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   7 | 200/507 batches | lr 0.0001 | ms/batch 395.62 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   7 | 300/507 batches | lr 0.0001 | ms/batch 395.14 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   7 | 400/507 batches | lr 0.0001 | ms/batch 395.15 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   7 | 500/507 batches | lr 0.0001 | ms/batch 395.23 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 220.68s | valid loss/mse 0.1023 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/507 batches | lr 0.0000 | ms/batch 398.04 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   8 | 200/507 batches | lr 0.0000 | ms/batch 394.19 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   8 | 300/507 batches | lr 0.0000 | ms/batch 394.04 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   8 | 400/507 batches | lr 0.0000 | ms/batch 394.25 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   8 | 500/507 batches | lr 0.0000 | ms/batch 394.36 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 220.03s | valid loss/mse 0.1064 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/507 batches | lr 0.0000 | ms/batch 398.29 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 200/507 batches | lr 0.0000 | ms/batch 393.97 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 300/507 batches | lr 0.0000 | ms/batch 393.95 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 400/507 batches | lr 0.0000 | ms/batch 394.04 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 500/507 batches | lr 0.0000 | ms/batch 393.79 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 219.87s | valid loss/mse 0.1018 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/507 batches | lr 0.0000 | ms/batch 398.29 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 200/507 batches | lr 0.0000 | ms/batch 393.76 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 300/507 batches | lr 0.0000 | ms/batch 393.93 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 400/507 batches | lr 0.0000 | ms/batch 393.83 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 500/507 batches | lr 0.0000 | ms/batch 393.98 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 219.87s | valid loss/mse 0.1025 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 10
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396858/scgpt/split4
scGPT - INFO - Running on 2024-07-29 15:09:20
scGPT - INFO - match 4579/5008 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/619 batches | lr 0.0001 | ms/batch 400.17 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 200/619 batches | lr 0.0001 | ms/batch 394.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 300/619 batches | lr 0.0001 | ms/batch 393.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/619 batches | lr 0.0001 | ms/batch 393.89 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 500/619 batches | lr 0.0001 | ms/batch 393.70 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 600/619 batches | lr 0.0001 | ms/batch 393.82 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 261.28s | valid loss/mse 0.0951 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0951
scGPT - INFO - | epoch   2 | 100/619 batches | lr 0.0001 | ms/batch 398.35 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 200/619 batches | lr 0.0001 | ms/batch 393.89 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 300/619 batches | lr 0.0001 | ms/batch 393.77 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 400/619 batches | lr 0.0001 | ms/batch 393.87 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 500/619 batches | lr 0.0001 | ms/batch 394.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 600/619 batches | lr 0.0001 | ms/batch 393.89 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 261.04s | valid loss/mse 0.0943 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0943
scGPT - INFO - | epoch   3 | 100/619 batches | lr 0.0001 | ms/batch 397.88 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 200/619 batches | lr 0.0001 | ms/batch 393.94 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 300/619 batches | lr 0.0001 | ms/batch 393.90 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 400/619 batches | lr 0.0001 | ms/batch 393.84 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 500/619 batches | lr 0.0001 | ms/batch 393.99 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 600/619 batches | lr 0.0001 | ms/batch 393.94 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 260.99s | valid loss/mse 0.0964 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/619 batches | lr 0.0001 | ms/batch 397.96 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 200/619 batches | lr 0.0001 | ms/batch 394.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 300/619 batches | lr 0.0001 | ms/batch 394.01 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 400/619 batches | lr 0.0001 | ms/batch 394.07 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 500/619 batches | lr 0.0001 | ms/batch 393.85 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 600/619 batches | lr 0.0001 | ms/batch 394.16 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 261.07s | valid loss/mse 0.0959 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/619 batches | lr 0.0001 | ms/batch 398.08 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 200/619 batches | lr 0.0001 | ms/batch 394.24 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 300/619 batches | lr 0.0001 | ms/batch 394.65 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 400/619 batches | lr 0.0001 | ms/batch 394.16 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 500/619 batches | lr 0.0001 | ms/batch 394.16 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 600/619 batches | lr 0.0001 | ms/batch 394.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 261.21s | valid loss/mse 0.0939 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0939
scGPT - INFO - | epoch   6 | 100/619 batches | lr 0.0001 | ms/batch 398.66 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 200/619 batches | lr 0.0001 | ms/batch 394.14 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 300/619 batches | lr 0.0001 | ms/batch 394.07 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 400/619 batches | lr 0.0001 | ms/batch 393.93 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 500/619 batches | lr 0.0001 | ms/batch 394.29 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 600/619 batches | lr 0.0001 | ms/batch 394.40 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 261.20s | valid loss/mse 0.0987 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/619 batches | lr 0.0001 | ms/batch 398.36 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 200/619 batches | lr 0.0001 | ms/batch 393.93 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 300/619 batches | lr 0.0001 | ms/batch 394.19 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 400/619 batches | lr 0.0001 | ms/batch 394.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 500/619 batches | lr 0.0001 | ms/batch 394.03 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 600/619 batches | lr 0.0001 | ms/batch 394.11 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 261.17s | valid loss/mse 0.0950 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/619 batches | lr 0.0000 | ms/batch 398.07 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 200/619 batches | lr 0.0000 | ms/batch 394.03 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 300/619 batches | lr 0.0000 | ms/batch 394.24 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 400/619 batches | lr 0.0000 | ms/batch 394.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 500/619 batches | lr 0.0000 | ms/batch 394.11 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 600/619 batches | lr 0.0000 | ms/batch 394.01 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 261.10s | valid loss/mse 0.0933 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0933
scGPT - INFO - | epoch   9 | 100/619 batches | lr 0.0000 | ms/batch 397.93 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 200/619 batches | lr 0.0000 | ms/batch 394.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 300/619 batches | lr 0.0000 | ms/batch 394.18 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 400/619 batches | lr 0.0000 | ms/batch 394.26 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 500/619 batches | lr 0.0000 | ms/batch 394.11 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 600/619 batches | lr 0.0000 | ms/batch 394.21 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 261.17s | valid loss/mse 0.0929 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0929
scGPT - INFO - | epoch  10 | 100/619 batches | lr 0.0000 | ms/batch 398.11 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 200/619 batches | lr 0.0000 | ms/batch 393.95 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 300/619 batches | lr 0.0000 | ms/batch 393.83 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 400/619 batches | lr 0.0000 | ms/batch 394.18 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 500/619 batches | lr 0.0000 | ms/batch 394.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 600/619 batches | lr 0.0000 | ms/batch 394.17 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 261.09s | valid loss/mse 0.0946 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/619 batches | lr 0.0000 | ms/batch 398.12 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 200/619 batches | lr 0.0000 | ms/batch 394.00 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 300/619 batches | lr 0.0000 | ms/batch 394.08 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 400/619 batches | lr 0.0000 | ms/batch 393.86 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 500/619 batches | lr 0.0000 | ms/batch 393.99 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 600/619 batches | lr 0.0000 | ms/batch 394.16 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 261.12s | valid loss/mse 0.0953 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/619 batches | lr 0.0000 | ms/batch 398.07 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 200/619 batches | lr 0.0000 | ms/batch 394.13 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 300/619 batches | lr 0.0000 | ms/batch 394.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 400/619 batches | lr 0.0000 | ms/batch 394.09 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 500/619 batches | lr 0.0000 | ms/batch 394.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  12 | 600/619 batches | lr 0.0000 | ms/batch 394.01 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 261.14s | valid loss/mse 0.0962 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/619 batches | lr 0.0000 | ms/batch 398.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 200/619 batches | lr 0.0000 | ms/batch 393.85 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 300/619 batches | lr 0.0000 | ms/batch 393.91 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 400/619 batches | lr 0.0000 | ms/batch 394.48 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 500/619 batches | lr 0.0000 | ms/batch 393.94 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 600/619 batches | lr 0.0000 | ms/batch 394.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 261.15s | valid loss/mse 0.0978 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/619 batches | lr 0.0000 | ms/batch 398.46 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 200/619 batches | lr 0.0000 | ms/batch 393.93 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 300/619 batches | lr 0.0000 | ms/batch 394.00 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 400/619 batches | lr 0.0000 | ms/batch 393.99 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 500/619 batches | lr 0.0000 | ms/batch 394.02 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 600/619 batches | lr 0.0000 | ms/batch 393.81 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 261.16s | valid loss/mse 0.0956 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 14
---Creating test_res
test_res saved successfully----
Split4 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396858/scgpt/split5
scGPT - INFO - Running on 2024-07-29 16:14:00
scGPT - INFO - match 4579/5008 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/529 batches | lr 0.0001 | ms/batch 399.43 | loss  0.16 | mse  0.16 |
scGPT - INFO - | epoch   1 | 200/529 batches | lr 0.0001 | ms/batch 394.24 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 300/529 batches | lr 0.0001 | ms/batch 394.05 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 400/529 batches | lr 0.0001 | ms/batch 394.20 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   1 | 500/529 batches | lr 0.0001 | ms/batch 394.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 228.88s | valid loss/mse 0.0997 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0997
scGPT - INFO - | epoch   2 | 100/529 batches | lr 0.0001 | ms/batch 398.41 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 200/529 batches | lr 0.0001 | ms/batch 394.54 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 300/529 batches | lr 0.0001 | ms/batch 394.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 400/529 batches | lr 0.0001 | ms/batch 394.36 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   2 | 500/529 batches | lr 0.0001 | ms/batch 394.25 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 228.74s | valid loss/mse 0.0997 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/529 batches | lr 0.0001 | ms/batch 398.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 200/529 batches | lr 0.0001 | ms/batch 394.20 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   3 | 300/529 batches | lr 0.0001 | ms/batch 394.34 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   3 | 400/529 batches | lr 0.0001 | ms/batch 394.17 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 500/529 batches | lr 0.0001 | ms/batch 394.41 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 228.70s | valid loss/mse 0.1081 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/529 batches | lr 0.0001 | ms/batch 398.20 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   4 | 200/529 batches | lr 0.0001 | ms/batch 394.43 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 300/529 batches | lr 0.0001 | ms/batch 394.27 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   4 | 400/529 batches | lr 0.0001 | ms/batch 394.77 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 500/529 batches | lr 0.0001 | ms/batch 394.20 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 228.80s | valid loss/mse 0.0968 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0968
scGPT - INFO - | epoch   5 | 100/529 batches | lr 0.0001 | ms/batch 399.32 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 200/529 batches | lr 0.0001 | ms/batch 394.28 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   5 | 300/529 batches | lr 0.0001 | ms/batch 394.38 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   5 | 400/529 batches | lr 0.0001 | ms/batch 394.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 500/529 batches | lr 0.0001 | ms/batch 394.06 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 228.76s | valid loss/mse 0.0997 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/529 batches | lr 0.0001 | ms/batch 397.93 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 200/529 batches | lr 0.0001 | ms/batch 394.02 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 300/529 batches | lr 0.0001 | ms/batch 394.21 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 400/529 batches | lr 0.0001 | ms/batch 394.37 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   6 | 500/529 batches | lr 0.0001 | ms/batch 394.36 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 228.70s | valid loss/mse 0.1036 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/529 batches | lr 0.0001 | ms/batch 398.41 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 200/529 batches | lr 0.0001 | ms/batch 394.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 300/529 batches | lr 0.0001 | ms/batch 394.58 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   7 | 400/529 batches | lr 0.0001 | ms/batch 394.16 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   7 | 500/529 batches | lr 0.0001 | ms/batch 394.37 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 228.73s | valid loss/mse 0.1005 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/529 batches | lr 0.0000 | ms/batch 398.23 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 200/529 batches | lr 0.0000 | ms/batch 394.16 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   8 | 300/529 batches | lr 0.0000 | ms/batch 394.35 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 400/529 batches | lr 0.0000 | ms/batch 394.41 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 500/529 batches | lr 0.0000 | ms/batch 394.30 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 228.71s | valid loss/mse 0.1020 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/529 batches | lr 0.0000 | ms/batch 397.80 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 200/529 batches | lr 0.0000 | ms/batch 394.42 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 300/529 batches | lr 0.0000 | ms/batch 394.38 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 400/529 batches | lr 0.0000 | ms/batch 394.24 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch   9 | 500/529 batches | lr 0.0000 | ms/batch 394.47 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 228.75s | valid loss/mse 0.0955 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0955
scGPT - INFO - | epoch  10 | 100/529 batches | lr 0.0000 | ms/batch 398.42 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 200/529 batches | lr 0.0000 | ms/batch 394.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 300/529 batches | lr 0.0000 | ms/batch 394.37 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 400/529 batches | lr 0.0000 | ms/batch 394.14 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  10 | 500/529 batches | lr 0.0000 | ms/batch 394.04 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 228.69s | valid loss/mse 0.0994 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/529 batches | lr 0.0000 | ms/batch 398.76 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 200/529 batches | lr 0.0000 | ms/batch 394.19 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  11 | 300/529 batches | lr 0.0000 | ms/batch 394.19 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  11 | 400/529 batches | lr 0.0000 | ms/batch 394.34 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  11 | 500/529 batches | lr 0.0000 | ms/batch 394.60 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 228.84s | valid loss/mse 0.0987 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/529 batches | lr 0.0000 | ms/batch 398.09 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  12 | 200/529 batches | lr 0.0000 | ms/batch 394.22 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  12 | 300/529 batches | lr 0.0000 | ms/batch 394.43 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  12 | 400/529 batches | lr 0.0000 | ms/batch 394.31 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  12 | 500/529 batches | lr 0.0000 | ms/batch 394.84 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 228.87s | valid loss/mse 0.0985 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/529 batches | lr 0.0000 | ms/batch 398.32 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 200/529 batches | lr 0.0000 | ms/batch 394.41 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  13 | 300/529 batches | lr 0.0000 | ms/batch 394.47 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 400/529 batches | lr 0.0000 | ms/batch 394.27 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  13 | 500/529 batches | lr 0.0000 | ms/batch 394.24 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 228.75s | valid loss/mse 0.0996 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/529 batches | lr 0.0000 | ms/batch 398.35 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  14 | 200/529 batches | lr 0.0000 | ms/batch 394.47 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  14 | 300/529 batches | lr 0.0000 | ms/batch 394.33 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  14 | 400/529 batches | lr 0.0000 | ms/batch 394.46 | loss  0.06 | mse  0.06 |
scGPT - INFO - | epoch  14 | 500/529 batches | lr 0.0000 | ms/batch 394.25 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 228.76s | valid loss/mse 0.0999 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 14
---Creating test_res
test_res saved successfully----
Split5 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396861/scgpt/split1
scGPT - INFO - Running on 2024-07-29 17:31:19
scGPT - INFO - match 4571/5007 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/429 batches | lr 0.0001 | ms/batch 409.29 | loss  0.22 | mse  0.22 |
scGPT - INFO - | epoch   1 | 200/429 batches | lr 0.0001 | ms/batch 395.18 | loss  0.10 | mse  0.10 |
scGPT - INFO - | epoch   1 | 300/429 batches | lr 0.0001 | ms/batch 394.87 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/429 batches | lr 0.0001 | ms/batch 394.70 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 184.88s | valid loss/mse 0.1048 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1048
scGPT - INFO - | epoch   2 | 100/429 batches | lr 0.0001 | ms/batch 398.89 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 200/429 batches | lr 0.0001 | ms/batch 394.95 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 300/429 batches | lr 0.0001 | ms/batch 395.05 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 400/429 batches | lr 0.0001 | ms/batch 395.04 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 183.99s | valid loss/mse 0.1038 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1038
scGPT - INFO - | epoch   3 | 100/429 batches | lr 0.0001 | ms/batch 399.17 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 200/429 batches | lr 0.0001 | ms/batch 394.68 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 300/429 batches | lr 0.0001 | ms/batch 394.68 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 400/429 batches | lr 0.0001 | ms/batch 394.54 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 183.80s | valid loss/mse 0.1015 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1015
scGPT - INFO - | epoch   4 | 100/429 batches | lr 0.0001 | ms/batch 398.32 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 200/429 batches | lr 0.0001 | ms/batch 394.19 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 300/429 batches | lr 0.0001 | ms/batch 393.94 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 400/429 batches | lr 0.0001 | ms/batch 394.06 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 183.50s | valid loss/mse 0.1001 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1001
scGPT - INFO - | epoch   5 | 100/429 batches | lr 0.0001 | ms/batch 397.52 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 200/429 batches | lr 0.0001 | ms/batch 393.60 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 300/429 batches | lr 0.0001 | ms/batch 393.50 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 400/429 batches | lr 0.0001 | ms/batch 397.03 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 183.74s | valid loss/mse 0.1019 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/429 batches | lr 0.0001 | ms/batch 397.31 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 200/429 batches | lr 0.0001 | ms/batch 393.70 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 300/429 batches | lr 0.0001 | ms/batch 393.76 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 400/429 batches | lr 0.0001 | ms/batch 394.32 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 183.32s | valid loss/mse 0.1001 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/429 batches | lr 0.0001 | ms/batch 397.68 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 200/429 batches | lr 0.0001 | ms/batch 393.47 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 300/429 batches | lr 0.0001 | ms/batch 394.06 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 400/429 batches | lr 0.0001 | ms/batch 393.44 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 183.28s | valid loss/mse 0.1021 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/429 batches | lr 0.0000 | ms/batch 397.84 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 200/429 batches | lr 0.0000 | ms/batch 394.27 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 300/429 batches | lr 0.0000 | ms/batch 393.52 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 400/429 batches | lr 0.0000 | ms/batch 393.55 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 183.35s | valid loss/mse 0.1038 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/429 batches | lr 0.0000 | ms/batch 397.76 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 200/429 batches | lr 0.0000 | ms/batch 393.74 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 300/429 batches | lr 0.0000 | ms/batch 393.85 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 400/429 batches | lr 0.0000 | ms/batch 393.55 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 183.30s | valid loss/mse 0.1010 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 9
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396861/scgpt/split2
scGPT - INFO - Running on 2024-07-29 18:08:21
scGPT - INFO - match 4571/5007 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/460 batches | lr 0.0001 | ms/batch 400.11 | loss  0.23 | mse  0.23 |
scGPT - INFO - | epoch   1 | 200/460 batches | lr 0.0001 | ms/batch 394.33 | loss  0.10 | mse  0.10 |
scGPT - INFO - | epoch   1 | 300/460 batches | lr 0.0001 | ms/batch 394.73 | loss  0.10 | mse  0.10 |
scGPT - INFO - | epoch   1 | 400/460 batches | lr 0.0001 | ms/batch 394.73 | loss  0.10 | mse  0.10 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 184.96s | valid loss/mse 0.1048 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1048
scGPT - INFO - | epoch   2 | 100/460 batches | lr 0.0001 | ms/batch 399.16 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 200/460 batches | lr 0.0001 | ms/batch 395.18 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 300/460 batches | lr 0.0001 | ms/batch 394.92 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 400/460 batches | lr 0.0001 | ms/batch 394.61 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 184.88s | valid loss/mse 0.1017 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1017
scGPT - INFO - | epoch   3 | 100/460 batches | lr 0.0001 | ms/batch 399.10 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 200/460 batches | lr 0.0001 | ms/batch 395.06 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 300/460 batches | lr 0.0001 | ms/batch 394.73 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 400/460 batches | lr 0.0001 | ms/batch 394.68 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 184.83s | valid loss/mse 0.1046 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/460 batches | lr 0.0001 | ms/batch 398.54 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 200/460 batches | lr 0.0001 | ms/batch 394.57 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 300/460 batches | lr 0.0001 | ms/batch 394.54 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 400/460 batches | lr 0.0001 | ms/batch 394.74 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 184.74s | valid loss/mse 0.1044 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/460 batches | lr 0.0001 | ms/batch 398.79 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 200/460 batches | lr 0.0001 | ms/batch 394.98 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 300/460 batches | lr 0.0001 | ms/batch 394.83 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 400/460 batches | lr 0.0001 | ms/batch 394.79 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 184.86s | valid loss/mse 0.1057 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/460 batches | lr 0.0001 | ms/batch 399.09 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 200/460 batches | lr 0.0001 | ms/batch 394.75 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 300/460 batches | lr 0.0001 | ms/batch 394.69 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 400/460 batches | lr 0.0001 | ms/batch 394.93 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 184.85s | valid loss/mse 0.1037 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/460 batches | lr 0.0001 | ms/batch 398.55 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 200/460 batches | lr 0.0001 | ms/batch 394.58 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 300/460 batches | lr 0.0001 | ms/batch 394.75 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 400/460 batches | lr 0.0001 | ms/batch 394.92 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 184.78s | valid loss/mse 0.1073 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 7
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396861/scgpt/split3
scGPT - INFO - Running on 2024-07-29 18:42:03
scGPT - INFO - match 4571/5007 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/440 batches | lr 0.0001 | ms/batch 400.57 | loss  0.28 | mse  0.28 |
scGPT - INFO - | epoch   1 | 200/440 batches | lr 0.0001 | ms/batch 394.84 | loss  0.10 | mse  0.10 |
scGPT - INFO - | epoch   1 | 300/440 batches | lr 0.0001 | ms/batch 395.04 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/440 batches | lr 0.0001 | ms/batch 395.15 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 176.45s | valid loss/mse 0.1036 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1036
scGPT - INFO - | epoch   2 | 100/440 batches | lr 0.0001 | ms/batch 399.01 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 200/440 batches | lr 0.0001 | ms/batch 394.19 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 300/440 batches | lr 0.0001 | ms/batch 394.71 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 400/440 batches | lr 0.0001 | ms/batch 394.48 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 176.08s | valid loss/mse 0.1009 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1009
scGPT - INFO - | epoch   3 | 100/440 batches | lr 0.0001 | ms/batch 397.99 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 200/440 batches | lr 0.0001 | ms/batch 394.06 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 300/440 batches | lr 0.0001 | ms/batch 393.75 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 400/440 batches | lr 0.0001 | ms/batch 393.95 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 175.77s | valid loss/mse 0.1024 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/440 batches | lr 0.0001 | ms/batch 397.90 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 200/440 batches | lr 0.0001 | ms/batch 394.18 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 300/440 batches | lr 0.0001 | ms/batch 394.15 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 400/440 batches | lr 0.0001 | ms/batch 393.96 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 175.81s | valid loss/mse 0.1025 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/440 batches | lr 0.0001 | ms/batch 398.41 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 200/440 batches | lr 0.0001 | ms/batch 393.98 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 300/440 batches | lr 0.0001 | ms/batch 393.78 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 400/440 batches | lr 0.0001 | ms/batch 393.93 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 175.79s | valid loss/mse 0.1040 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/440 batches | lr 0.0001 | ms/batch 397.83 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 200/440 batches | lr 0.0001 | ms/batch 394.21 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 300/440 batches | lr 0.0001 | ms/batch 394.28 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 400/440 batches | lr 0.0001 | ms/batch 394.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 175.88s | valid loss/mse 0.1044 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/440 batches | lr 0.0001 | ms/batch 398.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 200/440 batches | lr 0.0001 | ms/batch 394.41 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 300/440 batches | lr 0.0001 | ms/batch 394.49 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 400/440 batches | lr 0.0001 | ms/batch 394.07 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 175.92s | valid loss/mse 0.1022 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 7
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396861/scgpt/split4
scGPT - INFO - Running on 2024-07-29 19:16:32
scGPT - INFO - match 4571/5007 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/425 batches | lr 0.0001 | ms/batch 399.48 | loss  0.25 | mse  0.25 |
scGPT - INFO - | epoch   1 | 200/425 batches | lr 0.0001 | ms/batch 394.41 | loss  0.11 | mse  0.11 |
scGPT - INFO - | epoch   1 | 300/425 batches | lr 0.0001 | ms/batch 394.54 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 400/425 batches | lr 0.0001 | ms/batch 394.41 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 171.02s | valid loss/mse 0.1083 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1083
scGPT - INFO - | epoch   2 | 100/425 batches | lr 0.0001 | ms/batch 398.18 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 200/425 batches | lr 0.0001 | ms/batch 394.29 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 300/425 batches | lr 0.0001 | ms/batch 394.55 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 400/425 batches | lr 0.0001 | ms/batch 394.66 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 170.86s | valid loss/mse 0.1054 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1054
scGPT - INFO - | epoch   3 | 100/425 batches | lr 0.0001 | ms/batch 398.90 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 200/425 batches | lr 0.0001 | ms/batch 395.75 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 300/425 batches | lr 0.0001 | ms/batch 394.41 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 400/425 batches | lr 0.0001 | ms/batch 394.41 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 171.01s | valid loss/mse 0.1059 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/425 batches | lr 0.0001 | ms/batch 398.02 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 200/425 batches | lr 0.0001 | ms/batch 394.53 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 300/425 batches | lr 0.0001 | ms/batch 394.40 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 400/425 batches | lr 0.0001 | ms/batch 394.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 170.79s | valid loss/mse 0.1059 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/425 batches | lr 0.0001 | ms/batch 398.03 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 200/425 batches | lr 0.0001 | ms/batch 394.44 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 300/425 batches | lr 0.0001 | ms/batch 394.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 400/425 batches | lr 0.0001 | ms/batch 394.19 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 170.76s | valid loss/mse 0.1080 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/425 batches | lr 0.0001 | ms/batch 398.41 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 200/425 batches | lr 0.0001 | ms/batch 394.07 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 300/425 batches | lr 0.0001 | ms/batch 394.31 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 400/425 batches | lr 0.0001 | ms/batch 394.02 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 170.74s | valid loss/mse 0.1048 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1048
scGPT - INFO - | epoch   7 | 100/425 batches | lr 0.0001 | ms/batch 398.39 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 200/425 batches | lr 0.0001 | ms/batch 394.08 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 300/425 batches | lr 0.0001 | ms/batch 394.13 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 400/425 batches | lr 0.0001 | ms/batch 394.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 170.74s | valid loss/mse 0.1039 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1039
scGPT - INFO - | epoch   8 | 100/425 batches | lr 0.0000 | ms/batch 398.35 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 200/425 batches | lr 0.0000 | ms/batch 394.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 300/425 batches | lr 0.0000 | ms/batch 394.83 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 400/425 batches | lr 0.0000 | ms/batch 394.70 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 170.91s | valid loss/mse 0.1083 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/425 batches | lr 0.0000 | ms/batch 398.48 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 200/425 batches | lr 0.0000 | ms/batch 394.48 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 300/425 batches | lr 0.0000 | ms/batch 394.22 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 400/425 batches | lr 0.0000 | ms/batch 394.47 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 170.84s | valid loss/mse 0.1056 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/425 batches | lr 0.0000 | ms/batch 398.36 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  10 | 200/425 batches | lr 0.0000 | ms/batch 394.64 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  10 | 300/425 batches | lr 0.0000 | ms/batch 394.44 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  10 | 400/425 batches | lr 0.0000 | ms/batch 394.45 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 170.87s | valid loss/mse 0.1054 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/425 batches | lr 0.0000 | ms/batch 398.44 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  11 | 200/425 batches | lr 0.0000 | ms/batch 394.52 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  11 | 300/425 batches | lr 0.0000 | ms/batch 394.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  11 | 400/425 batches | lr 0.0000 | ms/batch 394.52 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 170.88s | valid loss/mse 0.1063 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/425 batches | lr 0.0000 | ms/batch 398.77 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  12 | 200/425 batches | lr 0.0000 | ms/batch 394.76 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  12 | 300/425 batches | lr 0.0000 | ms/batch 394.53 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  12 | 400/425 batches | lr 0.0000 | ms/batch 394.24 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 170.90s | valid loss/mse 0.1035 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1035
scGPT - INFO - | epoch  13 | 100/425 batches | lr 0.0000 | ms/batch 398.17 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  13 | 200/425 batches | lr 0.0000 | ms/batch 394.57 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  13 | 300/425 batches | lr 0.0000 | ms/batch 394.34 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  13 | 400/425 batches | lr 0.0000 | ms/batch 394.42 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 170.84s | valid loss/mse 0.1049 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/425 batches | lr 0.0000 | ms/batch 398.37 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  14 | 200/425 batches | lr 0.0000 | ms/batch 394.56 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  14 | 300/425 batches | lr 0.0000 | ms/batch 394.68 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  14 | 400/425 batches | lr 0.0000 | ms/batch 394.55 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 170.91s | valid loss/mse 0.1018 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1018
scGPT - INFO - | epoch  15 | 100/425 batches | lr 0.0000 | ms/batch 398.68 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  15 | 200/425 batches | lr 0.0000 | ms/batch 394.25 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  15 | 300/425 batches | lr 0.0000 | ms/batch 394.55 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  15 | 400/425 batches | lr 0.0000 | ms/batch 394.66 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 170.89s | valid loss/mse 0.1053 |
scGPT - INFO - -----------------------------------------------------------------------------------------
---Creating test_res
test_res saved successfully----
Split4 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/Dixit_GSM2396861/scgpt/split5
scGPT - INFO - Running on 2024-07-29 20:14:07
scGPT - INFO - match 4571/5007 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/396 batches | lr 0.0001 | ms/batch 399.34 | loss  0.23 | mse  0.23 |
scGPT - INFO - | epoch   1 | 200/396 batches | lr 0.0001 | ms/batch 394.22 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/396 batches | lr 0.0001 | ms/batch 394.57 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 163.42s | valid loss/mse 0.1068 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1068
scGPT - INFO - | epoch   2 | 100/396 batches | lr 0.0001 | ms/batch 398.28 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 200/396 batches | lr 0.0001 | ms/batch 394.16 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   2 | 300/396 batches | lr 0.0001 | ms/batch 394.14 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 163.20s | valid loss/mse 0.1036 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1036
scGPT - INFO - | epoch   3 | 100/396 batches | lr 0.0001 | ms/batch 397.92 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 200/396 batches | lr 0.0001 | ms/batch 394.10 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   3 | 300/396 batches | lr 0.0001 | ms/batch 393.96 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 163.15s | valid loss/mse 0.1039 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/396 batches | lr 0.0001 | ms/batch 398.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 200/396 batches | lr 0.0001 | ms/batch 394.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   4 | 300/396 batches | lr 0.0001 | ms/batch 394.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 163.28s | valid loss/mse 0.1044 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/396 batches | lr 0.0001 | ms/batch 398.35 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 200/396 batches | lr 0.0001 | ms/batch 394.28 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   5 | 300/396 batches | lr 0.0001 | ms/batch 394.21 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 163.24s | valid loss/mse 0.1033 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1033
scGPT - INFO - | epoch   6 | 100/396 batches | lr 0.0001 | ms/batch 398.56 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 200/396 batches | lr 0.0001 | ms/batch 394.57 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   6 | 300/396 batches | lr 0.0001 | ms/batch 394.50 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 163.33s | valid loss/mse 0.1021 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1021
scGPT - INFO - | epoch   7 | 100/396 batches | lr 0.0001 | ms/batch 398.20 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 200/396 batches | lr 0.0001 | ms/batch 394.65 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   7 | 300/396 batches | lr 0.0001 | ms/batch 394.45 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 163.28s | valid loss/mse 0.1040 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/396 batches | lr 0.0000 | ms/batch 398.03 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 200/396 batches | lr 0.0000 | ms/batch 394.42 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   8 | 300/396 batches | lr 0.0000 | ms/batch 394.33 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 163.25s | valid loss/mse 0.1044 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/396 batches | lr 0.0000 | ms/batch 398.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 200/396 batches | lr 0.0000 | ms/batch 394.45 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   9 | 300/396 batches | lr 0.0000 | ms/batch 394.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 163.21s | valid loss/mse 0.1026 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/396 batches | lr 0.0000 | ms/batch 397.83 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  10 | 200/396 batches | lr 0.0000 | ms/batch 394.04 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  10 | 300/396 batches | lr 0.0000 | ms/batch 393.81 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 163.04s | valid loss/mse 0.1039 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/396 batches | lr 0.0000 | ms/batch 397.71 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  11 | 200/396 batches | lr 0.0000 | ms/batch 393.78 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch  11 | 300/396 batches | lr 0.0000 | ms/batch 394.26 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 163.10s | valid loss/mse 0.1039 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 11
---Creating test_res
test_res saved successfully----
Split5 computation completed
