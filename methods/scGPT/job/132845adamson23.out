环境设置完成！
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406677_2/scgpt/split1
scGPT - INFO - Running on 2024-07-26 04:38:39
scGPT - INFO - match 4351/5001 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/153 batches | lr 0.0001 | ms/batch 406.18 | loss  0.18 | mse  0.18 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 69.79s | valid loss/mse 0.1519 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1519
scGPT - INFO - | epoch   2 | 100/153 batches | lr 0.0001 | ms/batch 397.17 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 69.00s | valid loss/mse 0.1371 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1371
scGPT - INFO - | epoch   3 | 100/153 batches | lr 0.0001 | ms/batch 396.86 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 68.84s | valid loss/mse 0.1615 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/153 batches | lr 0.0001 | ms/batch 396.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 68.82s | valid loss/mse 0.1279 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1279
scGPT - INFO - | epoch   5 | 100/153 batches | lr 0.0001 | ms/batch 396.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 68.87s | valid loss/mse 0.1335 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/153 batches | lr 0.0001 | ms/batch 396.81 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 68.84s | valid loss/mse 0.1602 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/153 batches | lr 0.0001 | ms/batch 396.72 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 68.85s | valid loss/mse 0.1425 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/153 batches | lr 0.0000 | ms/batch 397.12 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 68.83s | valid loss/mse 0.1543 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/153 batches | lr 0.0000 | ms/batch 396.14 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 68.70s | valid loss/mse 0.1398 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 9
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406677_2/scgpt/split2
scGPT - INFO - Running on 2024-07-26 05:04:20
scGPT - INFO - match 4351/5001 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/156 batches | lr 0.0001 | ms/batch 396.67 | loss  0.25 | mse  0.25 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 69.34s | valid loss/mse 0.1491 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1491
scGPT - INFO - | epoch   2 | 100/156 batches | lr 0.0001 | ms/batch 396.22 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 69.29s | valid loss/mse 0.1453 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1453
scGPT - INFO - | epoch   3 | 100/156 batches | lr 0.0001 | ms/batch 396.13 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 69.32s | valid loss/mse 0.1485 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/156 batches | lr 0.0001 | ms/batch 396.22 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 69.30s | valid loss/mse 0.1383 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1383
scGPT - INFO - | epoch   5 | 100/156 batches | lr 0.0001 | ms/batch 396.06 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 69.27s | valid loss/mse 0.1389 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/156 batches | lr 0.0001 | ms/batch 395.99 | loss  0.06 | mse  0.06 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 69.28s | valid loss/mse 0.1384 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/156 batches | lr 0.0001 | ms/batch 396.31 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 69.30s | valid loss/mse 0.1380 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1380
scGPT - INFO - | epoch   8 | 100/156 batches | lr 0.0000 | ms/batch 395.98 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 69.26s | valid loss/mse 0.1359 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1359
scGPT - INFO - | epoch   9 | 100/156 batches | lr 0.0000 | ms/batch 395.93 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 69.26s | valid loss/mse 0.1380 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/156 batches | lr 0.0000 | ms/batch 396.17 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 69.28s | valid loss/mse 0.1335 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1335
scGPT - INFO - | epoch  11 | 100/156 batches | lr 0.0000 | ms/batch 395.95 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 69.27s | valid loss/mse 0.1354 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/156 batches | lr 0.0000 | ms/batch 396.07 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 69.28s | valid loss/mse 0.1481 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/156 batches | lr 0.0000 | ms/batch 396.48 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 69.34s | valid loss/mse 0.1546 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/156 batches | lr 0.0000 | ms/batch 396.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 69.32s | valid loss/mse 0.1399 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  15 | 100/156 batches | lr 0.0000 | ms/batch 396.27 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  15 | time: 69.28s | valid loss/mse 0.1451 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 15
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406677_2/scgpt/split3
scGPT - INFO - Running on 2024-07-26 05:37:03
scGPT - INFO - match 4351/5001 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/153 batches | lr 0.0001 | ms/batch 397.09 | loss  0.20 | mse  0.20 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 60.18s | valid loss/mse 0.1574 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1574
scGPT - INFO - | epoch   2 | 100/153 batches | lr 0.0001 | ms/batch 396.56 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 60.11s | valid loss/mse 0.1704 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/153 batches | lr 0.0001 | ms/batch 396.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 60.12s | valid loss/mse 0.1493 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1493
scGPT - INFO - | epoch   4 | 100/153 batches | lr 0.0001 | ms/batch 396.50 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 60.09s | valid loss/mse 0.1429 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1429
scGPT - INFO - | epoch   5 | 100/153 batches | lr 0.0001 | ms/batch 396.33 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 60.09s | valid loss/mse 0.1571 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/153 batches | lr 0.0001 | ms/batch 396.53 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 60.11s | valid loss/mse 0.1599 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/153 batches | lr 0.0001 | ms/batch 396.56 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 60.12s | valid loss/mse 0.1544 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/153 batches | lr 0.0000 | ms/batch 396.59 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 60.12s | valid loss/mse 0.1663 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/153 batches | lr 0.0000 | ms/batch 396.45 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 60.12s | valid loss/mse 0.1217 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1217
scGPT - INFO - | epoch  10 | 100/153 batches | lr 0.0000 | ms/batch 396.73 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 60.14s | valid loss/mse 0.1557 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/153 batches | lr 0.0000 | ms/batch 396.74 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 60.13s | valid loss/mse 0.1474 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/153 batches | lr 0.0000 | ms/batch 396.25 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 60.07s | valid loss/mse 0.1407 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  13 | 100/153 batches | lr 0.0000 | ms/batch 396.63 | loss  0.04 | mse  0.04 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  13 | time: 60.12s | valid loss/mse 0.1621 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  14 | 100/153 batches | lr 0.0000 | ms/batch 396.89 | loss  0.05 | mse  0.05 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  14 | time: 60.14s | valid loss/mse 0.1538 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 14
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406677_2/scgpt/split4
scGPT - INFO - Running on 2024-07-26 06:10:21
scGPT - INFO - match 4351/5001 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/205 batches | lr 0.0001 | ms/batch 397.40 | loss  0.27 | mse  0.27 |
scGPT - INFO - | epoch   1 | 200/205 batches | lr 0.0001 | ms/batch 392.57 | loss  0.09 | mse  0.09 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 80.62s | valid loss/mse 0.1078 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1078
scGPT - INFO - | epoch   2 | 100/205 batches | lr 0.0001 | ms/batch 396.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/205 batches | lr 0.0001 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 80.49s | valid loss/mse 0.0896 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0896
scGPT - INFO - | epoch   3 | 100/205 batches | lr 0.0001 | ms/batch 396.39 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 200/205 batches | lr 0.0001 | ms/batch 392.53 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 80.49s | valid loss/mse 0.1243 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/205 batches | lr 0.0001 | ms/batch 396.46 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 200/205 batches | lr 0.0001 | ms/batch 392.64 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 80.51s | valid loss/mse 0.1036 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/205 batches | lr 0.0001 | ms/batch 396.41 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 200/205 batches | lr 0.0001 | ms/batch 392.70 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 80.51s | valid loss/mse 0.1257 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/205 batches | lr 0.0001 | ms/batch 396.59 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 200/205 batches | lr 0.0001 | ms/batch 392.68 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 80.53s | valid loss/mse 0.1121 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/205 batches | lr 0.0001 | ms/batch 396.38 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 200/205 batches | lr 0.0001 | ms/batch 392.74 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 80.51s | valid loss/mse 0.0896 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 7
---Creating test_res
test_res saved successfully----
Split4 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406677_2/scgpt/split5
scGPT - INFO - Running on 2024-07-26 06:35:05
scGPT - INFO - match 4351/5001 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/205 batches | lr 0.0001 | ms/batch 396.65 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 200/205 batches | lr 0.0001 | ms/batch 392.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 80.56s | valid loss/mse 0.1392 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1392
scGPT - INFO - | epoch   2 | 100/205 batches | lr 0.0001 | ms/batch 396.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/205 batches | lr 0.0001 | ms/batch 392.61 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 80.53s | valid loss/mse 0.1454 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/205 batches | lr 0.0001 | ms/batch 396.33 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   3 | 200/205 batches | lr 0.0001 | ms/batch 392.51 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 80.49s | valid loss/mse 0.1466 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/205 batches | lr 0.0001 | ms/batch 396.50 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   4 | 200/205 batches | lr 0.0001 | ms/batch 392.56 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 80.52s | valid loss/mse 0.1396 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/205 batches | lr 0.0001 | ms/batch 396.39 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   5 | 200/205 batches | lr 0.0001 | ms/batch 392.59 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 80.51s | valid loss/mse 0.1423 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/205 batches | lr 0.0001 | ms/batch 396.66 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   6 | 200/205 batches | lr 0.0001 | ms/batch 392.40 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 80.52s | valid loss/mse 0.1229 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.1229
scGPT - INFO - | epoch   7 | 100/205 batches | lr 0.0001 | ms/batch 396.93 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   7 | 200/205 batches | lr 0.0001 | ms/batch 392.41 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 80.54s | valid loss/mse 0.1315 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/205 batches | lr 0.0000 | ms/batch 396.22 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   8 | 200/205 batches | lr 0.0000 | ms/batch 392.32 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 80.46s | valid loss/mse 0.1253 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/205 batches | lr 0.0000 | ms/batch 396.28 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch   9 | 200/205 batches | lr 0.0000 | ms/batch 392.39 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 80.48s | valid loss/mse 0.1390 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/205 batches | lr 0.0000 | ms/batch 396.20 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  10 | 200/205 batches | lr 0.0000 | ms/batch 392.19 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 80.45s | valid loss/mse 0.1271 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/205 batches | lr 0.0000 | ms/batch 396.52 | loss  0.07 | mse  0.07 |
scGPT - INFO - | epoch  11 | 200/205 batches | lr 0.0000 | ms/batch 392.77 | loss  0.07 | mse  0.07 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 80.54s | valid loss/mse 0.1376 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 11
---Creating test_res
test_res saved successfully----
Split5 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406681_3/scgpt/split1
scGPT - INFO - Running on 2024-07-26 09:13:31
scGPT - INFO - match 4384/5063 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1265 batches | lr 0.0001 | ms/batch 404.54 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 200/1265 batches | lr 0.0001 | ms/batch 391.14 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1265 batches | lr 0.0001 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/1265 batches | lr 0.0001 | ms/batch 391.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/1265 batches | lr 0.0001 | ms/batch 391.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/1265 batches | lr 0.0001 | ms/batch 391.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/1265 batches | lr 0.0001 | ms/batch 391.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/1265 batches | lr 0.0001 | ms/batch 391.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1265 batches | lr 0.0001 | ms/batch 391.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1265 batches | lr 0.0001 | ms/batch 391.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1100/1265 batches | lr 0.0001 | ms/batch 391.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1200/1265 batches | lr 0.0001 | ms/batch 391.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 514.27s | valid loss/mse 0.0938 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0938
scGPT - INFO - | epoch   2 | 100/1265 batches | lr 0.0001 | ms/batch 395.83 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1265 batches | lr 0.0001 | ms/batch 391.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1265 batches | lr 0.0001 | ms/batch 391.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1265 batches | lr 0.0001 | ms/batch 391.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1265 batches | lr 0.0001 | ms/batch 391.65 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1265 batches | lr 0.0001 | ms/batch 391.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1265 batches | lr 0.0001 | ms/batch 391.71 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1265 batches | lr 0.0001 | ms/batch 391.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1265 batches | lr 0.0001 | ms/batch 391.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1265 batches | lr 0.0001 | ms/batch 391.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1265 batches | lr 0.0001 | ms/batch 391.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1200/1265 batches | lr 0.0001 | ms/batch 391.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 513.35s | valid loss/mse 0.0893 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0893
scGPT - INFO - | epoch   3 | 100/1265 batches | lr 0.0001 | ms/batch 395.70 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1265 batches | lr 0.0001 | ms/batch 391.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1265 batches | lr 0.0001 | ms/batch 391.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1265 batches | lr 0.0001 | ms/batch 391.66 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1265 batches | lr 0.0001 | ms/batch 391.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1265 batches | lr 0.0001 | ms/batch 391.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1265 batches | lr 0.0001 | ms/batch 391.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1265 batches | lr 0.0001 | ms/batch 391.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1265 batches | lr 0.0001 | ms/batch 391.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1265 batches | lr 0.0001 | ms/batch 391.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1265 batches | lr 0.0001 | ms/batch 391.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1200/1265 batches | lr 0.0001 | ms/batch 391.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 513.40s | valid loss/mse 0.0896 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1265 batches | lr 0.0001 | ms/batch 395.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1265 batches | lr 0.0001 | ms/batch 391.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1265 batches | lr 0.0001 | ms/batch 391.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1265 batches | lr 0.0001 | ms/batch 391.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1265 batches | lr 0.0001 | ms/batch 391.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1265 batches | lr 0.0001 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1265 batches | lr 0.0001 | ms/batch 391.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1265 batches | lr 0.0001 | ms/batch 391.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1265 batches | lr 0.0001 | ms/batch 391.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1265 batches | lr 0.0001 | ms/batch 391.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1265 batches | lr 0.0001 | ms/batch 391.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1200/1265 batches | lr 0.0001 | ms/batch 391.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 513.32s | valid loss/mse 0.0898 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1265 batches | lr 0.0001 | ms/batch 395.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1265 batches | lr 0.0001 | ms/batch 391.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1265 batches | lr 0.0001 | ms/batch 391.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1265 batches | lr 0.0001 | ms/batch 391.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1265 batches | lr 0.0001 | ms/batch 391.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1265 batches | lr 0.0001 | ms/batch 391.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1265 batches | lr 0.0001 | ms/batch 391.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1265 batches | lr 0.0001 | ms/batch 391.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1265 batches | lr 0.0001 | ms/batch 391.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1265 batches | lr 0.0001 | ms/batch 391.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1265 batches | lr 0.0001 | ms/batch 391.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1200/1265 batches | lr 0.0001 | ms/batch 391.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 513.01s | valid loss/mse 0.0889 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0889
scGPT - INFO - | epoch   6 | 100/1265 batches | lr 0.0001 | ms/batch 395.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1265 batches | lr 0.0001 | ms/batch 391.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1265 batches | lr 0.0001 | ms/batch 391.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1265 batches | lr 0.0001 | ms/batch 391.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1265 batches | lr 0.0001 | ms/batch 391.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1265 batches | lr 0.0001 | ms/batch 391.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1265 batches | lr 0.0001 | ms/batch 391.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1265 batches | lr 0.0001 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1265 batches | lr 0.0001 | ms/batch 391.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1265 batches | lr 0.0001 | ms/batch 391.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1265 batches | lr 0.0001 | ms/batch 391.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1200/1265 batches | lr 0.0001 | ms/batch 391.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 513.18s | valid loss/mse 0.0890 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1265 batches | lr 0.0001 | ms/batch 395.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1265 batches | lr 0.0001 | ms/batch 391.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1265 batches | lr 0.0001 | ms/batch 391.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1265 batches | lr 0.0001 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1265 batches | lr 0.0001 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1265 batches | lr 0.0001 | ms/batch 391.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1265 batches | lr 0.0001 | ms/batch 391.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1265 batches | lr 0.0001 | ms/batch 391.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1265 batches | lr 0.0001 | ms/batch 391.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1265 batches | lr 0.0001 | ms/batch 391.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1265 batches | lr 0.0001 | ms/batch 391.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1200/1265 batches | lr 0.0001 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 513.09s | valid loss/mse 0.0881 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0881
scGPT - INFO - | epoch   8 | 100/1265 batches | lr 0.0000 | ms/batch 395.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/1265 batches | lr 0.0000 | ms/batch 391.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/1265 batches | lr 0.0000 | ms/batch 391.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/1265 batches | lr 0.0000 | ms/batch 391.71 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/1265 batches | lr 0.0000 | ms/batch 391.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/1265 batches | lr 0.0000 | ms/batch 391.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/1265 batches | lr 0.0000 | ms/batch 391.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/1265 batches | lr 0.0000 | ms/batch 391.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/1265 batches | lr 0.0000 | ms/batch 391.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1000/1265 batches | lr 0.0000 | ms/batch 391.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1100/1265 batches | lr 0.0000 | ms/batch 391.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1200/1265 batches | lr 0.0000 | ms/batch 391.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 513.35s | valid loss/mse 0.0885 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1265 batches | lr 0.0000 | ms/batch 395.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/1265 batches | lr 0.0000 | ms/batch 391.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/1265 batches | lr 0.0000 | ms/batch 391.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/1265 batches | lr 0.0000 | ms/batch 391.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/1265 batches | lr 0.0000 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/1265 batches | lr 0.0000 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/1265 batches | lr 0.0000 | ms/batch 391.07 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/1265 batches | lr 0.0000 | ms/batch 391.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/1265 batches | lr 0.0000 | ms/batch 391.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1000/1265 batches | lr 0.0000 | ms/batch 390.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1100/1265 batches | lr 0.0000 | ms/batch 391.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1200/1265 batches | lr 0.0000 | ms/batch 391.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 512.91s | valid loss/mse 0.0897 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/1265 batches | lr 0.0000 | ms/batch 395.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 200/1265 batches | lr 0.0000 | ms/batch 391.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 300/1265 batches | lr 0.0000 | ms/batch 391.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 400/1265 batches | lr 0.0000 | ms/batch 391.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 500/1265 batches | lr 0.0000 | ms/batch 391.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 600/1265 batches | lr 0.0000 | ms/batch 391.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 700/1265 batches | lr 0.0000 | ms/batch 391.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 800/1265 batches | lr 0.0000 | ms/batch 391.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 900/1265 batches | lr 0.0000 | ms/batch 391.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1000/1265 batches | lr 0.0000 | ms/batch 391.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1100/1265 batches | lr 0.0000 | ms/batch 391.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1200/1265 batches | lr 0.0000 | ms/batch 391.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 513.06s | valid loss/mse 0.0888 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/1265 batches | lr 0.0000 | ms/batch 395.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 200/1265 batches | lr 0.0000 | ms/batch 391.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 300/1265 batches | lr 0.0000 | ms/batch 391.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 400/1265 batches | lr 0.0000 | ms/batch 391.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 500/1265 batches | lr 0.0000 | ms/batch 391.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 600/1265 batches | lr 0.0000 | ms/batch 391.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 700/1265 batches | lr 0.0000 | ms/batch 391.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 800/1265 batches | lr 0.0000 | ms/batch 391.59 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 900/1265 batches | lr 0.0000 | ms/batch 391.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1000/1265 batches | lr 0.0000 | ms/batch 391.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1100/1265 batches | lr 0.0000 | ms/batch 391.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1200/1265 batches | lr 0.0000 | ms/batch 391.68 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 513.35s | valid loss/mse 0.0893 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  12 | 100/1265 batches | lr 0.0000 | ms/batch 395.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 200/1265 batches | lr 0.0000 | ms/batch 391.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 300/1265 batches | lr 0.0000 | ms/batch 391.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 400/1265 batches | lr 0.0000 | ms/batch 391.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 500/1265 batches | lr 0.0000 | ms/batch 391.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 600/1265 batches | lr 0.0000 | ms/batch 391.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 700/1265 batches | lr 0.0000 | ms/batch 391.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 800/1265 batches | lr 0.0000 | ms/batch 391.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 900/1265 batches | lr 0.0000 | ms/batch 391.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 1000/1265 batches | lr 0.0000 | ms/batch 391.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 1100/1265 batches | lr 0.0000 | ms/batch 391.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  12 | 1200/1265 batches | lr 0.0000 | ms/batch 391.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  12 | time: 513.23s | valid loss/mse 0.0890 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 12
---Creating test_res
test_res saved successfully----
Split1 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406681_3/scgpt/split2
scGPT - INFO - Running on 2024-07-26 11:30:52
scGPT - INFO - match 4384/5063 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1243 batches | lr 0.0001 | ms/batch 396.43 | loss  0.17 | mse  0.17 |
scGPT - INFO - | epoch   1 | 200/1243 batches | lr 0.0001 | ms/batch 391.38 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1243 batches | lr 0.0001 | ms/batch 391.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/1243 batches | lr 0.0001 | ms/batch 391.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/1243 batches | lr 0.0001 | ms/batch 391.84 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/1243 batches | lr 0.0001 | ms/batch 392.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/1243 batches | lr 0.0001 | ms/batch 391.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/1243 batches | lr 0.0001 | ms/batch 391.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1243 batches | lr 0.0001 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1243 batches | lr 0.0001 | ms/batch 392.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1100/1243 batches | lr 0.0001 | ms/batch 392.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1200/1243 batches | lr 0.0001 | ms/batch 391.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 509.72s | valid loss/mse 0.0885 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0885
scGPT - INFO - | epoch   2 | 100/1243 batches | lr 0.0001 | ms/batch 396.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1243 batches | lr 0.0001 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1243 batches | lr 0.0001 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1243 batches | lr 0.0001 | ms/batch 392.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1243 batches | lr 0.0001 | ms/batch 391.73 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1243 batches | lr 0.0001 | ms/batch 392.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1243 batches | lr 0.0001 | ms/batch 392.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1243 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1243 batches | lr 0.0001 | ms/batch 391.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1243 batches | lr 0.0001 | ms/batch 391.71 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1243 batches | lr 0.0001 | ms/batch 391.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1200/1243 batches | lr 0.0001 | ms/batch 392.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 509.71s | valid loss/mse 0.0860 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0860
scGPT - INFO - | epoch   3 | 100/1243 batches | lr 0.0001 | ms/batch 396.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1243 batches | lr 0.0001 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1243 batches | lr 0.0001 | ms/batch 391.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1243 batches | lr 0.0001 | ms/batch 391.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1243 batches | lr 0.0001 | ms/batch 391.91 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1243 batches | lr 0.0001 | ms/batch 391.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1243 batches | lr 0.0001 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1243 batches | lr 0.0001 | ms/batch 391.81 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1243 batches | lr 0.0001 | ms/batch 391.87 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1243 batches | lr 0.0001 | ms/batch 392.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1243 batches | lr 0.0001 | ms/batch 392.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1200/1243 batches | lr 0.0001 | ms/batch 392.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 509.67s | valid loss/mse 0.0870 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1243 batches | lr 0.0001 | ms/batch 396.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1243 batches | lr 0.0001 | ms/batch 391.81 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1243 batches | lr 0.0001 | ms/batch 391.82 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1243 batches | lr 0.0001 | ms/batch 391.92 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1243 batches | lr 0.0001 | ms/batch 391.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1243 batches | lr 0.0001 | ms/batch 391.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1243 batches | lr 0.0001 | ms/batch 392.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1243 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1243 batches | lr 0.0001 | ms/batch 392.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1243 batches | lr 0.0001 | ms/batch 391.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1243 batches | lr 0.0001 | ms/batch 391.98 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1200/1243 batches | lr 0.0001 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 509.67s | valid loss/mse 0.0867 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1243 batches | lr 0.0001 | ms/batch 395.91 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1243 batches | lr 0.0001 | ms/batch 392.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1243 batches | lr 0.0001 | ms/batch 392.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1243 batches | lr 0.0001 | ms/batch 391.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1243 batches | lr 0.0001 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1243 batches | lr 0.0001 | ms/batch 392.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1243 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1243 batches | lr 0.0001 | ms/batch 392.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1243 batches | lr 0.0001 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1243 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1243 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1200/1243 batches | lr 0.0001 | ms/batch 392.07 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 509.73s | valid loss/mse 0.0864 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1243 batches | lr 0.0001 | ms/batch 395.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1243 batches | lr 0.0001 | ms/batch 391.84 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1243 batches | lr 0.0001 | ms/batch 391.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1243 batches | lr 0.0001 | ms/batch 391.86 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1243 batches | lr 0.0001 | ms/batch 391.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1243 batches | lr 0.0001 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1243 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1243 batches | lr 0.0001 | ms/batch 392.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1243 batches | lr 0.0001 | ms/batch 391.82 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1243 batches | lr 0.0001 | ms/batch 392.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1243 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1200/1243 batches | lr 0.0001 | ms/batch 393.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 509.79s | valid loss/mse 0.0869 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1243 batches | lr 0.0001 | ms/batch 395.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1243 batches | lr 0.0001 | ms/batch 391.87 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1243 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1243 batches | lr 0.0001 | ms/batch 391.88 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1243 batches | lr 0.0001 | ms/batch 392.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1243 batches | lr 0.0001 | ms/batch 392.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1243 batches | lr 0.0001 | ms/batch 391.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1243 batches | lr 0.0001 | ms/batch 392.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1243 batches | lr 0.0001 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1243 batches | lr 0.0001 | ms/batch 392.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1243 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1200/1243 batches | lr 0.0001 | ms/batch 392.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 509.78s | valid loss/mse 0.0865 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 7
---Creating test_res
test_res saved successfully----
Split2 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406681_3/scgpt/split3
scGPT - INFO - Running on 2024-07-26 13:04:30
scGPT - INFO - match 4384/5063 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1285 batches | lr 0.0001 | ms/batch 397.23 | loss  0.18 | mse  0.18 |
scGPT - INFO - | epoch   1 | 200/1285 batches | lr 0.0001 | ms/batch 392.52 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1285 batches | lr 0.0001 | ms/batch 392.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/1285 batches | lr 0.0001 | ms/batch 392.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/1285 batches | lr 0.0001 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/1285 batches | lr 0.0001 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/1285 batches | lr 0.0001 | ms/batch 392.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/1285 batches | lr 0.0001 | ms/batch 392.62 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1285 batches | lr 0.0001 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1285 batches | lr 0.0001 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1100/1285 batches | lr 0.0001 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1200/1285 batches | lr 0.0001 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 517.78s | valid loss/mse 0.0863 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0863
scGPT - INFO - | epoch   2 | 100/1285 batches | lr 0.0001 | ms/batch 396.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1285 batches | lr 0.0001 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1285 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1285 batches | lr 0.0001 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1285 batches | lr 0.0001 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1285 batches | lr 0.0001 | ms/batch 392.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1285 batches | lr 0.0001 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1285 batches | lr 0.0001 | ms/batch 392.09 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1285 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1285 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1285 batches | lr 0.0001 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1200/1285 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 517.20s | valid loss/mse 0.0872 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1285 batches | lr 0.0001 | ms/batch 396.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1285 batches | lr 0.0001 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1285 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1285 batches | lr 0.0001 | ms/batch 392.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1285 batches | lr 0.0001 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1285 batches | lr 0.0001 | ms/batch 392.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1285 batches | lr 0.0001 | ms/batch 392.63 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1285 batches | lr 0.0001 | ms/batch 392.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1285 batches | lr 0.0001 | ms/batch 392.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1285 batches | lr 0.0001 | ms/batch 392.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1285 batches | lr 0.0001 | ms/batch 392.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1200/1285 batches | lr 0.0001 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 517.58s | valid loss/mse 0.0863 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0863
scGPT - INFO - | epoch   4 | 100/1285 batches | lr 0.0001 | ms/batch 396.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1285 batches | lr 0.0001 | ms/batch 392.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1285 batches | lr 0.0001 | ms/batch 392.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1285 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1285 batches | lr 0.0001 | ms/batch 392.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1285 batches | lr 0.0001 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1285 batches | lr 0.0001 | ms/batch 392.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1285 batches | lr 0.0001 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1285 batches | lr 0.0001 | ms/batch 392.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1285 batches | lr 0.0001 | ms/batch 392.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1285 batches | lr 0.0001 | ms/batch 392.55 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1200/1285 batches | lr 0.0001 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 517.49s | valid loss/mse 0.0866 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1285 batches | lr 0.0001 | ms/batch 396.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1285 batches | lr 0.0001 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1285 batches | lr 0.0001 | ms/batch 392.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1285 batches | lr 0.0001 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1285 batches | lr 0.0001 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1285 batches | lr 0.0001 | ms/batch 392.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1285 batches | lr 0.0001 | ms/batch 392.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1285 batches | lr 0.0001 | ms/batch 392.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1285 batches | lr 0.0001 | ms/batch 392.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1285 batches | lr 0.0001 | ms/batch 392.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1285 batches | lr 0.0001 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1200/1285 batches | lr 0.0001 | ms/batch 392.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 517.37s | valid loss/mse 0.0852 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0852
scGPT - INFO - | epoch   6 | 100/1285 batches | lr 0.0001 | ms/batch 396.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1285 batches | lr 0.0001 | ms/batch 391.91 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1285 batches | lr 0.0001 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1285 batches | lr 0.0001 | ms/batch 392.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1285 batches | lr 0.0001 | ms/batch 392.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1285 batches | lr 0.0001 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1285 batches | lr 0.0001 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1285 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1285 batches | lr 0.0001 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1285 batches | lr 0.0001 | ms/batch 392.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1285 batches | lr 0.0001 | ms/batch 392.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1200/1285 batches | lr 0.0001 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 517.44s | valid loss/mse 0.0863 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1285 batches | lr 0.0001 | ms/batch 396.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1285 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1285 batches | lr 0.0001 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1285 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1285 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1285 batches | lr 0.0001 | ms/batch 392.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1285 batches | lr 0.0001 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1285 batches | lr 0.0001 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1285 batches | lr 0.0001 | ms/batch 392.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1285 batches | lr 0.0001 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1285 batches | lr 0.0001 | ms/batch 392.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1200/1285 batches | lr 0.0001 | ms/batch 392.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 517.27s | valid loss/mse 0.0866 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1285 batches | lr 0.0000 | ms/batch 396.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/1285 batches | lr 0.0000 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/1285 batches | lr 0.0000 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/1285 batches | lr 0.0000 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/1285 batches | lr 0.0000 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/1285 batches | lr 0.0000 | ms/batch 392.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/1285 batches | lr 0.0000 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/1285 batches | lr 0.0000 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/1285 batches | lr 0.0000 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1000/1285 batches | lr 0.0000 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1100/1285 batches | lr 0.0000 | ms/batch 392.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1200/1285 batches | lr 0.0000 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 517.48s | valid loss/mse 0.0853 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1285 batches | lr 0.0000 | ms/batch 396.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/1285 batches | lr 0.0000 | ms/batch 392.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/1285 batches | lr 0.0000 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/1285 batches | lr 0.0000 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/1285 batches | lr 0.0000 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/1285 batches | lr 0.0000 | ms/batch 392.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/1285 batches | lr 0.0000 | ms/batch 392.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/1285 batches | lr 0.0000 | ms/batch 392.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/1285 batches | lr 0.0000 | ms/batch 392.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1000/1285 batches | lr 0.0000 | ms/batch 392.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1100/1285 batches | lr 0.0000 | ms/batch 392.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1200/1285 batches | lr 0.0000 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 517.43s | valid loss/mse 0.0856 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/1285 batches | lr 0.0000 | ms/batch 396.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 200/1285 batches | lr 0.0000 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 300/1285 batches | lr 0.0000 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 400/1285 batches | lr 0.0000 | ms/batch 391.99 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 500/1285 batches | lr 0.0000 | ms/batch 392.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 600/1285 batches | lr 0.0000 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 700/1285 batches | lr 0.0000 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 800/1285 batches | lr 0.0000 | ms/batch 392.07 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 900/1285 batches | lr 0.0000 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1000/1285 batches | lr 0.0000 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1100/1285 batches | lr 0.0000 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1200/1285 batches | lr 0.0000 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 517.35s | valid loss/mse 0.0861 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 10
---Creating test_res
test_res saved successfully----
Split3 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406681_3/scgpt/split4
scGPT - INFO - Running on 2024-07-26 15:05:57
scGPT - INFO - match 4384/5063 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1257 batches | lr 0.0001 | ms/batch 398.09 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 200/1257 batches | lr 0.0001 | ms/batch 392.39 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1257 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/1257 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 500/1257 batches | lr 0.0001 | ms/batch 391.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/1257 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 700/1257 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/1257 batches | lr 0.0001 | ms/batch 391.98 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1257 batches | lr 0.0001 | ms/batch 392.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1257 batches | lr 0.0001 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1100/1257 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1200/1257 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 508.86s | valid loss/mse 0.0818 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0818
scGPT - INFO - | epoch   2 | 100/1257 batches | lr 0.0001 | ms/batch 396.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1257 batches | lr 0.0001 | ms/batch 392.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1257 batches | lr 0.0001 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1257 batches | lr 0.0001 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1257 batches | lr 0.0001 | ms/batch 392.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1257 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1257 batches | lr 0.0001 | ms/batch 392.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1257 batches | lr 0.0001 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1257 batches | lr 0.0001 | ms/batch 392.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1257 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1257 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1200/1257 batches | lr 0.0001 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 508.66s | valid loss/mse 0.0825 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   3 | 100/1257 batches | lr 0.0001 | ms/batch 396.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1257 batches | lr 0.0001 | ms/batch 392.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1257 batches | lr 0.0001 | ms/batch 392.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1257 batches | lr 0.0001 | ms/batch 392.60 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1257 batches | lr 0.0001 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1257 batches | lr 0.0001 | ms/batch 392.58 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1257 batches | lr 0.0001 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1257 batches | lr 0.0001 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1257 batches | lr 0.0001 | ms/batch 392.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1257 batches | lr 0.0001 | ms/batch 392.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1257 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1200/1257 batches | lr 0.0001 | ms/batch 391.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 508.67s | valid loss/mse 0.0829 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1257 batches | lr 0.0001 | ms/batch 396.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1257 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1257 batches | lr 0.0001 | ms/batch 392.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1257 batches | lr 0.0001 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1257 batches | lr 0.0001 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1257 batches | lr 0.0001 | ms/batch 392.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1257 batches | lr 0.0001 | ms/batch 392.82 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1257 batches | lr 0.0001 | ms/batch 392.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1257 batches | lr 0.0001 | ms/batch 392.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1257 batches | lr 0.0001 | ms/batch 392.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1257 batches | lr 0.0001 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1200/1257 batches | lr 0.0001 | ms/batch 392.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 508.76s | valid loss/mse 0.0823 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   5 | 100/1257 batches | lr 0.0001 | ms/batch 396.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1257 batches | lr 0.0001 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1257 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1257 batches | lr 0.0001 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1257 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1257 batches | lr 0.0001 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1257 batches | lr 0.0001 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1257 batches | lr 0.0001 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1257 batches | lr 0.0001 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1257 batches | lr 0.0001 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1257 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1200/1257 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 508.57s | valid loss/mse 0.0816 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0816
scGPT - INFO - | epoch   6 | 100/1257 batches | lr 0.0001 | ms/batch 396.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1257 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1257 batches | lr 0.0001 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1257 batches | lr 0.0001 | ms/batch 392.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1257 batches | lr 0.0001 | ms/batch 392.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1257 batches | lr 0.0001 | ms/batch 392.07 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1257 batches | lr 0.0001 | ms/batch 391.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1257 batches | lr 0.0001 | ms/batch 392.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1257 batches | lr 0.0001 | ms/batch 392.28 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1257 batches | lr 0.0001 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1257 batches | lr 0.0001 | ms/batch 392.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1200/1257 batches | lr 0.0001 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 508.51s | valid loss/mse 0.0816 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0816
scGPT - INFO - | epoch   7 | 100/1257 batches | lr 0.0001 | ms/batch 396.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1257 batches | lr 0.0001 | ms/batch 392.47 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1257 batches | lr 0.0001 | ms/batch 392.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1257 batches | lr 0.0001 | ms/batch 391.94 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1257 batches | lr 0.0001 | ms/batch 391.91 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1257 batches | lr 0.0001 | ms/batch 392.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1257 batches | lr 0.0001 | ms/batch 392.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1257 batches | lr 0.0001 | ms/batch 391.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1257 batches | lr 0.0001 | ms/batch 391.84 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1257 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1257 batches | lr 0.0001 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1200/1257 batches | lr 0.0001 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 508.41s | valid loss/mse 0.0819 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1257 batches | lr 0.0000 | ms/batch 396.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/1257 batches | lr 0.0000 | ms/batch 392.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/1257 batches | lr 0.0000 | ms/batch 392.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/1257 batches | lr 0.0000 | ms/batch 392.05 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/1257 batches | lr 0.0000 | ms/batch 392.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/1257 batches | lr 0.0000 | ms/batch 392.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/1257 batches | lr 0.0000 | ms/batch 392.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/1257 batches | lr 0.0000 | ms/batch 392.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/1257 batches | lr 0.0000 | ms/batch 392.15 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1000/1257 batches | lr 0.0000 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1100/1257 batches | lr 0.0000 | ms/batch 392.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1200/1257 batches | lr 0.0000 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 508.42s | valid loss/mse 0.0827 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1257 batches | lr 0.0000 | ms/batch 396.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/1257 batches | lr 0.0000 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/1257 batches | lr 0.0000 | ms/batch 391.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/1257 batches | lr 0.0000 | ms/batch 391.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/1257 batches | lr 0.0000 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/1257 batches | lr 0.0000 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/1257 batches | lr 0.0000 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/1257 batches | lr 0.0000 | ms/batch 391.93 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/1257 batches | lr 0.0000 | ms/batch 391.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1000/1257 batches | lr 0.0000 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1100/1257 batches | lr 0.0000 | ms/batch 392.02 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1200/1257 batches | lr 0.0000 | ms/batch 391.89 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 508.32s | valid loss/mse 0.0817 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  10 | 100/1257 batches | lr 0.0000 | ms/batch 396.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 200/1257 batches | lr 0.0000 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 300/1257 batches | lr 0.0000 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 400/1257 batches | lr 0.0000 | ms/batch 392.71 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 500/1257 batches | lr 0.0000 | ms/batch 392.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 600/1257 batches | lr 0.0000 | ms/batch 391.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 700/1257 batches | lr 0.0000 | ms/batch 391.75 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 800/1257 batches | lr 0.0000 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 900/1257 batches | lr 0.0000 | ms/batch 392.04 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1000/1257 batches | lr 0.0000 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1100/1257 batches | lr 0.0000 | ms/batch 391.96 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  10 | 1200/1257 batches | lr 0.0000 | ms/batch 391.81 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  10 | time: 508.34s | valid loss/mse 0.0821 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch  11 | 100/1257 batches | lr 0.0000 | ms/batch 396.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 200/1257 batches | lr 0.0000 | ms/batch 391.85 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 300/1257 batches | lr 0.0000 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 400/1257 batches | lr 0.0000 | ms/batch 392.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 500/1257 batches | lr 0.0000 | ms/batch 392.01 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 600/1257 batches | lr 0.0000 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 700/1257 batches | lr 0.0000 | ms/batch 391.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 800/1257 batches | lr 0.0000 | ms/batch 391.72 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 900/1257 batches | lr 0.0000 | ms/batch 391.67 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1000/1257 batches | lr 0.0000 | ms/batch 391.80 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1100/1257 batches | lr 0.0000 | ms/batch 391.97 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch  11 | 1200/1257 batches | lr 0.0000 | ms/batch 391.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch  11 | time: 508.21s | valid loss/mse 0.0825 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 11
---Creating test_res
test_res saved successfully----
Split4 computation completed
saving to /home/share/huadjyin/home/zhoumin3/zhoumin/model_benchmark/01_A_results/AdamsonWeissman2016_GSM2406681_3/scgpt/split5
scGPT - INFO - Running on 2024-07-26 17:15:34
scGPT - INFO - match 4384/5063 genes in vocabulary of size 60697.
scGPT - INFO - Resume model from /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/best_model.pt, the model args will override the config /home/share/huadjyin/home/zhoumin3/zhoumin/scgpt/scGPT_human/args.json.
Using simple batchnorm instead of domain specific batchnorm
scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - | epoch   1 | 100/1349 batches | lr 0.0001 | ms/batch 397.82 | loss  0.19 | mse  0.19 |
scGPT - INFO - | epoch   1 | 200/1349 batches | lr 0.0001 | ms/batch 392.70 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 300/1349 batches | lr 0.0001 | ms/batch 392.77 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 400/1349 batches | lr 0.0001 | ms/batch 392.78 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 500/1349 batches | lr 0.0001 | ms/batch 392.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 600/1349 batches | lr 0.0001 | ms/batch 392.23 | loss  0.09 | mse  0.09 |
scGPT - INFO - | epoch   1 | 700/1349 batches | lr 0.0001 | ms/batch 392.20 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 800/1349 batches | lr 0.0001 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 900/1349 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1000/1349 batches | lr 0.0001 | ms/batch 392.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1100/1349 batches | lr 0.0001 | ms/batch 392.69 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1200/1349 batches | lr 0.0001 | ms/batch 392.61 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   1 | 1300/1349 batches | lr 0.0001 | ms/batch 392.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   1 | time: 543.07s | valid loss/mse 0.0860 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0860
scGPT - INFO - | epoch   2 | 100/1349 batches | lr 0.0001 | ms/batch 396.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 200/1349 batches | lr 0.0001 | ms/batch 392.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 300/1349 batches | lr 0.0001 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 400/1349 batches | lr 0.0001 | ms/batch 392.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 500/1349 batches | lr 0.0001 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 600/1349 batches | lr 0.0001 | ms/batch 392.50 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 700/1349 batches | lr 0.0001 | ms/batch 392.78 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 800/1349 batches | lr 0.0001 | ms/batch 392.81 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 900/1349 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1000/1349 batches | lr 0.0001 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1100/1349 batches | lr 0.0001 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1200/1349 batches | lr 0.0001 | ms/batch 392.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   2 | 1300/1349 batches | lr 0.0001 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   2 | time: 542.82s | valid loss/mse 0.0843 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0843
scGPT - INFO - | epoch   3 | 100/1349 batches | lr 0.0001 | ms/batch 396.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 200/1349 batches | lr 0.0001 | ms/batch 392.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 300/1349 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 400/1349 batches | lr 0.0001 | ms/batch 392.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 500/1349 batches | lr 0.0001 | ms/batch 392.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 600/1349 batches | lr 0.0001 | ms/batch 392.49 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 700/1349 batches | lr 0.0001 | ms/batch 392.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 800/1349 batches | lr 0.0001 | ms/batch 392.52 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 900/1349 batches | lr 0.0001 | ms/batch 392.46 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1000/1349 batches | lr 0.0001 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1100/1349 batches | lr 0.0001 | ms/batch 392.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1200/1349 batches | lr 0.0001 | ms/batch 392.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   3 | 1300/1349 batches | lr 0.0001 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   3 | time: 542.65s | valid loss/mse 0.0850 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   4 | 100/1349 batches | lr 0.0001 | ms/batch 396.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 200/1349 batches | lr 0.0001 | ms/batch 392.23 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 300/1349 batches | lr 0.0001 | ms/batch 392.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 400/1349 batches | lr 0.0001 | ms/batch 392.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 500/1349 batches | lr 0.0001 | ms/batch 391.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 600/1349 batches | lr 0.0001 | ms/batch 391.98 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 700/1349 batches | lr 0.0001 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 800/1349 batches | lr 0.0001 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 900/1349 batches | lr 0.0001 | ms/batch 391.98 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1000/1349 batches | lr 0.0001 | ms/batch 391.90 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1100/1349 batches | lr 0.0001 | ms/batch 391.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1200/1349 batches | lr 0.0001 | ms/batch 391.79 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   4 | 1300/1349 batches | lr 0.0001 | ms/batch 391.95 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   4 | time: 542.27s | valid loss/mse 0.0829 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Best model with score 0.0829
scGPT - INFO - | epoch   5 | 100/1349 batches | lr 0.0001 | ms/batch 396.37 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 200/1349 batches | lr 0.0001 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 300/1349 batches | lr 0.0001 | ms/batch 392.16 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 400/1349 batches | lr 0.0001 | ms/batch 392.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 500/1349 batches | lr 0.0001 | ms/batch 392.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 600/1349 batches | lr 0.0001 | ms/batch 392.18 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 700/1349 batches | lr 0.0001 | ms/batch 392.21 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 800/1349 batches | lr 0.0001 | ms/batch 392.10 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 900/1349 batches | lr 0.0001 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1000/1349 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1100/1349 batches | lr 0.0001 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1200/1349 batches | lr 0.0001 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   5 | 1300/1349 batches | lr 0.0001 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   5 | time: 542.53s | valid loss/mse 0.0840 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   6 | 100/1349 batches | lr 0.0001 | ms/batch 396.57 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 200/1349 batches | lr 0.0001 | ms/batch 392.11 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 300/1349 batches | lr 0.0001 | ms/batch 392.25 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 400/1349 batches | lr 0.0001 | ms/batch 392.06 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 500/1349 batches | lr 0.0001 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 600/1349 batches | lr 0.0001 | ms/batch 392.54 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 700/1349 batches | lr 0.0001 | ms/batch 392.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 800/1349 batches | lr 0.0001 | ms/batch 392.24 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 900/1349 batches | lr 0.0001 | ms/batch 391.87 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1000/1349 batches | lr 0.0001 | ms/batch 392.29 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1100/1349 batches | lr 0.0001 | ms/batch 392.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1200/1349 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   6 | 1300/1349 batches | lr 0.0001 | ms/batch 392.12 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   6 | time: 542.49s | valid loss/mse 0.0846 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   7 | 100/1349 batches | lr 0.0001 | ms/batch 396.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 200/1349 batches | lr 0.0001 | ms/batch 392.00 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 300/1349 batches | lr 0.0001 | ms/batch 391.98 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 400/1349 batches | lr 0.0001 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 500/1349 batches | lr 0.0001 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 600/1349 batches | lr 0.0001 | ms/batch 392.39 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 700/1349 batches | lr 0.0001 | ms/batch 392.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 800/1349 batches | lr 0.0001 | ms/batch 392.64 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 900/1349 batches | lr 0.0001 | ms/batch 392.56 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1000/1349 batches | lr 0.0001 | ms/batch 392.45 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1100/1349 batches | lr 0.0001 | ms/batch 392.42 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1200/1349 batches | lr 0.0001 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   7 | 1300/1349 batches | lr 0.0001 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   7 | time: 542.62s | valid loss/mse 0.0842 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   8 | 100/1349 batches | lr 0.0000 | ms/batch 396.27 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 200/1349 batches | lr 0.0000 | ms/batch 392.43 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 300/1349 batches | lr 0.0000 | ms/batch 392.40 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 400/1349 batches | lr 0.0000 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 500/1349 batches | lr 0.0000 | ms/batch 392.26 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 600/1349 batches | lr 0.0000 | ms/batch 392.03 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 700/1349 batches | lr 0.0000 | ms/batch 392.13 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 800/1349 batches | lr 0.0000 | ms/batch 392.14 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 900/1349 batches | lr 0.0000 | ms/batch 392.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1000/1349 batches | lr 0.0000 | ms/batch 392.33 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1100/1349 batches | lr 0.0000 | ms/batch 392.30 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1200/1349 batches | lr 0.0000 | ms/batch 392.36 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   8 | 1300/1349 batches | lr 0.0000 | ms/batch 392.53 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   8 | time: 542.60s | valid loss/mse 0.0842 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | epoch   9 | 100/1349 batches | lr 0.0000 | ms/batch 396.17 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 200/1349 batches | lr 0.0000 | ms/batch 392.35 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 300/1349 batches | lr 0.0000 | ms/batch 392.38 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 400/1349 batches | lr 0.0000 | ms/batch 392.34 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 500/1349 batches | lr 0.0000 | ms/batch 392.32 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 600/1349 batches | lr 0.0000 | ms/batch 392.41 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 700/1349 batches | lr 0.0000 | ms/batch 392.31 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 800/1349 batches | lr 0.0000 | ms/batch 392.19 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 900/1349 batches | lr 0.0000 | ms/batch 392.22 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1000/1349 batches | lr 0.0000 | ms/batch 392.08 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1100/1349 batches | lr 0.0000 | ms/batch 392.51 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1200/1349 batches | lr 0.0000 | ms/batch 392.48 | loss  0.08 | mse  0.08 |
scGPT - INFO - | epoch   9 | 1300/1349 batches | lr 0.0000 | ms/batch 392.44 | loss  0.08 | mse  0.08 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - | end of epoch   9 | time: 542.63s | valid loss/mse 0.0838 |
scGPT - INFO - -----------------------------------------------------------------------------------------
scGPT - INFO - Early stop at epoch 9
---Creating test_res
test_res saved successfully----
Split5 computation completed
