cuda-11.8.0 loaded successful
gcc-9.3.0 loaded successful
cmake-3.27.0 loaded successful
openmpi-4.1.2 loaded successful
Openblas-0.3.25 loaded successful
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x
  self.hub = sentry_sdk.Hub(client)
Creating pyg object for each cell in the data...
  0%|          | 0/9 [00:00<?, ?it/s] 11%|‚ñà         | 1/9 [00:09<01:14,  9.31s/it] 22%|‚ñà‚ñà‚ñè       | 2/9 [00:21<01:16, 10.93s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [00:33<01:08, 11.40s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [00:39<00:26,  6.61s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [00:48<00:21,  7.30s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [01:08<00:21, 10.95s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [01:08<00:07,  7.78s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:19<00:00,  8.69s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:19<00:00,  8.79s/it]
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_arrayed_rna/data_pyg/cell_graphs.pkl
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_215239-rbgiy092
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/rbgiy092
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.013 MB of 0.033 MB uploadedwandb: / 0.033 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÜ
wandb:      val_mre ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 49.59467
wandb:               epoch 8
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4997/5000 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 21:52:57
wandb:            val_loss 0.08617
wandb:             val_mre 69071.99805
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_arrayed_RNA_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/rbgiy092
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_215239-rbgiy092/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_220044-5dlawnbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/5dlawnbm
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.035 MB uploadedwandb: | 0.013 MB of 0.035 MB uploadedwandb: / 0.035 MB of 0.035 MB uploadedwandb: - 0.035 MB of 0.035 MB uploadedwandb: \ 0.035 MB of 0.035 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñà
wandb:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÇ
wandb:      val_mre ‚ñà‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÇ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 49.11904
wandb:               epoch 13
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4997/5000 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 22:00:59
wandb:            val_loss 0.09272
wandb:             val_mre 71520.875
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_arrayed_RNA_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/5dlawnbm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_220044-5dlawnbm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_221253-udndfduq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/udndfduq
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.013 MB of 0.033 MB uploadedwandb: / 0.033 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñÅ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÉ
wandb:      val_mre ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 45.08282
wandb:               epoch 9
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4997/5000 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 22:13:08
wandb:            val_loss 0.08805
wandb:             val_mre 65271.83984
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_arrayed_RNA_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/udndfduq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_221253-udndfduq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_222132-fb387il8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/fb387il8
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.003 MB of 0.033 MB uploadedwandb: / 0.033 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb: | 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñá
wandb:      val_mre ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 41.78931
wandb:               epoch 11
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4997/5000 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 22:21:47
wandb:            val_loss 0.10997
wandb:             val_mre 102259.78564
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_arrayed_RNA_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/fb387il8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_222132-fb387il8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_223106-kjglebru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_arrayed_RNA_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/kjglebru
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_arrayed_rna/splits/papalexisatija2021_eccite_arrayed_rna_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.032 MB uploadedwandb: | 0.002 MB of 0.032 MB uploadedwandb: / 0.027 MB of 0.032 MB uploadedwandb: - 0.027 MB of 0.032 MB uploadedwandb: \ 0.027 MB of 0.032 MB uploadedwandb: | 0.032 MB of 0.032 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÉ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÇ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb:     val_loss ‚ñÅ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñà
wandb:      val_mre ‚ñÅ‚ñÉ‚ñá‚ñà‚ñà‚ñÜ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 52.54839
wandb:               epoch 6
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4997/5000 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 22:31:21
wandb:            val_loss 0.08848
wandb:             val_mre 76462.875
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_arrayed_RNA_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/kjglebru
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_223106-kjglebru/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x
  self.hub = sentry_sdk.Hub(client)
Creating pyg object for each cell in the data...
  0%|          | 0/25 [00:00<?, ?it/s]  4%|‚ñç         | 1/25 [00:45<18:21, 45.88s/it]  8%|‚ñä         | 2/25 [01:37<18:51, 49.21s/it] 12%|‚ñà‚ñè        | 3/25 [02:04<14:15, 38.88s/it] 16%|‚ñà‚ñå        | 4/25 [02:38<13:04, 37.34s/it] 20%|‚ñà‚ñà        | 5/25 [02:39<08:00, 24.03s/it] 24%|‚ñà‚ñà‚ñç       | 6/25 [03:42<11:47, 37.21s/it] 28%|‚ñà‚ñà‚ñä       | 7/25 [04:10<10:16, 34.26s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [05:09<11:57, 42.20s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [05:54<11:28, 43.03s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [06:33<10:27, 41.84s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [07:11<09:28, 40.60s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [07:58<09:11, 42.45s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [09:15<10:35, 52.94s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 14/25 [09:45<08:27, 46.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [10:54<08:48, 52.90s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [11:28<07:07, 47.47s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [11:44<05:03, 37.91s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 18/25 [12:02<03:43, 31.98s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [13:04<04:04, 40.78s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [13:43<03:21, 40.26s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [14:31<02:50, 42.71s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [15:43<02:34, 51.52s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [16:39<01:45, 52.82s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [16:45<00:38, 38.84s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [16:48<00:00, 28.00s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [16:48<00:00, 40.34s/it]
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_rna/data_pyg/cell_graphs.pkl
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_225523-g70aybxk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/g70aybxk
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.001 MB of 0.033 MB uploadedwandb: / 0.031 MB of 0.033 MB uploadedwandb: - 0.031 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb:     val_loss ‚ñÅ‚ñà‚ñá‚ñá‚ñà‚ñÉ
wandb:      val_mre ‚ñá‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 153.44457
wandb:               epoch 6
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4999/5007 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 22:55:38
wandb:            val_loss 0.14791
wandb:             val_mre 100138.23765
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_RNA_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/g70aybxk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_225523-g70aybxk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240726_232110-b1xfq9e3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/b1xfq9e3
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.001 MB of 0.038 MB uploadedwandb: / 0.032 MB of 0.038 MB uploadedwandb: - 0.032 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÜ
wandb:      val_mre ‚ñÅ‚ñá‚ñÜ‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:        elapsed_time 153.07554
wandb:               epoch 15
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4999/5007 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-26 23:21:24
wandb:            val_loss 0.15229
wandb:             val_mre 94650.35263
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_RNA_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/b1xfq9e3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_232110-b1xfq9e3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_000952-vfk51ih3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/vfk51ih3
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.001 MB uploadedwandb: - 0.001 MB of 0.034 MB uploadedwandb: \ 0.008 MB of 0.034 MB uploadedwandb: | 0.013 MB of 0.034 MB uploadedwandb: / 0.013 MB of 0.034 MB uploadedwandb: - 0.034 MB of 0.034 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñÑ‚ñÉ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:     val_loss ‚ñá‚ñÉ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:      val_mre ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÅ‚ñà‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 150.67516
wandb:               epoch 8
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4999/5007 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 00:10:07
wandb:            val_loss 0.14532
wandb:             val_mre 102495.76531
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_RNA_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/vfk51ih3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_000952-vfk51ih3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_004143-k79v2nuf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/k79v2nuf
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.028 MB of 0.033 MB uploadedwandb: / 0.028 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñÇ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb:     val_loss ‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñà‚ñÜ
wandb:      val_mre ‚ñÖ‚ñà‚ñÉ‚ñÑ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 165.09073
wandb:               epoch 6
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4999/5007 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 00:41:55
wandb:            val_loss 0.15224
wandb:             val_mre 98518.77009
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_RNA_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/k79v2nuf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_004143-k79v2nuf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_010551-3ktzabf0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PapalexiSatija2021_eccite_RNA_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/3ktzabf0
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/papalexisatija2021_eccite_rna/splits/papalexisatija2021_eccite_rna_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:6
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.038 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÅ
wandb:      val_mre ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:        elapsed_time 150.79904
wandb:               epoch 15
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4999/5007 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 01:06:04
wandb:            val_loss 0.13923
wandb:             val_mre 101568.45985
wandb: 
wandb: üöÄ View run PapalexiSatija2021_eccite_RNA_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/3ktzabf0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_010551-3ktzabf0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x
  self.hub = sentry_sdk.Hub(client)
Creating pyg object for each cell in the data...
  0%|          | 0/19 [00:00<?, ?it/s]  5%|‚ñå         | 1/19 [00:06<01:50,  6.16s/it] 11%|‚ñà         | 2/19 [00:10<01:29,  5.27s/it] 16%|‚ñà‚ñå        | 3/19 [00:12<00:59,  3.71s/it] 21%|‚ñà‚ñà        | 4/19 [00:17<01:01,  4.08s/it] 26%|‚ñà‚ñà‚ñã       | 5/19 [00:19<00:50,  3.58s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 6/19 [00:28<01:06,  5.13s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 7/19 [00:31<00:54,  4.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 8/19 [00:35<00:49,  4.51s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 9/19 [00:39<00:41,  4.13s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 10/19 [00:44<00:39,  4.34s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 11/19 [00:46<00:31,  3.92s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 12/19 [00:51<00:28,  4.07s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 13/19 [00:54<00:21,  3.62s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 14/19 [00:57<00:17,  3.57s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 15/19 [01:02<00:16,  4.04s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 16/19 [01:05<00:11,  3.70s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/19 [01:07<00:06,  3.33s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 18/19 [01:10<00:03,  3.01s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [01:12<00:00,  2.92s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [01:12<00:00,  3.84s/it]
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/shifrutmarson2018/data_pyg/cell_graphs.pkl
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_015709-bm1lya9y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/bm1lya9y
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/shifrutmarson2018/splits/shifrutmarson2018_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.053 MB uploadedwandb: | 0.001 MB of 0.053 MB uploadedwandb: / 0.052 MB of 0.053 MB uploadedwandb: - 0.052 MB of 0.053 MB uploadedwandb: \ 0.053 MB of 0.053 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñá‚ñÖ‚ñÅ‚ñÉ‚ñÑ
wandb:      val_mre ‚ñá‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 551.38362
wandb:               epoch 15
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4647/5008 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 01:57:27
wandb:            val_loss 0.06472
wandb:             val_mre 36628.3716
wandb: 
wandb: üöÄ View run ShifrutMarson2018_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/bm1lya9y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_015709-bm1lya9y/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_041712-slcbx31p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShifrutMarson2018_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/slcbx31p
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/shifrutmarson2018/splits/shifrutmarson2018_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/ShifrutMarson2018.py", line 491, in <module>
    test_res = eval_perturb(test_loader, best_model, device, pert_data)
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/ShifrutMarson2018.py", line 277, in eval_perturb
    p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 322, in pred_perturb
    output_dict = self(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 206, in forward
    transformer_output = self._encode(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 141, in _encode
    output = self.transformer_encoder(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 315, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 591, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 599, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.96 GiB (GPU 0; 39.41 GiB total capacity; 15.03 GiB already allocated; 9.73 GiB free; 28.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/ShifrutMarson2018.py", line 491, in <module>
    test_res = eval_perturb(test_loader, best_model, device, pert_data)
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/ShifrutMarson2018.py", line 277, in eval_perturb
    p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 322, in pred_perturb
    output_dict = self(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 206, in forward
    transformer_output = self._encode(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 141, in _encode
    output = self.transformer_encoder(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 315, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 591, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 599, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.96 GiB (GPU 0; 39.41 GiB total capacity; 15.03 GiB already allocated; 9.73 GiB free; 28.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.045 MB uploadedwandb: | 0.043 MB of 0.045 MB uploadedwandb: / 0.045 MB of 0.045 MB uploadedwandb: - 0.045 MB of 0.045 MB uploadedwandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb:        epoch ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà
wandb:     val_loss ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñÉ
wandb:      val_mre ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñà
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 543.76261
wandb:               epoch 6
wandb: loading_params_info Loading params trans...
wandb:          match_info match 4647/5008 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 04:17:28
wandb:            val_loss 0.0622
wandb:             val_mre 44714.18399
wandb: 
wandb: üöÄ View run ShifrutMarson2018_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/slcbx31p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_041712-slcbx31p/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x
  self.hub = sentry_sdk.Hub(client)
Creating pyg object for each cell in the data...
  0%|          | 0/203 [00:00<?, ?it/s]  0%|          | 1/203 [00:20<1:07:21, 20.01s/it]  1%|          | 2/203 [01:01<1:49:52, 32.80s/it]  1%|‚ñè         | 3/203 [02:08<2:40:34, 48.17s/it]  2%|‚ñè         | 4/203 [02:50<2:32:19, 45.93s/it]  2%|‚ñè         | 5/203 [03:52<2:49:54, 51.49s/it]  3%|‚ñé         | 6/203 [04:31<2:36:06, 47.55s/it]  3%|‚ñé         | 7/203 [06:18<3:38:46, 66.97s/it]  4%|‚ñç         | 8/203 [06:42<2:52:46, 53.16s/it]  4%|‚ñç         | 9/203 [08:10<3:27:34, 64.20s/it]  5%|‚ñç         | 10/203 [08:21<2:33:17, 47.65s/it]  5%|‚ñå         | 11/203 [08:25<1:50:01, 34.38s/it]  6%|‚ñå         | 12/203 [08:56<1:46:07, 33.34s/it]  6%|‚ñã         | 13/203 [09:23<1:39:13, 31.33s/it]  7%|‚ñã         | 14/203 [10:31<2:13:47, 42.47s/it]  7%|‚ñã         | 15/203 [11:22<2:21:12, 45.07s/it]  8%|‚ñä         | 16/203 [12:00<2:13:34, 42.86s/it]  8%|‚ñä         | 17/203 [12:37<2:07:11, 41.03s/it]  9%|‚ñâ         | 18/203 [12:39<1:30:29, 29.35s/it]  9%|‚ñâ         | 19/203 [13:15<1:36:13, 31.38s/it] 10%|‚ñâ         | 20/203 [13:43<1:32:16, 30.25s/it] 10%|‚ñà         | 21/203 [14:15<1:33:09, 30.71s/it] 11%|‚ñà         | 22/203 [14:39<1:27:16, 28.93s/it] 11%|‚ñà‚ñè        | 23/203 [15:03<1:21:46, 27.26s/it] 12%|‚ñà‚ñè        | 24/203 [15:24<1:15:45, 25.40s/it] 12%|‚ñà‚ñè        | 25/203 [15:34<1:01:49, 20.84s/it] 13%|‚ñà‚ñé        | 26/203 [15:50<57:39, 19.55s/it]   13%|‚ñà‚ñé        | 27/203 [16:36<1:20:01, 27.28s/it] 14%|‚ñà‚ñç        | 28/203 [17:30<1:43:32, 35.50s/it] 14%|‚ñà‚ñç        | 29/203 [17:58<1:35:42, 33.00s/it] 15%|‚ñà‚ñç        | 30/203 [18:05<1:13:14, 25.40s/it] 15%|‚ñà‚ñå        | 31/203 [18:25<1:07:36, 23.58s/it] 16%|‚ñà‚ñå        | 32/203 [19:19<1:33:35, 32.84s/it] 16%|‚ñà‚ñã        | 33/203 [19:21<1:07:08, 23.70s/it] 17%|‚ñà‚ñã        | 34/203 [20:11<1:28:42, 31.49s/it] 17%|‚ñà‚ñã        | 35/203 [20:13<1:03:17, 22.60s/it] 18%|‚ñà‚ñä        | 36/203 [20:36<1:02:54, 22.60s/it] 18%|‚ñà‚ñä        | 37/203 [21:04<1:07:26, 24.37s/it] 19%|‚ñà‚ñä        | 38/203 [21:43<1:19:04, 28.75s/it] 19%|‚ñà‚ñâ        | 39/203 [22:36<1:38:01, 35.86s/it] 20%|‚ñà‚ñâ        | 40/203 [22:42<1:13:42, 27.13s/it] 20%|‚ñà‚ñà        | 41/203 [22:46<54:13, 20.08s/it]   21%|‚ñà‚ñà        | 42/203 [23:10<57:29, 21.42s/it] 21%|‚ñà‚ñà        | 43/203 [23:38<1:02:17, 23.36s/it] 22%|‚ñà‚ñà‚ñè       | 44/203 [23:51<53:03, 20.02s/it]   22%|‚ñà‚ñà‚ñè       | 45/203 [24:32<1:09:55, 26.55s/it] 23%|‚ñà‚ñà‚ñé       | 46/203 [25:16<1:22:41, 31.60s/it] 23%|‚ñà‚ñà‚ñé       | 47/203 [26:18<1:46:11, 40.84s/it] 24%|‚ñà‚ñà‚ñé       | 48/203 [26:35<1:27:16, 33.79s/it] 24%|‚ñà‚ñà‚ñç       | 49/203 [27:06<1:24:33, 32.94s/it] 25%|‚ñà‚ñà‚ñç       | 50/203 [27:46<1:28:53, 34.86s/it] 25%|‚ñà‚ñà‚ñå       | 51/203 [27:48<1:03:15, 24.97s/it] 26%|‚ñà‚ñà‚ñå       | 52/203 [28:13<1:03:06, 25.08s/it] 26%|‚ñà‚ñà‚ñå       | 53/203 [28:33<58:33, 23.42s/it]   27%|‚ñà‚ñà‚ñã       | 54/203 [29:01<1:02:11, 25.04s/it] 27%|‚ñà‚ñà‚ñã       | 55/203 [29:52<1:20:36, 32.68s/it] 28%|‚ñà‚ñà‚ñä       | 56/203 [30:34<1:27:06, 35.56s/it] 28%|‚ñà‚ñà‚ñä       | 57/203 [30:57<1:17:22, 31.80s/it] 29%|‚ñà‚ñà‚ñä       | 58/203 [31:27<1:15:05, 31.08s/it] 29%|‚ñà‚ñà‚ñâ       | 59/203 [31:49<1:08:25, 28.51s/it] 30%|‚ñà‚ñà‚ñâ       | 60/203 [32:16<1:07:07, 28.16s/it] 30%|‚ñà‚ñà‚ñà       | 61/203 [33:12<1:26:01, 36.35s/it] 31%|‚ñà‚ñà‚ñà       | 62/203 [33:36<1:16:34, 32.59s/it] 31%|‚ñà‚ñà‚ñà       | 63/203 [34:21<1:25:01, 36.44s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 64/203 [34:45<1:16:01, 32.82s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 65/203 [35:39<1:29:56, 39.10s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 66/203 [35:40<1:02:48, 27.50s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 67/203 [36:25<1:14:23, 32.82s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 68/203 [37:10<1:22:22, 36.61s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 69/203 [37:45<1:20:45, 36.16s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 70/203 [38:18<1:17:41, 35.05s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 71/203 [39:08<1:27:08, 39.61s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 72/203 [39:42<1:22:23, 37.73s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 73/203 [40:04<1:11:51, 33.17s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 74/203 [40:32<1:07:45, 31.52s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 75/203 [40:36<50:06, 23.49s/it]   37%|‚ñà‚ñà‚ñà‚ñã      | 76/203 [41:23<1:04:12, 30.34s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 77/203 [42:03<1:09:56, 33.30s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 78/203 [42:32<1:06:43, 32.03s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 79/203 [42:49<57:02, 27.60s/it]   39%|‚ñà‚ñà‚ñà‚ñâ      | 80/203 [43:07<50:22, 24.57s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 81/203 [43:26<46:25, 22.84s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 82/203 [44:13<1:01:07, 30.31s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 83/203 [44:42<59:22, 29.69s/it]   41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/203 [45:07<56:32, 28.50s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 85/203 [45:39<57:55, 29.46s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 86/203 [45:56<50:24, 25.85s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/203 [46:02<38:14, 19.78s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 88/203 [46:38<47:05, 24.57s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/203 [46:57<43:21, 22.82s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 90/203 [47:41<55:01, 29.21s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 91/203 [48:56<1:20:33, 43.16s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/203 [49:18<1:07:58, 36.74s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 93/203 [49:20<47:54, 26.13s/it]   46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/203 [49:51<50:35, 27.85s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 95/203 [50:33<57:30, 31.95s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 96/203 [50:47<47:29, 26.64s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/203 [50:49<33:49, 19.15s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 98/203 [51:16<37:50, 21.62s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/203 [51:25<30:42, 17.71s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 100/203 [52:20<49:48, 29.01s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 101/203 [52:53<51:25, 30.25s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/203 [53:17<47:44, 28.36s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 103/203 [53:23<36:01, 21.62s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 104/203 [53:51<38:51, 23.55s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 105/203 [54:00<31:24, 19.23s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 106/203 [54:27<34:28, 21.32s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/203 [55:10<44:49, 28.01s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 108/203 [55:22<36:25, 23.00s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 109/203 [56:13<49:17, 31.46s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 110/203 [56:36<45:01, 29.05s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 111/203 [56:57<40:45, 26.59s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/203 [57:16<36:51, 24.30s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 113/203 [57:41<36:39, 24.43s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 114/203 [58:05<36:12, 24.41s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 115/203 [58:18<30:42, 20.93s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 116/203 [58:58<38:53, 26.82s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/203 [59:53<50:21, 35.13s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 118/203 [1:00:06<40:13, 28.39s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 119/203 [1:00:11<29:59, 21.43s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 120/203 [1:00:36<31:08, 22.51s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 121/203 [1:00:52<28:09, 20.60s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/203 [1:01:11<27:02, 20.03s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 123/203 [1:01:40<30:14, 22.68s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 124/203 [1:01:57<27:45, 21.09s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 125/203 [1:02:16<26:28, 20.37s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 126/203 [1:02:41<28:10, 21.96s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/203 [1:03:19<33:54, 26.76s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 128/203 [1:03:55<37:00, 29.60s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 129/203 [1:04:13<31:56, 25.89s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 130/203 [1:04:35<30:00, 24.67s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 131/203 [1:04:39<22:19, 18.60s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/203 [1:04:57<21:46, 18.40s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 133/203 [1:05:12<20:09, 17.27s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 134/203 [1:05:29<20:00, 17.41s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 135/203 [1:05:36<16:10, 14.27s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 136/203 [1:05:42<13:05, 11.72s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 137/203 [1:05:55<13:16, 12.07s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 138/203 [1:06:04<12:02, 11.12s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 139/203 [1:06:46<21:58, 20.60s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 140/203 [1:07:04<20:45, 19.77s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 141/203 [1:07:27<21:28, 20.78s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 142/203 [1:07:42<19:16, 18.97s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 143/203 [1:07:50<15:38, 15.65s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 144/203 [1:08:05<15:16, 15.53s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 145/203 [1:08:13<12:45, 13.21s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 146/203 [1:08:21<11:01, 11.60s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 147/203 [1:08:34<11:09, 11.96s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 148/203 [1:09:04<15:59, 17.45s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 149/203 [1:09:10<12:40, 14.09s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 150/203 [1:09:13<09:30, 10.76s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 151/203 [1:09:22<08:44, 10.08s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 152/203 [1:10:09<18:05, 21.28s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 153/203 [1:10:16<14:11, 17.04s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 154/203 [1:10:37<14:48, 18.13s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 155/203 [1:10:44<11:52, 14.85s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 156/203 [1:10:50<09:30, 12.13s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 157/203 [1:10:56<07:58, 10.41s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 158/203 [1:11:11<08:51, 11.82s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 159/203 [1:11:13<06:23,  8.72s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 160/203 [1:11:33<08:41, 12.13s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 161/203 [1:11:45<08:25, 12.05s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 162/203 [1:12:00<08:57, 13.10s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 163/203 [1:12:12<08:23, 12.58s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 164/203 [1:12:20<07:24, 11.39s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 165/203 [1:12:27<06:14,  9.84s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 166/203 [1:12:38<06:22, 10.33s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 167/203 [1:12:44<05:18,  8.84s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 168/203 [1:12:50<04:48,  8.24s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 169/203 [1:13:02<05:11,  9.16s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 170/203 [1:13:05<04:06,  7.48s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 171/203 [1:13:25<05:55, 11.11s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 172/203 [1:13:32<05:04,  9.82s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 173/203 [1:13:37<04:15,  8.53s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 174/203 [1:13:46<04:09,  8.60s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 175/203 [1:13:55<04:02,  8.65s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 176/203 [1:13:56<02:53,  6.41s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 177/203 [1:13:59<02:24,  5.54s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 178/203 [1:14:06<02:28,  5.94s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 179/203 [1:14:10<02:05,  5.24s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 180/203 [1:14:18<02:21,  6.16s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 181/203 [1:14:29<02:43,  7.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 182/203 [1:14:32<02:08,  6.14s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 183/203 [1:14:33<01:33,  4.65s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 184/203 [1:14:48<02:30,  7.94s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 185/203 [1:14:50<01:50,  6.14s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 186/203 [1:14:57<01:45,  6.21s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 187/203 [1:15:09<02:07,  7.99s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 188/203 [1:15:10<01:27,  5.83s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 189/203 [1:15:11<01:03,  4.52s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 190/203 [1:15:15<00:57,  4.43s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 191/203 [1:15:16<00:38,  3.24s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 192/203 [1:15:17<00:29,  2.64s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 193/203 [1:15:18<00:21,  2.15s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 194/203 [1:15:24<00:29,  3.32s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 195/203 [1:15:25<00:19,  2.46s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 196/203 [1:15:26<00:15,  2.25s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 197/203 [1:15:33<00:21,  3.55s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 198/203 [1:15:35<00:14,  2.98s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 199/203 [1:15:36<00:09,  2.38s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 200/203 [1:15:37<00:06,  2.15s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 201/203 [1:15:40<00:04,  2.26s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 202/203 [1:15:40<00:01,  1.75s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [1:15:41<00:00,  1.49s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [1:15:41<00:00, 22.37s/it]
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/xucao2023/data_pyg/cell_graphs.pkl
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_062950-4me68lo1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/4me68lo1
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/xucao2023/splits/xucao2023_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.056 MB uploadedwandb: | 0.056 MB of 0.056 MB uploadedwandb: / 0.056 MB of 0.056 MB uploadedwandb: - 0.056 MB of 0.056 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñÖ‚ñÖ
wandb:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:      val_mre ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:     early_stop_info Early stop at epoch ...
wandb:        elapsed_time 774.46876
wandb:               epoch 13
wandb: loading_params_info Loading params trans...
wandb:          match_info match 5164/5172 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 06:30:09
wandb:            val_loss 0.01676
wandb:             val_mre 17201.67324
wandb: 
wandb: üöÄ View run XuCao2023_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/4me68lo1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_062950-4me68lo1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/wandb/run-20240727_095402-m5ffgzgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run XuCao2023_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/m5ffgzgo
Local copy of pyg dataset is detected. Loading...
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/xucao2023/splits/xucao2023_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:51
Done!
Creating dataloaders....
Done!
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/XuCao2023.py", line 491, in <module>
    test_res = eval_perturb(test_loader, best_model, device, pert_data)
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/XuCao2023.py", line 277, in eval_perturb
    p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 322, in pred_perturb
    output_dict = self(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 206, in forward
    transformer_output = self._encode(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 141, in _encode
    output = self.transformer_encoder(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 315, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 591, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 599, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.76 GiB (GPU 0; 39.41 GiB total capacity; 15.89 GiB already allocated; 7.11 GiB free; 30.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/XuCao2023.py", line 491, in <module>
    test_res = eval_perturb(test_loader, best_model, device, pert_data)
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03scgpt/script/XuCao2023.py", line 277, in eval_perturb
    p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 322, in pred_perturb
    output_dict = self(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 206, in forward
    transformer_output = self._encode(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/scgpt/model/generation_model.py", line 141, in _encode
    output = self.transformer_encoder(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 315, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 591, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/transformer.py", line 599, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/share/huadjyin/home/zhoumin3/.conda/envs/scgpt/lib/python3.9/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.76 GiB (GPU 0; 39.41 GiB total capacity; 15.89 GiB already allocated; 7.11 GiB free; 30.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.065 MB uploadedwandb: | 0.001 MB of 0.069 MB uploadedwandb: / 0.069 MB of 0.069 MB uploadedwandb: - 0.069 MB of 0.069 MB uploadedwandb: 
wandb: Run history:
wandb: elapsed_time ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñà
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:     val_loss ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      val_mre ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:     best_model_info Best model with scor...
wandb:        elapsed_time 820.55827
wandb:               epoch 15
wandb: loading_params_info Loading params trans...
wandb:          match_info match 5164/5172 gene...
wandb:   resume_model_info Resume model from /h...
wandb:            run_time 2024-07-27 09:54:22
wandb:            val_loss 0.0167
wandb:             val_mre 15678.50524
wandb: 
wandb: üöÄ View run XuCao2023_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt/runs/m5ffgzgo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/01_dataset_all_scgpt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_095402-m5ffgzgo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
