cuda-11.8.0 loaded successful
gcc-12.2.0 loaded successful
cmake-3.27.0 loaded successful
openmpi-4.1.2 loaded successful
Openblas-0.3.25 loaded successful
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/237 [00:00<?, ?it/s]  0%|          | 1/237 [00:37<2:25:49, 37.07s/it]  1%|          | 2/237 [02:15<4:45:29, 72.89s/it]  1%|â–         | 3/237 [04:11<6:01:36, 92.72s/it]  2%|â–         | 4/237 [06:18<6:53:17, 106.43s/it]  2%|â–         | 5/237 [09:29<8:49:35, 136.96s/it]  3%|â–Ž         | 6/237 [10:45<7:26:28, 115.97s/it]  3%|â–Ž         | 7/237 [11:04<5:23:28, 84.38s/it]   3%|â–Ž         | 8/237 [11:42<4:26:19, 69.78s/it]  4%|â–         | 9/237 [11:59<3:21:48, 53.11s/it]  4%|â–         | 10/237 [13:22<3:55:44, 62.31s/it]  5%|â–         | 11/237 [14:29<3:59:50, 63.67s/it]  5%|â–Œ         | 12/237 [18:08<6:56:50, 111.16s/it]  5%|â–Œ         | 13/237 [19:09<5:57:32, 95.77s/it]   6%|â–Œ         | 14/237 [21:54<7:13:57, 116.76s/it]  6%|â–‹         | 15/237 [22:54<6:08:59, 99.73s/it]   7%|â–‹         | 16/237 [24:43<6:17:27, 102.48s/it]  7%|â–‹         | 17/237 [27:17<7:12:58, 118.08s/it]  8%|â–Š         | 18/237 [29:06<7:00:51, 115.30s/it]  8%|â–Š         | 19/237 [29:29<5:18:14, 87.59s/it]   8%|â–Š         | 20/237 [30:12<4:27:39, 74.01s/it]  9%|â–‰         | 21/237 [31:21<4:21:43, 72.70s/it]  9%|â–‰         | 22/237 [32:47<4:34:45, 76.68s/it] 10%|â–‰         | 23/237 [33:41<4:09:13, 69.87s/it] 10%|â–ˆ         | 24/237 [33:51<3:03:39, 51.73s/it] 11%|â–ˆ         | 25/237 [34:39<2:59:27, 50.79s/it] 11%|â–ˆ         | 26/237 [35:46<3:15:39, 55.64s/it] 11%|â–ˆâ–        | 27/237 [37:53<4:29:47, 77.08s/it] 12%|â–ˆâ–        | 28/237 [38:13<3:28:24, 59.83s/it] 12%|â–ˆâ–        | 29/237 [38:15<2:27:01, 42.41s/it] 13%|â–ˆâ–Ž        | 30/237 [39:45<3:16:12, 56.87s/it] 13%|â–ˆâ–Ž        | 31/237 [41:35<4:09:49, 72.76s/it] 14%|â–ˆâ–Ž        | 32/237 [43:23<4:44:04, 83.14s/it] 14%|â–ˆâ–        | 33/237 [44:06<4:02:23, 71.29s/it] 14%|â–ˆâ–        | 34/237 [46:25<5:10:11, 91.68s/it] 15%|â–ˆâ–        | 35/237 [47:00<4:10:40, 74.46s/it] 15%|â–ˆâ–Œ        | 36/237 [47:53<3:48:12, 68.12s/it] 16%|â–ˆâ–Œ        | 37/237 [48:44<3:30:10, 63.05s/it] 16%|â–ˆâ–Œ        | 38/237 [50:40<4:21:44, 78.92s/it] 16%|â–ˆâ–‹        | 39/237 [51:11<3:32:44, 64.47s/it] 17%|â–ˆâ–‹        | 40/237 [52:53<4:08:11, 75.59s/it] 17%|â–ˆâ–‹        | 41/237 [53:53<3:51:40, 70.92s/it] 18%|â–ˆâ–Š        | 42/237 [55:09<3:56:08, 72.66s/it] 18%|â–ˆâ–Š        | 43/237 [57:27<4:58:28, 92.31s/it] 19%|â–ˆâ–Š        | 44/237 [59:00<4:56:55, 92.31s/it] 19%|â–ˆâ–‰        | 45/237 [1:02:03<6:23:02, 119.70s/it] 19%|â–ˆâ–‰        | 46/237 [1:02:50<5:11:11, 97.76s/it]  20%|â–ˆâ–‰        | 47/237 [1:03:44<4:28:17, 84.72s/it] 20%|â–ˆâ–ˆ        | 48/237 [1:06:45<5:58:05, 113.68s/it] 21%|â–ˆâ–ˆ        | 49/237 [1:07:41<5:01:48, 96.32s/it]  21%|â–ˆâ–ˆ        | 50/237 [1:08:24<4:09:47, 80.15s/it] 22%|â–ˆâ–ˆâ–       | 51/237 [1:09:26<3:52:06, 74.87s/it] 22%|â–ˆâ–ˆâ–       | 52/237 [1:10:20<3:31:09, 68.49s/it] 22%|â–ˆâ–ˆâ–       | 53/237 [1:12:42<4:38:06, 90.68s/it] 23%|â–ˆâ–ˆâ–Ž       | 54/237 [1:14:10<4:33:48, 89.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 55/237 [1:16:09<4:58:43, 98.48s/it] 24%|â–ˆâ–ˆâ–Ž       | 56/237 [1:16:23<3:40:27, 73.08s/it] 24%|â–ˆâ–ˆâ–       | 57/237 [1:17:11<3:17:20, 65.78s/it] 24%|â–ˆâ–ˆâ–       | 58/237 [1:21:45<6:22:07, 128.09s/it] 25%|â–ˆâ–ˆâ–       | 59/237 [1:23:34<6:03:08, 122.41s/it] 25%|â–ˆâ–ˆâ–Œ       | 60/237 [1:24:49<5:19:06, 108.17s/it] 26%|â–ˆâ–ˆâ–Œ       | 61/237 [1:27:00<5:37:16, 114.98s/it] 26%|â–ˆâ–ˆâ–Œ       | 62/237 [1:27:59<4:46:48, 98.33s/it]  27%|â–ˆâ–ˆâ–‹       | 63/237 [1:29:27<4:36:04, 95.20s/it] 27%|â–ˆâ–ˆâ–‹       | 64/237 [1:30:10<3:49:09, 79.48s/it] 27%|â–ˆâ–ˆâ–‹       | 65/237 [1:33:54<5:52:13, 122.87s/it] 28%|â–ˆâ–ˆâ–Š       | 66/237 [1:34:49<4:51:47, 102.38s/it] 28%|â–ˆâ–ˆâ–Š       | 67/237 [1:35:47<4:12:45, 89.21s/it]  29%|â–ˆâ–ˆâ–Š       | 68/237 [1:37:31<4:23:25, 93.53s/it] 29%|â–ˆâ–ˆâ–‰       | 69/237 [1:37:47<3:17:00, 70.36s/it] 30%|â–ˆâ–ˆâ–‰       | 70/237 [1:41:29<5:22:07, 115.73s/it] 30%|â–ˆâ–ˆâ–‰       | 71/237 [1:42:51<4:52:52, 105.86s/it] 30%|â–ˆâ–ˆâ–ˆ       | 72/237 [1:44:10<4:28:18, 97.56s/it]  31%|â–ˆâ–ˆâ–ˆ       | 73/237 [1:46:00<4:36:59, 101.34s/it] 31%|â–ˆâ–ˆâ–ˆ       | 74/237 [1:46:59<4:00:36, 88.57s/it]  32%|â–ˆâ–ˆâ–ˆâ–      | 75/237 [1:49:05<4:30:03, 100.02s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 76/237 [1:51:12<4:50:15, 108.17s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 77/237 [1:52:55<4:44:01, 106.51s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 78/237 [1:53:22<3:38:49, 82.57s/it]  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/237 [1:59:31<7:23:42, 168.49s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 80/237 [2:01:10<6:26:11, 147.59s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 81/237 [2:03:37<6:23:13, 147.39s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 82/237 [2:04:48<5:21:51, 124.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 83/237 [2:06:23<4:56:57, 115.70s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/237 [2:07:51<4:33:43, 107.34s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/237 [2:08:11<3:25:32, 81.13s/it]  36%|â–ˆâ–ˆâ–ˆâ–‹      | 86/237 [2:10:17<3:58:32, 94.78s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 87/237 [2:12:22<4:19:19, 103.73s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 88/237 [2:14:31<4:36:07, 111.19s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 89/237 [2:15:59<4:17:23, 104.35s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 90/237 [2:17:28<4:04:13, 99.69s/it]  38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/237 [2:18:16<3:25:16, 84.36s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 92/237 [2:19:35<3:19:40, 82.62s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 93/237 [2:20:31<2:59:05, 74.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 94/237 [2:24:12<4:42:52, 118.69s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 95/237 [2:27:44<5:46:56, 146.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 96/237 [2:29:00<4:54:56, 125.51s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/237 [2:30:43<4:36:54, 118.67s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 98/237 [2:31:38<3:50:54, 99.68s/it]  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/237 [2:33:40<4:04:38, 106.37s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/237 [2:36:44<4:55:52, 129.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 101/237 [2:39:41<5:25:59, 143.82s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 102/237 [2:40:21<4:13:20, 112.60s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/237 [2:41:52<3:56:53, 106.07s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 104/237 [2:44:31<4:30:20, 121.96s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/237 [2:45:10<3:33:52, 97.22s/it]  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/237 [2:46:40<3:27:14, 94.92s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 107/237 [2:47:27<2:54:20, 80.46s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 108/237 [2:48:32<2:43:02, 75.83s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/237 [2:49:58<2:48:16, 78.88s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 110/237 [2:50:43<2:25:52, 68.91s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/237 [2:52:51<3:01:29, 86.43s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/237 [2:53:55<2:46:14, 79.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 113/237 [2:54:40<2:23:14, 69.31s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 114/237 [2:56:45<2:56:15, 85.98s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/237 [2:59:11<3:31:26, 103.99s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 116/237 [3:00:03<2:58:32, 88.53s/it]  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 117/237 [3:02:57<3:48:15, 114.13s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 118/237 [3:03:42<3:05:28, 93.52s/it]  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 119/237 [3:05:01<2:55:13, 89.10s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 120/237 [3:06:26<2:51:10, 87.78s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/237 [3:07:34<2:38:24, 81.94s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 122/237 [3:09:30<2:56:38, 92.16s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/237 [3:11:01<2:54:21, 91.77s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 124/237 [3:11:56<2:31:44, 80.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 125/237 [3:12:04<1:50:01, 58.94s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 126/237 [3:13:32<2:05:10, 67.66s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/237 [3:13:54<1:38:49, 53.90s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 128/237 [3:15:54<2:13:57, 73.73s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/237 [3:16:52<2:04:15, 69.03s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/237 [3:17:29<1:45:59, 59.44s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 131/237 [3:17:55<1:27:16, 49.40s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 132/237 [3:20:27<2:20:23, 80.22s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 133/237 [3:21:50<2:20:13, 80.90s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/237 [3:22:55<2:11:03, 76.35s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 135/237 [3:25:53<3:01:20, 106.67s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 136/237 [3:26:18<2:18:35, 82.33s/it]  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 137/237 [3:27:27<2:10:14, 78.15s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 138/237 [3:28:44<2:08:35, 77.93s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/237 [3:29:30<1:51:26, 68.23s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 140/237 [3:31:10<2:06:03, 77.97s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/237 [3:32:24<2:02:33, 76.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 142/237 [3:32:51<1:37:50, 61.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 143/237 [3:33:58<1:39:15, 63.35s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 144/237 [3:34:35<1:26:01, 55.50s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/237 [3:35:43<1:30:41, 59.14s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 146/237 [3:37:44<1:57:44, 77.63s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/237 [3:38:49<1:50:57, 73.97s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/237 [3:39:43<1:40:40, 67.87s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 149/237 [3:40:39<1:34:25, 64.38s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/237 [3:41:34<1:29:21, 61.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/237 [3:43:06<1:41:11, 70.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 152/237 [3:45:18<2:06:18, 89.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153/237 [3:46:28<1:56:51, 83.47s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/237 [3:47:35<1:48:44, 78.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 155/237 [3:49:44<2:07:57, 93.62s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 156/237 [3:52:24<2:33:22, 113.61s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/237 [3:53:17<2:06:57, 95.22s/it]  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 158/237 [3:55:23<2:17:38, 104.54s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 159/237 [3:56:34<2:02:42, 94.38s/it]  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 160/237 [3:57:34<1:48:01, 84.17s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 161/237 [3:58:58<1:46:22, 83.97s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 162/237 [4:00:13<1:41:49, 81.46s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 163/237 [4:01:00<1:27:35, 71.02s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 164/237 [4:01:57<1:21:24, 66.92s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/237 [4:03:05<1:20:36, 67.17s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 166/237 [4:04:39<1:29:10, 75.36s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 167/237 [4:06:03<1:30:47, 77.82s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 168/237 [4:06:36<1:14:09, 64.49s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 169/237 [4:08:22<1:27:12, 76.94s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 170/237 [4:10:26<1:41:35, 90.97s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/237 [4:10:43<1:15:32, 68.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 172/237 [4:12:28<1:26:18, 79.67s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 173/237 [4:13:37<1:21:25, 76.33s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 174/237 [4:14:22<1:10:24, 67.06s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 175/237 [4:15:26<1:08:17, 66.09s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 176/237 [4:16:42<1:10:26, 69.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 177/237 [4:18:15<1:16:16, 76.27s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 178/237 [4:19:02<1:06:12, 67.33s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 179/237 [4:19:53<1:00:30, 62.60s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/237 [4:20:50<57:49, 60.88s/it]   76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 181/237 [4:21:13<46:11, 49.50s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 182/237 [4:22:16<49:02, 53.50s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 183/237 [4:23:24<52:00, 57.79s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 184/237 [4:24:20<50:47, 57.50s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 185/237 [4:25:49<57:53, 66.80s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 186/237 [4:27:03<58:40, 69.04s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 187/237 [4:27:39<49:09, 58.98s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 188/237 [4:28:33<47:09, 57.74s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 189/237 [4:28:48<35:45, 44.70s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 190/237 [4:29:56<40:36, 51.84s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 191/237 [4:30:47<39:35, 51.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 192/237 [4:31:43<39:31, 52.69s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 193/237 [4:33:28<50:19, 68.63s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 194/237 [4:34:51<52:10, 72.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/237 [4:37:05<1:03:43, 91.03s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 196/237 [4:38:12<57:25, 84.04s/it]   83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 197/237 [4:38:48<46:27, 69.69s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 198/237 [4:39:47<43:10, 66.43s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 199/237 [4:42:08<56:14, 88.81s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 200/237 [4:43:17<51:06, 82.88s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 201/237 [4:44:13<44:44, 74.57s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 202/237 [4:45:00<38:43, 66.37s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 203/237 [4:47:31<52:03, 91.86s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 204/237 [4:48:15<42:32, 77.36s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 205/237 [4:48:29<31:06, 58.33s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 206/237 [4:49:34<31:17, 60.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 207/237 [4:50:24<28:38, 57.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 208/237 [4:50:52<23:29, 48.61s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 209/237 [4:51:50<23:58, 51.38s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 210/237 [4:52:45<23:37, 52.49s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 211/237 [4:53:15<19:50, 45.80s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 212/237 [4:55:07<27:14, 65.39s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 213/237 [4:55:47<23:08, 57.85s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 214/237 [4:56:28<20:17, 52.93s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 215/237 [4:57:09<18:07, 49.41s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 216/237 [4:57:42<15:30, 44.33s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 217/237 [4:58:11<13:17, 39.90s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 218/237 [4:58:48<12:21, 39.01s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 219/237 [4:59:36<12:29, 41.66s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 220/237 [5:00:57<15:09, 53.50s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 221/237 [5:01:45<13:46, 51.68s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 222/237 [5:03:15<15:49, 63.32s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 223/237 [5:03:29<11:17, 48.38s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 224/237 [5:04:19<10:35, 48.85s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 225/237 [5:05:09<09:51, 49.30s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 226/237 [5:05:24<07:09, 39.06s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 227/237 [5:05:35<05:06, 30.65s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 228/237 [5:08:01<09:46, 65.17s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 229/237 [5:08:32<07:19, 54.90s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 230/237 [5:08:38<04:42, 40.33s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 231/237 [5:08:46<03:03, 30.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 232/237 [5:09:35<03:00, 36.05s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 233/237 [5:10:12<02:25, 36.48s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 234/237 [5:10:26<01:28, 29.48s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 235/237 [5:10:36<00:47, 23.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 236/237 [5:10:48<00:20, 20.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [5:11:10<00:00, 20.92s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237/237 [5:11:10<00:00, 78.78s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/normanweissman2019/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/normanweissman2019/splits/normanweissman2019_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:9
combo_seen1:52
combo_seen2:18
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_070526-f05mufn5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NormanWeissman2019_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/f05mufn5
  0%|          | 0/3397 [00:00<?, ?it/s]  0%|          | 4/3397 [00:00<01:29, 37.71it/s]  0%|          | 11/3397 [00:00<01:02, 54.17it/s]  1%|          | 18/3397 [00:00<00:56, 59.29it/s]  1%|          | 25/3397 [00:00<00:54, 62.32it/s]  1%|          | 35/3397 [00:00<00:46, 71.60it/s]  1%|â–         | 45/3397 [00:00<00:43, 77.94it/s]  2%|â–         | 54/3397 [00:00<00:42, 79.07it/s]  2%|â–         | 62/3397 [00:00<00:43, 76.29it/s]  2%|â–         | 71/3397 [00:00<00:41, 80.02it/s]  2%|â–         | 80/3397 [00:01<00:41, 79.80it/s]  3%|â–Ž         | 89/3397 [00:01<00:42, 77.42it/s]  3%|â–Ž         | 98/3397 [00:01<00:41, 80.04it/s]  3%|â–Ž         | 107/3397 [00:01<00:42, 77.50it/s]  3%|â–Ž         | 116/3397 [00:01<00:40, 80.50it/s]  4%|â–Ž         | 125/3397 [00:01<00:40, 80.90it/s]  4%|â–         | 134/3397 [00:01<00:41, 78.41it/s]  4%|â–         | 144/3397 [00:01<00:41, 79.20it/s]  4%|â–         | 152/3397 [00:02<00:40, 79.32it/s]  5%|â–         | 160/3397 [00:02<00:40, 79.28it/s]  5%|â–         | 169/3397 [00:02<00:39, 82.25it/s]  5%|â–Œ         | 178/3397 [00:02<00:39, 80.92it/s]  6%|â–Œ         | 187/3397 [00:02<00:41, 76.69it/s]  6%|â–Œ         | 195/3397 [00:02<00:43, 73.58it/s]  6%|â–Œ         | 203/3397 [00:02<00:44, 71.83it/s]  6%|â–Œ         | 211/3397 [00:02<00:45, 70.64it/s]  6%|â–‹         | 219/3397 [00:02<00:45, 69.43it/s]  7%|â–‹         | 226/3397 [00:03<00:45, 69.09it/s]  7%|â–‹         | 233/3397 [00:03<00:47, 66.04it/s]  7%|â–‹         | 241/3397 [00:03<00:46, 68.51it/s]  7%|â–‹         | 248/3397 [00:03<00:46, 67.54it/s]  8%|â–Š         | 255/3397 [00:03<00:46, 67.24it/s]  8%|â–Š         | 262/3397 [00:03<00:48, 64.29it/s]  8%|â–Š         | 270/3397 [00:03<00:46, 67.56it/s]  8%|â–Š         | 277/3397 [00:03<00:46, 67.75it/s]  8%|â–Š         | 284/3397 [00:03<00:46, 67.38it/s]  9%|â–Š         | 291/3397 [00:04<00:46, 67.14it/s]  9%|â–‰         | 298/3397 [00:04<00:46, 67.09it/s]  9%|â–‰         | 305/3397 [00:04<00:45, 67.45it/s]  9%|â–‰         | 312/3397 [00:04<00:46, 66.67it/s]  9%|â–‰         | 319/3397 [00:04<00:46, 65.77it/s] 10%|â–‰         | 326/3397 [00:04<00:47, 65.25it/s] 10%|â–‰         | 333/3397 [00:04<00:46, 65.79it/s] 10%|â–ˆ         | 340/3397 [00:04<00:46, 66.36it/s] 10%|â–ˆ         | 347/3397 [00:04<00:45, 66.81it/s] 10%|â–ˆ         | 354/3397 [00:04<00:45, 67.09it/s] 11%|â–ˆ         | 361/3397 [00:05<00:47, 64.03it/s] 11%|â–ˆ         | 369/3397 [00:05<00:44, 67.52it/s] 11%|â–ˆ         | 376/3397 [00:05<00:44, 67.32it/s] 11%|â–ˆâ–        | 383/3397 [00:05<00:45, 66.64it/s] 11%|â–ˆâ–        | 390/3397 [00:05<00:45, 65.92it/s] 12%|â–ˆâ–        | 397/3397 [00:05<00:45, 65.92it/s] 12%|â–ˆâ–        | 404/3397 [00:05<00:45, 66.26it/s] 12%|â–ˆâ–        | 411/3397 [00:05<00:45, 66.26it/s] 12%|â–ˆâ–        | 418/3397 [00:05<00:45, 66.18it/s] 13%|â–ˆâ–Ž        | 425/3397 [00:06<00:44, 66.48it/s] 13%|â–ˆâ–Ž        | 432/3397 [00:06<00:44, 66.97it/s] 13%|â–ˆâ–Ž        | 439/3397 [00:06<00:43, 67.30it/s] 13%|â–ˆâ–Ž        | 446/3397 [00:06<00:43, 67.48it/s] 13%|â–ˆâ–Ž        | 453/3397 [00:06<00:43, 67.73it/s] 14%|â–ˆâ–Ž        | 460/3397 [00:06<00:44, 66.39it/s] 14%|â–ˆâ–Ž        | 467/3397 [00:06<00:44, 65.68it/s] 14%|â–ˆâ–        | 474/3397 [00:06<00:44, 65.56it/s] 14%|â–ˆâ–        | 481/3397 [00:06<00:43, 66.34it/s] 14%|â–ˆâ–        | 488/3397 [00:06<00:43, 66.66it/s] 15%|â–ˆâ–        | 495/3397 [00:07<00:43, 66.97it/s] 15%|â–ˆâ–        | 502/3397 [00:07<00:43, 67.02it/s] 15%|â–ˆâ–        | 509/3397 [00:07<00:43, 65.79it/s] 15%|â–ˆâ–Œ        | 516/3397 [00:07<00:44, 64.53it/s] 15%|â–ˆâ–Œ        | 523/3397 [00:07<00:45, 63.30it/s] 16%|â–ˆâ–Œ        | 530/3397 [00:07<00:45, 63.19it/s] 16%|â–ˆâ–Œ        | 537/3397 [00:07<00:45, 63.34it/s] 16%|â–ˆâ–Œ        | 544/3397 [00:07<00:45, 62.33it/s] 16%|â–ˆâ–Œ        | 551/3397 [00:07<00:45, 61.94it/s] 16%|â–ˆâ–‹        | 558/3397 [00:08<00:45, 62.67it/s] 17%|â–ˆâ–‹        | 565/3397 [00:08<00:45, 62.63it/s] 17%|â–ˆâ–‹        | 572/3397 [00:08<00:45, 62.35it/s] 17%|â–ˆâ–‹        | 579/3397 [00:08<00:47, 59.61it/s] 17%|â–ˆâ–‹        | 586/3397 [00:08<00:45, 61.26it/s] 17%|â–ˆâ–‹        | 593/3397 [00:08<00:44, 62.42it/s] 18%|â–ˆâ–Š        | 600/3397 [00:08<00:44, 62.36it/s] 18%|â–ˆâ–Š        | 607/3397 [00:08<00:44, 62.68it/s] 18%|â–ˆâ–Š        | 614/3397 [00:08<00:44, 62.20it/s] 18%|â–ˆâ–Š        | 621/3397 [00:09<00:45, 61.61it/s] 18%|â–ˆâ–Š        | 628/3397 [00:09<00:44, 61.72it/s] 19%|â–ˆâ–Š        | 635/3397 [00:09<00:44, 62.75it/s] 19%|â–ˆâ–‰        | 642/3397 [00:09<00:43, 62.99it/s] 19%|â–ˆâ–‰        | 649/3397 [00:09<00:43, 62.88it/s] 19%|â–ˆâ–‰        | 656/3397 [00:09<00:44, 60.98it/s] 20%|â–ˆâ–‰        | 664/3397 [00:09<00:42, 64.10it/s] 20%|â–ˆâ–‰        | 671/3397 [00:09<00:42, 63.66it/s] 20%|â–ˆâ–‰        | 678/3397 [00:10<00:43, 62.77it/s] 20%|â–ˆâ–ˆ        | 685/3397 [00:10<00:42, 63.32it/s] 20%|â–ˆâ–ˆ        | 692/3397 [00:10<00:43, 62.57it/s] 21%|â–ˆâ–ˆ        | 699/3397 [00:10<00:43, 62.02it/s] 21%|â–ˆâ–ˆ        | 706/3397 [00:10<00:43, 62.25it/s] 21%|â–ˆâ–ˆ        | 713/3397 [00:10<00:42, 63.46it/s] 21%|â–ˆâ–ˆ        | 720/3397 [00:10<00:42, 62.90it/s] 21%|â–ˆâ–ˆâ–       | 727/3397 [00:10<00:42, 62.56it/s] 22%|â–ˆâ–ˆâ–       | 734/3397 [00:10<00:41, 63.57it/s] 22%|â–ˆâ–ˆâ–       | 741/3397 [00:11<00:41, 63.45it/s] 22%|â–ˆâ–ˆâ–       | 748/3397 [00:11<00:42, 62.85it/s] 22%|â–ˆâ–ˆâ–       | 755/3397 [00:11<00:42, 62.46it/s] 22%|â–ˆâ–ˆâ–       | 762/3397 [00:11<00:41, 62.92it/s] 23%|â–ˆâ–ˆâ–Ž       | 769/3397 [00:11<00:42, 61.93it/s] 23%|â–ˆâ–ˆâ–Ž       | 776/3397 [00:11<00:42, 61.32it/s] 23%|â–ˆâ–ˆâ–Ž       | 783/3397 [00:11<00:41, 62.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 790/3397 [00:11<00:41, 63.27it/s] 23%|â–ˆâ–ˆâ–Ž       | 797/3397 [00:11<00:41, 63.03it/s] 24%|â–ˆâ–ˆâ–Ž       | 804/3397 [00:12<00:41, 62.93it/s] 24%|â–ˆâ–ˆâ–       | 811/3397 [00:12<00:42, 60.86it/s] 24%|â–ˆâ–ˆâ–       | 819/3397 [00:12<00:40, 63.81it/s] 24%|â–ˆâ–ˆâ–       | 826/3397 [00:12<00:40, 64.09it/s] 25%|â–ˆâ–ˆâ–       | 833/3397 [00:12<00:40, 62.83it/s] 25%|â–ˆâ–ˆâ–       | 840/3397 [00:12<00:41, 62.35it/s] 25%|â–ˆâ–ˆâ–       | 847/3397 [00:12<00:42, 60.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 854/3397 [00:12<00:40, 62.41it/s] 25%|â–ˆâ–ˆâ–Œ       | 863/3397 [00:12<00:36, 68.62it/s] 26%|â–ˆâ–ˆâ–Œ       | 873/3397 [00:13<00:34, 73.20it/s] 26%|â–ˆâ–ˆâ–Œ       | 883/3397 [00:13<00:32, 76.29it/s] 26%|â–ˆâ–ˆâ–‹       | 893/3397 [00:13<00:31, 80.52it/s] 27%|â–ˆâ–ˆâ–‹       | 902/3397 [00:13<00:30, 81.53it/s] 27%|â–ˆâ–ˆâ–‹       | 911/3397 [00:13<00:31, 78.63it/s] 27%|â–ˆâ–ˆâ–‹       | 921/3397 [00:13<00:31, 79.28it/s] 27%|â–ˆâ–ˆâ–‹       | 931/3397 [00:13<00:30, 80.04it/s] 28%|â–ˆâ–ˆâ–Š       | 941/3397 [00:13<00:29, 83.24it/s] 28%|â–ˆâ–ˆâ–Š       | 950/3397 [00:13<00:29, 83.25it/s] 28%|â–ˆâ–ˆâ–Š       | 959/3397 [00:14<00:30, 80.95it/s] 28%|â–ˆâ–ˆâ–Š       | 968/3397 [00:14<00:29, 81.15it/s] 29%|â–ˆâ–ˆâ–‰       | 978/3397 [00:14<00:29, 81.75it/s] 29%|â–ˆâ–ˆâ–‰       | 988/3397 [00:14<00:28, 84.08it/s] 29%|â–ˆâ–ˆâ–‰       | 997/3397 [00:14<00:29, 80.52it/s] 30%|â–ˆâ–ˆâ–‰       | 1007/3397 [00:14<00:29, 80.95it/s] 30%|â–ˆâ–ˆâ–‰       | 1016/3397 [00:14<00:29, 81.14it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1026/3397 [00:14<00:29, 81.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1035/3397 [00:15<00:28, 81.96it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1045/3397 [00:15<00:28, 82.09it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1054/3397 [00:15<00:28, 82.39it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1063/3397 [00:15<00:28, 82.01it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1072/3397 [00:15<00:28, 81.80it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1082/3397 [00:15<00:27, 84.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1091/3397 [00:15<00:27, 83.51it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1100/3397 [00:15<00:28, 80.78it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1109/3397 [00:15<00:28, 81.53it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1118/3397 [00:16<00:27, 81.69it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1128/3397 [00:16<00:27, 82.05it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1137/3397 [00:16<00:32, 70.41it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1150/3397 [00:16<00:26, 83.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1160/3397 [00:16<00:27, 82.72it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1170/3397 [00:16<00:26, 82.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1180/3397 [00:16<00:25, 85.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1189/3397 [00:16<00:25, 84.96it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1198/3397 [00:16<00:25, 84.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1207/3397 [00:17<00:26, 84.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1216/3397 [00:17<00:26, 83.25it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1225/3397 [00:17<00:26, 80.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1234/3397 [00:17<00:27, 79.57it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1243/3397 [00:17<00:27, 78.99it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1251/3397 [00:17<00:27, 79.03it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1260/3397 [00:17<00:26, 81.79it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1269/3397 [00:17<00:27, 78.22it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1278/3397 [00:17<00:26, 80.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1287/3397 [00:18<00:26, 80.18it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1296/3397 [00:18<00:26, 79.82it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1305/3397 [00:18<00:26, 79.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1313/3397 [00:18<00:26, 79.54it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1321/3397 [00:18<00:26, 79.13it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1329/3397 [00:18<00:26, 79.10it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1337/3397 [00:18<00:26, 79.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1345/3397 [00:18<00:26, 78.84it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1353/3397 [00:18<00:26, 76.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1362/3397 [00:19<00:26, 76.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1371/3397 [00:19<00:25, 80.11it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1380/3397 [00:19<00:25, 79.74it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1388/3397 [00:19<00:25, 79.42it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1396/3397 [00:19<00:25, 79.14it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1404/3397 [00:19<00:25, 79.23it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1412/3397 [00:19<00:26, 76.29it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1421/3397 [00:19<00:24, 79.76it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1430/3397 [00:19<00:24, 79.09it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1438/3397 [00:20<00:24, 78.46it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1446/3397 [00:20<00:24, 78.36it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1454/3397 [00:20<00:24, 78.29it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1462/3397 [00:20<00:24, 78.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1470/3397 [00:20<00:25, 75.77it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1479/3397 [00:20<00:24, 79.03it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1487/3397 [00:20<00:24, 78.87it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1495/3397 [00:20<00:25, 75.91it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1503/3397 [00:20<00:24, 76.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1512/3397 [00:20<00:23, 79.89it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1521/3397 [00:21<00:25, 73.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1530/3397 [00:21<00:24, 77.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1538/3397 [00:21<00:23, 77.84it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1546/3397 [00:21<00:24, 75.24it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1555/3397 [00:21<00:23, 78.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1563/3397 [00:21<00:23, 77.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1572/3397 [00:21<00:22, 79.76it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1581/3397 [00:21<00:22, 81.01it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1590/3397 [00:21<00:22, 79.39it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1600/3397 [00:22<00:21, 83.29it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1609/3397 [00:22<00:21, 83.21it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1618/3397 [00:22<00:21, 80.95it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1628/3397 [00:22<00:21, 83.81it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1637/3397 [00:22<00:21, 83.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1646/3397 [00:22<00:20, 83.58it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1655/3397 [00:22<00:20, 83.51it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1664/3397 [00:22<00:20, 83.33it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1673/3397 [00:22<00:21, 80.81it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1682/3397 [00:23<00:20, 82.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1691/3397 [00:23<00:20, 83.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1700/3397 [00:23<00:20, 83.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1709/3397 [00:23<00:20, 83.30it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1718/3397 [00:23<00:20, 83.51it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1727/3397 [00:23<00:19, 83.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1736/3397 [00:23<00:19, 83.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1745/3397 [00:23<00:20, 80.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1755/3397 [00:23<00:19, 83.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1764/3397 [00:24<00:19, 83.59it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1773/3397 [00:24<00:19, 83.45it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1782/3397 [00:24<00:20, 80.42it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1792/3397 [00:24<00:19, 81.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1801/3397 [00:24<00:19, 81.78it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1811/3397 [00:24<00:18, 84.85it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1820/3397 [00:24<00:18, 84.33it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1829/3397 [00:24<00:20, 76.19it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1839/3397 [00:25<00:19, 78.90it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1849/3397 [00:25<00:19, 80.97it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1859/3397 [00:25<00:18, 84.40it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1869/3397 [00:25<00:17, 87.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1878/3397 [00:25<00:18, 83.91it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1889/3397 [00:25<00:17, 87.06it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1900/3397 [00:25<00:16, 90.19it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1910/3397 [00:25<00:16, 88.70it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1920/3397 [00:25<00:16, 90.03it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1930/3397 [00:26<00:16, 89.55it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1940/3397 [00:26<00:16, 90.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1950/3397 [00:26<00:16, 89.37it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1960/3397 [00:26<00:16, 89.16it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1970/3397 [00:26<00:16, 88.92it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1980/3397 [00:26<00:15, 91.49it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1990/3397 [00:26<00:15, 91.65it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2000/3397 [00:26<00:15, 87.81it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2009/3397 [00:26<00:15, 87.88it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2019/3397 [00:27<00:15, 88.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2029/3397 [00:27<00:15, 90.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2039/3397 [00:27<00:14, 90.96it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2049/3397 [00:27<00:14, 89.96it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2059/3397 [00:27<00:14, 89.32it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2069/3397 [00:27<00:14, 90.07it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2079/3397 [00:27<00:15, 86.43it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2089/3397 [00:27<00:14, 88.03it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2099/3397 [00:27<00:14, 90.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2109/3397 [00:28<00:14, 89.43it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2119/3397 [00:28<00:14, 90.48it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2129/3397 [00:28<00:14, 89.01it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2138/3397 [00:28<00:14, 86.07it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2148/3397 [00:28<00:14, 89.06it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2157/3397 [00:28<00:14, 85.94it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2167/3397 [00:28<00:13, 87.91it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2176/3397 [00:28<00:13, 87.30it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2186/3397 [00:28<00:13, 90.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2196/3397 [00:28<00:13, 89.87it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2206/3397 [00:29<00:13, 86.11it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2215/3397 [00:29<00:13, 87.06it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2225/3397 [00:29<00:13, 86.68it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2234/3397 [00:29<00:13, 84.36it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2244/3397 [00:29<00:13, 88.31it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2253/3397 [00:29<00:13, 87.33it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2263/3397 [00:29<00:12, 90.24it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2273/3397 [00:29<00:12, 89.01it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2282/3397 [00:29<00:13, 85.32it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2291/3397 [00:30<00:12, 85.97it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2300/3397 [00:30<00:12, 85.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2310/3397 [00:30<00:12, 86.12it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2320/3397 [00:30<00:12, 89.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2329/3397 [00:30<00:12, 85.87it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2338/3397 [00:30<00:12, 86.62it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2348/3397 [00:30<00:11, 89.10it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2357/3397 [00:30<00:12, 85.91it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2366/3397 [00:30<00:11, 86.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2375/3397 [00:31<00:11, 85.86it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2385/3397 [00:31<00:11, 89.30it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2394/3397 [00:31<00:11, 89.13it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2403/3397 [00:31<00:11, 87.89it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2412/3397 [00:31<00:11, 87.36it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2421/3397 [00:31<00:11, 86.90it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2430/3397 [00:31<00:11, 87.08it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2439/3397 [00:31<00:11, 85.94it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2448/3397 [00:31<00:10, 86.35it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2457/3397 [00:32<00:10, 86.51it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2466/3397 [00:32<00:11, 83.86it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2475/3397 [00:32<00:10, 84.96it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2485/3397 [00:32<00:10, 87.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2494/3397 [00:32<00:10, 85.03it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2504/3397 [00:32<00:10, 87.76it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2513/3397 [00:32<00:10, 86.67it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2523/3397 [00:32<00:09, 87.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2532/3397 [00:32<00:09, 87.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2541/3397 [00:32<00:09, 85.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2550/3397 [00:33<00:10, 82.92it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2559/3397 [00:33<00:10, 80.13it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2568/3397 [00:33<00:10, 78.89it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2577/3397 [00:33<00:10, 79.02it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2586/3397 [00:33<00:10, 79.39it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2595/3397 [00:33<00:10, 79.90it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2604/3397 [00:33<00:10, 78.78it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2614/3397 [00:33<00:09, 82.56it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2623/3397 [00:34<00:09, 80.02it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2633/3397 [00:34<00:09, 84.47it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2642/3397 [00:34<00:09, 83.31it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2652/3397 [00:34<00:08, 86.35it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2661/3397 [00:34<00:08, 84.55it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2670/3397 [00:34<00:08, 85.50it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2679/3397 [00:34<00:08, 86.38it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2688/3397 [00:34<00:08, 86.89it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2698/3397 [00:34<00:07, 89.78it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2707/3397 [00:34<00:07, 89.05it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2716/3397 [00:35<00:07, 86.01it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2726/3397 [00:35<00:07, 85.35it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2735/3397 [00:35<00:07, 84.96it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2745/3397 [00:35<00:07, 89.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2754/3397 [00:35<00:07, 86.42it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2763/3397 [00:35<00:07, 82.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2773/3397 [00:35<00:07, 85.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2782/3397 [00:35<00:07, 84.48it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2792/3397 [00:35<00:06, 87.10it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2802/3397 [00:36<00:06, 89.33it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2811/3397 [00:36<00:06, 87.14it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2822/3397 [00:36<00:06, 89.10it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2831/3397 [00:36<00:06, 89.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2841/3397 [00:36<00:06, 89.51it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2852/3397 [00:36<00:05, 92.22it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2862/3397 [00:36<00:06, 86.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2871/3397 [00:36<00:06, 83.05it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2880/3397 [00:37<00:06, 77.94it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2889/3397 [00:37<00:06, 79.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2898/3397 [00:37<00:06, 78.38it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2906/3397 [00:37<00:06, 78.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2914/3397 [00:37<00:06, 78.94it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2924/3397 [00:37<00:05, 82.33it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2934/3397 [00:37<00:05, 87.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2943/3397 [00:37<00:05, 86.01it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2952/3397 [00:37<00:05, 86.68it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2962/3397 [00:37<00:04, 90.48it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2972/3397 [00:38<00:04, 90.38it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2982/3397 [00:38<00:04, 90.23it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2992/3397 [00:38<00:04, 90.13it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3002/3397 [00:38<00:04, 90.08it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3012/3397 [00:38<00:04, 88.10it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3021/3397 [00:38<00:04, 80.22it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3030/3397 [00:38<00:04, 74.14it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3038/3397 [00:38<00:04, 71.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3046/3397 [00:39<00:04, 70.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3054/3397 [00:39<00:05, 68.34it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3062/3397 [00:39<00:04, 69.46it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3069/3397 [00:39<00:04, 67.29it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3076/3397 [00:39<00:04, 64.82it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3084/3397 [00:39<00:04, 67.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3092/3397 [00:39<00:04, 67.89it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3100/3397 [00:39<00:04, 68.19it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3109/3397 [00:39<00:03, 73.09it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3118/3397 [00:40<00:03, 76.33it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3127/3397 [00:40<00:03, 78.97it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3135/3397 [00:40<00:03, 79.25it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3145/3397 [00:40<00:03, 82.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3155/3397 [00:40<00:02, 86.77it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3164/3397 [00:40<00:02, 84.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3174/3397 [00:40<00:02, 88.41it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3183/3397 [00:40<00:02, 88.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3192/3397 [00:40<00:02, 84.69it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3203/3397 [00:41<00:02, 90.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3213/3397 [00:41<00:02, 90.38it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3223/3397 [00:41<00:01, 91.05it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3233/3397 [00:41<00:01, 87.24it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3243/3397 [00:41<00:01, 87.98it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3253/3397 [00:41<00:01, 89.42it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3262/3397 [00:41<00:01, 73.02it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3277/3397 [00:41<00:01, 90.96it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3287/3397 [00:41<00:01, 90.22it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3297/3397 [00:42<00:01, 87.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3307/3397 [00:42<00:00, 90.37it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3317/3397 [00:42<00:00, 88.23it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3327/3397 [00:42<00:00, 90.69it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3337/3397 [00:42<00:00, 87.09it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3346/3397 [00:42<00:00, 85.38it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3355/3397 [00:42<00:00, 77.89it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3363/3397 [00:42<00:00, 71.95it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3371/3397 [00:43<00:00, 69.10it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3379/3397 [00:43<00:00, 70.06it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3387/3397 [00:43<00:00, 67.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3395/3397 [00:43<00:00, 68.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3397/3397 [00:43<00:00, 78.18it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.9800
Epoch 1 Step 51 Train Loss: 0.5325
Epoch 1 Step 101 Train Loss: 0.4841
Epoch 1 Step 151 Train Loss: 0.5130
Epoch 1 Step 201 Train Loss: 0.4861
Epoch 1 Step 251 Train Loss: 0.4555
Epoch 1 Step 301 Train Loss: 0.5027
Epoch 1 Step 351 Train Loss: 0.5097
Epoch 1 Step 401 Train Loss: 0.4649
Epoch 1 Step 451 Train Loss: 0.5283
Epoch 1 Step 501 Train Loss: 0.4847
Epoch 1 Step 551 Train Loss: 0.5116
Epoch 1 Step 601 Train Loss: 0.5329
Epoch 1 Step 651 Train Loss: 0.5002
Epoch 1 Step 701 Train Loss: 0.5334
Epoch 1 Step 751 Train Loss: 0.5052
Epoch 1 Step 801 Train Loss: 0.4916
Epoch 1 Step 851 Train Loss: 0.4996
Epoch 1 Step 901 Train Loss: 0.4504
Epoch 1: Train Overall MSE: 0.0029 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.1068 Validation Top 20 DE MSE: 0.1762. 
Epoch 2 Step 1 Train Loss: 0.4718
Epoch 2 Step 51 Train Loss: 0.4926
Epoch 2 Step 101 Train Loss: 0.4760
Epoch 2 Step 151 Train Loss: 0.4917
Epoch 2 Step 201 Train Loss: 0.4793
Epoch 2 Step 251 Train Loss: 0.5239
Epoch 2 Step 301 Train Loss: 0.4770
Epoch 2 Step 351 Train Loss: 0.4622
Epoch 2 Step 401 Train Loss: 0.4706
Epoch 2 Step 451 Train Loss: 0.4952
Epoch 2 Step 501 Train Loss: 0.4869
Epoch 2 Step 551 Train Loss: 0.4782
Epoch 2 Step 601 Train Loss: 0.5048
Epoch 2 Step 651 Train Loss: 0.4507
Epoch 2 Step 701 Train Loss: 0.4937
Epoch 2 Step 751 Train Loss: 0.4538
Epoch 2 Step 801 Train Loss: 0.5215
Epoch 2 Step 851 Train Loss: 0.4841
Epoch 2 Step 901 Train Loss: 0.5022
Epoch 2: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0933 Validation Top 20 DE MSE: 0.1593. 
Epoch 3 Step 1 Train Loss: 0.4981
Epoch 3 Step 51 Train Loss: 0.4384
Epoch 3 Step 101 Train Loss: 0.4492
Epoch 3 Step 151 Train Loss: 0.4799
Epoch 3 Step 201 Train Loss: 0.4936
Epoch 3 Step 251 Train Loss: 0.4988
Epoch 3 Step 301 Train Loss: 0.5630
Epoch 3 Step 351 Train Loss: 0.5181
Epoch 3 Step 401 Train Loss: 0.4392
Epoch 3 Step 451 Train Loss: 0.5040
Epoch 3 Step 501 Train Loss: 0.4821
Epoch 3 Step 551 Train Loss: 0.4666
Epoch 3 Step 601 Train Loss: 0.4872
Epoch 3 Step 651 Train Loss: 0.5142
Epoch 3 Step 701 Train Loss: 0.5085
Epoch 3 Step 751 Train Loss: 0.4794
Epoch 3 Step 801 Train Loss: 0.4922
Epoch 3 Step 851 Train Loss: 0.4839
Epoch 3 Step 901 Train Loss: 0.4957
Epoch 3: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0042. 
Train Top 20 DE MSE: 0.0946 Validation Top 20 DE MSE: 0.1615. 
Epoch 4 Step 1 Train Loss: 0.4981
Epoch 4 Step 51 Train Loss: 0.4761
Epoch 4 Step 101 Train Loss: 0.5109
Epoch 4 Step 151 Train Loss: 0.4913
Epoch 4 Step 201 Train Loss: 0.4938
Epoch 4 Step 251 Train Loss: 0.4535
Epoch 4 Step 301 Train Loss: 0.5465
Epoch 4 Step 351 Train Loss: 0.4706
Epoch 4 Step 401 Train Loss: 0.5209
Epoch 4 Step 451 Train Loss: 0.4940
Epoch 4 Step 501 Train Loss: 0.4841
Epoch 4 Step 551 Train Loss: 0.4445
Epoch 4 Step 601 Train Loss: 0.5072
Epoch 4 Step 651 Train Loss: 0.4498
Epoch 4 Step 701 Train Loss: 0.4665
Epoch 4 Step 751 Train Loss: 0.5414
Epoch 4 Step 801 Train Loss: 0.4870
Epoch 4 Step 851 Train Loss: 0.4573
Epoch 4 Step 901 Train Loss: 0.4660
Epoch 4: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0690 Validation Top 20 DE MSE: 0.1202. 
Epoch 5 Step 1 Train Loss: 0.4902
Epoch 5 Step 51 Train Loss: 0.4886
Epoch 5 Step 101 Train Loss: 0.4935
Epoch 5 Step 151 Train Loss: 0.5039
Epoch 5 Step 201 Train Loss: 0.4812
Epoch 5 Step 251 Train Loss: 0.4916
Epoch 5 Step 301 Train Loss: 0.4921
Epoch 5 Step 351 Train Loss: 0.4885
Epoch 5 Step 401 Train Loss: 0.5446
Epoch 5 Step 451 Train Loss: 0.4692
Epoch 5 Step 501 Train Loss: 0.4777
Epoch 5 Step 551 Train Loss: 0.4754
Epoch 5 Step 601 Train Loss: 0.4829
Epoch 5 Step 651 Train Loss: 0.4960
Epoch 5 Step 701 Train Loss: 0.4719
Epoch 5 Step 751 Train Loss: 0.5180
Epoch 5 Step 801 Train Loss: 0.4880
Epoch 5 Step 851 Train Loss: 0.4790
Epoch 5 Step 901 Train Loss: 0.4717
Epoch 5: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0672 Validation Top 20 DE MSE: 0.1211. 
Epoch 6 Step 1 Train Loss: 0.4594
Epoch 6 Step 51 Train Loss: 0.4965
Epoch 6 Step 101 Train Loss: 0.4796
Epoch 6 Step 151 Train Loss: 0.4919
Epoch 6 Step 201 Train Loss: 0.4663
Epoch 6 Step 251 Train Loss: 0.5906
Epoch 6 Step 301 Train Loss: 0.4536
Epoch 6 Step 351 Train Loss: 0.4633
Epoch 6 Step 401 Train Loss: 0.4878
Epoch 6 Step 451 Train Loss: 0.5081
Epoch 6 Step 501 Train Loss: 0.4847
Epoch 6 Step 551 Train Loss: 0.4922
Epoch 6 Step 601 Train Loss: 0.4926
Epoch 6 Step 651 Train Loss: 0.5076
Epoch 6 Step 701 Train Loss: 0.5073
Epoch 6 Step 751 Train Loss: 0.4650
Epoch 6 Step 801 Train Loss: 0.4775
Epoch 6 Step 851 Train Loss: 0.4839
Epoch 6 Step 901 Train Loss: 0.5155
Epoch 6: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.0649 Validation Top 20 DE MSE: 0.1210. 
Epoch 7 Step 1 Train Loss: 0.5059
Epoch 7 Step 51 Train Loss: 0.5277
Epoch 7 Step 101 Train Loss: 0.4904
Epoch 7 Step 151 Train Loss: 0.4742
Epoch 7 Step 201 Train Loss: 0.4945
Epoch 7 Step 251 Train Loss: 0.4545
Epoch 7 Step 301 Train Loss: 0.4930
Epoch 7 Step 351 Train Loss: 0.5227
Epoch 7 Step 401 Train Loss: 0.5129
Epoch 7 Step 451 Train Loss: 0.5242
Epoch 7 Step 501 Train Loss: 0.4926
Epoch 7 Step 551 Train Loss: 0.4860
Epoch 7 Step 601 Train Loss: 0.5274
Epoch 7 Step 651 Train Loss: 0.4763
Epoch 7 Step 701 Train Loss: 0.4974
Epoch 7 Step 751 Train Loss: 0.4995
Epoch 7 Step 801 Train Loss: 0.4829
Epoch 7 Step 851 Train Loss: 0.4510
Epoch 7 Step 901 Train Loss: 0.4979
Epoch 7: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0633 Validation Top 20 DE MSE: 0.1183. 
Epoch 8 Step 1 Train Loss: 0.4781
Epoch 8 Step 51 Train Loss: 0.4911
Epoch 8 Step 101 Train Loss: 0.4758
Epoch 8 Step 151 Train Loss: 0.4905
Epoch 8 Step 201 Train Loss: 0.4927
Epoch 8 Step 251 Train Loss: 0.4612
Epoch 8 Step 301 Train Loss: 0.4789
Epoch 8 Step 351 Train Loss: 0.5124
Epoch 8 Step 401 Train Loss: 0.4853
Epoch 8 Step 451 Train Loss: 0.4662
Epoch 8 Step 501 Train Loss: 0.4952
Epoch 8 Step 551 Train Loss: 0.5006
Epoch 8 Step 601 Train Loss: 0.4928
Epoch 8 Step 651 Train Loss: 0.4976
Epoch 8 Step 701 Train Loss: 0.5016
Epoch 8 Step 751 Train Loss: 0.4742
Epoch 8 Step 801 Train Loss: 0.4674
Epoch 8 Step 851 Train Loss: 0.4996
Epoch 8 Step 901 Train Loss: 0.5157
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0639 Validation Top 20 DE MSE: 0.1204. 
Epoch 9 Step 1 Train Loss: 0.4364
Epoch 9 Step 51 Train Loss: 0.4965
Epoch 9 Step 101 Train Loss: 0.4753
Epoch 9 Step 151 Train Loss: 0.5064
Epoch 9 Step 201 Train Loss: 0.4788
Epoch 9 Step 251 Train Loss: 0.4867
Epoch 9 Step 301 Train Loss: 0.4894
Epoch 9 Step 351 Train Loss: 0.5022
Epoch 9 Step 401 Train Loss: 0.4859
Epoch 9 Step 451 Train Loss: 0.4866
Epoch 9 Step 501 Train Loss: 0.4804
Epoch 9 Step 551 Train Loss: 0.4966
Epoch 9 Step 601 Train Loss: 0.5096
Epoch 9 Step 651 Train Loss: 0.4919
Epoch 9 Step 701 Train Loss: 0.5016
Epoch 9 Step 751 Train Loss: 0.5015
Epoch 9 Step 801 Train Loss: 0.5041
Epoch 9 Step 851 Train Loss: 0.4581
Epoch 9 Step 901 Train Loss: 0.5449
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0654 Validation Top 20 DE MSE: 0.1203. 
Epoch 10 Step 1 Train Loss: 0.5056
Epoch 10 Step 51 Train Loss: 0.4605
Epoch 10 Step 101 Train Loss: 0.4782
Epoch 10 Step 151 Train Loss: 0.4686
Epoch 10 Step 201 Train Loss: 0.4671
Epoch 10 Step 251 Train Loss: 0.4925
Epoch 10 Step 301 Train Loss: 0.4745
Epoch 10 Step 351 Train Loss: 0.5142
Epoch 10 Step 401 Train Loss: 0.4811
Epoch 10 Step 451 Train Loss: 0.5741
Epoch 10 Step 501 Train Loss: 0.5610
Epoch 10 Step 551 Train Loss: 0.5074
Epoch 10 Step 601 Train Loss: 0.5327
Epoch 10 Step 651 Train Loss: 0.4866
Epoch 10 Step 701 Train Loss: 0.5013
Epoch 10 Step 751 Train Loss: 0.5270
Epoch 10 Step 801 Train Loss: 0.5146
Epoch 10 Step 851 Train Loss: 0.4814
Epoch 10 Step 901 Train Loss: 0.5047
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0611 Validation Top 20 DE MSE: 0.1181. 
Epoch 11 Step 1 Train Loss: 0.5088
Epoch 11 Step 51 Train Loss: 0.5111
Epoch 11 Step 101 Train Loss: 0.4634
Epoch 11 Step 151 Train Loss: 0.4731
Epoch 11 Step 201 Train Loss: 0.5088
Epoch 11 Step 251 Train Loss: 0.4814
Epoch 11 Step 301 Train Loss: 0.4943
Epoch 11 Step 351 Train Loss: 0.4840
Epoch 11 Step 401 Train Loss: 0.5192
Epoch 11 Step 451 Train Loss: 0.4588
Epoch 11 Step 501 Train Loss: 0.4862
Epoch 11 Step 551 Train Loss: 0.4993
Epoch 11 Step 601 Train Loss: 0.4621
Epoch 11 Step 651 Train Loss: 0.4867
Epoch 11 Step 701 Train Loss: 0.5153
Epoch 11 Step 751 Train Loss: 0.5558
Epoch 11 Step 801 Train Loss: 0.4949
Epoch 11 Step 851 Train Loss: 0.4579
Epoch 11 Step 901 Train Loss: 0.4944
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0616 Validation Top 20 DE MSE: 0.1182. 
Epoch 12 Step 1 Train Loss: 0.5413
Epoch 12 Step 51 Train Loss: 0.5140
Epoch 12 Step 101 Train Loss: 0.4844
Epoch 12 Step 151 Train Loss: 0.4683
Epoch 12 Step 201 Train Loss: 0.4911
Epoch 12 Step 251 Train Loss: 0.4788
Epoch 12 Step 301 Train Loss: 0.4905
Epoch 12 Step 351 Train Loss: 0.4678
Epoch 12 Step 401 Train Loss: 0.5010
Epoch 12 Step 451 Train Loss: 0.4781
Epoch 12 Step 501 Train Loss: 0.4573
Epoch 12 Step 551 Train Loss: 0.5072
Epoch 12 Step 601 Train Loss: 0.4927
Epoch 12 Step 651 Train Loss: 0.5114
Epoch 12 Step 701 Train Loss: 0.4973
Epoch 12 Step 751 Train Loss: 0.5116
Epoch 12 Step 801 Train Loss: 0.5103
Epoch 12 Step 851 Train Loss: 0.4958
Epoch 12 Step 901 Train Loss: 0.4963
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0598 Validation Top 20 DE MSE: 0.1160. 
Epoch 13 Step 1 Train Loss: 0.4991
Epoch 13 Step 51 Train Loss: 0.5081
Epoch 13 Step 101 Train Loss: 0.4962
Epoch 13 Step 151 Train Loss: 0.4472
Epoch 13 Step 201 Train Loss: 0.4918
Epoch 13 Step 251 Train Loss: 0.5100
Epoch 13 Step 301 Train Loss: 0.4755
Epoch 13 Step 351 Train Loss: 0.5134
Epoch 13 Step 401 Train Loss: 0.4893
Epoch 13 Step 451 Train Loss: 0.4595
Epoch 13 Step 501 Train Loss: 0.4655
Epoch 13 Step 551 Train Loss: 0.4851
Epoch 13 Step 601 Train Loss: 0.5263
Epoch 13 Step 651 Train Loss: 0.4840
Epoch 13 Step 701 Train Loss: 0.5052
Epoch 13 Step 751 Train Loss: 0.4889
Epoch 13 Step 801 Train Loss: 0.4736
Epoch 13 Step 851 Train Loss: 0.4838
Epoch 13 Step 901 Train Loss: 0.5087
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0629 Validation Top 20 DE MSE: 0.1209. 
Epoch 14 Step 1 Train Loss: 0.4450
Epoch 14 Step 51 Train Loss: 0.4882
Epoch 14 Step 101 Train Loss: 0.4874
Epoch 14 Step 151 Train Loss: 0.5389
Epoch 14 Step 201 Train Loss: 0.5054
Epoch 14 Step 251 Train Loss: 0.5144
Epoch 14 Step 301 Train Loss: 0.4690
Epoch 14 Step 351 Train Loss: 0.5180
Epoch 14 Step 401 Train Loss: 0.5171
Epoch 14 Step 451 Train Loss: 0.4987
Epoch 14 Step 501 Train Loss: 0.4880
Epoch 14 Step 551 Train Loss: 0.4842
Epoch 14 Step 601 Train Loss: 0.4698
Epoch 14 Step 651 Train Loss: 0.4592
Epoch 14 Step 701 Train Loss: 0.4808
Epoch 14 Step 751 Train Loss: 0.4800
Epoch 14 Step 801 Train Loss: 0.4807
Epoch 14 Step 851 Train Loss: 0.4858
Epoch 14 Step 901 Train Loss: 0.5076
Epoch 14: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0616 Validation Top 20 DE MSE: 0.1191. 
Epoch 15 Step 1 Train Loss: 0.5115
Epoch 15 Step 51 Train Loss: 0.4817
Epoch 15 Step 101 Train Loss: 0.5048
Epoch 15 Step 151 Train Loss: 0.4614
Epoch 15 Step 201 Train Loss: 0.4985
Epoch 15 Step 251 Train Loss: 0.5456
Epoch 15 Step 301 Train Loss: 0.5061
Epoch 15 Step 351 Train Loss: 0.4593
Epoch 15 Step 401 Train Loss: 0.5119
Epoch 15 Step 451 Train Loss: 0.4783
Epoch 15 Step 501 Train Loss: 0.4944
Epoch 15 Step 551 Train Loss: 0.4863
Epoch 15 Step 601 Train Loss: 0.4491
Epoch 15 Step 651 Train Loss: 0.5465
Epoch 15 Step 701 Train Loss: 0.5252
Epoch 15 Step 751 Train Loss: 0.5310
Epoch 15 Step 801 Train Loss: 0.4933
Epoch 15 Step 851 Train Loss: 0.4706
Epoch 15 Step 901 Train Loss: 0.4857
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0628 Validation Top 20 DE MSE: 0.1193. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1353
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0041747307
test_combo_seen0_pearson: 0.9906295249815743
test_combo_seen0_mse_de: 0.12471807
test_combo_seen0_pearson_de: 0.6408400112764902
test_combo_seen1_mse: 0.0043264013
test_combo_seen1_pearson: 0.9904732855809082
test_combo_seen1_mse_de: 0.14305666
test_combo_seen1_pearson_de: 0.7721785082489555
test_combo_seen2_mse: 0.003254756
test_combo_seen2_pearson: 0.9931054704710383
test_combo_seen2_mse_de: 0.10167843
test_combo_seen2_pearson_de: 0.9614470367754504
test_unseen_single_mse: 0.0023774148
test_unseen_single_pearson: 0.994504199380085
test_unseen_single_mse_de: 0.14631253
test_unseen_single_pearson_de: 0.9094431234467616
test_combo_seen0_pearson_delta: 0.6064367657908829
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.08333333333333333
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8999999999999999
test_combo_seen0_mse_top20_de_non_dropout: 0.17390497
test_combo_seen1_pearson_delta: 0.6193475647383241
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.10192307692307691
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8961538461538461
test_combo_seen1_mse_top20_de_non_dropout: 0.17785579
test_combo_seen2_pearson_delta: 0.6629130996593506
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.07222222222222223
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.9583333333333334
test_combo_seen2_mse_top20_de_non_dropout: 0.108274885
test_unseen_single_pearson_delta: 0.4157319045778884
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2611111111111111
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9129629629629631
test_unseen_single_mse_top20_de_non_dropout: 0.16606934
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.004 MB of 0.028 MB uploadedwandb: / 0.014 MB of 0.028 MB uploadedwandb: - 0.014 MB of 0.028 MB uploadedwandb: \ 0.014 MB of 0.028 MB uploadedwandb: | 0.028 MB of 0.028 MB uploadedwandb: / 0.028 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen0_mse â–
wandb:                                      test_combo_seen0_mse_de â–
wandb:                    test_combo_seen0_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen0_pearson â–
wandb:                                  test_combo_seen0_pearson_de â–
wandb:                               test_combo_seen0_pearson_delta â–
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–†â–†â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–ƒâ–„â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‡â–†â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb:                                                train_pearson â–â–ƒâ–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–‡â–…â–ƒâ–…â–ƒâ–…â–„â–…â–ƒâ–â–†â–†â–â–ƒâ–†â–…â–†â–„â–…â–ƒâ–ˆâ–„â–ƒâ–„â–…â–‚â–ˆâ–„â–†â–ƒâ–…â–„â–†â–ƒâ–‚â–ƒâ–…â–„â–‚â–„
wandb:                                                   val_de_mse â–ˆâ–†â–†â–â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–
wandb:                                               val_de_pearson â–â–…â–…â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡
wandb:                                                      val_mse â–†â–†â–ˆâ–†â–‚â–„â–‚â–‚â–â–â–â–‚â–â–‚â–‚
wandb:                                                  val_pearson â–â–‚â–â–ƒâ–‡â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.08333
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.9
wandb:                                         test_combo_seen0_mse 0.00417
wandb:                                      test_combo_seen0_mse_de 0.12472
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.1739
wandb:                                     test_combo_seen0_pearson 0.99063
wandb:                                  test_combo_seen0_pearson_de 0.64084
wandb:                               test_combo_seen0_pearson_delta 0.60644
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.10192
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.89615
wandb:                                         test_combo_seen1_mse 0.00433
wandb:                                      test_combo_seen1_mse_de 0.14306
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.17786
wandb:                                     test_combo_seen1_pearson 0.99047
wandb:                                  test_combo_seen1_pearson_de 0.77218
wandb:                               test_combo_seen1_pearson_delta 0.61935
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.07222
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.95833
wandb:                                         test_combo_seen2_mse 0.00325
wandb:                                      test_combo_seen2_mse_de 0.10168
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.10827
wandb:                                     test_combo_seen2_pearson 0.99311
wandb:                                  test_combo_seen2_pearson_de 0.96145
wandb:                               test_combo_seen2_pearson_delta 0.66291
wandb:                                                  test_de_mse 0.1353
wandb:                                              test_de_pearson 0.82813
wandb:               test_frac_opposite_direction_top20_non_dropout 0.13585
wandb:                          test_frac_sigma_below_1_non_dropout 0.91132
wandb:                                                     test_mse 0.00364
wandb:                                test_mse_top20_de_non_dropout 0.1627
wandb:                                                 test_pearson 0.99196
wandb:                                           test_pearson_delta 0.57378
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.26111
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.91296
wandb:                                       test_unseen_single_mse 0.00238
wandb:                                    test_unseen_single_mse_de 0.14631
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.16607
wandb:                                   test_unseen_single_pearson 0.9945
wandb:                                test_unseen_single_pearson_de 0.90944
wandb:                             test_unseen_single_pearson_delta 0.41573
wandb:                                                 train_de_mse 0.06284
wandb:                                             train_de_pearson 0.92881
wandb:                                                    train_mse 0.00174
wandb:                                                train_pearson 0.99629
wandb:                                                training_loss 0.47168
wandb:                                                   val_de_mse 0.11931
wandb:                                               val_de_pearson 0.83016
wandb:                                                      val_mse 0.00325
wandb:                                                  val_pearson 0.9928
wandb: 
wandb: ðŸš€ View run NormanWeissman2019_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/f05mufn5
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_070526-f05mufn5/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/normanweissman2019/splits/normanweissman2019_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:7
combo_seen1:52
combo_seen2:18
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_080254-mpwdrfot
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NormanWeissman2019_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/mpwdrfot
Start Training...
Epoch 1 Step 1 Train Loss: 1.1452
Epoch 1 Step 51 Train Loss: 0.4989
Epoch 1 Step 101 Train Loss: 0.5059
Epoch 1 Step 151 Train Loss: 0.5209
Epoch 1 Step 201 Train Loss: 0.4603
Epoch 1 Step 251 Train Loss: 0.5096
Epoch 1 Step 301 Train Loss: 0.5046
Epoch 1 Step 351 Train Loss: 0.4622
Epoch 1 Step 401 Train Loss: 0.5089
Epoch 1 Step 451 Train Loss: 0.5207
Epoch 1 Step 501 Train Loss: 0.5136
Epoch 1 Step 551 Train Loss: 0.4552
Epoch 1 Step 601 Train Loss: 0.4373
Epoch 1 Step 651 Train Loss: 0.4866
Epoch 1 Step 701 Train Loss: 0.4814
Epoch 1 Step 751 Train Loss: 0.4745
Epoch 1 Step 801 Train Loss: 0.5187
Epoch 1 Step 851 Train Loss: 0.4540
Epoch 1 Step 901 Train Loss: 0.5036
Epoch 1: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.1187 Validation Top 20 DE MSE: 0.1974. 
Epoch 2 Step 1 Train Loss: 0.4855
Epoch 2 Step 51 Train Loss: 0.4467
Epoch 2 Step 101 Train Loss: 0.4989
Epoch 2 Step 151 Train Loss: 0.5044
Epoch 2 Step 201 Train Loss: 0.4692
Epoch 2 Step 251 Train Loss: 0.5055
Epoch 2 Step 301 Train Loss: 0.4605
Epoch 2 Step 351 Train Loss: 0.4913
Epoch 2 Step 401 Train Loss: 0.4821
Epoch 2 Step 451 Train Loss: 0.4781
Epoch 2 Step 501 Train Loss: 0.4867
Epoch 2 Step 551 Train Loss: 0.4630
Epoch 2 Step 601 Train Loss: 0.4804
Epoch 2 Step 651 Train Loss: 0.4855
Epoch 2 Step 701 Train Loss: 0.4839
Epoch 2 Step 751 Train Loss: 0.4330
Epoch 2 Step 801 Train Loss: 0.4489
Epoch 2 Step 851 Train Loss: 0.4943
Epoch 2 Step 901 Train Loss: 0.4492
Epoch 2: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.1214 Validation Top 20 DE MSE: 0.1999. 
Epoch 3 Step 1 Train Loss: 0.4870
Epoch 3 Step 51 Train Loss: 0.4781
Epoch 3 Step 101 Train Loss: 0.4815
Epoch 3 Step 151 Train Loss: 0.4978
Epoch 3 Step 201 Train Loss: 0.4929
Epoch 3 Step 251 Train Loss: 0.4758
Epoch 3 Step 301 Train Loss: 0.4770
Epoch 3 Step 351 Train Loss: 0.4853
Epoch 3 Step 401 Train Loss: 0.4829
Epoch 3 Step 451 Train Loss: 0.4711
Epoch 3 Step 501 Train Loss: 0.4594
Epoch 3 Step 551 Train Loss: 0.4412
Epoch 3 Step 601 Train Loss: 0.4907
Epoch 3 Step 651 Train Loss: 0.4721
Epoch 3 Step 701 Train Loss: 0.4651
Epoch 3 Step 751 Train Loss: 0.4873
Epoch 3 Step 801 Train Loss: 0.4717
Epoch 3 Step 851 Train Loss: 0.4840
Epoch 3 Step 901 Train Loss: 0.4812
Epoch 3: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0627 Validation Top 20 DE MSE: 0.1337. 
Epoch 4 Step 1 Train Loss: 0.5277
Epoch 4 Step 51 Train Loss: 0.5040
Epoch 4 Step 101 Train Loss: 0.4709
Epoch 4 Step 151 Train Loss: 0.4655
Epoch 4 Step 201 Train Loss: 0.5117
Epoch 4 Step 251 Train Loss: 0.5136
Epoch 4 Step 301 Train Loss: 0.5149
Epoch 4 Step 351 Train Loss: 0.4543
Epoch 4 Step 401 Train Loss: 0.5011
Epoch 4 Step 451 Train Loss: 0.4929
Epoch 4 Step 501 Train Loss: 0.4913
Epoch 4 Step 551 Train Loss: 0.5280
Epoch 4 Step 601 Train Loss: 0.5233
Epoch 4 Step 651 Train Loss: 0.4757
Epoch 4 Step 701 Train Loss: 0.5191
Epoch 4 Step 751 Train Loss: 0.4914
Epoch 4 Step 801 Train Loss: 0.5130
Epoch 4 Step 851 Train Loss: 0.4898
Epoch 4 Step 901 Train Loss: 0.4870
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0760 Validation Top 20 DE MSE: 0.1383. 
Epoch 5 Step 1 Train Loss: 0.4811
Epoch 5 Step 51 Train Loss: 0.4965
Epoch 5 Step 101 Train Loss: 0.4463
Epoch 5 Step 151 Train Loss: 0.5236
Epoch 5 Step 201 Train Loss: 0.5096
Epoch 5 Step 251 Train Loss: 0.5157
Epoch 5 Step 301 Train Loss: 0.5098
Epoch 5 Step 351 Train Loss: 0.5500
Epoch 5 Step 401 Train Loss: 0.5025
Epoch 5 Step 451 Train Loss: 0.4829
Epoch 5 Step 501 Train Loss: 0.4758
Epoch 5 Step 551 Train Loss: 0.4494
Epoch 5 Step 601 Train Loss: 0.5044
Epoch 5 Step 651 Train Loss: 0.4848
Epoch 5 Step 701 Train Loss: 0.4747
Epoch 5 Step 751 Train Loss: 0.5172
Epoch 5 Step 801 Train Loss: 0.5296
Epoch 5 Step 851 Train Loss: 0.4891
Epoch 5 Step 901 Train Loss: 0.5177
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0600 Validation Top 20 DE MSE: 0.1213. 
Epoch 6 Step 1 Train Loss: 0.4871
Epoch 6 Step 51 Train Loss: 0.4922
Epoch 6 Step 101 Train Loss: 0.5168
Epoch 6 Step 151 Train Loss: 0.4649
Epoch 6 Step 201 Train Loss: 0.4845
Epoch 6 Step 251 Train Loss: 0.4760
Epoch 6 Step 301 Train Loss: 0.5180
Epoch 6 Step 351 Train Loss: 0.5080
Epoch 6 Step 401 Train Loss: 0.4704
Epoch 6 Step 451 Train Loss: 0.5130
Epoch 6 Step 501 Train Loss: 0.4803
Epoch 6 Step 551 Train Loss: 0.5097
Epoch 6 Step 601 Train Loss: 0.5085
Epoch 6 Step 651 Train Loss: 0.4703
Epoch 6 Step 701 Train Loss: 0.5284
Epoch 6 Step 751 Train Loss: 0.4844
Epoch 6 Step 801 Train Loss: 0.5266
Epoch 6 Step 851 Train Loss: 0.5063
Epoch 6 Step 901 Train Loss: 0.4907
Epoch 6: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0530 Validation Top 20 DE MSE: 0.1281. 
Epoch 7 Step 1 Train Loss: 0.4895
Epoch 7 Step 51 Train Loss: 0.4813
Epoch 7 Step 101 Train Loss: 0.4866
Epoch 7 Step 151 Train Loss: 0.4865
Epoch 7 Step 201 Train Loss: 0.5432
Epoch 7 Step 251 Train Loss: 0.4998
Epoch 7 Step 301 Train Loss: 0.5217
Epoch 7 Step 351 Train Loss: 0.5095
Epoch 7 Step 401 Train Loss: 0.4550
Epoch 7 Step 451 Train Loss: 0.4845
Epoch 7 Step 501 Train Loss: 0.5020
Epoch 7 Step 551 Train Loss: 0.4996
Epoch 7 Step 601 Train Loss: 0.4936
Epoch 7 Step 651 Train Loss: 0.5322
Epoch 7 Step 701 Train Loss: 0.4735
Epoch 7 Step 751 Train Loss: 0.5124
Epoch 7 Step 801 Train Loss: 0.5065
Epoch 7 Step 851 Train Loss: 0.5371
Epoch 7 Step 901 Train Loss: 0.4855
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0504 Validation Top 20 DE MSE: 0.1227. 
Epoch 8 Step 1 Train Loss: 0.4817
Epoch 8 Step 51 Train Loss: 0.5027
Epoch 8 Step 101 Train Loss: 0.4636
Epoch 8 Step 151 Train Loss: 0.4961
Epoch 8 Step 201 Train Loss: 0.4939
Epoch 8 Step 251 Train Loss: 0.5137
Epoch 8 Step 301 Train Loss: 0.4955
Epoch 8 Step 351 Train Loss: 0.5269
Epoch 8 Step 401 Train Loss: 0.4894
Epoch 8 Step 451 Train Loss: 0.4982
Epoch 8 Step 501 Train Loss: 0.4848
Epoch 8 Step 551 Train Loss: 0.5004
Epoch 8 Step 601 Train Loss: 0.5049
Epoch 8 Step 651 Train Loss: 0.5356
Epoch 8 Step 701 Train Loss: 0.5028
Epoch 8 Step 751 Train Loss: 0.4621
Epoch 8 Step 801 Train Loss: 0.4819
Epoch 8 Step 851 Train Loss: 0.5320
Epoch 8 Step 901 Train Loss: 0.5044
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0505 Validation Top 20 DE MSE: 0.1218. 
Epoch 9 Step 1 Train Loss: 0.4635
Epoch 9 Step 51 Train Loss: 0.5059
Epoch 9 Step 101 Train Loss: 0.4935
Epoch 9 Step 151 Train Loss: 0.5052
Epoch 9 Step 201 Train Loss: 0.4795
Epoch 9 Step 251 Train Loss: 0.4774
Epoch 9 Step 301 Train Loss: 0.4684
Epoch 9 Step 351 Train Loss: 0.4832
Epoch 9 Step 401 Train Loss: 0.5287
Epoch 9 Step 451 Train Loss: 0.5028
Epoch 9 Step 501 Train Loss: 0.5104
Epoch 9 Step 551 Train Loss: 0.5045
Epoch 9 Step 601 Train Loss: 0.4941
Epoch 9 Step 651 Train Loss: 0.4732
Epoch 9 Step 701 Train Loss: 0.4720
Epoch 9 Step 751 Train Loss: 0.5101
Epoch 9 Step 801 Train Loss: 0.5073
Epoch 9 Step 851 Train Loss: 0.4605
Epoch 9 Step 901 Train Loss: 0.5269
Epoch 9: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0511 Validation Top 20 DE MSE: 0.1232. 
Epoch 10 Step 1 Train Loss: 0.4868
Epoch 10 Step 51 Train Loss: 0.5205
Epoch 10 Step 101 Train Loss: 0.4818
Epoch 10 Step 151 Train Loss: 0.4776
Epoch 10 Step 201 Train Loss: 0.4892
Epoch 10 Step 251 Train Loss: 0.5528
Epoch 10 Step 301 Train Loss: 0.5279
Epoch 10 Step 351 Train Loss: 0.4843
Epoch 10 Step 401 Train Loss: 0.4572
Epoch 10 Step 451 Train Loss: 0.4825
Epoch 10 Step 501 Train Loss: 0.4902
Epoch 10 Step 551 Train Loss: 0.4784
Epoch 10 Step 601 Train Loss: 0.5163
Epoch 10 Step 651 Train Loss: 0.5077
Epoch 10 Step 701 Train Loss: 0.5583
Epoch 10 Step 751 Train Loss: 0.5223
Epoch 10 Step 801 Train Loss: 0.4782
Epoch 10 Step 851 Train Loss: 0.5022
Epoch 10 Step 901 Train Loss: 0.5360
Epoch 10: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0486 Validation Top 20 DE MSE: 0.1206. 
Epoch 11 Step 1 Train Loss: 0.5141
Epoch 11 Step 51 Train Loss: 0.4967
Epoch 11 Step 101 Train Loss: 0.4996
Epoch 11 Step 151 Train Loss: 0.4597
Epoch 11 Step 201 Train Loss: 0.5075
Epoch 11 Step 251 Train Loss: 0.5015
Epoch 11 Step 301 Train Loss: 0.4942
Epoch 11 Step 351 Train Loss: 0.5299
Epoch 11 Step 401 Train Loss: 0.4943
Epoch 11 Step 451 Train Loss: 0.5037
Epoch 11 Step 501 Train Loss: 0.4956
Epoch 11 Step 551 Train Loss: 0.4661
Epoch 11 Step 601 Train Loss: 0.5002
Epoch 11 Step 651 Train Loss: 0.4792
Epoch 11 Step 701 Train Loss: 0.4831
Epoch 11 Step 751 Train Loss: 0.4768
Epoch 11 Step 801 Train Loss: 0.5487
Epoch 11 Step 851 Train Loss: 0.4896
Epoch 11 Step 901 Train Loss: 0.4781
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0491 Validation Top 20 DE MSE: 0.1202. 
Epoch 12 Step 1 Train Loss: 0.4895
Epoch 12 Step 51 Train Loss: 0.4857
Epoch 12 Step 101 Train Loss: 0.4963
Epoch 12 Step 151 Train Loss: 0.4728
Epoch 12 Step 201 Train Loss: 0.4805
Epoch 12 Step 251 Train Loss: 0.5059
Epoch 12 Step 301 Train Loss: 0.4773
Epoch 12 Step 351 Train Loss: 0.4895
Epoch 12 Step 401 Train Loss: 0.4768
Epoch 12 Step 451 Train Loss: 0.5067
Epoch 12 Step 501 Train Loss: 0.5368
Epoch 12 Step 551 Train Loss: 0.5007
Epoch 12 Step 601 Train Loss: 0.5062
Epoch 12 Step 651 Train Loss: 0.5372
Epoch 12 Step 701 Train Loss: 0.5330
Epoch 12 Step 751 Train Loss: 0.5075
Epoch 12 Step 801 Train Loss: 0.4630
Epoch 12 Step 851 Train Loss: 0.4714
Epoch 12 Step 901 Train Loss: 0.5284
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0489 Validation Top 20 DE MSE: 0.1201. 
Epoch 13 Step 1 Train Loss: 0.5017
Epoch 13 Step 51 Train Loss: 0.5161
Epoch 13 Step 101 Train Loss: 0.4612
Epoch 13 Step 151 Train Loss: 0.4914
Epoch 13 Step 201 Train Loss: 0.5038
Epoch 13 Step 251 Train Loss: 0.4831
Epoch 13 Step 301 Train Loss: 0.4863
Epoch 13 Step 351 Train Loss: 0.4376
Epoch 13 Step 401 Train Loss: 0.5262
Epoch 13 Step 451 Train Loss: 0.4968
Epoch 13 Step 501 Train Loss: 0.5430
Epoch 13 Step 551 Train Loss: 0.5605
Epoch 13 Step 601 Train Loss: 0.4985
Epoch 13 Step 651 Train Loss: 0.4817
Epoch 13 Step 701 Train Loss: 0.5306
Epoch 13 Step 751 Train Loss: 0.5190
Epoch 13 Step 801 Train Loss: 0.4718
Epoch 13 Step 851 Train Loss: 0.4711
Epoch 13 Step 901 Train Loss: 0.5056
Epoch 13: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0506 Validation Top 20 DE MSE: 0.1209. 
Epoch 14 Step 1 Train Loss: 0.4939
Epoch 14 Step 51 Train Loss: 0.4771
Epoch 14 Step 101 Train Loss: 0.5079
Epoch 14 Step 151 Train Loss: 0.4671
Epoch 14 Step 201 Train Loss: 0.4640
Epoch 14 Step 251 Train Loss: 0.5130
Epoch 14 Step 301 Train Loss: 0.4989
Epoch 14 Step 351 Train Loss: 0.4905
Epoch 14 Step 401 Train Loss: 0.4858
Epoch 14 Step 451 Train Loss: 0.4859
Epoch 14 Step 501 Train Loss: 0.4688
Epoch 14 Step 551 Train Loss: 0.5057
Epoch 14 Step 601 Train Loss: 0.5056
Epoch 14 Step 651 Train Loss: 0.5466
Epoch 14 Step 701 Train Loss: 0.5109
Epoch 14 Step 751 Train Loss: 0.4772
Epoch 14 Step 801 Train Loss: 0.5370
Epoch 14 Step 851 Train Loss: 0.4870
Epoch 14 Step 901 Train Loss: 0.4808
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0470 Validation Top 20 DE MSE: 0.1191. 
Epoch 15 Step 1 Train Loss: 0.4625
Epoch 15 Step 51 Train Loss: 0.5011
Epoch 15 Step 101 Train Loss: 0.4985
Epoch 15 Step 151 Train Loss: 0.5204
Epoch 15 Step 201 Train Loss: 0.5018
Epoch 15 Step 251 Train Loss: 0.5135
Epoch 15 Step 301 Train Loss: 0.4951
Epoch 15 Step 351 Train Loss: 0.5111
Epoch 15 Step 401 Train Loss: 0.4515
Epoch 15 Step 451 Train Loss: 0.5418
Epoch 15 Step 501 Train Loss: 0.4784
Epoch 15 Step 551 Train Loss: 0.4790
Epoch 15 Step 601 Train Loss: 0.5286
Epoch 15 Step 651 Train Loss: 0.4938
Epoch 15 Step 701 Train Loss: 0.5288
Epoch 15 Step 751 Train Loss: 0.4696
Epoch 15 Step 801 Train Loss: 0.5000
Epoch 15 Step 851 Train Loss: 0.4830
Epoch 15 Step 901 Train Loss: 0.5228
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0511 Validation Top 20 DE MSE: 0.1227. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1260
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.004042563
test_combo_seen0_pearson: 0.9908552184809637
test_combo_seen0_mse_de: 0.122952126
test_combo_seen0_pearson_de: 0.8171515089568102
test_combo_seen1_mse: 0.003579012
test_combo_seen1_pearson: 0.9920393654103877
test_combo_seen1_mse_de: 0.13596135
test_combo_seen1_pearson_de: 0.8037734123030262
test_combo_seen2_mse: 0.0030196684
test_combo_seen2_pearson: 0.9933876047778529
test_combo_seen2_mse_de: 0.06740161
test_combo_seen2_pearson_de: 0.8140831372233213
test_unseen_single_mse: 0.002456894
test_unseen_single_pearson: 0.9943507032961278
test_unseen_single_mse_de: 0.14652513
test_unseen_single_pearson_de: 0.9093292827028814
test_combo_seen0_pearson_delta: 0.6496458703275
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.08571428571428572
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.9142857142857143
test_combo_seen0_mse_top20_de_non_dropout: 0.17530139
test_combo_seen1_pearson_delta: 0.6482022597480925
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.10865384615384616
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.901923076923077
test_combo_seen1_mse_top20_de_non_dropout: 0.1609617
test_combo_seen2_pearson_delta: 0.6922128253123392
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.06666666666666667
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.9555555555555555
test_combo_seen2_mse_top20_de_non_dropout: 0.08344141
test_unseen_single_pearson_delta: 0.44053899461800144
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.21481481481481485
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9092592592592593
test_unseen_single_mse_top20_de_non_dropout: 0.1735568
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.027 MB uploadedwandb: - 0.004 MB of 0.027 MB uploadedwandb: \ 0.024 MB of 0.027 MB uploadedwandb: | 0.027 MB of 0.027 MB uploadedwandb: / 0.027 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen0_mse â–
wandb:                                      test_combo_seen0_mse_de â–
wandb:                    test_combo_seen0_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen0_pearson â–
wandb:                                  test_combo_seen0_pearson_de â–
wandb:                               test_combo_seen0_pearson_delta â–
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ˆâ–‚â–„â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–ƒâ–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‡â–‚â–„â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‚â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–…â–â–†â–‚â–„â–ƒâ–ƒâ–…â–ƒâ–ƒâ–‚â–†â–„â–„â–ƒâ–„â–ƒâ–â–‚â–…â–…â–ƒâ–‚â–‚â–…â–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–ˆâ–„â–„â–…â–‚â–„â–„â–„
wandb:                                                   val_de_mse â–ˆâ–ˆâ–‚â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–ƒâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–ˆâ–†â–â–„â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:                                                  val_pearson â–â–ƒâ–ˆâ–†â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.08571
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.91429
wandb:                                         test_combo_seen0_mse 0.00404
wandb:                                      test_combo_seen0_mse_de 0.12295
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.1753
wandb:                                     test_combo_seen0_pearson 0.99086
wandb:                                  test_combo_seen0_pearson_de 0.81715
wandb:                               test_combo_seen0_pearson_delta 0.64965
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.10865
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.90192
wandb:                                         test_combo_seen1_mse 0.00358
wandb:                                      test_combo_seen1_mse_de 0.13596
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.16096
wandb:                                     test_combo_seen1_pearson 0.99204
wandb:                                  test_combo_seen1_pearson_de 0.80377
wandb:                               test_combo_seen1_pearson_delta 0.6482
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.06667
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.95556
wandb:                                         test_combo_seen2_mse 0.00302
wandb:                                      test_combo_seen2_mse_de 0.0674
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.08344
wandb:                                     test_combo_seen2_pearson 0.99339
wandb:                                  test_combo_seen2_pearson_de 0.81408
wandb:                               test_combo_seen2_pearson_delta 0.69221
wandb:                                                  test_de_mse 0.12596
wandb:                                              test_de_pearson 0.83386
wandb:               test_frac_opposite_direction_top20_non_dropout 0.1274
wandb:                          test_frac_sigma_below_1_non_dropout 0.91394
wandb:                                                     test_mse 0.00322
wandb:                                test_mse_top20_de_non_dropout 0.15178
wandb:                                                 test_pearson 0.99279
wandb:                                           test_pearson_delta 0.602
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.21481
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.90926
wandb:                                       test_unseen_single_mse 0.00246
wandb:                                    test_unseen_single_mse_de 0.14653
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.17356
wandb:                                   test_unseen_single_pearson 0.99435
wandb:                                test_unseen_single_pearson_de 0.90933
wandb:                             test_unseen_single_pearson_delta 0.44054
wandb:                                                 train_de_mse 0.05112
wandb:                                             train_de_pearson 0.93274
wandb:                                                    train_mse 0.00156
wandb:                                                train_pearson 0.99665
wandb:                                                training_loss 0.47574
wandb:                                                   val_de_mse 0.1227
wandb:                                               val_de_pearson 0.79582
wandb:                                                      val_mse 0.00295
wandb:                                                  val_pearson 0.99353
wandb: 
wandb: ðŸš€ View run NormanWeissman2019_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/mpwdrfot
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_080254-mpwdrfot/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/normanweissman2019/splits/normanweissman2019_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:2
combo_seen1:39
combo_seen2:23
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_090034-6h6wuj2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NormanWeissman2019_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/6h6wuj2o
Start Training...
Epoch 1 Step 1 Train Loss: 1.0038
Epoch 1 Step 51 Train Loss: 0.5183
Epoch 1 Step 101 Train Loss: 0.5268
Epoch 1 Step 151 Train Loss: 0.5216
Epoch 1 Step 201 Train Loss: 0.5050
Epoch 1 Step 251 Train Loss: 0.5408
Epoch 1 Step 301 Train Loss: 0.5259
Epoch 1 Step 351 Train Loss: 0.5337
Epoch 1 Step 401 Train Loss: 0.4546
Epoch 1 Step 451 Train Loss: 0.5098
Epoch 1 Step 501 Train Loss: 0.5007
Epoch 1 Step 551 Train Loss: 0.5296
Epoch 1 Step 601 Train Loss: 0.4892
Epoch 1 Step 651 Train Loss: 0.4646
Epoch 1 Step 701 Train Loss: 0.4525
Epoch 1 Step 751 Train Loss: 0.5464
Epoch 1 Step 801 Train Loss: 0.4609
Epoch 1 Step 851 Train Loss: 0.4731
Epoch 1 Step 901 Train Loss: 0.4779
Epoch 1 Step 951 Train Loss: 0.4990
Epoch 1: Train Overall MSE: 0.0052 Validation Overall MSE: 0.0063. 
Train Top 20 DE MSE: 0.1107 Validation Top 20 DE MSE: 0.2235. 
Epoch 2 Step 1 Train Loss: 0.5046
Epoch 2 Step 51 Train Loss: 0.4726
Epoch 2 Step 101 Train Loss: 0.5037
Epoch 2 Step 151 Train Loss: 0.5264
Epoch 2 Step 201 Train Loss: 0.4896
Epoch 2 Step 251 Train Loss: 0.4557
Epoch 2 Step 301 Train Loss: 0.5231
Epoch 2 Step 351 Train Loss: 0.4839
Epoch 2 Step 401 Train Loss: 0.5262
Epoch 2 Step 451 Train Loss: 0.4938
Epoch 2 Step 501 Train Loss: 0.4834
Epoch 2 Step 551 Train Loss: 0.4880
Epoch 2 Step 601 Train Loss: 0.4740
Epoch 2 Step 651 Train Loss: 0.4925
Epoch 2 Step 701 Train Loss: 0.4962
Epoch 2 Step 751 Train Loss: 0.5032
Epoch 2 Step 801 Train Loss: 0.4893
Epoch 2 Step 851 Train Loss: 0.4785
Epoch 2 Step 901 Train Loss: 0.5072
Epoch 2 Step 951 Train Loss: 0.4972
Epoch 2: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0038. 
Train Top 20 DE MSE: 0.0841 Validation Top 20 DE MSE: 0.1813. 
Epoch 3 Step 1 Train Loss: 0.4971
Epoch 3 Step 51 Train Loss: 0.4981
Epoch 3 Step 101 Train Loss: 0.4892
Epoch 3 Step 151 Train Loss: 0.5314
Epoch 3 Step 201 Train Loss: 0.4804
Epoch 3 Step 251 Train Loss: 0.5031
Epoch 3 Step 301 Train Loss: 0.5296
Epoch 3 Step 351 Train Loss: 0.4865
Epoch 3 Step 401 Train Loss: 0.5164
Epoch 3 Step 451 Train Loss: 0.5248
Epoch 3 Step 501 Train Loss: 0.4973
Epoch 3 Step 551 Train Loss: 0.5177
Epoch 3 Step 601 Train Loss: 0.5190
Epoch 3 Step 651 Train Loss: 0.4739
Epoch 3 Step 701 Train Loss: 0.5083
Epoch 3 Step 751 Train Loss: 0.5241
Epoch 3 Step 801 Train Loss: 0.5497
Epoch 3 Step 851 Train Loss: 0.5075
Epoch 3 Step 901 Train Loss: 0.4913
Epoch 3 Step 951 Train Loss: 0.4796
Epoch 3: Train Overall MSE: 0.0041 Validation Overall MSE: 0.0049. 
Train Top 20 DE MSE: 0.0998 Validation Top 20 DE MSE: 0.1968. 
Epoch 4 Step 1 Train Loss: 0.5038
Epoch 4 Step 51 Train Loss: 0.5000
Epoch 4 Step 101 Train Loss: 0.4980
Epoch 4 Step 151 Train Loss: 0.4688
Epoch 4 Step 201 Train Loss: 0.4928
Epoch 4 Step 251 Train Loss: 0.5162
Epoch 4 Step 301 Train Loss: 0.4803
Epoch 4 Step 351 Train Loss: 0.4554
Epoch 4 Step 401 Train Loss: 0.5012
Epoch 4 Step 451 Train Loss: 0.4972
Epoch 4 Step 501 Train Loss: 0.5301
Epoch 4 Step 551 Train Loss: 0.4880
Epoch 4 Step 601 Train Loss: 0.4752
Epoch 4 Step 651 Train Loss: 0.5199
Epoch 4 Step 701 Train Loss: 0.5189
Epoch 4 Step 751 Train Loss: 0.5268
Epoch 4 Step 801 Train Loss: 0.5093
Epoch 4 Step 851 Train Loss: 0.4788
Epoch 4 Step 901 Train Loss: 0.4598
Epoch 4 Step 951 Train Loss: 0.5153
Epoch 4: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.1004 Validation Top 20 DE MSE: 0.1979. 
Epoch 5 Step 1 Train Loss: 0.5457
Epoch 5 Step 51 Train Loss: 0.4766
Epoch 5 Step 101 Train Loss: 0.4738
Epoch 5 Step 151 Train Loss: 0.4728
Epoch 5 Step 201 Train Loss: 0.5232
Epoch 5 Step 251 Train Loss: 0.5414
Epoch 5 Step 301 Train Loss: 0.5394
Epoch 5 Step 351 Train Loss: 0.5303
Epoch 5 Step 401 Train Loss: 0.4803
Epoch 5 Step 451 Train Loss: 0.5279
Epoch 5 Step 501 Train Loss: 0.5181
Epoch 5 Step 551 Train Loss: 0.5300
Epoch 5 Step 601 Train Loss: 0.5118
Epoch 5 Step 651 Train Loss: 0.4823
Epoch 5 Step 701 Train Loss: 0.5112
Epoch 5 Step 751 Train Loss: 0.5280
Epoch 5 Step 801 Train Loss: 0.4799
Epoch 5 Step 851 Train Loss: 0.5025
Epoch 5 Step 901 Train Loss: 0.5050
Epoch 5 Step 951 Train Loss: 0.5417
Epoch 5: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0635 Validation Top 20 DE MSE: 0.1422. 
Epoch 6 Step 1 Train Loss: 0.5283
Epoch 6 Step 51 Train Loss: 0.5546
Epoch 6 Step 101 Train Loss: 0.5032
Epoch 6 Step 151 Train Loss: 0.5268
Epoch 6 Step 201 Train Loss: 0.4960
Epoch 6 Step 251 Train Loss: 0.5258
Epoch 6 Step 301 Train Loss: 0.5174
Epoch 6 Step 351 Train Loss: 0.4721
Epoch 6 Step 401 Train Loss: 0.5581
Epoch 6 Step 451 Train Loss: 0.4802
Epoch 6 Step 501 Train Loss: 0.5172
Epoch 6 Step 551 Train Loss: 0.5016
Epoch 6 Step 601 Train Loss: 0.5217
Epoch 6 Step 651 Train Loss: 0.5176
Epoch 6 Step 701 Train Loss: 0.4840
Epoch 6 Step 751 Train Loss: 0.5171
Epoch 6 Step 801 Train Loss: 0.5213
Epoch 6 Step 851 Train Loss: 0.4704
Epoch 6 Step 901 Train Loss: 0.4928
Epoch 6 Step 951 Train Loss: 0.4704
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0636 Validation Top 20 DE MSE: 0.1601. 
Epoch 7 Step 1 Train Loss: 0.5233
Epoch 7 Step 51 Train Loss: 0.4845
Epoch 7 Step 101 Train Loss: 0.4818
Epoch 7 Step 151 Train Loss: 0.5339
Epoch 7 Step 201 Train Loss: 0.4855
Epoch 7 Step 251 Train Loss: 0.4864
Epoch 7 Step 301 Train Loss: 0.5417
Epoch 7 Step 351 Train Loss: 0.4926
Epoch 7 Step 401 Train Loss: 0.5166
Epoch 7 Step 451 Train Loss: 0.5211
Epoch 7 Step 501 Train Loss: 0.5430
Epoch 7 Step 551 Train Loss: 0.5045
Epoch 7 Step 601 Train Loss: 0.5019
Epoch 7 Step 651 Train Loss: 0.4950
Epoch 7 Step 701 Train Loss: 0.5123
Epoch 7 Step 751 Train Loss: 0.5437
Epoch 7 Step 801 Train Loss: 0.5104
Epoch 7 Step 851 Train Loss: 0.4872
Epoch 7 Step 901 Train Loss: 0.5404
Epoch 7 Step 951 Train Loss: 0.5465
Epoch 7: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0609 Validation Top 20 DE MSE: 0.1453. 
Epoch 8 Step 1 Train Loss: 0.5131
Epoch 8 Step 51 Train Loss: 0.5092
Epoch 8 Step 101 Train Loss: 0.4884
Epoch 8 Step 151 Train Loss: 0.5259
Epoch 8 Step 201 Train Loss: 0.5244
Epoch 8 Step 251 Train Loss: 0.5357
Epoch 8 Step 301 Train Loss: 0.4833
Epoch 8 Step 351 Train Loss: 0.4901
Epoch 8 Step 401 Train Loss: 0.4857
Epoch 8 Step 451 Train Loss: 0.4896
Epoch 8 Step 501 Train Loss: 0.5124
Epoch 8 Step 551 Train Loss: 0.5352
Epoch 8 Step 601 Train Loss: 0.5097
Epoch 8 Step 651 Train Loss: 0.4997
Epoch 8 Step 701 Train Loss: 0.5148
Epoch 8 Step 751 Train Loss: 0.4982
Epoch 8 Step 801 Train Loss: 0.5430
Epoch 8 Step 851 Train Loss: 0.5209
Epoch 8 Step 901 Train Loss: 0.4909
Epoch 8 Step 951 Train Loss: 0.5239
Epoch 8: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0610 Validation Top 20 DE MSE: 0.1490. 
Epoch 9 Step 1 Train Loss: 0.5266
Epoch 9 Step 51 Train Loss: 0.5191
Epoch 9 Step 101 Train Loss: 0.5390
Epoch 9 Step 151 Train Loss: 0.5215
Epoch 9 Step 201 Train Loss: 0.4984
Epoch 9 Step 251 Train Loss: 0.5474
Epoch 9 Step 301 Train Loss: 0.4681
Epoch 9 Step 351 Train Loss: 0.5265
Epoch 9 Step 401 Train Loss: 0.5467
Epoch 9 Step 451 Train Loss: 0.4772
Epoch 9 Step 501 Train Loss: 0.4597
Epoch 9 Step 551 Train Loss: 0.5174
Epoch 9 Step 601 Train Loss: 0.5215
Epoch 9 Step 651 Train Loss: 0.4689
Epoch 9 Step 701 Train Loss: 0.4810
Epoch 9 Step 751 Train Loss: 0.5106
Epoch 9 Step 801 Train Loss: 0.5064
Epoch 9 Step 851 Train Loss: 0.4995
Epoch 9 Step 901 Train Loss: 0.4788
Epoch 9 Step 951 Train Loss: 0.5118
Epoch 9: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0601 Validation Top 20 DE MSE: 0.1493. 
Epoch 10 Step 1 Train Loss: 0.5094
Epoch 10 Step 51 Train Loss: 0.5589
Epoch 10 Step 101 Train Loss: 0.5453
Epoch 10 Step 151 Train Loss: 0.5024
Epoch 10 Step 201 Train Loss: 0.4795
Epoch 10 Step 251 Train Loss: 0.5326
Epoch 10 Step 301 Train Loss: 0.4894
Epoch 10 Step 351 Train Loss: 0.5051
Epoch 10 Step 401 Train Loss: 0.5015
Epoch 10 Step 451 Train Loss: 0.5110
Epoch 10 Step 501 Train Loss: 0.4968
Epoch 10 Step 551 Train Loss: 0.5362
Epoch 10 Step 601 Train Loss: 0.5235
Epoch 10 Step 651 Train Loss: 0.5020
Epoch 10 Step 701 Train Loss: 0.5385
Epoch 10 Step 751 Train Loss: 0.5026
Epoch 10 Step 801 Train Loss: 0.4965
Epoch 10 Step 851 Train Loss: 0.4740
Epoch 10 Step 901 Train Loss: 0.5397
Epoch 10 Step 951 Train Loss: 0.4716
Epoch 10: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0593 Validation Top 20 DE MSE: 0.1493. 
Epoch 11 Step 1 Train Loss: 0.5314
Epoch 11 Step 51 Train Loss: 0.5179
Epoch 11 Step 101 Train Loss: 0.4763
Epoch 11 Step 151 Train Loss: 0.5469
Epoch 11 Step 201 Train Loss: 0.5391
Epoch 11 Step 251 Train Loss: 0.5070
Epoch 11 Step 301 Train Loss: 0.5364
Epoch 11 Step 351 Train Loss: 0.5114
Epoch 11 Step 401 Train Loss: 0.4615
Epoch 11 Step 451 Train Loss: 0.5069
Epoch 11 Step 501 Train Loss: 0.4993
Epoch 11 Step 551 Train Loss: 0.4780
Epoch 11 Step 601 Train Loss: 0.5283
Epoch 11 Step 651 Train Loss: 0.5139
Epoch 11 Step 701 Train Loss: 0.5168
Epoch 11 Step 751 Train Loss: 0.4920
Epoch 11 Step 801 Train Loss: 0.4861
Epoch 11 Step 851 Train Loss: 0.5641
Epoch 11 Step 901 Train Loss: 0.4824
Epoch 11 Step 951 Train Loss: 0.5266
Epoch 11: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0579 Validation Top 20 DE MSE: 0.1481. 
Epoch 12 Step 1 Train Loss: 0.5304
Epoch 12 Step 51 Train Loss: 0.5068
Epoch 12 Step 101 Train Loss: 0.5108
Epoch 12 Step 151 Train Loss: 0.5200
Epoch 12 Step 201 Train Loss: 0.4882
Epoch 12 Step 251 Train Loss: 0.5416
Epoch 12 Step 301 Train Loss: 0.5763
Epoch 12 Step 351 Train Loss: 0.5156
Epoch 12 Step 401 Train Loss: 0.5235
Epoch 12 Step 451 Train Loss: 0.4754
Epoch 12 Step 501 Train Loss: 0.4613
Epoch 12 Step 551 Train Loss: 0.5399
Epoch 12 Step 601 Train Loss: 0.5567
Epoch 12 Step 651 Train Loss: 0.4795
Epoch 12 Step 701 Train Loss: 0.5255
Epoch 12 Step 751 Train Loss: 0.4985
Epoch 12 Step 801 Train Loss: 0.5063
Epoch 12 Step 851 Train Loss: 0.5318
Epoch 12 Step 901 Train Loss: 0.5296
Epoch 12 Step 951 Train Loss: 0.5383
Epoch 12: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0592 Validation Top 20 DE MSE: 0.1478. 
Epoch 13 Step 1 Train Loss: 0.4978
Epoch 13 Step 51 Train Loss: 0.5157
Epoch 13 Step 101 Train Loss: 0.5284
Epoch 13 Step 151 Train Loss: 0.4727
Epoch 13 Step 201 Train Loss: 0.4911
Epoch 13 Step 251 Train Loss: 0.4796
Epoch 13 Step 301 Train Loss: 0.5235
Epoch 13 Step 351 Train Loss: 0.5103
Epoch 13 Step 401 Train Loss: 0.5169
Epoch 13 Step 451 Train Loss: 0.4772
Epoch 13 Step 501 Train Loss: 0.5033
Epoch 13 Step 551 Train Loss: 0.5303
Epoch 13 Step 601 Train Loss: 0.5358
Epoch 13 Step 651 Train Loss: 0.5198
Epoch 13 Step 701 Train Loss: 0.5106
Epoch 13 Step 751 Train Loss: 0.5288
Epoch 13 Step 801 Train Loss: 0.5539
Epoch 13 Step 851 Train Loss: 0.5165
Epoch 13 Step 901 Train Loss: 0.5224
Epoch 13 Step 951 Train Loss: 0.4943
Epoch 13: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0586 Validation Top 20 DE MSE: 0.1457. 
Epoch 14 Step 1 Train Loss: 0.5142
Epoch 14 Step 51 Train Loss: 0.5161
Epoch 14 Step 101 Train Loss: 0.4968
Epoch 14 Step 151 Train Loss: 0.4794
Epoch 14 Step 201 Train Loss: 0.4778
Epoch 14 Step 251 Train Loss: 0.4939
Epoch 14 Step 301 Train Loss: 0.5456
Epoch 14 Step 351 Train Loss: 0.5378
Epoch 14 Step 401 Train Loss: 0.5171
Epoch 14 Step 451 Train Loss: 0.5315
Epoch 14 Step 501 Train Loss: 0.4622
Epoch 14 Step 551 Train Loss: 0.4875
Epoch 14 Step 601 Train Loss: 0.5581
Epoch 14 Step 651 Train Loss: 0.4989
Epoch 14 Step 701 Train Loss: 0.5348
Epoch 14 Step 751 Train Loss: 0.4850
Epoch 14 Step 801 Train Loss: 0.4984
Epoch 14 Step 851 Train Loss: 0.4934
Epoch 14 Step 901 Train Loss: 0.5595
Epoch 14 Step 951 Train Loss: 0.5125
Epoch 14: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0594 Validation Top 20 DE MSE: 0.1484. 
Epoch 15 Step 1 Train Loss: 0.4876
Epoch 15 Step 51 Train Loss: 0.4996
Epoch 15 Step 101 Train Loss: 0.4865
Epoch 15 Step 151 Train Loss: 0.4708
Epoch 15 Step 201 Train Loss: 0.5149
Epoch 15 Step 251 Train Loss: 0.5022
Epoch 15 Step 301 Train Loss: 0.5125
Epoch 15 Step 351 Train Loss: 0.4698
Epoch 15 Step 401 Train Loss: 0.4947
Epoch 15 Step 451 Train Loss: 0.5182
Epoch 15 Step 501 Train Loss: 0.4820
Epoch 15 Step 551 Train Loss: 0.4717
Epoch 15 Step 601 Train Loss: 0.5055
Epoch 15 Step 651 Train Loss: 0.4908
Epoch 15 Step 701 Train Loss: 0.5029
Epoch 15 Step 751 Train Loss: 0.4728
Epoch 15 Step 801 Train Loss: 0.5189
Epoch 15 Step 851 Train Loss: 0.5270
Epoch 15 Step 901 Train Loss: 0.5400
Epoch 15 Step 951 Train Loss: 0.5017
Epoch 15: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0625 Validation Top 20 DE MSE: 0.1529. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1259
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.00462944
test_combo_seen0_pearson: 0.9892935301231013
test_combo_seen0_mse_de: 0.40298843
test_combo_seen0_pearson_de: 0.9329635011122701
test_combo_seen1_mse: 0.0034998965
test_combo_seen1_pearson: 0.9922371821157483
test_combo_seen1_mse_de: 0.12632085
test_combo_seen1_pearson_de: 0.8307599085258162
test_combo_seen2_mse: 0.0037311297
test_combo_seen2_pearson: 0.9917945562031018
test_combo_seen2_mse_de: 0.11056985
test_combo_seen2_pearson_de: 0.8791082788097041
test_unseen_single_mse: 0.0021853666
test_unseen_single_pearson: 0.9950746590905821
test_unseen_single_mse_de: 0.11794855
test_unseen_single_pearson_de: 0.9408808556867321
test_combo_seen0_pearson_delta: 0.6824783632035764
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.025
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.675
test_combo_seen0_mse_top20_de_non_dropout: 0.4211732
test_combo_seen1_pearson_delta: 0.6106962519777855
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.11794871794871799
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.9128205128205129
test_combo_seen1_mse_top20_de_non_dropout: 0.1575112
test_combo_seen2_pearson_delta: 0.6564890644115273
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.09130434782608696
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.9152173913043479
test_combo_seen2_mse_top20_de_non_dropout: 0.13065442
test_unseen_single_pearson_delta: 0.4456660143552475
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9240740740740739
test_unseen_single_mse_top20_de_non_dropout: 0.13099147
Done!
wandb: - 0.001 MB of 0.024 MB uploadedwandb: \ 0.001 MB of 0.028 MB uploadedwandb: | 0.013 MB of 0.028 MB uploadedwandb: / 0.013 MB of 0.028 MB uploadedwandb: - 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen0_mse â–
wandb:                                      test_combo_seen0_mse_de â–
wandb:                    test_combo_seen0_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen0_pearson â–
wandb:                                  test_combo_seen0_pearson_de â–
wandb:                               test_combo_seen0_pearson_delta â–
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–‡â–‡â–‚â–‚â–â–â–â–â–â–â–â–â–‚
wandb:                                             train_de_pearson â–â–„â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–†â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–†â–‡â–â–ƒâ–‚â–‚â–„â–„â–†â–„â–‚â–‡â–ˆâ–‡â–†â–†â–…â–†â–ƒâ–â–†â–†â–ˆâ–‡â–…â–‡â–…â–‚â–…â–„â–…â–ˆâ–„â–†â–†â–ƒâ–ƒâ–ƒâ–„â–…
wandb:                                                   val_de_mse â–ˆâ–„â–†â–†â–â–ƒâ–â–‚â–‚â–‚â–‚â–â–â–‚â–‚
wandb:                                               val_de_pearson â–â–…â–„â–„â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:                                                      val_mse â–ˆâ–ƒâ–…â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–†â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.025
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.675
wandb:                                         test_combo_seen0_mse 0.00463
wandb:                                      test_combo_seen0_mse_de 0.40299
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.42117
wandb:                                     test_combo_seen0_pearson 0.98929
wandb:                                  test_combo_seen0_pearson_de 0.93296
wandb:                               test_combo_seen0_pearson_delta 0.68248
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.11795
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.91282
wandb:                                         test_combo_seen1_mse 0.0035
wandb:                                      test_combo_seen1_mse_de 0.12632
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.15751
wandb:                                     test_combo_seen1_pearson 0.99224
wandb:                                  test_combo_seen1_pearson_de 0.83076
wandb:                               test_combo_seen1_pearson_delta 0.6107
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.0913
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.91522
wandb:                                         test_combo_seen2_mse 0.00373
wandb:                                      test_combo_seen2_mse_de 0.11057
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.13065
wandb:                                     test_combo_seen2_pearson 0.99179
wandb:                                  test_combo_seen2_pearson_de 0.87911
wandb:                               test_combo_seen2_pearson_delta 0.65649
wandb:                                                  test_de_mse 0.12594
wandb:                                              test_de_pearson 0.8779
wandb:               test_frac_opposite_direction_top20_non_dropout 0.14835
wandb:                          test_frac_sigma_below_1_non_dropout 0.91154
wandb:                                                     test_mse 0.00319
wandb:                                test_mse_top20_de_non_dropout 0.14865
wandb:                                                 test_pearson 0.9929
wandb:                                           test_pearson_delta 0.57488
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.92407
wandb:                                       test_unseen_single_mse 0.00219
wandb:                                    test_unseen_single_mse_de 0.11795
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.13099
wandb:                                   test_unseen_single_pearson 0.99507
wandb:                                test_unseen_single_pearson_de 0.94088
wandb:                             test_unseen_single_pearson_delta 0.44567
wandb:                                                 train_de_mse 0.06245
wandb:                                             train_de_pearson 0.87715
wandb:                                                    train_mse 0.00187
wandb:                                                train_pearson 0.996
wandb:                                                training_loss 0.48408
wandb:                                                   val_de_mse 0.15287
wandb:                                               val_de_pearson 0.82994
wandb:                                                      val_mse 0.003
wandb:                                                  val_pearson 0.99333
wandb: 
wandb: ðŸš€ View run NormanWeissman2019_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/6h6wuj2o
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_090034-6h6wuj2o/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/normanweissman2019/splits/normanweissman2019_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:6
combo_seen1:44
combo_seen2:21
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_100022-cuhtwlgi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NormanWeissman2019_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/cuhtwlgi
Start Training...
Epoch 1 Step 1 Train Loss: 1.0549
Epoch 1 Step 51 Train Loss: 0.5373
Epoch 1 Step 101 Train Loss: 0.5104
Epoch 1 Step 151 Train Loss: 0.5425
Epoch 1 Step 201 Train Loss: 0.5537
Epoch 1 Step 251 Train Loss: 0.5271
Epoch 1 Step 301 Train Loss: 0.5005
Epoch 1 Step 351 Train Loss: 0.6197
Epoch 1 Step 401 Train Loss: 0.5231
Epoch 1 Step 451 Train Loss: 0.5723
Epoch 1 Step 501 Train Loss: 0.5577
Epoch 1 Step 551 Train Loss: 0.5801
Epoch 1 Step 601 Train Loss: 0.5103
Epoch 1 Step 651 Train Loss: 0.5167
Epoch 1 Step 701 Train Loss: 0.5063
Epoch 1 Step 751 Train Loss: 0.5027
Epoch 1 Step 801 Train Loss: 0.5062
Epoch 1 Step 851 Train Loss: 0.5006
Epoch 1 Step 901 Train Loss: 0.5687
Epoch 1: Train Overall MSE: 0.0034 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.1159 Validation Top 20 DE MSE: 0.1759. 
Epoch 2 Step 1 Train Loss: 0.5503
Epoch 2 Step 51 Train Loss: 0.5188
Epoch 2 Step 101 Train Loss: 0.5006
Epoch 2 Step 151 Train Loss: 0.5155
Epoch 2 Step 201 Train Loss: 0.5531
Epoch 2 Step 251 Train Loss: 0.5107
Epoch 2 Step 301 Train Loss: 0.4895
Epoch 2 Step 351 Train Loss: 0.4460
Epoch 2 Step 401 Train Loss: 0.4708
Epoch 2 Step 451 Train Loss: 0.4650
Epoch 2 Step 501 Train Loss: 0.4924
Epoch 2 Step 551 Train Loss: 0.4652
Epoch 2 Step 601 Train Loss: 0.4559
Epoch 2 Step 651 Train Loss: 0.4864
Epoch 2 Step 701 Train Loss: 0.5546
Epoch 2 Step 751 Train Loss: 0.4744
Epoch 2 Step 801 Train Loss: 0.5578
Epoch 2 Step 851 Train Loss: 0.4846
Epoch 2 Step 901 Train Loss: 0.4819
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0935 Validation Top 20 DE MSE: 0.1673. 
Epoch 3 Step 1 Train Loss: 0.5023
Epoch 3 Step 51 Train Loss: 0.4791
Epoch 3 Step 101 Train Loss: 0.4867
Epoch 3 Step 151 Train Loss: 0.4986
Epoch 3 Step 201 Train Loss: 0.5284
Epoch 3 Step 251 Train Loss: 0.5103
Epoch 3 Step 301 Train Loss: 0.4806
Epoch 3 Step 351 Train Loss: 0.4765
Epoch 3 Step 401 Train Loss: 0.4945
Epoch 3 Step 451 Train Loss: 0.5116
Epoch 3 Step 501 Train Loss: 0.4957
Epoch 3 Step 551 Train Loss: 0.5317
Epoch 3 Step 601 Train Loss: 0.5563
Epoch 3 Step 651 Train Loss: 0.4984
Epoch 3 Step 701 Train Loss: 0.5096
Epoch 3 Step 751 Train Loss: 0.4989
Epoch 3 Step 801 Train Loss: 0.4975
Epoch 3 Step 851 Train Loss: 0.5522
Epoch 3 Step 901 Train Loss: 0.5028
Epoch 3: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0844 Validation Top 20 DE MSE: 0.1570. 
Epoch 4 Step 1 Train Loss: 0.5195
Epoch 4 Step 51 Train Loss: 0.5231
Epoch 4 Step 101 Train Loss: 0.4860
Epoch 4 Step 151 Train Loss: 0.5140
Epoch 4 Step 201 Train Loss: 0.5227
Epoch 4 Step 251 Train Loss: 0.5103
Epoch 4 Step 301 Train Loss: 0.5004
Epoch 4 Step 351 Train Loss: 0.4981
Epoch 4 Step 401 Train Loss: 0.5001
Epoch 4 Step 451 Train Loss: 0.5475
Epoch 4 Step 501 Train Loss: 0.5158
Epoch 4 Step 551 Train Loss: 0.5060
Epoch 4 Step 601 Train Loss: 0.4629
Epoch 4 Step 651 Train Loss: 0.4984
Epoch 4 Step 701 Train Loss: 0.5052
Epoch 4 Step 751 Train Loss: 0.4908
Epoch 4 Step 801 Train Loss: 0.5329
Epoch 4 Step 851 Train Loss: 0.4878
Epoch 4 Step 901 Train Loss: 0.5225
Epoch 4: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0744 Validation Top 20 DE MSE: 0.1477. 
Epoch 5 Step 1 Train Loss: 0.4954
Epoch 5 Step 51 Train Loss: 0.5065
Epoch 5 Step 101 Train Loss: 0.5217
Epoch 5 Step 151 Train Loss: 0.4803
Epoch 5 Step 201 Train Loss: 0.5371
Epoch 5 Step 251 Train Loss: 0.4766
Epoch 5 Step 301 Train Loss: 0.5215
Epoch 5 Step 351 Train Loss: 0.5235
Epoch 5 Step 401 Train Loss: 0.5127
Epoch 5 Step 451 Train Loss: 0.5062
Epoch 5 Step 501 Train Loss: 0.5132
Epoch 5 Step 551 Train Loss: 0.4897
Epoch 5 Step 601 Train Loss: 0.5389
Epoch 5 Step 651 Train Loss: 0.5555
Epoch 5 Step 701 Train Loss: 0.5010
Epoch 5 Step 751 Train Loss: 0.5054
Epoch 5 Step 801 Train Loss: 0.5154
Epoch 5 Step 851 Train Loss: 0.5388
Epoch 5 Step 901 Train Loss: 0.5190
Epoch 5: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0687 Validation Top 20 DE MSE: 0.1666. 
Epoch 6 Step 1 Train Loss: 0.4884
Epoch 6 Step 51 Train Loss: 0.5340
Epoch 6 Step 101 Train Loss: 0.4998
Epoch 6 Step 151 Train Loss: 0.5175
Epoch 6 Step 201 Train Loss: 0.5080
Epoch 6 Step 251 Train Loss: 0.5284
Epoch 6 Step 301 Train Loss: 0.5241
Epoch 6 Step 351 Train Loss: 0.5180
Epoch 6 Step 401 Train Loss: 0.4805
Epoch 6 Step 451 Train Loss: 0.4921
Epoch 6 Step 501 Train Loss: 0.5402
Epoch 6 Step 551 Train Loss: 0.5152
Epoch 6 Step 601 Train Loss: 0.5172
Epoch 6 Step 651 Train Loss: 0.5019
Epoch 6 Step 701 Train Loss: 0.5241
Epoch 6 Step 751 Train Loss: 0.5321
Epoch 6 Step 801 Train Loss: 0.4983
Epoch 6 Step 851 Train Loss: 0.5344
Epoch 6 Step 901 Train Loss: 0.5019
Epoch 6: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0645 Validation Top 20 DE MSE: 0.1639. 
Epoch 7 Step 1 Train Loss: 0.5794
Epoch 7 Step 51 Train Loss: 0.5012
Epoch 7 Step 101 Train Loss: 0.4894
Epoch 7 Step 151 Train Loss: 0.5068
Epoch 7 Step 201 Train Loss: 0.5182
Epoch 7 Step 251 Train Loss: 0.5491
Epoch 7 Step 301 Train Loss: 0.6279
Epoch 7 Step 351 Train Loss: 0.4827
Epoch 7 Step 401 Train Loss: 0.5311
Epoch 7 Step 451 Train Loss: 0.5340
Epoch 7 Step 501 Train Loss: 0.5013
Epoch 7 Step 551 Train Loss: 0.4461
Epoch 7 Step 601 Train Loss: 0.5189
Epoch 7 Step 651 Train Loss: 0.5112
Epoch 7 Step 701 Train Loss: 0.4896
Epoch 7 Step 751 Train Loss: 0.5177
Epoch 7 Step 801 Train Loss: 0.5333
Epoch 7 Step 851 Train Loss: 0.5181
Epoch 7 Step 901 Train Loss: 0.4760
Epoch 7: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0599 Validation Top 20 DE MSE: 0.1561. 
Epoch 8 Step 1 Train Loss: 0.5458
Epoch 8 Step 51 Train Loss: 0.5094
Epoch 8 Step 101 Train Loss: 0.5082
Epoch 8 Step 151 Train Loss: 0.4996
Epoch 8 Step 201 Train Loss: 0.5407
Epoch 8 Step 251 Train Loss: 0.5003
Epoch 8 Step 301 Train Loss: 0.5102
Epoch 8 Step 351 Train Loss: 0.5024
Epoch 8 Step 401 Train Loss: 0.5024
Epoch 8 Step 451 Train Loss: 0.5184
Epoch 8 Step 501 Train Loss: 0.4718
Epoch 8 Step 551 Train Loss: 0.5244
Epoch 8 Step 601 Train Loss: 0.4975
Epoch 8 Step 651 Train Loss: 0.5181
Epoch 8 Step 701 Train Loss: 0.5239
Epoch 8 Step 751 Train Loss: 0.5355
Epoch 8 Step 801 Train Loss: 0.4680
Epoch 8 Step 851 Train Loss: 0.5517
Epoch 8 Step 901 Train Loss: 0.4851
Epoch 8: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0593 Validation Top 20 DE MSE: 0.1489. 
Epoch 9 Step 1 Train Loss: 0.5062
Epoch 9 Step 51 Train Loss: 0.5044
Epoch 9 Step 101 Train Loss: 0.5061
Epoch 9 Step 151 Train Loss: 0.4858
Epoch 9 Step 201 Train Loss: 0.4787
Epoch 9 Step 251 Train Loss: 0.4837
Epoch 9 Step 301 Train Loss: 0.5412
Epoch 9 Step 351 Train Loss: 0.5174
Epoch 9 Step 401 Train Loss: 0.4992
Epoch 9 Step 451 Train Loss: 0.4983
Epoch 9 Step 501 Train Loss: 0.4896
Epoch 9 Step 551 Train Loss: 0.5002
Epoch 9 Step 601 Train Loss: 0.4607
Epoch 9 Step 651 Train Loss: 0.4912
Epoch 9 Step 701 Train Loss: 0.4848
Epoch 9 Step 751 Train Loss: 0.5666
Epoch 9 Step 801 Train Loss: 0.4797
Epoch 9 Step 851 Train Loss: 0.5133
Epoch 9 Step 901 Train Loss: 0.4777
Epoch 9: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0614 Validation Top 20 DE MSE: 0.1520. 
Epoch 10 Step 1 Train Loss: 0.5202
Epoch 10 Step 51 Train Loss: 0.5171
Epoch 10 Step 101 Train Loss: 0.4755
Epoch 10 Step 151 Train Loss: 0.5142
Epoch 10 Step 201 Train Loss: 0.5201
Epoch 10 Step 251 Train Loss: 0.5146
Epoch 10 Step 301 Train Loss: 0.5233
Epoch 10 Step 351 Train Loss: 0.5184
Epoch 10 Step 401 Train Loss: 0.5089
Epoch 10 Step 451 Train Loss: 0.4968
Epoch 10 Step 501 Train Loss: 0.4986
Epoch 10 Step 551 Train Loss: 0.4967
Epoch 10 Step 601 Train Loss: 0.5151
Epoch 10 Step 651 Train Loss: 0.4855
Epoch 10 Step 701 Train Loss: 0.5092
Epoch 10 Step 751 Train Loss: 0.5467
Epoch 10 Step 801 Train Loss: 0.5196
Epoch 10 Step 851 Train Loss: 0.5391
Epoch 10 Step 901 Train Loss: 0.5097
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0606 Validation Top 20 DE MSE: 0.1531. 
Epoch 11 Step 1 Train Loss: 0.5323
Epoch 11 Step 51 Train Loss: 0.5309
Epoch 11 Step 101 Train Loss: 0.5794
Epoch 11 Step 151 Train Loss: 0.4930
Epoch 11 Step 201 Train Loss: 0.5377
Epoch 11 Step 251 Train Loss: 0.5052
Epoch 11 Step 301 Train Loss: 0.4996
Epoch 11 Step 351 Train Loss: 0.5026
Epoch 11 Step 401 Train Loss: 0.4943
Epoch 11 Step 451 Train Loss: 0.4967
Epoch 11 Step 501 Train Loss: 0.5117
Epoch 11 Step 551 Train Loss: 0.5025
Epoch 11 Step 601 Train Loss: 0.4990
Epoch 11 Step 651 Train Loss: 0.6361
Epoch 11 Step 701 Train Loss: 0.5291
Epoch 11 Step 751 Train Loss: 0.5466
Epoch 11 Step 801 Train Loss: 0.5423
Epoch 11 Step 851 Train Loss: 0.5077
Epoch 11 Step 901 Train Loss: 0.5198
Epoch 11: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0588 Validation Top 20 DE MSE: 0.1543. 
Epoch 12 Step 1 Train Loss: 0.5058
Epoch 12 Step 51 Train Loss: 0.5303
Epoch 12 Step 101 Train Loss: 0.5095
Epoch 12 Step 151 Train Loss: 0.5021
Epoch 12 Step 201 Train Loss: 0.5425
Epoch 12 Step 251 Train Loss: 0.5372
Epoch 12 Step 301 Train Loss: 0.5383
Epoch 12 Step 351 Train Loss: 0.4857
Epoch 12 Step 401 Train Loss: 0.5012
Epoch 12 Step 451 Train Loss: 0.5232
Epoch 12 Step 501 Train Loss: 0.5851
Epoch 12 Step 551 Train Loss: 0.5571
Epoch 12 Step 601 Train Loss: 0.4690
Epoch 12 Step 651 Train Loss: 0.5047
Epoch 12 Step 701 Train Loss: 0.5326
Epoch 12 Step 751 Train Loss: 0.5160
Epoch 12 Step 801 Train Loss: 0.5591
Epoch 12 Step 851 Train Loss: 0.5097
Epoch 12 Step 901 Train Loss: 0.5358
Epoch 12: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0580 Validation Top 20 DE MSE: 0.1528. 
Epoch 13 Step 1 Train Loss: 0.5112
Epoch 13 Step 51 Train Loss: 0.5295
Epoch 13 Step 101 Train Loss: 0.5111
Epoch 13 Step 151 Train Loss: 0.5373
Epoch 13 Step 201 Train Loss: 0.4849
Epoch 13 Step 251 Train Loss: 0.5207
Epoch 13 Step 301 Train Loss: 0.5663
Epoch 13 Step 351 Train Loss: 0.5271
Epoch 13 Step 401 Train Loss: 0.5033
Epoch 13 Step 451 Train Loss: 0.5008
Epoch 13 Step 501 Train Loss: 0.5243
Epoch 13 Step 551 Train Loss: 0.5050
Epoch 13 Step 601 Train Loss: 0.5250
Epoch 13 Step 651 Train Loss: 0.4877
Epoch 13 Step 701 Train Loss: 0.5399
Epoch 13 Step 751 Train Loss: 0.5459
Epoch 13 Step 801 Train Loss: 0.4978
Epoch 13 Step 851 Train Loss: 0.5207
Epoch 13 Step 901 Train Loss: 0.5295
Epoch 13: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0601 Validation Top 20 DE MSE: 0.1540. 
Epoch 14 Step 1 Train Loss: 0.4931
Epoch 14 Step 51 Train Loss: 0.6081
Epoch 14 Step 101 Train Loss: 0.4881
Epoch 14 Step 151 Train Loss: 0.5244
Epoch 14 Step 201 Train Loss: 0.5148
Epoch 14 Step 251 Train Loss: 0.4843
Epoch 14 Step 301 Train Loss: 0.5357
Epoch 14 Step 351 Train Loss: 0.5244
Epoch 14 Step 401 Train Loss: 0.5088
Epoch 14 Step 451 Train Loss: 0.5300
Epoch 14 Step 501 Train Loss: 0.4873
Epoch 14 Step 551 Train Loss: 0.5406
Epoch 14 Step 601 Train Loss: 0.4798
Epoch 14 Step 651 Train Loss: 0.4954
Epoch 14 Step 701 Train Loss: 0.5161
Epoch 14 Step 751 Train Loss: 0.5206
Epoch 14 Step 801 Train Loss: 0.4817
Epoch 14 Step 851 Train Loss: 0.5265
Epoch 14 Step 901 Train Loss: 0.5269
Epoch 14: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0600 Validation Top 20 DE MSE: 0.1525. 
Epoch 15 Step 1 Train Loss: 0.5252
Epoch 15 Step 51 Train Loss: 0.4870
Epoch 15 Step 101 Train Loss: 0.5098
Epoch 15 Step 151 Train Loss: 0.4953
Epoch 15 Step 201 Train Loss: 0.5267
Epoch 15 Step 251 Train Loss: 0.5356
Epoch 15 Step 301 Train Loss: 0.4829
Epoch 15 Step 351 Train Loss: 0.5554
Epoch 15 Step 401 Train Loss: 0.5315
Epoch 15 Step 451 Train Loss: 0.4759
Epoch 15 Step 501 Train Loss: 0.4966
Epoch 15 Step 551 Train Loss: 0.5184
Epoch 15 Step 601 Train Loss: 0.5044
Epoch 15 Step 651 Train Loss: 0.5211
Epoch 15 Step 701 Train Loss: 0.4947
Epoch 15 Step 751 Train Loss: 0.4655
Epoch 15 Step 801 Train Loss: 0.5045
Epoch 15 Step 851 Train Loss: 0.4888
Epoch 15 Step 901 Train Loss: 0.4897
Epoch 15: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0620 Validation Top 20 DE MSE: 0.1584. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1587
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0041983197
test_combo_seen0_pearson: 0.9904691751214335
test_combo_seen0_mse_de: 0.24032535
test_combo_seen0_pearson_de: 0.7897796607856488
test_combo_seen1_mse: 0.0034096085
test_combo_seen1_pearson: 0.9923445860394279
test_combo_seen1_mse_de: 0.16412002
test_combo_seen1_pearson_de: 0.8445537784565492
test_combo_seen2_mse: 0.0036904875
test_combo_seen2_pearson: 0.9917459527075514
test_combo_seen2_mse_de: 0.10837364
test_combo_seen2_pearson_de: 0.784845774196627
test_unseen_single_mse: 0.002208838
test_unseen_single_pearson: 0.994959578695566
test_unseen_single_mse_de: 0.17084023
test_unseen_single_pearson_de: 0.9536935543204488
test_combo_seen0_pearson_delta: 0.4508107954604601
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.20833333333333334
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8416666666666668
test_combo_seen0_mse_top20_de_non_dropout: 0.2859618
test_combo_seen1_pearson_delta: 0.5587048629524181
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.11022727272727272
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.905681818181818
test_combo_seen1_mse_top20_de_non_dropout: 0.19070023
test_combo_seen2_pearson_delta: 0.6355357817046343
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.09523809523809526
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.911904761904762
test_combo_seen2_mse_top20_de_non_dropout: 0.13950387
test_unseen_single_pearson_delta: 0.3886410605466138
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.24999999999999997
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9037037037037037
test_unseen_single_mse_top20_de_non_dropout: 0.18435489
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.023 MB of 0.027 MB uploadedwandb: / 0.027 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen0_mse â–
wandb:                                      test_combo_seen0_mse_de â–
wandb:                    test_combo_seen0_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen0_pearson â–
wandb:                                  test_combo_seen0_pearson_de â–
wandb:                               test_combo_seen0_pearson_delta â–
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–„â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                                                train_pearson â–â–…â–†â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–„â–„â–â–ƒâ–â–„â–ƒâ–…â–ƒâ–ƒâ–†â–„â–„â–„â–„â–„â–ƒâ–„â–‚â–‡â–„â–„â–ƒâ–…â–ƒâ–‚â–‚â–„â–„â–‡â–…â–ƒâ–…â–…â–ƒâ–„â–†â–ƒâ–†
wandb:                                                   val_de_mse â–ˆâ–†â–ƒâ–â–†â–…â–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–„
wandb:                                               val_de_pearson â–‚â–â–…â–ˆâ–‚â–ƒâ–†â–ˆâ–‡â–†â–†â–†â–†â–‡â–„
wandb:                                                      val_mse â–ˆâ–…â–â–â–ƒâ–„â–…â–„â–„â–„â–…â–…â–„â–„â–„
wandb:                                                  val_pearson â–â–„â–ˆâ–ˆâ–†â–†â–…â–…â–†â–†â–…â–…â–†â–†â–†
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.20833
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.84167
wandb:                                         test_combo_seen0_mse 0.0042
wandb:                                      test_combo_seen0_mse_de 0.24033
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.28596
wandb:                                     test_combo_seen0_pearson 0.99047
wandb:                                  test_combo_seen0_pearson_de 0.78978
wandb:                               test_combo_seen0_pearson_delta 0.45081
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.11023
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.90568
wandb:                                         test_combo_seen1_mse 0.00341
wandb:                                      test_combo_seen1_mse_de 0.16412
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.1907
wandb:                                     test_combo_seen1_pearson 0.99234
wandb:                                  test_combo_seen1_pearson_de 0.84455
wandb:                               test_combo_seen1_pearson_delta 0.5587
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.09524
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.9119
wandb:                                         test_combo_seen2_mse 0.00369
wandb:                                      test_combo_seen2_mse_de 0.10837
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.1395
wandb:                                     test_combo_seen2_pearson 0.99175
wandb:                                  test_combo_seen2_pearson_de 0.78485
wandb:                               test_combo_seen2_pearson_delta 0.63554
wandb:                                                  test_de_mse 0.15869
wandb:                                              test_de_pearson 0.85847
wandb:               test_frac_opposite_direction_top20_non_dropout 0.15153
wandb:                          test_frac_sigma_below_1_non_dropout 0.90255
wandb:                                                     test_mse 0.00319
wandb:                                test_mse_top20_de_non_dropout 0.18381
wandb:                                                 test_pearson 0.99282
wandb:                                           test_pearson_delta 0.52171
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9037
wandb:                                       test_unseen_single_mse 0.00221
wandb:                                    test_unseen_single_mse_de 0.17084
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.18435
wandb:                                   test_unseen_single_pearson 0.99496
wandb:                                test_unseen_single_pearson_de 0.95369
wandb:                             test_unseen_single_pearson_delta 0.38864
wandb:                                                 train_de_mse 0.06195
wandb:                                             train_de_pearson 0.90079
wandb:                                                    train_mse 0.00205
wandb:                                                train_pearson 0.99564
wandb:                                                training_loss 0.51234
wandb:                                                   val_de_mse 0.15836
wandb:                                               val_de_pearson 0.7676
wandb:                                                      val_mse 0.00324
wandb:                                                  val_pearson 0.99269
wandb: 
wandb: ðŸš€ View run NormanWeissman2019_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/cuhtwlgi
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_100022-cuhtwlgi/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/normanweissman2019/splits/normanweissman2019_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:10
combo_seen1:58
combo_seen2:16
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_105818-27ctmduo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NormanWeissman2019_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/27ctmduo
Start Training...
Epoch 1 Step 1 Train Loss: 1.3530
Epoch 1 Step 51 Train Loss: 0.5206
Epoch 1 Step 101 Train Loss: 0.5527
Epoch 1 Step 151 Train Loss: 0.4733
Epoch 1 Step 201 Train Loss: 0.5424
Epoch 1 Step 251 Train Loss: 0.5022
Epoch 1 Step 301 Train Loss: 0.4860
Epoch 1 Step 351 Train Loss: 0.4960
Epoch 1 Step 401 Train Loss: 0.4967
Epoch 1 Step 451 Train Loss: 0.4854
Epoch 1 Step 501 Train Loss: 0.5197
Epoch 1 Step 551 Train Loss: 0.4938
Epoch 1 Step 601 Train Loss: 0.4272
Epoch 1 Step 651 Train Loss: 0.4993
Epoch 1 Step 701 Train Loss: 0.4908
Epoch 1 Step 751 Train Loss: 0.4711
Epoch 1 Step 801 Train Loss: 0.4799
Epoch 1 Step 851 Train Loss: 0.5247
Epoch 1 Step 901 Train Loss: 0.4783
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.1120 Validation Top 20 DE MSE: 0.2036. 
Epoch 2 Step 1 Train Loss: 0.5305
Epoch 2 Step 51 Train Loss: 0.4620
Epoch 2 Step 101 Train Loss: 0.4918
Epoch 2 Step 151 Train Loss: 0.5382
Epoch 2 Step 201 Train Loss: 0.4699
Epoch 2 Step 251 Train Loss: 0.4308
Epoch 2 Step 301 Train Loss: 0.4790
Epoch 2 Step 351 Train Loss: 0.5250
Epoch 2 Step 401 Train Loss: 0.4927
Epoch 2 Step 451 Train Loss: 0.4969
Epoch 2 Step 501 Train Loss: 0.5391
Epoch 2 Step 551 Train Loss: 0.4880
Epoch 2 Step 601 Train Loss: 0.4906
Epoch 2 Step 651 Train Loss: 0.4918
Epoch 2 Step 701 Train Loss: 0.5141
Epoch 2 Step 751 Train Loss: 0.4619
Epoch 2 Step 801 Train Loss: 0.5036
Epoch 2 Step 851 Train Loss: 0.4804
Epoch 2 Step 901 Train Loss: 0.5049
Epoch 2: Train Overall MSE: 0.0035 Validation Overall MSE: 0.0046. 
Train Top 20 DE MSE: 0.0750 Validation Top 20 DE MSE: 0.1127. 
Epoch 3 Step 1 Train Loss: 0.4574
Epoch 3 Step 51 Train Loss: 0.4873
Epoch 3 Step 101 Train Loss: 0.4738
Epoch 3 Step 151 Train Loss: 0.5048
Epoch 3 Step 201 Train Loss: 0.5318
Epoch 3 Step 251 Train Loss: 0.5173
Epoch 3 Step 301 Train Loss: 0.5138
Epoch 3 Step 351 Train Loss: 0.5100
Epoch 3 Step 401 Train Loss: 0.5040
Epoch 3 Step 451 Train Loss: 0.4773
Epoch 3 Step 501 Train Loss: 0.4599
Epoch 3 Step 551 Train Loss: 0.4931
Epoch 3 Step 601 Train Loss: 0.4985
Epoch 3 Step 651 Train Loss: 0.5377
Epoch 3 Step 701 Train Loss: 0.4901
Epoch 3 Step 751 Train Loss: 0.4844
Epoch 3 Step 801 Train Loss: 0.5156
Epoch 3 Step 851 Train Loss: 0.4910
Epoch 3 Step 901 Train Loss: 0.4548
Epoch 3: Train Overall MSE: 0.0029 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0717 Validation Top 20 DE MSE: 0.1111. 
Epoch 4 Step 1 Train Loss: 0.4729
Epoch 4 Step 51 Train Loss: 0.4818
Epoch 4 Step 101 Train Loss: 0.5032
Epoch 4 Step 151 Train Loss: 0.5165
Epoch 4 Step 201 Train Loss: 0.4940
Epoch 4 Step 251 Train Loss: 0.4859
Epoch 4 Step 301 Train Loss: 0.4672
Epoch 4 Step 351 Train Loss: 0.5098
Epoch 4 Step 401 Train Loss: 0.5110
Epoch 4 Step 451 Train Loss: 0.4986
Epoch 4 Step 501 Train Loss: 0.5341
Epoch 4 Step 551 Train Loss: 0.5071
Epoch 4 Step 601 Train Loss: 0.4943
Epoch 4 Step 651 Train Loss: 0.4909
Epoch 4 Step 701 Train Loss: 0.4516
Epoch 4 Step 751 Train Loss: 0.4678
Epoch 4 Step 801 Train Loss: 0.4777
Epoch 4 Step 851 Train Loss: 0.5142
Epoch 4 Step 901 Train Loss: 0.4829
Epoch 4: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0600 Validation Top 20 DE MSE: 0.1535. 
Epoch 5 Step 1 Train Loss: 0.4655
Epoch 5 Step 51 Train Loss: 0.4871
Epoch 5 Step 101 Train Loss: 0.5080
Epoch 5 Step 151 Train Loss: 0.5111
Epoch 5 Step 201 Train Loss: 0.5473
Epoch 5 Step 251 Train Loss: 0.4837
Epoch 5 Step 301 Train Loss: 0.4956
Epoch 5 Step 351 Train Loss: 0.4556
Epoch 5 Step 401 Train Loss: 0.5354
Epoch 5 Step 451 Train Loss: 0.5197
Epoch 5 Step 501 Train Loss: 0.4785
Epoch 5 Step 551 Train Loss: 0.5787
Epoch 5 Step 601 Train Loss: 0.4911
Epoch 5 Step 651 Train Loss: 0.4981
Epoch 5 Step 701 Train Loss: 0.4635
Epoch 5 Step 751 Train Loss: 0.5000
Epoch 5 Step 801 Train Loss: 0.4960
Epoch 5 Step 851 Train Loss: 0.5091
Epoch 5 Step 901 Train Loss: 0.4820
Epoch 5: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0638 Validation Top 20 DE MSE: 0.1497. 
Epoch 6 Step 1 Train Loss: 0.4875
Epoch 6 Step 51 Train Loss: 0.5435
Epoch 6 Step 101 Train Loss: 0.5127
Epoch 6 Step 151 Train Loss: 0.5044
Epoch 6 Step 201 Train Loss: 0.5165
Epoch 6 Step 251 Train Loss: 0.5318
Epoch 6 Step 301 Train Loss: 0.5218
Epoch 6 Step 351 Train Loss: 0.4878
Epoch 6 Step 401 Train Loss: 0.4933
Epoch 6 Step 451 Train Loss: 0.4674
Epoch 6 Step 501 Train Loss: 0.4768
Epoch 6 Step 551 Train Loss: 0.5165
Epoch 6 Step 601 Train Loss: 0.4920
Epoch 6 Step 651 Train Loss: 0.5240
Epoch 6 Step 701 Train Loss: 0.5033
Epoch 6 Step 751 Train Loss: 0.4926
Epoch 6 Step 801 Train Loss: 0.4940
Epoch 6 Step 851 Train Loss: 0.5345
Epoch 6 Step 901 Train Loss: 0.5493
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0580 Validation Top 20 DE MSE: 0.1417. 
Epoch 7 Step 1 Train Loss: 0.4641
Epoch 7 Step 51 Train Loss: 0.5467
Epoch 7 Step 101 Train Loss: 0.5019
Epoch 7 Step 151 Train Loss: 0.5145
Epoch 7 Step 201 Train Loss: 0.5031
Epoch 7 Step 251 Train Loss: 0.5067
Epoch 7 Step 301 Train Loss: 0.4963
Epoch 7 Step 351 Train Loss: 0.5032
Epoch 7 Step 401 Train Loss: 0.4920
Epoch 7 Step 451 Train Loss: 0.5388
Epoch 7 Step 501 Train Loss: 0.5014
Epoch 7 Step 551 Train Loss: 0.5054
Epoch 7 Step 601 Train Loss: 0.4919
Epoch 7 Step 651 Train Loss: 0.4727
Epoch 7 Step 701 Train Loss: 0.5108
Epoch 7 Step 751 Train Loss: 0.5290
Epoch 7 Step 801 Train Loss: 0.4974
Epoch 7 Step 851 Train Loss: 0.4871
Epoch 7 Step 901 Train Loss: 0.5148
Epoch 7: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0560 Validation Top 20 DE MSE: 0.1313. 
Epoch 8 Step 1 Train Loss: 0.4970
Epoch 8 Step 51 Train Loss: 0.5263
Epoch 8 Step 101 Train Loss: 0.4994
Epoch 8 Step 151 Train Loss: 0.4793
Epoch 8 Step 201 Train Loss: 0.4711
Epoch 8 Step 251 Train Loss: 0.5508
Epoch 8 Step 301 Train Loss: 0.5453
Epoch 8 Step 351 Train Loss: 0.5065
Epoch 8 Step 401 Train Loss: 0.4719
Epoch 8 Step 451 Train Loss: 0.5326
Epoch 8 Step 501 Train Loss: 0.4915
Epoch 8 Step 551 Train Loss: 0.5548
Epoch 8 Step 601 Train Loss: 0.5065
Epoch 8 Step 651 Train Loss: 0.4936
Epoch 8 Step 701 Train Loss: 0.5322
Epoch 8 Step 751 Train Loss: 0.4944
Epoch 8 Step 801 Train Loss: 0.5052
Epoch 8 Step 851 Train Loss: 0.5006
Epoch 8 Step 901 Train Loss: 0.5113
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0554 Validation Top 20 DE MSE: 0.1402. 
Epoch 9 Step 1 Train Loss: 0.5215
Epoch 9 Step 51 Train Loss: 0.5226
Epoch 9 Step 101 Train Loss: 0.5045
Epoch 9 Step 151 Train Loss: 0.5280
Epoch 9 Step 201 Train Loss: 0.5716
Epoch 9 Step 251 Train Loss: 0.5201
Epoch 9 Step 301 Train Loss: 0.4982
Epoch 9 Step 351 Train Loss: 0.5093
Epoch 9 Step 401 Train Loss: 0.4978
Epoch 9 Step 451 Train Loss: 0.5092
Epoch 9 Step 501 Train Loss: 0.4598
Epoch 9 Step 551 Train Loss: 0.5002
Epoch 9 Step 601 Train Loss: 0.5011
Epoch 9 Step 651 Train Loss: 0.4924
Epoch 9 Step 701 Train Loss: 0.5792
Epoch 9 Step 751 Train Loss: 0.5565
Epoch 9 Step 801 Train Loss: 0.5071
Epoch 9 Step 851 Train Loss: 0.5277
Epoch 9 Step 901 Train Loss: 0.4830
Epoch 9: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0534 Validation Top 20 DE MSE: 0.1343. 
Epoch 10 Step 1 Train Loss: 0.5319
Epoch 10 Step 51 Train Loss: 0.4737
Epoch 10 Step 101 Train Loss: 0.5092
Epoch 10 Step 151 Train Loss: 0.5037
Epoch 10 Step 201 Train Loss: 0.4853
Epoch 10 Step 251 Train Loss: 0.5459
Epoch 10 Step 301 Train Loss: 0.5380
Epoch 10 Step 351 Train Loss: 0.5188
Epoch 10 Step 401 Train Loss: 0.5082
Epoch 10 Step 451 Train Loss: 0.4930
Epoch 10 Step 501 Train Loss: 0.4724
Epoch 10 Step 551 Train Loss: 0.4897
Epoch 10 Step 601 Train Loss: 0.4695
Epoch 10 Step 651 Train Loss: 0.4845
Epoch 10 Step 701 Train Loss: 0.4886
Epoch 10 Step 751 Train Loss: 0.4977
Epoch 10 Step 801 Train Loss: 0.5132
Epoch 10 Step 851 Train Loss: 0.4872
Epoch 10 Step 901 Train Loss: 0.5050
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0569 Validation Top 20 DE MSE: 0.1373. 
Epoch 11 Step 1 Train Loss: 0.5433
Epoch 11 Step 51 Train Loss: 0.4921
Epoch 11 Step 101 Train Loss: 0.5530
Epoch 11 Step 151 Train Loss: 0.4928
Epoch 11 Step 201 Train Loss: 0.5260
Epoch 11 Step 251 Train Loss: 0.5054
Epoch 11 Step 301 Train Loss: 0.5004
Epoch 11 Step 351 Train Loss: 0.5062
Epoch 11 Step 401 Train Loss: 0.4914
Epoch 11 Step 451 Train Loss: 0.4789
Epoch 11 Step 501 Train Loss: 0.4679
Epoch 11 Step 551 Train Loss: 0.4869
Epoch 11 Step 601 Train Loss: 0.5072
Epoch 11 Step 651 Train Loss: 0.4831
Epoch 11 Step 701 Train Loss: 0.4957
Epoch 11 Step 751 Train Loss: 0.5027
Epoch 11 Step 801 Train Loss: 0.5008
Epoch 11 Step 851 Train Loss: 0.5163
Epoch 11 Step 901 Train Loss: 0.5212
Epoch 11: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0556 Validation Top 20 DE MSE: 0.1379. 
Epoch 12 Step 1 Train Loss: 0.4884
Epoch 12 Step 51 Train Loss: 0.4729
Epoch 12 Step 101 Train Loss: 0.5192
Epoch 12 Step 151 Train Loss: 0.4902
Epoch 12 Step 201 Train Loss: 0.4917
Epoch 12 Step 251 Train Loss: 0.4668
Epoch 12 Step 301 Train Loss: 0.4851
Epoch 12 Step 351 Train Loss: 0.5116
Epoch 12 Step 401 Train Loss: 0.5070
Epoch 12 Step 451 Train Loss: 0.5458
Epoch 12 Step 501 Train Loss: 0.5036
Epoch 12 Step 551 Train Loss: 0.5357
Epoch 12 Step 601 Train Loss: 0.5034
Epoch 12 Step 651 Train Loss: 0.5504
Epoch 12 Step 701 Train Loss: 0.5097
Epoch 12 Step 751 Train Loss: 0.5166
Epoch 12 Step 801 Train Loss: 0.4981
Epoch 12 Step 851 Train Loss: 0.5342
Epoch 12 Step 901 Train Loss: 0.4922
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0537 Validation Top 20 DE MSE: 0.1357. 
Epoch 13 Step 1 Train Loss: 0.4915
Epoch 13 Step 51 Train Loss: 0.4878
Epoch 13 Step 101 Train Loss: 0.5150
Epoch 13 Step 151 Train Loss: 0.5163
Epoch 13 Step 201 Train Loss: 0.4986
Epoch 13 Step 251 Train Loss: 0.4996
Epoch 13 Step 301 Train Loss: 0.4531
Epoch 13 Step 351 Train Loss: 0.4965
Epoch 13 Step 401 Train Loss: 0.5266
Epoch 13 Step 451 Train Loss: 0.4926
Epoch 13 Step 501 Train Loss: 0.5351
Epoch 13 Step 551 Train Loss: 0.4983
Epoch 13 Step 601 Train Loss: 0.4949
Epoch 13 Step 651 Train Loss: 0.5202
Epoch 13 Step 701 Train Loss: 0.5059
Epoch 13 Step 751 Train Loss: 0.4551
Epoch 13 Step 801 Train Loss: 0.4950
Epoch 13 Step 851 Train Loss: 0.5150
Epoch 13 Step 901 Train Loss: 0.5687
Epoch 13: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0529 Validation Top 20 DE MSE: 0.1291. 
Epoch 14 Step 1 Train Loss: 0.5028
Epoch 14 Step 51 Train Loss: 0.4921
Epoch 14 Step 101 Train Loss: 0.5233
Epoch 14 Step 151 Train Loss: 0.4766
Epoch 14 Step 201 Train Loss: 0.4694
Epoch 14 Step 251 Train Loss: 0.5200
Epoch 14 Step 301 Train Loss: 0.5380
Epoch 14 Step 351 Train Loss: 0.4819
Epoch 14 Step 401 Train Loss: 0.4919
Epoch 14 Step 451 Train Loss: 0.5061
Epoch 14 Step 501 Train Loss: 0.5050
Epoch 14 Step 551 Train Loss: 0.5194
Epoch 14 Step 601 Train Loss: 0.5150
Epoch 14 Step 651 Train Loss: 0.4937
Epoch 14 Step 701 Train Loss: 0.4805
Epoch 14 Step 751 Train Loss: 0.5008
Epoch 14 Step 801 Train Loss: 0.4846
Epoch 14 Step 851 Train Loss: 0.4815
Epoch 14 Step 901 Train Loss: 0.5013
Epoch 14: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0556 Validation Top 20 DE MSE: 0.1343. 
Epoch 15 Step 1 Train Loss: 0.5360
Epoch 15 Step 51 Train Loss: 0.5277
Epoch 15 Step 101 Train Loss: 0.4864
Epoch 15 Step 151 Train Loss: 0.4705
Epoch 15 Step 201 Train Loss: 0.5110
Epoch 15 Step 251 Train Loss: 0.4955
Epoch 15 Step 301 Train Loss: 0.4906
Epoch 15 Step 351 Train Loss: 0.5532
Epoch 15 Step 401 Train Loss: 0.4888
Epoch 15 Step 451 Train Loss: 0.5041
Epoch 15 Step 501 Train Loss: 0.4826
Epoch 15 Step 551 Train Loss: 0.4695
Epoch 15 Step 601 Train Loss: 0.5438
Epoch 15 Step 651 Train Loss: 0.4661
Epoch 15 Step 701 Train Loss: 0.5248
Epoch 15 Step 751 Train Loss: 0.4949
Epoch 15 Step 801 Train Loss: 0.4827
Epoch 15 Step 851 Train Loss: 0.5187
Epoch 15 Step 901 Train Loss: 0.4945
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0551 Validation Top 20 DE MSE: 0.1330. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1411
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0065666414
test_combo_seen0_pearson: 0.9853470044985253
test_combo_seen0_mse_de: 0.18185215
test_combo_seen0_pearson_de: 0.8258912517054599
test_combo_seen1_mse: 0.0057818685
test_combo_seen1_pearson: 0.9872103793109659
test_combo_seen1_mse_de: 0.12205839
test_combo_seen1_pearson_de: 0.7445349075276015
test_combo_seen2_mse: 0.0060302154
test_combo_seen2_pearson: 0.9864477628736095
test_combo_seen2_mse_de: 0.17203295
test_combo_seen2_pearson_de: 0.8871471909785459
test_unseen_single_mse: 0.0028705385
test_unseen_single_pearson: 0.9935992135921943
test_unseen_single_mse_de: 0.1485101
test_unseen_single_pearson_de: 0.9460732580520925
test_combo_seen0_pearson_delta: 0.47101054561747846
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.17500000000000002
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8400000000000001
test_combo_seen0_mse_top20_de_non_dropout: 0.22375579
test_combo_seen1_pearson_delta: 0.537652316736208
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.17068965517241377
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.864655172413793
test_combo_seen1_mse_top20_de_non_dropout: 0.18230453
test_combo_seen2_pearson_delta: 0.5744090947035503
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.05625
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.865625
test_combo_seen2_mse_top20_de_non_dropout: 0.19247821
test_unseen_single_pearson_delta: 0.45480719709789513
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.1962962962962963
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9074074074074073
test_unseen_single_mse_top20_de_non_dropout: 0.15862411
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.027 MB uploadedwandb: | 0.004 MB of 0.027 MB uploadedwandb: / 0.027 MB of 0.027 MB uploadedwandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen0_mse â–
wandb:                                      test_combo_seen0_mse_de â–
wandb:                    test_combo_seen0_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen0_pearson â–
wandb:                                  test_combo_seen0_pearson_de â–
wandb:                               test_combo_seen0_pearson_delta â–
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–‚â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–‡â–ˆâ–†â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–â–ƒâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–…â–†â–‚â–„â–ˆâ–‚â–„â–…â–â–…â–ƒâ–„â–ƒâ–†â–‡â–…â–‚â–„â–ƒâ–†â–„â–…â–ƒâ–…â–ˆâ–‚â–„â–ƒâ–†â–„â–…â–‡â–„â–…â–„â–â–†â–‡â–…â–…
wandb:                                                   val_de_mse â–ˆâ–â–â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:                                               val_de_pearson â–â–ˆâ–ˆâ–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ
wandb:                                                      val_mse â–†â–ˆâ–†â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–‚â–â–ƒâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.175
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.84
wandb:                                         test_combo_seen0_mse 0.00657
wandb:                                      test_combo_seen0_mse_de 0.18185
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.22376
wandb:                                     test_combo_seen0_pearson 0.98535
wandb:                                  test_combo_seen0_pearson_de 0.82589
wandb:                               test_combo_seen0_pearson_delta 0.47101
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.17069
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.86466
wandb:                                         test_combo_seen1_mse 0.00578
wandb:                                      test_combo_seen1_mse_de 0.12206
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.1823
wandb:                                     test_combo_seen1_pearson 0.98721
wandb:                                  test_combo_seen1_pearson_de 0.74453
wandb:                               test_combo_seen1_pearson_delta 0.53765
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.05625
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.86562
wandb:                                         test_combo_seen2_mse 0.00603
wandb:                                      test_combo_seen2_mse_de 0.17203
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.19248
wandb:                                     test_combo_seen2_pearson 0.98645
wandb:                                  test_combo_seen2_pearson_de 0.88715
wandb:                               test_combo_seen2_pearson_delta 0.57441
wandb:                                                  test_de_mse 0.14108
wandb:                                              test_de_pearson 0.82144
wandb:               test_frac_opposite_direction_top20_non_dropout 0.16081
wandb:                          test_frac_sigma_below_1_non_dropout 0.87297
wandb:                                                     test_mse 0.00518
wandb:                                test_mse_top20_de_non_dropout 0.18175
wandb:                                                 test_pearson 0.98849
wandb:                                           test_pearson_delta 0.5168
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.1963
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.90741
wandb:                                       test_unseen_single_mse 0.00287
wandb:                                    test_unseen_single_mse_de 0.14851
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15862
wandb:                                   test_unseen_single_pearson 0.9936
wandb:                                test_unseen_single_pearson_de 0.94607
wandb:                             test_unseen_single_pearson_delta 0.45481
wandb:                                                 train_de_mse 0.05512
wandb:                                             train_de_pearson 0.91468
wandb:                                                    train_mse 0.00171
wandb:                                                train_pearson 0.99637
wandb:                                                training_loss 0.50004
wandb:                                                   val_de_mse 0.13304
wandb:                                               val_de_pearson 0.87666
wandb:                                                      val_mse 0.00256
wandb:                                                  val_pearson 0.99424
wandb: 
wandb: ðŸš€ View run NormanWeissman2019_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/27ctmduo
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_105818-27ctmduo/logs
