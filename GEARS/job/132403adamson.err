cuda-11.8.0 loaded successful
gcc-12.2.0 loaded successful
cmake-3.27.0 loaded successful
openmpi-4.1.2 loaded successful
Openblas-0.3.25 loaded successful
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/8 [00:00<?, ?it/s] 12%|â–ˆâ–Ž        | 1/8 [00:07<00:54,  7.75s/it] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:16<00:48,  8.06s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:16<00:22,  4.47s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:26<00:26,  6.67s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:34<00:22,  7.36s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:43<00:15,  7.82s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:54<00:08,  8.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:01<00:00,  8.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:01<00:00,  7.71s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406675_1/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406675_1/splits/adamsonweissman2016_gsm2406675_1_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_010907-u6lmzw3m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406675_1_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u6lmzw3m
  0%|          | 0/3895 [00:00<?, ?it/s]  0%|          | 8/3895 [00:00<00:53, 72.74it/s]  0%|          | 17/3895 [00:00<00:49, 78.07it/s]  1%|          | 27/3895 [00:00<00:46, 84.01it/s]  1%|          | 37/3895 [00:00<00:44, 87.06it/s]  1%|          | 47/3895 [00:00<00:43, 88.65it/s]  1%|â–         | 57/3895 [00:00<00:42, 89.42it/s]  2%|â–         | 67/3895 [00:00<00:42, 90.42it/s]  2%|â–         | 77/3895 [00:00<00:43, 87.94it/s]  2%|â–         | 88/3895 [00:00<00:41, 91.64it/s]  3%|â–Ž         | 98/3895 [00:01<00:41, 91.41it/s]  3%|â–Ž         | 108/3895 [00:01<00:41, 90.77it/s]  3%|â–Ž         | 118/3895 [00:01<00:41, 90.84it/s]  3%|â–Ž         | 128/3895 [00:01<00:43, 87.39it/s]  4%|â–Ž         | 138/3895 [00:01<00:42, 87.49it/s]  4%|â–         | 147/3895 [00:01<00:42, 87.83it/s]  4%|â–         | 158/3895 [00:01<00:40, 91.49it/s]  4%|â–         | 168/3895 [00:01<00:42, 88.52it/s]  5%|â–         | 179/3895 [00:02<00:40, 91.82it/s]  5%|â–         | 189/3895 [00:02<00:41, 88.47it/s]  5%|â–Œ         | 199/3895 [00:02<00:42, 87.18it/s]  5%|â–Œ         | 208/3895 [00:02<00:42, 87.09it/s]  6%|â–Œ         | 217/3895 [00:02<00:43, 83.94it/s]  6%|â–Œ         | 226/3895 [00:02<00:44, 82.12it/s]  6%|â–Œ         | 235/3895 [00:02<00:45, 81.00it/s]  6%|â–‹         | 244/3895 [00:02<00:46, 78.22it/s]  6%|â–‹         | 252/3895 [00:02<00:46, 78.59it/s]  7%|â–‹         | 261/3895 [00:03<00:44, 81.68it/s]  7%|â–‹         | 270/3895 [00:03<00:44, 80.97it/s]  7%|â–‹         | 279/3895 [00:03<00:44, 80.93it/s]  7%|â–‹         | 288/3895 [00:03<00:44, 80.58it/s]  8%|â–Š         | 297/3895 [00:03<00:44, 80.57it/s]  8%|â–Š         | 306/3895 [00:03<00:44, 80.40it/s]  8%|â–Š         | 315/3895 [00:03<00:46, 77.40it/s]  8%|â–Š         | 324/3895 [00:03<00:45, 78.02it/s]  9%|â–Š         | 333/3895 [00:03<00:44, 80.91it/s]  9%|â–‰         | 342/3895 [00:04<00:44, 80.73it/s]  9%|â–‰         | 351/3895 [00:04<00:43, 80.92it/s]  9%|â–‰         | 360/3895 [00:04<00:42, 82.65it/s]  9%|â–‰         | 369/3895 [00:04<00:42, 83.85it/s] 10%|â–‰         | 378/3895 [00:04<00:41, 84.71it/s] 10%|â–‰         | 387/3895 [00:04<00:40, 86.12it/s] 10%|â–ˆ         | 396/3895 [00:04<00:41, 83.83it/s] 10%|â–ˆ         | 406/3895 [00:04<00:39, 87.71it/s] 11%|â–ˆ         | 415/3895 [00:04<00:39, 87.69it/s] 11%|â–ˆ         | 424/3895 [00:04<00:39, 87.87it/s] 11%|â–ˆ         | 433/3895 [00:05<00:39, 87.69it/s] 11%|â–ˆâ–        | 442/3895 [00:05<00:39, 88.00it/s] 12%|â–ˆâ–        | 451/3895 [00:05<00:39, 87.84it/s] 12%|â–ˆâ–        | 461/3895 [00:05<00:38, 89.12it/s] 12%|â–ˆâ–        | 470/3895 [00:05<00:39, 86.82it/s] 12%|â–ˆâ–        | 481/3895 [00:05<00:37, 90.67it/s] 13%|â–ˆâ–Ž        | 491/3895 [00:05<00:37, 90.04it/s] 13%|â–ˆâ–Ž        | 501/3895 [00:05<00:37, 89.93it/s] 13%|â–ˆâ–Ž        | 510/3895 [00:05<00:38, 87.57it/s] 13%|â–ˆâ–Ž        | 520/3895 [00:06<00:37, 90.71it/s] 14%|â–ˆâ–Ž        | 530/3895 [00:06<00:38, 88.12it/s] 14%|â–ˆâ–        | 540/3895 [00:06<00:36, 91.03it/s] 14%|â–ˆâ–        | 550/3895 [00:06<00:37, 90.31it/s] 14%|â–ˆâ–        | 560/3895 [00:06<00:38, 86.74it/s] 15%|â–ˆâ–        | 569/3895 [00:06<00:40, 82.26it/s] 15%|â–ˆâ–        | 579/3895 [00:06<00:39, 84.30it/s] 15%|â–ˆâ–Œ        | 588/3895 [00:06<00:39, 82.98it/s] 15%|â–ˆâ–Œ        | 597/3895 [00:06<00:40, 82.16it/s] 16%|â–ˆâ–Œ        | 606/3895 [00:07<00:40, 81.53it/s] 16%|â–ˆâ–Œ        | 615/3895 [00:07<00:41, 78.53it/s] 16%|â–ˆâ–Œ        | 624/3895 [00:07<00:41, 78.99it/s] 16%|â–ˆâ–‹        | 634/3895 [00:07<00:39, 82.01it/s] 17%|â–ˆâ–‹        | 643/3895 [00:07<00:39, 81.45it/s] 17%|â–ˆâ–‹        | 652/3895 [00:07<00:39, 81.20it/s] 17%|â–ˆâ–‹        | 661/3895 [00:07<00:39, 81.24it/s] 17%|â–ˆâ–‹        | 670/3895 [00:07<00:39, 81.12it/s] 17%|â–ˆâ–‹        | 679/3895 [00:08<00:39, 80.90it/s] 18%|â–ˆâ–Š        | 688/3895 [00:08<00:39, 80.74it/s] 18%|â–ˆâ–Š        | 697/3895 [00:08<00:39, 80.59it/s] 18%|â–ˆâ–Š        | 706/3895 [00:08<00:39, 80.33it/s] 18%|â–ˆâ–Š        | 715/3895 [00:08<00:39, 80.55it/s] 19%|â–ˆâ–Š        | 724/3895 [00:08<00:39, 80.54it/s] 19%|â–ˆâ–‰        | 733/3895 [00:08<00:39, 80.41it/s] 19%|â–ˆâ–‰        | 742/3895 [00:08<00:39, 80.41it/s] 19%|â–ˆâ–‰        | 751/3895 [00:08<00:39, 80.45it/s] 20%|â–ˆâ–‰        | 760/3895 [00:09<00:38, 80.79it/s] 20%|â–ˆâ–‰        | 769/3895 [00:09<00:38, 80.76it/s] 20%|â–ˆâ–‰        | 778/3895 [00:09<00:38, 80.78it/s] 20%|â–ˆâ–ˆ        | 787/3895 [00:09<00:38, 80.95it/s] 20%|â–ˆâ–ˆ        | 796/3895 [00:09<00:38, 80.85it/s] 21%|â–ˆâ–ˆ        | 805/3895 [00:09<00:38, 80.70it/s] 21%|â–ˆâ–ˆ        | 814/3895 [00:09<00:38, 80.80it/s] 21%|â–ˆâ–ˆ        | 823/3895 [00:09<00:38, 80.38it/s] 21%|â–ˆâ–ˆâ–       | 832/3895 [00:09<00:38, 80.15it/s] 22%|â–ˆâ–ˆâ–       | 841/3895 [00:10<00:37, 80.39it/s] 22%|â–ˆâ–ˆâ–       | 850/3895 [00:10<00:37, 81.45it/s] 22%|â–ˆâ–ˆâ–       | 859/3895 [00:10<00:37, 80.36it/s] 22%|â–ˆâ–ˆâ–       | 868/3895 [00:10<00:37, 80.98it/s] 23%|â–ˆâ–ˆâ–Ž       | 878/3895 [00:10<00:35, 85.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 887/3895 [00:10<00:35, 83.57it/s] 23%|â–ˆâ–ˆâ–Ž       | 897/3895 [00:10<00:34, 87.53it/s] 23%|â–ˆâ–ˆâ–Ž       | 906/3895 [00:10<00:34, 86.94it/s] 23%|â–ˆâ–ˆâ–Ž       | 915/3895 [00:10<00:34, 87.05it/s] 24%|â–ˆâ–ˆâ–Ž       | 924/3895 [00:10<00:35, 84.04it/s] 24%|â–ˆâ–ˆâ–       | 933/3895 [00:11<00:34, 85.32it/s] 24%|â–ˆâ–ˆâ–       | 943/3895 [00:11<00:33, 89.10it/s] 24%|â–ˆâ–ˆâ–       | 952/3895 [00:11<00:33, 88.58it/s] 25%|â–ˆâ–ˆâ–       | 961/3895 [00:11<00:33, 86.48it/s] 25%|â–ˆâ–ˆâ–       | 971/3895 [00:11<00:32, 89.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 981/3895 [00:11<00:33, 87.19it/s] 25%|â–ˆâ–ˆâ–Œ       | 991/3895 [00:11<00:32, 90.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 1001/3895 [00:11<00:33, 86.85it/s] 26%|â–ˆâ–ˆâ–Œ       | 1010/3895 [00:11<00:33, 87.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 1020/3895 [00:12<00:31, 90.14it/s] 26%|â–ˆâ–ˆâ–‹       | 1030/3895 [00:12<00:32, 87.11it/s] 27%|â–ˆâ–ˆâ–‹       | 1039/3895 [00:12<00:32, 87.15it/s] 27%|â–ˆâ–ˆâ–‹       | 1049/3895 [00:12<00:31, 89.48it/s] 27%|â–ˆâ–ˆâ–‹       | 1059/3895 [00:12<00:32, 86.92it/s] 27%|â–ˆâ–ˆâ–‹       | 1068/3895 [00:12<00:32, 86.99it/s] 28%|â–ˆâ–ˆâ–Š       | 1077/3895 [00:12<00:34, 82.32it/s] 28%|â–ˆâ–ˆâ–Š       | 1087/3895 [00:12<00:32, 85.23it/s] 28%|â–ˆâ–ˆâ–Š       | 1096/3895 [00:12<00:33, 82.85it/s] 28%|â–ˆâ–ˆâ–Š       | 1107/3895 [00:13<00:32, 84.52it/s] 29%|â–ˆâ–ˆâ–Š       | 1116/3895 [00:13<00:32, 85.98it/s] 29%|â–ˆâ–ˆâ–‰       | 1126/3895 [00:13<00:32, 86.38it/s] 29%|â–ˆâ–ˆâ–‰       | 1135/3895 [00:13<00:32, 83.74it/s] 29%|â–ˆâ–ˆâ–‰       | 1144/3895 [00:13<00:32, 83.85it/s] 30%|â–ˆâ–ˆâ–‰       | 1156/3895 [00:13<00:31, 87.05it/s] 30%|â–ˆâ–ˆâ–‰       | 1165/3895 [00:13<00:31, 86.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1175/3895 [00:13<00:30, 89.59it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1185/3895 [00:13<00:30, 88.02it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1195/3895 [00:14<00:29, 91.31it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1205/3895 [00:14<00:29, 90.53it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1215/3895 [00:14<00:29, 89.88it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1225/3895 [00:14<00:30, 88.73it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1235/3895 [00:14<00:30, 86.59it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1245/3895 [00:14<00:30, 85.90it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1255/3895 [00:14<00:29, 88.72it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1264/3895 [00:14<00:30, 85.91it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1273/3895 [00:14<00:30, 85.54it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1283/3895 [00:15<00:29, 89.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1292/3895 [00:15<00:29, 88.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1301/3895 [00:15<00:29, 87.93it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1311/3895 [00:15<00:29, 87.90it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1322/3895 [00:15<00:28, 91.56it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1332/3895 [00:15<00:28, 89.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1341/3895 [00:15<00:29, 87.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1352/3895 [00:15<00:28, 89.21it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1363/3895 [00:15<00:27, 93.34it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1373/3895 [00:16<00:27, 90.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1383/3895 [00:16<00:27, 90.65it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1394/3895 [00:16<00:26, 93.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1404/3895 [00:16<00:27, 90.00it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1415/3895 [00:16<00:26, 93.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1425/3895 [00:16<00:26, 93.37it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1435/3895 [00:16<00:27, 90.30it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1445/3895 [00:16<00:26, 92.13it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1455/3895 [00:16<00:27, 89.69it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1465/3895 [00:17<00:28, 85.13it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1474/3895 [00:17<00:30, 78.39it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1482/3895 [00:17<00:32, 73.85it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1492/3895 [00:17<00:30, 78.55it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1501/3895 [00:17<00:29, 80.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1510/3895 [00:17<00:28, 82.99it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1519/3895 [00:17<00:28, 83.99it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1529/3895 [00:17<00:27, 84.84it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1540/3895 [00:18<00:27, 86.79it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1550/3895 [00:18<00:26, 89.07it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1559/3895 [00:18<00:27, 86.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1569/3895 [00:18<00:25, 89.98it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1579/3895 [00:18<00:25, 89.24it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1588/3895 [00:18<00:26, 86.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1597/3895 [00:18<00:28, 80.07it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1606/3895 [00:18<00:27, 81.97it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1615/3895 [00:18<00:30, 75.98it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1623/3895 [00:19<00:32, 70.92it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1631/3895 [00:19<00:33, 67.79it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1638/3895 [00:19<00:33, 67.21it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1645/3895 [00:19<00:34, 65.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1652/3895 [00:19<00:35, 62.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1660/3895 [00:19<00:35, 62.80it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1668/3895 [00:19<00:33, 66.81it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1675/3895 [00:19<00:33, 66.66it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1683/3895 [00:20<00:32, 68.48it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1690/3895 [00:20<00:32, 66.96it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1698/3895 [00:20<00:31, 68.93it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1706/3895 [00:20<00:30, 71.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1714/3895 [00:20<00:29, 73.05it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1722/3895 [00:20<00:29, 73.95it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1730/3895 [00:20<00:28, 75.05it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1738/3895 [00:20<00:29, 72.85it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1747/3895 [00:20<00:28, 76.05it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1755/3895 [00:20<00:28, 75.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1765/3895 [00:21<00:25, 82.15it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1774/3895 [00:21<00:26, 80.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1786/3895 [00:21<00:23, 91.02it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1796/3895 [00:21<00:22, 93.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1806/3895 [00:21<00:22, 94.19it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1816/3895 [00:21<00:21, 94.76it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1827/3895 [00:21<00:21, 97.96it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1837/3895 [00:21<00:21, 96.51it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1847/3895 [00:21<00:21, 95.54it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1857/3895 [00:22<00:22, 92.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1868/3895 [00:22<00:21, 95.74it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1878/3895 [00:22<00:21, 94.99it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1888/3895 [00:22<00:21, 92.46it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1898/3895 [00:22<00:23, 83.29it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1907/3895 [00:22<00:24, 80.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1916/3895 [00:22<00:25, 77.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1924/3895 [00:22<00:25, 75.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1932/3895 [00:23<00:26, 72.83it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1940/3895 [00:23<00:27, 72.08it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1948/3895 [00:23<00:27, 72.01it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1956/3895 [00:23<00:27, 70.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1965/3895 [00:23<00:26, 72.67it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1973/3895 [00:23<00:27, 68.89it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1980/3895 [00:23<00:28, 67.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1988/3895 [00:23<00:27, 69.47it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1996/3895 [00:23<00:26, 71.24it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2005/3895 [00:24<00:25, 74.83it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2014/3895 [00:24<00:25, 73.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2022/3895 [00:24<00:25, 72.97it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2030/3895 [00:24<00:25, 73.31it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2038/3895 [00:24<00:25, 72.05it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2046/3895 [00:24<00:26, 70.75it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2054/3895 [00:24<00:26, 70.81it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2062/3895 [00:24<00:25, 71.71it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2070/3895 [00:24<00:26, 70.00it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2078/3895 [00:25<00:26, 68.51it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2085/3895 [00:25<00:26, 68.78it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2092/3895 [00:25<00:27, 66.72it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2099/3895 [00:25<00:26, 66.86it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2106/3895 [00:25<00:27, 64.06it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2114/3895 [00:25<00:27, 65.56it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2122/3895 [00:25<00:26, 68.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2129/3895 [00:25<00:25, 68.55it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2138/3895 [00:25<00:23, 74.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2146/3895 [00:26<00:23, 75.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2157/3895 [00:26<00:21, 81.70it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2167/3895 [00:26<00:19, 86.70it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2177/3895 [00:26<00:19, 87.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2187/3895 [00:26<00:19, 89.35it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2198/3895 [00:26<00:18, 92.60it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2208/3895 [00:26<00:18, 89.09it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2218/3895 [00:26<00:18, 90.80it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2228/3895 [00:26<00:17, 93.14it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2238/3895 [00:27<00:18, 88.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2249/3895 [00:27<00:17, 92.08it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2259/3895 [00:27<00:17, 91.69it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2270/3895 [00:27<00:17, 91.98it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2280/3895 [00:27<00:17, 90.35it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2290/3895 [00:27<00:17, 92.00it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2301/3895 [00:27<00:16, 94.41it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2311/3895 [00:27<00:16, 94.11it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2321/3895 [00:27<00:17, 91.00it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2332/3895 [00:28<00:16, 93.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2342/3895 [00:28<00:16, 94.36it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2352/3895 [00:28<00:16, 93.85it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2362/3895 [00:28<00:16, 91.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2373/3895 [00:28<00:16, 94.54it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2383/3895 [00:28<00:16, 90.43it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2394/3895 [00:28<00:15, 94.45it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2404/3895 [00:28<00:15, 93.37it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2414/3895 [00:28<00:16, 91.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2424/3895 [00:29<00:15, 92.14it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2435/3895 [00:29<00:15, 92.27it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2445/3895 [00:29<00:15, 92.95it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2456/3895 [00:29<00:15, 92.31it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2467/3895 [00:29<00:14, 96.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2477/3895 [00:29<00:14, 95.31it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2487/3895 [00:29<00:15, 91.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2497/3895 [00:29<00:15, 92.83it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2507/3895 [00:29<00:14, 92.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2517/3895 [00:30<00:14, 92.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2527/3895 [00:30<00:14, 93.69it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2537/3895 [00:30<00:14, 93.04it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2547/3895 [00:30<00:15, 89.81it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2558/3895 [00:30<00:14, 91.34it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2569/3895 [00:30<00:13, 94.78it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2579/3895 [00:30<00:14, 93.67it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2589/3895 [00:30<00:14, 91.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2601/3895 [00:30<00:13, 95.08it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2611/3895 [00:31<00:13, 94.46it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2621/3895 [00:31<00:13, 92.92it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2632/3895 [00:31<00:12, 97.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2642/3895 [00:31<00:12, 97.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2652/3895 [00:31<00:13, 92.93it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2662/3895 [00:31<00:13, 90.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2673/3895 [00:31<00:12, 94.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2683/3895 [00:31<00:12, 95.85it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2693/3895 [00:31<00:13, 91.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2704/3895 [00:32<00:13, 90.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2714/3895 [00:32<00:12, 92.13it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2726/3895 [00:32<00:12, 97.14it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2736/3895 [00:32<00:12, 93.86it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2746/3895 [00:32<00:12, 95.02it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2756/3895 [00:32<00:12, 94.06it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2767/3895 [00:32<00:11, 97.93it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2777/3895 [00:32<00:11, 96.03it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2787/3895 [00:32<00:12, 91.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2797/3895 [00:33<00:12, 90.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2807/3895 [00:33<00:12, 85.02it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2817/3895 [00:33<00:12, 88.00it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2827/3895 [00:33<00:11, 89.37it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2837/3895 [00:33<00:11, 92.15it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2847/3895 [00:33<00:11, 93.74it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2857/3895 [00:33<00:11, 94.11it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2867/3895 [00:33<00:10, 94.61it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2877/3895 [00:33<00:10, 94.92it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2887/3895 [00:33<00:10, 95.85it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2897/3895 [00:34<00:10, 93.60it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2909/3895 [00:34<00:10, 94.28it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2921/3895 [00:34<00:10, 96.39it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2932/3895 [00:34<00:09, 98.98it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2943/3895 [00:34<00:09, 101.54it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2954/3895 [00:34<00:09, 100.57it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2965/3895 [00:34<00:09, 97.00it/s]  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2975/3895 [00:34<00:09, 97.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2985/3895 [00:34<00:09, 97.33it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2996/3895 [00:35<00:09, 98.23it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3007/3895 [00:35<00:08, 100.32it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 3018/3895 [00:35<00:09, 94.96it/s]  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3030/3895 [00:35<00:08, 97.80it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3041/3895 [00:35<00:08, 100.75it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3052/3895 [00:35<00:08, 99.99it/s]  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3063/3895 [00:35<00:08, 95.98it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3073/3895 [00:35<00:08, 96.56it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3083/3895 [00:36<00:08, 96.32it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3094/3895 [00:36<00:08, 97.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3105/3895 [00:36<00:07, 99.01it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 3115/3895 [00:36<00:08, 95.98it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3126/3895 [00:36<00:07, 96.43it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3136/3895 [00:36<00:13, 57.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3168/3895 [00:36<00:06, 104.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3182/3895 [00:37<00:06, 104.28it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3195/3895 [00:37<00:06, 102.50it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3207/3895 [00:37<00:12, 53.66it/s]  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3217/3895 [00:37<00:13, 48.50it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3232/3895 [00:38<00:10, 60.54it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3250/3895 [00:38<00:08, 72.21it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3283/3895 [00:38<00:06, 89.26it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3294/3895 [00:38<00:07, 82.16it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3303/3895 [00:39<00:10, 58.32it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3333/3895 [00:39<00:06, 88.33it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3375/3895 [00:39<00:03, 138.95it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3395/3895 [00:39<00:03, 128.56it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3412/3895 [00:39<00:03, 121.47it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3427/3895 [00:39<00:04, 114.20it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3441/3895 [00:40<00:04, 107.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3453/3895 [00:40<00:04, 104.70it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3465/3895 [00:40<00:04, 101.89it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3476/3895 [00:40<00:04, 99.28it/s]  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3487/3895 [00:40<00:04, 98.40it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3498/3895 [00:40<00:03, 100.01it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3509/3895 [00:40<00:04, 95.90it/s]  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3519/3895 [00:40<00:03, 96.13it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3530/3895 [00:40<00:03, 97.34it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3540/3895 [00:41<00:03, 94.94it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3550/3895 [00:41<00:03, 94.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3561/3895 [00:41<00:03, 95.10it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3571/3895 [00:41<00:03, 94.66it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3581/3895 [00:41<00:03, 93.72it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3591/3895 [00:41<00:03, 95.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3601/3895 [00:41<00:03, 94.81it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3611/3895 [00:41<00:02, 94.86it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3622/3895 [00:41<00:02, 95.07it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3632/3895 [00:42<00:02, 94.59it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3642/3895 [00:42<00:02, 95.68it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3653/3895 [00:42<00:02, 98.32it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3663/3895 [00:42<00:02, 94.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3674/3895 [00:42<00:02, 94.73it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3684/3895 [00:42<00:02, 94.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3694/3895 [00:42<00:02, 95.35it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3705/3895 [00:42<00:01, 97.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3715/3895 [00:42<00:01, 94.84it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3725/3895 [00:42<00:01, 95.20it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3736/3895 [00:43<00:01, 98.08it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3746/3895 [00:43<00:01, 93.65it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3756/3895 [00:43<00:01, 93.51it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3767/3895 [00:43<00:01, 97.54it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3777/3895 [00:43<00:01, 97.00it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3787/3895 [00:43<00:01, 90.76it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3798/3895 [00:43<00:01, 94.37it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3809/3895 [00:43<00:00, 97.67it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3819/3895 [00:43<00:00, 94.78it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3829/3895 [00:44<00:00, 94.98it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3840/3895 [00:44<00:00, 95.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3850/3895 [00:44<00:00, 92.27it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3862/3895 [00:44<00:00, 95.54it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3872/3895 [00:44<00:00, 95.07it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3882/3895 [00:44<00:00, 95.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3892/3895 [00:44<00:00, 95.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3895/3895 [00:44<00:00, 86.99it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.4644
Epoch 1 Step 51 Train Loss: 0.2958
Epoch 1: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0341 Validation Top 20 DE MSE: 0.0540. 
Epoch 2 Step 1 Train Loss: 0.2897
Epoch 2 Step 51 Train Loss: 0.3056
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0566. 
Epoch 3 Step 1 Train Loss: 0.2882
Epoch 3 Step 51 Train Loss: 0.2888
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0576. 
Epoch 4 Step 1 Train Loss: 0.2920
Epoch 4 Step 51 Train Loss: 0.2895
Epoch 4: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0584. 
Epoch 5 Step 1 Train Loss: 0.2719
Epoch 5 Step 51 Train Loss: 0.2759
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0569. 
Epoch 6 Step 1 Train Loss: 0.2773
Epoch 6 Step 51 Train Loss: 0.2790
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0573. 
Epoch 7 Step 1 Train Loss: 0.2781
Epoch 7 Step 51 Train Loss: 0.2766
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0575. 
Epoch 8 Step 1 Train Loss: 0.2710
Epoch 8 Step 51 Train Loss: 0.2708
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0574. 
Epoch 9 Step 1 Train Loss: 0.2899
Epoch 9 Step 51 Train Loss: 0.2752
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0579. 
Epoch 10 Step 1 Train Loss: 0.2798
Epoch 10 Step 51 Train Loss: 0.2905
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0579. 
Epoch 11 Step 1 Train Loss: 0.2787
Epoch 11 Step 51 Train Loss: 0.2770
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0581. 
Epoch 12 Step 1 Train Loss: 0.2821
Epoch 12 Step 51 Train Loss: 0.2834
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0579. 
Epoch 13 Step 1 Train Loss: 0.2799
Epoch 13 Step 51 Train Loss: 0.2800
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0569. 
Epoch 14 Step 1 Train Loss: 0.2872
Epoch 14 Step 51 Train Loss: 0.2788
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0580. 
Epoch 15 Step 1 Train Loss: 0.2765
Epoch 15 Step 51 Train Loss: 0.2762
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0566. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1257
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0016077107
test_unseen_single_pearson: 0.9953244713457574
test_unseen_single_mse_de: 0.12570862
test_unseen_single_pearson_de: 0.9512387303839681
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.0488381926651809
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.45
test_unseen_single_frac_sigma_below_1_non_dropout: 0.875
test_unseen_single_mse_top20_de_non_dropout: 0.12570862
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.018 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–„â–ˆâ–â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–„â–„â–„â–ƒ
wandb:                                                train_pearson â–…â–â–ˆâ–‡â–†â–†â–†â–…â–†â–†â–…â–…â–…â–…â–†
wandb:                                                training_loss â–ˆâ–„â–…â–„â–„â–„â–„â–ƒâ–‚â–‚â–â–‚â–‚â–…â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–„â–‚â–„â–ƒâ–â–â–ƒâ–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–‚
wandb:                                                   val_de_mse â–â–…â–‡â–ˆâ–†â–†â–‡â–†â–‡â–‡â–ˆâ–‡â–†â–‡â–…
wandb:                                               val_de_pearson â–ˆâ–‚â–â–â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–…
wandb:                                                      val_mse â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–‚â–â–â–â–
wandb:                                                  val_pearson â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.12571
wandb:                                              test_de_pearson 0.95124
wandb:               test_frac_opposite_direction_top20_non_dropout 0.45
wandb:                          test_frac_sigma_below_1_non_dropout 0.875
wandb:                                                     test_mse 0.00161
wandb:                                test_mse_top20_de_non_dropout 0.12571
wandb:                                                 test_pearson 0.99532
wandb:                                           test_pearson_delta 0.04884
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.45
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.875
wandb:                                       test_unseen_single_mse 0.00161
wandb:                                    test_unseen_single_mse_de 0.12571
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.12571
wandb:                                   test_unseen_single_pearson 0.99532
wandb:                                test_unseen_single_pearson_de 0.95124
wandb:                             test_unseen_single_pearson_delta 0.04884
wandb:                                                 train_de_mse 0.0038
wandb:                                             train_de_pearson 0.99885
wandb:                                                    train_mse 0.00093
wandb:                                                train_pearson 0.99732
wandb:                                                training_loss 0.27451
wandb:                                                   val_de_mse 0.05656
wandb:                                               val_de_pearson 0.97456
wandb:                                                      val_mse 0.00081
wandb:                                                  val_pearson 0.99762
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406675_1_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/u6lmzw3m
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_010907-u6lmzw3m/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406675_1/splits/adamsonweissman2016_gsm2406675_1_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_011322-bjdfevpw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406675_1_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/bjdfevpw
Start Training...
Epoch 1 Step 1 Train Loss: 0.5150
Epoch 1 Step 51 Train Loss: 0.2945
Epoch 1: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0324 Validation Top 20 DE MSE: 0.0093. 
Epoch 2 Step 1 Train Loss: 0.3035
Epoch 2 Step 51 Train Loss: 0.2884
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0188 Validation Top 20 DE MSE: 0.0080. 
Epoch 3 Step 1 Train Loss: 0.2777
Epoch 3 Step 51 Train Loss: 0.2869
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0071. 
Epoch 4 Step 1 Train Loss: 0.2902
Epoch 4 Step 51 Train Loss: 0.2881
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0052 Validation Top 20 DE MSE: 0.0052. 
Epoch 5 Step 1 Train Loss: 0.2866
Epoch 5 Step 51 Train Loss: 0.2886
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0067. 
Epoch 6 Step 1 Train Loss: 0.2939
Epoch 6 Step 51 Train Loss: 0.2781
Epoch 6: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0066. 
Epoch 7 Step 1 Train Loss: 0.2682
Epoch 7 Step 51 Train Loss: 0.2791
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0066. 
Epoch 8 Step 1 Train Loss: 0.2701
Epoch 8 Step 51 Train Loss: 0.2902
Epoch 8: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0069. 
Epoch 9 Step 1 Train Loss: 0.2751
Epoch 9 Step 51 Train Loss: 0.2849
Epoch 9: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0069. 
Epoch 10 Step 1 Train Loss: 0.2901
Epoch 10 Step 51 Train Loss: 0.2929
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0066. 
Epoch 11 Step 1 Train Loss: 0.2692
Epoch 11 Step 51 Train Loss: 0.2750
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0071. 
Epoch 12 Step 1 Train Loss: 0.2897
Epoch 12 Step 51 Train Loss: 0.2870
Epoch 12: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0029 Validation Top 20 DE MSE: 0.0068. 
Epoch 13 Step 1 Train Loss: 0.2779
Epoch 13 Step 51 Train Loss: 0.2749
Epoch 13: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0068. 
Epoch 14 Step 1 Train Loss: 0.2843
Epoch 14 Step 51 Train Loss: 0.2855
Epoch 14: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0066. 
Epoch 15 Step 1 Train Loss: 0.2816
Epoch 15 Step 51 Train Loss: 0.2895
Epoch 15: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0065. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1601
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0019932943
test_unseen_single_pearson: 0.9941748879371417
test_unseen_single_mse_de: 0.16006954
test_unseen_single_pearson_de: 0.9347595046456912
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.08279207990965781
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.5
test_unseen_single_frac_sigma_below_1_non_dropout: 0.825
test_unseen_single_mse_top20_de_non_dropout: 0.16006954
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.002 MB of 0.018 MB uploadedwandb: / 0.002 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–„â–…â–‚â–â–…â–„â–‡â–‡â–…â–ˆâ–‡â–†â–‡â–†â–†
wandb:                                                train_pearson â–…â–„â–†â–ˆâ–„â–…â–â–‚â–„â–â–‚â–ƒâ–‚â–ƒâ–ƒ
wandb:                                                training_loss â–ˆâ–…â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–„â–‚â–‚â–‚â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–…â–‚â–‚â–„â–‚â–‚â–„â–ƒâ–â–ƒâ–„â–„â–ƒ
wandb:                                                   val_de_mse â–ˆâ–†â–„â–â–„â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–„â–ƒâ–ƒ
wandb:                                               val_de_pearson â–â–ƒâ–…â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–‚â–‚â–â–ƒâ–„â–…â–ˆâ–†â–‡â–†â–…â–…â–‡â–†â–‡
wandb:                                                  val_pearson â–‡â–‡â–ˆâ–†â–…â–„â–â–ƒâ–‚â–ƒâ–„â–„â–‚â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.16007
wandb:                                              test_de_pearson 0.93476
wandb:               test_frac_opposite_direction_top20_non_dropout 0.5
wandb:                          test_frac_sigma_below_1_non_dropout 0.825
wandb:                                                     test_mse 0.00199
wandb:                                test_mse_top20_de_non_dropout 0.16007
wandb:                                                 test_pearson 0.99417
wandb:                                           test_pearson_delta 0.08279
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.5
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.825
wandb:                                       test_unseen_single_mse 0.00199
wandb:                                    test_unseen_single_mse_de 0.16007
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.16007
wandb:                                   test_unseen_single_pearson 0.99417
wandb:                                test_unseen_single_pearson_de 0.93476
wandb:                             test_unseen_single_pearson_delta 0.08279
wandb:                                                 train_de_mse 0.00285
wandb:                                             train_de_pearson 0.99905
wandb:                                                    train_mse 0.00084
wandb:                                                train_pearson 0.99758
wandb:                                                training_loss 0.28473
wandb:                                                   val_de_mse 0.00654
wandb:                                               val_de_pearson 0.99825
wandb:                                                      val_mse 0.00145
wandb:                                                  val_pearson 0.99577
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406675_1_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/bjdfevpw
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_011322-bjdfevpw/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406675_1/splits/adamsonweissman2016_gsm2406675_1_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_011602-vsgl6rwl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406675_1_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/vsgl6rwl
Start Training...
Epoch 1 Step 1 Train Loss: 0.5943
Epoch 1 Step 51 Train Loss: 0.2869
Epoch 1: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0614 Validation Top 20 DE MSE: 0.0540. 
Epoch 2 Step 1 Train Loss: 0.2954
Epoch 2 Step 51 Train Loss: 0.2749
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0176 Validation Top 20 DE MSE: 0.0518. 
Epoch 3 Step 1 Train Loss: 0.2916
Epoch 3 Step 51 Train Loss: 0.2830
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0483. 
Epoch 4 Step 1 Train Loss: 0.2869
Epoch 4 Step 51 Train Loss: 0.2846
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0487. 
Epoch 5 Step 1 Train Loss: 0.2779
Epoch 5 Step 51 Train Loss: 0.2955
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0056 Validation Top 20 DE MSE: 0.0490. 
Epoch 6 Step 1 Train Loss: 0.2970
Epoch 6 Step 51 Train Loss: 0.2801
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0483. 
Epoch 7 Step 1 Train Loss: 0.2865
Epoch 7 Step 51 Train Loss: 0.2713
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0484. 
Epoch 8 Step 1 Train Loss: 0.2987
Epoch 8 Step 51 Train Loss: 0.3056
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0046 Validation Top 20 DE MSE: 0.0485. 
Epoch 9 Step 1 Train Loss: 0.2974
Epoch 9 Step 51 Train Loss: 0.2833
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0478. 
Epoch 10 Step 1 Train Loss: 0.2888
Epoch 10 Step 51 Train Loss: 0.2864
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0481. 
Epoch 11 Step 1 Train Loss: 0.2776
Epoch 11 Step 51 Train Loss: 0.2925
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0479. 
Epoch 12 Step 1 Train Loss: 0.2852
Epoch 12 Step 51 Train Loss: 0.2744
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0481. 
Epoch 13 Step 1 Train Loss: 0.2926
Epoch 13 Step 51 Train Loss: 0.2877
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0480. 
Epoch 14 Step 1 Train Loss: 0.2746
Epoch 14 Step 51 Train Loss: 0.2864
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0479. 
Epoch 15 Step 1 Train Loss: 0.2968
Epoch 15 Step 51 Train Loss: 0.2768
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0478. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0414
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0011114548
test_unseen_single_pearson: 0.9967770106949632
test_unseen_single_mse_de: 0.041375924
test_unseen_single_pearson_de: 0.98744889163195
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.19400001627413932
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.225
test_unseen_single_frac_sigma_below_1_non_dropout: 0.875
test_unseen_single_mse_top20_de_non_dropout: 0.04146516
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.002 MB of 0.018 MB uploadedwandb: | 0.002 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–‚â–ˆâ–…â–â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–„â–ƒ
wandb:                                                train_pearson â–†â–â–„â–ˆâ–†â–‡â–†â–†â–…â–†â–†â–†â–†â–†â–†
wandb:                                                training_loss â–ˆâ–„â–…â–„â–‚â–ƒâ–‚â–„â–ƒâ–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–ƒâ–‚â–ƒâ–‚â–
wandb:                                                   val_de_mse â–ˆâ–†â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–„â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–â–‚â–â–„â–ƒâ–„â–‡â–„â–†â–…â–…â–„â–‡â–†â–ˆ
wandb:                                                  val_pearson â–ˆâ–†â–ˆâ–…â–†â–…â–‚â–…â–ƒâ–„â–„â–…â–‚â–ƒâ–
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.04138
wandb:                                              test_de_pearson 0.98745
wandb:               test_frac_opposite_direction_top20_non_dropout 0.225
wandb:                          test_frac_sigma_below_1_non_dropout 0.875
wandb:                                                     test_mse 0.00111
wandb:                                test_mse_top20_de_non_dropout 0.04147
wandb:                                                 test_pearson 0.99678
wandb:                                           test_pearson_delta 0.194
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.225
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.875
wandb:                                       test_unseen_single_mse 0.00111
wandb:                                    test_unseen_single_mse_de 0.04138
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.04147
wandb:                                   test_unseen_single_pearson 0.99678
wandb:                                test_unseen_single_pearson_de 0.98745
wandb:                             test_unseen_single_pearson_delta 0.194
wandb:                                                 train_de_mse 0.0042
wandb:                                             train_de_pearson 0.99847
wandb:                                                    train_mse 0.00088
wandb:                                                train_pearson 0.99751
wandb:                                                training_loss 0.2873
wandb:                                                   val_de_mse 0.04782
wandb:                                               val_de_pearson 0.98045
wandb:                                                      val_mse 0.00117
wandb:                                                  val_pearson 0.99656
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406675_1_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/vsgl6rwl
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_011602-vsgl6rwl/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406675_1/splits/adamsonweissman2016_gsm2406675_1_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_011848-7bo9heaf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406675_1_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/7bo9heaf
Start Training...
Epoch 1 Step 1 Train Loss: 0.4840
Epoch 1 Step 51 Train Loss: 0.2982
Epoch 1: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0491 Validation Top 20 DE MSE: 0.0454. 
Epoch 2 Step 1 Train Loss: 0.3023
Epoch 2 Step 51 Train Loss: 0.3037
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.0534. 
Epoch 3 Step 1 Train Loss: 0.2800
Epoch 3 Step 51 Train Loss: 0.2775
Epoch 3: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0081 Validation Top 20 DE MSE: 0.0531. 
Epoch 4 Step 1 Train Loss: 0.2879
Epoch 4 Step 51 Train Loss: 0.2805
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0046 Validation Top 20 DE MSE: 0.0533. 
Epoch 5 Step 1 Train Loss: 0.2917
Epoch 5 Step 51 Train Loss: 0.2794
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0544. 
Epoch 6 Step 1 Train Loss: 0.2728
Epoch 6 Step 51 Train Loss: 0.2839
Epoch 6: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0559. 
Epoch 7 Step 1 Train Loss: 0.2932
Epoch 7 Step 51 Train Loss: 0.2869
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0024 Validation Top 20 DE MSE: 0.0572. 
Epoch 8 Step 1 Train Loss: 0.2803
Epoch 8 Step 51 Train Loss: 0.2733
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0028 Validation Top 20 DE MSE: 0.0556. 
Epoch 9 Step 1 Train Loss: 0.2919
Epoch 9 Step 51 Train Loss: 0.2881
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0559. 
Epoch 10 Step 1 Train Loss: 0.2766
Epoch 10 Step 51 Train Loss: 0.2810
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0026 Validation Top 20 DE MSE: 0.0567. 
Epoch 11 Step 1 Train Loss: 0.2781
Epoch 11 Step 51 Train Loss: 0.3056
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0572. 
Epoch 12 Step 1 Train Loss: 0.2710
Epoch 12 Step 51 Train Loss: 0.2875
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0567. 
Epoch 13 Step 1 Train Loss: 0.2855
Epoch 13 Step 51 Train Loss: 0.2827
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0563. 
Epoch 14 Step 1 Train Loss: 0.2805
Epoch 14 Step 51 Train Loss: 0.2814
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0025 Validation Top 20 DE MSE: 0.0574. 
Epoch 15 Step 1 Train Loss: 0.2959
Epoch 15 Step 51 Train Loss: 0.2954
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0027 Validation Top 20 DE MSE: 0.0559. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1147
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014181912
test_unseen_single_pearson: 0.9958586891419114
test_unseen_single_mse_de: 0.11471352
test_unseen_single_pearson_de: 0.9489988635141565
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.01770787822051464
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.6000000000000001
test_unseen_single_frac_sigma_below_1_non_dropout: 0.85
test_unseen_single_mse_top20_de_non_dropout: 0.11489011
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.014 MB uploadedwandb: | 0.008 MB of 0.018 MB uploadedwandb: / 0.008 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–„â–ˆâ–â–‚â–ƒâ–ƒâ–…â–„â–„â–„â–…â–…â–…â–…â–…
wandb:                                                train_pearson â–…â–â–ˆâ–‡â–†â–†â–„â–…â–…â–…â–„â–„â–„â–„â–„
wandb:                                                training_loss â–†â–ˆâ–ƒâ–…â–†â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–‚â–„â–„â–ƒâ–…â–ƒâ–…â–‚â–â–„â–‚â–‚â–â–ƒâ–„â–ƒâ–„â–…â–„â–ƒâ–„â–„â–ƒâ–ƒâ–
wandb:                                                   val_de_mse â–â–†â–…â–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡
wandb:                                               val_de_pearson â–ˆâ–„â–ƒâ–…â–†â–ƒâ–‚â–…â–…â–â–‡â–‚â–†â–â–„
wandb:                                                      val_mse â–…â–ˆâ–â–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–ƒâ–„â–„â–ƒ
wandb:                                                  val_pearson â–„â–â–ˆâ–‡â–‡â–†â–…â–†â–†â–†â–„â–†â–…â–…â–†
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.11471
wandb:                                              test_de_pearson 0.949
wandb:               test_frac_opposite_direction_top20_non_dropout 0.6
wandb:                          test_frac_sigma_below_1_non_dropout 0.85
wandb:                                                     test_mse 0.00142
wandb:                                test_mse_top20_de_non_dropout 0.11489
wandb:                                                 test_pearson 0.99586
wandb:                                           test_pearson_delta 0.01771
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.6
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.85
wandb:                                       test_unseen_single_mse 0.00142
wandb:                                    test_unseen_single_mse_de 0.11471
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.11489
wandb:                                   test_unseen_single_pearson 0.99586
wandb:                                test_unseen_single_pearson_de 0.949
wandb:                             test_unseen_single_pearson_delta 0.01771
wandb:                                                 train_de_mse 0.00269
wandb:                                             train_de_pearson 0.99915
wandb:                                                    train_mse 0.00091
wandb:                                                train_pearson 0.99737
wandb:                                                training_loss 0.27957
wandb:                                                   val_de_mse 0.05591
wandb:                                               val_de_pearson 0.99464
wandb:                                                      val_mse 0.00072
wandb:                                                  val_pearson 0.9979
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406675_1_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/7bo9heaf
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_011848-7bo9heaf/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406675_1/splits/adamsonweissman2016_gsm2406675_1_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_012128-x8ompbdp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406675_1_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/x8ompbdp
Start Training...
Epoch 1 Step 1 Train Loss: 0.4555
Epoch 1 Step 51 Train Loss: 0.3045
Epoch 1: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0515 Validation Top 20 DE MSE: 0.0626. 
Epoch 2 Step 1 Train Loss: 0.2906
Epoch 2 Step 51 Train Loss: 0.2865
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0179 Validation Top 20 DE MSE: 0.0577. 
Epoch 3 Step 1 Train Loss: 0.2781
Epoch 3 Step 51 Train Loss: 0.2906
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0569. 
Epoch 4 Step 1 Train Loss: 0.2771
Epoch 4 Step 51 Train Loss: 0.2860
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0046 Validation Top 20 DE MSE: 0.0578. 
Epoch 5 Step 1 Train Loss: 0.2746
Epoch 5 Step 51 Train Loss: 0.2803
Epoch 5: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0570. 
Epoch 6 Step 1 Train Loss: 0.2843
Epoch 6 Step 51 Train Loss: 0.2785
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0572. 
Epoch 7 Step 1 Train Loss: 0.2855
Epoch 7 Step 51 Train Loss: 0.2636
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0574. 
Epoch 8 Step 1 Train Loss: 0.2924
Epoch 8 Step 51 Train Loss: 0.2695
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0583. 
Epoch 9 Step 1 Train Loss: 0.2810
Epoch 9 Step 51 Train Loss: 0.2832
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0596. 
Epoch 10 Step 1 Train Loss: 0.2749
Epoch 10 Step 51 Train Loss: 0.2741
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0597. 
Epoch 11 Step 1 Train Loss: 0.2882
Epoch 11 Step 51 Train Loss: 0.2868
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0580. 
Epoch 12 Step 1 Train Loss: 0.2808
Epoch 12 Step 51 Train Loss: 0.2862
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0570. 
Epoch 13 Step 1 Train Loss: 0.2836
Epoch 13 Step 51 Train Loss: 0.2730
Epoch 13: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0590. 
Epoch 14 Step 1 Train Loss: 0.2817
Epoch 14 Step 51 Train Loss: 0.2733
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0589. 
Epoch 15 Step 1 Train Loss: 0.2745
Epoch 15 Step 51 Train Loss: 0.2904
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0591. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1349
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014489281
test_unseen_single_pearson: 0.9957780153440421
test_unseen_single_mse_de: 0.1349142
test_unseen_single_pearson_de: 0.9510380668680899
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.051098747436829524
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.575
test_unseen_single_frac_sigma_below_1_non_dropout: 0.825
test_unseen_single_mse_top20_de_non_dropout: 0.1349142
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.001 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–ƒ
wandb:                                                train_pearson â–â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–†â–†â–…â–†â–†â–…â–†â–†
wandb:                                                training_loss â–ˆâ–„â–…â–…â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–„â–â–ƒâ–‚â–„â–‚â–„â–„â–‚â–‚â–‚â–â–â–„â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒ
wandb:                                                   val_de_mse â–ˆâ–‚â–â–‚â–â–â–‚â–ƒâ–„â–„â–‚â–â–„â–ƒâ–„
wandb:                                               val_de_pearson â–â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–†â–†â–‡â–ˆâ–‡â–‡â–‡
wandb:                                                      val_mse â–ˆâ–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚
wandb:                                                  val_pearson â–â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–‡â–†â–†â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.13491
wandb:                                              test_de_pearson 0.95104
wandb:               test_frac_opposite_direction_top20_non_dropout 0.575
wandb:                          test_frac_sigma_below_1_non_dropout 0.825
wandb:                                                     test_mse 0.00145
wandb:                                test_mse_top20_de_non_dropout 0.13491
wandb:                                                 test_pearson 0.99578
wandb:                                           test_pearson_delta 0.0511
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.575
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.825
wandb:                                       test_unseen_single_mse 0.00145
wandb:                                    test_unseen_single_mse_de 0.13491
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.13491
wandb:                                   test_unseen_single_pearson 0.99578
wandb:                                test_unseen_single_pearson_de 0.95104
wandb:                             test_unseen_single_pearson_delta 0.0511
wandb:                                                 train_de_mse 0.00419
wandb:                                             train_de_pearson 0.99879
wandb:                                                    train_mse 0.00093
wandb:                                                train_pearson 0.99731
wandb:                                                training_loss 0.2837
wandb:                                                   val_de_mse 0.05908
wandb:                                               val_de_pearson 0.97448
wandb:                                                      val_mse 0.00098
wandb:                                                  val_pearson 0.99713
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406675_1_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/x8ompbdp
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_012128-x8ompbdp/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/9 [00:00<?, ?it/s] 11%|â–ˆ         | 1/9 [00:00<00:03,  2.27it/s] 22%|â–ˆâ–ˆâ–       | 2/9 [01:19<05:26, 46.67s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [02:45<06:28, 64.82s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [04:08<05:58, 71.67s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [05:25<04:54, 73.57s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [06:50<03:52, 77.51s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [08:10<02:36, 78.38s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [08:10<00:53, 53.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [08:10<00:00, 36.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [08:10<00:00, 54.55s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406677_2/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406677_2/splits/adamsonweissman2016_gsm2406677_2_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_013308-e3zuzl6v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406677_2_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/e3zuzl6v
  0%|          | 0/3722 [00:00<?, ?it/s]  0%|          | 7/3722 [00:00<00:58, 63.18it/s]  0%|          | 16/3722 [00:00<00:49, 75.38it/s]  1%|          | 25/3722 [00:00<00:45, 81.00it/s]  1%|          | 35/3722 [00:00<00:43, 84.32it/s]  1%|          | 44/3722 [00:00<00:45, 80.23it/s]  1%|â–         | 53/3722 [00:00<00:47, 77.83it/s]  2%|â–         | 61/3722 [00:00<00:47, 76.34it/s]  2%|â–         | 69/3722 [00:00<00:48, 75.28it/s]  2%|â–         | 77/3722 [00:01<00:48, 74.62it/s]  2%|â–         | 85/3722 [00:01<00:48, 74.96it/s]  3%|â–Ž         | 94/3722 [00:01<00:47, 75.58it/s]  3%|â–Ž         | 104/3722 [00:01<00:44, 81.76it/s]  3%|â–Ž         | 113/3722 [00:01<00:43, 83.83it/s]  3%|â–Ž         | 122/3722 [00:01<00:42, 85.53it/s]  4%|â–Ž         | 131/3722 [00:01<00:42, 84.26it/s]  4%|â–         | 140/3722 [00:01<00:45, 79.20it/s]  4%|â–         | 149/3722 [00:01<00:46, 77.06it/s]  4%|â–         | 158/3722 [00:02<00:45, 79.06it/s]  4%|â–         | 166/3722 [00:02<00:45, 77.59it/s]  5%|â–         | 176/3722 [00:02<00:43, 81.30it/s]  5%|â–         | 185/3722 [00:02<00:42, 82.76it/s]  5%|â–Œ         | 194/3722 [00:02<00:43, 80.52it/s]  5%|â–Œ         | 204/3722 [00:02<00:41, 85.03it/s]  6%|â–Œ         | 214/3722 [00:02<00:40, 86.52it/s]  6%|â–Œ         | 223/3722 [00:02<00:40, 86.59it/s]  6%|â–Œ         | 232/3722 [00:02<00:40, 87.12it/s]  6%|â–‹         | 241/3722 [00:02<00:39, 87.65it/s]  7%|â–‹         | 250/3722 [00:03<00:39, 87.90it/s]  7%|â–‹         | 260/3722 [00:03<00:38, 88.90it/s]  7%|â–‹         | 269/3722 [00:03<00:38, 88.96it/s]  7%|â–‹         | 279/3722 [00:03<00:39, 86.73it/s]  8%|â–Š         | 290/3722 [00:03<00:37, 91.28it/s]  8%|â–Š         | 300/3722 [00:03<00:37, 91.44it/s]  8%|â–Š         | 310/3722 [00:03<00:37, 91.30it/s]  9%|â–Š         | 320/3722 [00:03<00:38, 87.88it/s]  9%|â–‰         | 329/3722 [00:04<00:44, 76.69it/s]  9%|â–‰         | 337/3722 [00:04<00:44, 76.55it/s]  9%|â–‰         | 346/3722 [00:04<00:43, 78.09it/s] 10%|â–‰         | 354/3722 [00:04<00:44, 75.91it/s] 10%|â–‰         | 362/3722 [00:04<00:46, 72.93it/s] 10%|â–‰         | 370/3722 [00:04<00:49, 68.28it/s] 10%|â–ˆ         | 377/3722 [00:04<00:49, 67.43it/s] 10%|â–ˆ         | 384/3722 [00:04<00:50, 66.26it/s] 11%|â–ˆ         | 391/3722 [00:04<00:51, 64.97it/s] 11%|â–ˆ         | 398/3722 [00:05<00:51, 64.58it/s] 11%|â–ˆ         | 405/3722 [00:05<00:51, 64.14it/s] 11%|â–ˆ         | 412/3722 [00:05<00:53, 62.21it/s] 11%|â–ˆâ–        | 420/3722 [00:05<00:50, 64.98it/s] 11%|â–ˆâ–        | 427/3722 [00:05<00:51, 63.38it/s] 12%|â–ˆâ–        | 434/3722 [00:05<00:53, 61.98it/s] 12%|â–ˆâ–        | 441/3722 [00:05<00:52, 63.08it/s] 12%|â–ˆâ–        | 448/3722 [00:05<00:51, 63.77it/s] 12%|â–ˆâ–        | 455/3722 [00:05<00:50, 64.60it/s] 12%|â–ˆâ–        | 462/3722 [00:06<00:50, 64.40it/s] 13%|â–ˆâ–Ž        | 469/3722 [00:06<00:50, 64.11it/s] 13%|â–ˆâ–Ž        | 476/3722 [00:06<00:50, 63.77it/s] 13%|â–ˆâ–Ž        | 483/3722 [00:06<00:51, 63.45it/s] 13%|â–ˆâ–Ž        | 490/3722 [00:06<00:51, 62.85it/s] 13%|â–ˆâ–Ž        | 497/3722 [00:06<00:51, 62.54it/s] 14%|â–ˆâ–Ž        | 504/3722 [00:06<00:50, 63.49it/s] 14%|â–ˆâ–Ž        | 511/3722 [00:06<00:50, 63.87it/s] 14%|â–ˆâ–        | 518/3722 [00:06<00:50, 64.02it/s] 14%|â–ˆâ–        | 525/3722 [00:07<00:49, 64.22it/s] 14%|â–ˆâ–        | 533/3722 [00:07<00:47, 66.73it/s] 15%|â–ˆâ–        | 541/3722 [00:07<00:45, 70.17it/s] 15%|â–ˆâ–        | 549/3722 [00:07<00:48, 65.42it/s] 15%|â–ˆâ–Œ        | 560/3722 [00:07<00:41, 76.56it/s] 15%|â–ˆâ–Œ        | 568/3722 [00:07<00:41, 76.02it/s] 15%|â–ˆâ–Œ        | 576/3722 [00:07<00:40, 76.86it/s] 16%|â–ˆâ–Œ        | 584/3722 [00:07<00:42, 74.58it/s] 16%|â–ˆâ–Œ        | 593/3722 [00:07<00:40, 77.34it/s] 16%|â–ˆâ–Œ        | 601/3722 [00:08<00:40, 76.76it/s] 16%|â–ˆâ–‹        | 612/3722 [00:08<00:37, 83.83it/s] 17%|â–ˆâ–‹        | 621/3722 [00:08<00:36, 85.53it/s] 17%|â–ˆâ–‹        | 631/3722 [00:08<00:36, 85.61it/s] 17%|â–ˆâ–‹        | 642/3722 [00:08<00:35, 87.53it/s] 18%|â–ˆâ–Š        | 652/3722 [00:08<00:34, 89.81it/s] 18%|â–ˆâ–Š        | 661/3722 [00:08<00:34, 89.44it/s] 18%|â–ˆâ–Š        | 670/3722 [00:08<00:34, 89.45it/s] 18%|â–ˆâ–Š        | 679/3722 [00:08<00:35, 86.73it/s] 18%|â–ˆâ–Š        | 688/3722 [00:09<00:37, 80.17it/s] 19%|â–ˆâ–Š        | 697/3722 [00:09<00:47, 64.33it/s] 19%|â–ˆâ–‰        | 709/3722 [00:09<00:39, 75.79it/s] 19%|â–ˆâ–‰        | 718/3722 [00:09<00:42, 70.60it/s] 20%|â–ˆâ–‰        | 726/3722 [00:09<00:43, 68.98it/s] 20%|â–ˆâ–‰        | 734/3722 [00:09<00:43, 68.00it/s] 20%|â–ˆâ–‰        | 742/3722 [00:09<00:44, 67.20it/s] 20%|â–ˆâ–ˆ        | 749/3722 [00:09<00:44, 66.52it/s] 20%|â–ˆâ–ˆ        | 756/3722 [00:10<00:45, 65.28it/s] 20%|â–ˆâ–ˆ        | 763/3722 [00:10<00:46, 63.94it/s] 21%|â–ˆâ–ˆ        | 770/3722 [00:10<00:47, 62.55it/s] 21%|â–ˆâ–ˆ        | 777/3722 [00:10<00:46, 63.15it/s] 21%|â–ˆâ–ˆ        | 784/3722 [00:10<00:46, 63.82it/s] 21%|â–ˆâ–ˆâ–       | 791/3722 [00:10<00:46, 63.46it/s] 21%|â–ˆâ–ˆâ–       | 798/3722 [00:10<00:46, 62.82it/s] 22%|â–ˆâ–ˆâ–       | 806/3722 [00:10<00:44, 66.20it/s] 22%|â–ˆâ–ˆâ–       | 813/3722 [00:10<00:43, 66.14it/s] 22%|â–ˆâ–ˆâ–       | 820/3722 [00:11<00:45, 64.14it/s] 22%|â–ˆâ–ˆâ–       | 828/3722 [00:11<00:43, 66.73it/s] 22%|â–ˆâ–ˆâ–       | 835/3722 [00:11<00:42, 67.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 842/3722 [00:11<00:43, 66.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 849/3722 [00:11<00:43, 66.41it/s] 23%|â–ˆâ–ˆâ–Ž       | 857/3722 [00:11<00:41, 68.40it/s] 23%|â–ˆâ–ˆâ–Ž       | 864/3722 [00:11<00:42, 67.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 871/3722 [00:11<00:43, 65.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 878/3722 [00:11<00:42, 66.46it/s] 24%|â–ˆâ–ˆâ–       | 885/3722 [00:12<00:45, 61.90it/s] 24%|â–ˆâ–ˆâ–       | 892/3722 [00:12<00:44, 63.51it/s] 24%|â–ˆâ–ˆâ–       | 902/3722 [00:12<00:38, 72.67it/s] 24%|â–ˆâ–ˆâ–       | 911/3722 [00:12<00:38, 73.93it/s] 25%|â–ˆâ–ˆâ–       | 919/3722 [00:12<00:37, 74.75it/s] 25%|â–ˆâ–ˆâ–       | 928/3722 [00:12<00:35, 78.31it/s] 25%|â–ˆâ–ˆâ–Œ       | 938/3722 [00:12<00:33, 82.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 947/3722 [00:12<00:33, 82.12it/s] 26%|â–ˆâ–ˆâ–Œ       | 956/3722 [00:12<00:33, 82.93it/s] 26%|â–ˆâ–ˆâ–Œ       | 965/3722 [00:13<00:35, 78.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 973/3722 [00:13<00:35, 77.95it/s] 26%|â–ˆâ–ˆâ–‹       | 983/3722 [00:13<00:32, 83.71it/s] 27%|â–ˆâ–ˆâ–‹       | 992/3722 [00:13<00:33, 82.70it/s] 27%|â–ˆâ–ˆâ–‹       | 1001/3722 [00:13<00:32, 82.50it/s] 27%|â–ˆâ–ˆâ–‹       | 1010/3722 [00:13<00:33, 81.74it/s] 27%|â–ˆâ–ˆâ–‹       | 1019/3722 [00:13<00:33, 80.96it/s] 28%|â–ˆâ–ˆâ–Š       | 1028/3722 [00:13<00:32, 82.57it/s] 28%|â–ˆâ–ˆâ–Š       | 1037/3722 [00:13<00:32, 82.14it/s] 28%|â–ˆâ–ˆâ–Š       | 1046/3722 [00:14<00:32, 83.23it/s] 28%|â–ˆâ–ˆâ–Š       | 1055/3722 [00:14<00:32, 81.36it/s] 29%|â–ˆâ–ˆâ–Š       | 1065/3722 [00:14<00:32, 80.59it/s] 29%|â–ˆâ–ˆâ–‰       | 1074/3722 [00:14<00:31, 83.11it/s] 29%|â–ˆâ–ˆâ–‰       | 1083/3722 [00:14<00:32, 81.65it/s] 29%|â–ˆâ–ˆâ–‰       | 1093/3722 [00:14<00:31, 83.90it/s] 30%|â–ˆâ–ˆâ–‰       | 1102/3722 [00:14<00:31, 84.45it/s] 30%|â–ˆâ–ˆâ–‰       | 1111/3722 [00:14<00:31, 82.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1121/3722 [00:14<00:30, 84.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1130/3722 [00:15<00:30, 85.18it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1139/3722 [00:15<00:30, 85.14it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1148/3722 [00:15<00:30, 83.80it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1157/3722 [00:15<00:30, 84.12it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1166/3722 [00:15<00:31, 80.85it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1176/3722 [00:15<00:29, 84.95it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1185/3722 [00:15<00:29, 85.84it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1194/3722 [00:15<00:30, 84.19it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1203/3722 [00:15<00:30, 82.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1212/3722 [00:16<00:30, 82.53it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1221/3722 [00:16<00:30, 82.04it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1231/3722 [00:16<00:28, 86.55it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1240/3722 [00:16<00:29, 83.10it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1250/3722 [00:16<00:29, 84.46it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1259/3722 [00:16<00:28, 85.13it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1268/3722 [00:16<00:28, 86.20it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1277/3722 [00:16<00:28, 86.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1286/3722 [00:16<00:28, 86.91it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1295/3722 [00:16<00:28, 85.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1304/3722 [00:17<00:28, 84.85it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1313/3722 [00:17<00:28, 85.39it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1323/3722 [00:17<00:27, 87.26it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1332/3722 [00:17<00:27, 87.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1341/3722 [00:17<00:27, 87.04it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1350/3722 [00:17<00:27, 85.31it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1360/3722 [00:17<00:26, 89.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1369/3722 [00:17<00:26, 87.97it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1378/3722 [00:17<00:26, 87.06it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1388/3722 [00:18<00:28, 82.53it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1399/3722 [00:18<00:27, 84.61it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1411/3722 [00:18<00:25, 91.75it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1421/3722 [00:18<00:25, 89.53it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1431/3722 [00:18<00:25, 89.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1440/3722 [00:18<00:27, 84.28it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1449/3722 [00:18<00:27, 83.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1460/3722 [00:18<00:25, 88.77it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1469/3722 [00:18<00:25, 87.93it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1478/3722 [00:19<00:27, 81.52it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1487/3722 [00:19<00:27, 82.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1497/3722 [00:19<00:27, 82.34it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1508/3722 [00:19<00:25, 86.63it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1518/3722 [00:19<00:24, 89.82it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1528/3722 [00:19<00:24, 91.38it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1538/3722 [00:19<00:23, 92.20it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1548/3722 [00:19<00:23, 91.89it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1558/3722 [00:19<00:23, 90.45it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1568/3722 [00:20<00:23, 91.85it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1578/3722 [00:20<00:23, 89.72it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1589/3722 [00:20<00:22, 94.03it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1599/3722 [00:20<00:23, 90.90it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1610/3722 [00:20<00:22, 95.02it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1620/3722 [00:20<00:22, 93.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1630/3722 [00:20<00:23, 90.29it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1640/3722 [00:20<00:22, 92.35it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1650/3722 [00:21<00:23, 89.07it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1661/3722 [00:21<00:22, 92.22it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1671/3722 [00:21<00:22, 90.62it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1681/3722 [00:21<00:22, 91.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1691/3722 [00:21<00:22, 90.61it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1701/3722 [00:21<00:22, 89.19it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1711/3722 [00:21<00:22, 91.34it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1721/3722 [00:21<00:22, 87.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1730/3722 [00:21<00:23, 85.36it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1739/3722 [00:22<00:23, 83.83it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1749/3722 [00:22<00:22, 86.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1758/3722 [00:22<00:23, 84.53it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1767/3722 [00:22<00:22, 85.73it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1776/3722 [00:22<00:22, 85.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1785/3722 [00:22<00:22, 85.96it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1794/3722 [00:22<00:22, 85.50it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1803/3722 [00:22<00:22, 85.86it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1812/3722 [00:22<00:22, 83.07it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1821/3722 [00:22<00:23, 80.88it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1831/3722 [00:23<00:22, 84.26it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1840/3722 [00:23<00:23, 80.08it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1850/3722 [00:23<00:21, 85.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1859/3722 [00:23<00:21, 85.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1868/3722 [00:23<00:21, 84.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1877/3722 [00:23<00:22, 80.70it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1886/3722 [00:23<00:23, 79.08it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1894/3722 [00:23<00:24, 75.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1902/3722 [00:24<00:24, 74.28it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1910/3722 [00:24<00:24, 72.74it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1918/3722 [00:24<00:24, 72.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1927/3722 [00:24<00:23, 76.21it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1935/3722 [00:24<00:24, 73.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1943/3722 [00:24<00:26, 66.60it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1951/3722 [00:24<00:25, 69.54it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1961/3722 [00:24<00:23, 76.54it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1970/3722 [00:24<00:22, 77.24it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1980/3722 [00:25<00:20, 83.01it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1989/3722 [00:25<00:22, 78.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1997/3722 [00:25<00:21, 78.55it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2005/3722 [00:25<00:22, 76.94it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2013/3722 [00:25<00:24, 71.15it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2021/3722 [00:25<00:23, 73.24it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2029/3722 [00:25<00:23, 72.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2037/3722 [00:25<00:22, 73.46it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2045/3722 [00:25<00:23, 72.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2053/3722 [00:26<00:22, 74.09it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2061/3722 [00:26<00:22, 72.70it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2071/3722 [00:26<00:22, 74.79it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2079/3722 [00:26<00:22, 74.46it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2087/3722 [00:26<00:22, 73.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2096/3722 [00:26<00:21, 74.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2104/3722 [00:26<00:21, 73.61it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2112/3722 [00:26<00:21, 73.30it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2120/3722 [00:26<00:22, 72.65it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2129/3722 [00:27<00:21, 73.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2138/3722 [00:27<00:20, 75.93it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2146/3722 [00:27<00:21, 74.05it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2154/3722 [00:27<00:21, 72.19it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2162/3722 [00:27<00:21, 71.79it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2171/3722 [00:27<00:20, 74.39it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2180/3722 [00:27<00:20, 75.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2188/3722 [00:27<00:20, 75.56it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2197/3722 [00:27<00:19, 77.40it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2205/3722 [00:28<00:19, 76.51it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2213/3722 [00:28<00:19, 77.01it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2222/3722 [00:28<00:19, 77.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2230/3722 [00:28<00:19, 75.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2240/3722 [00:28<00:18, 81.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2249/3722 [00:28<00:17, 82.41it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2259/3722 [00:28<00:17, 83.39it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2270/3722 [00:28<00:16, 88.39it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2279/3722 [00:28<00:16, 84.97it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2288/3722 [00:29<00:17, 80.96it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2297/3722 [00:29<00:19, 72.57it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2306/3722 [00:29<00:18, 76.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2315/3722 [00:29<00:17, 78.27it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2323/3722 [00:29<00:18, 75.61it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2331/3722 [00:29<00:18, 76.10it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2339/3722 [00:29<00:17, 77.14it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2347/3722 [00:29<00:18, 76.09it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2355/3722 [00:30<00:20, 68.12it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2362/3722 [00:30<00:20, 67.09it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2372/3722 [00:30<00:18, 72.12it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2382/3722 [00:30<00:17, 77.86it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2392/3722 [00:30<00:15, 83.28it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2401/3722 [00:30<00:15, 85.02it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2411/3722 [00:30<00:15, 85.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2420/3722 [00:30<00:15, 83.01it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2430/3722 [00:30<00:15, 85.20it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2439/3722 [00:31<00:15, 81.62it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2449/3722 [00:31<00:14, 85.09it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2458/3722 [00:31<00:14, 84.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2467/3722 [00:31<00:15, 82.21it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2476/3722 [00:31<00:14, 84.11it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2485/3722 [00:31<00:14, 83.06it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2494/3722 [00:31<00:14, 83.15it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2503/3722 [00:31<00:15, 79.91it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2512/3722 [00:31<00:15, 79.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2521/3722 [00:32<00:14, 81.35it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2530/3722 [00:32<00:14, 81.98it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2539/3722 [00:32<00:14, 79.08it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2547/3722 [00:32<00:14, 78.48it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2555/3722 [00:32<00:14, 77.95it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2563/3722 [00:32<00:15, 75.86it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2571/3722 [00:32<00:15, 72.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2579/3722 [00:32<00:16, 70.72it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2587/3722 [00:32<00:16, 70.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2595/3722 [00:33<00:16, 68.73it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2603/3722 [00:33<00:15, 70.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2611/3722 [00:33<00:15, 71.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2620/3722 [00:33<00:15, 71.75it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2629/3722 [00:33<00:14, 76.37it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2638/3722 [00:33<00:14, 77.38it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2646/3722 [00:33<00:13, 77.53it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2654/3722 [00:33<00:13, 77.69it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2662/3722 [00:33<00:14, 73.40it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2671/3722 [00:34<00:13, 77.07it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2679/3722 [00:34<00:13, 76.25it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2687/3722 [00:35<00:42, 24.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2705/3722 [00:35<00:26, 38.70it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2713/3722 [00:35<00:23, 42.36it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2759/3722 [00:35<00:09, 105.13it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2778/3722 [00:35<00:09, 95.41it/s]  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2794/3722 [00:35<00:10, 89.15it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2807/3722 [00:36<00:10, 84.93it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2819/3722 [00:36<00:11, 81.66it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2830/3722 [00:36<00:11, 80.42it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2840/3722 [00:36<00:10, 80.80it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2849/3722 [00:36<00:11, 77.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2858/3722 [00:36<00:10, 78.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2867/3722 [00:36<00:11, 77.50it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2876/3722 [00:37<00:11, 74.04it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2885/3722 [00:37<00:11, 74.19it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2894/3722 [00:37<00:11, 75.25it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2902/3722 [00:37<00:10, 74.57it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2910/3722 [00:37<00:10, 73.85it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2918/3722 [00:37<00:11, 71.82it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2927/3722 [00:37<00:10, 73.58it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2935/3722 [00:37<00:10, 75.11it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2943/3722 [00:37<00:10, 75.95it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2951/3722 [00:37<00:10, 76.94it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2959/3722 [00:38<00:09, 76.39it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2967/3722 [00:38<00:09, 75.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2975/3722 [00:38<00:09, 75.93it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2983/3722 [00:38<00:09, 76.09it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2991/3722 [00:38<00:09, 76.31it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2999/3722 [00:38<00:09, 74.90it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3009/3722 [00:38<00:09, 77.84it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3019/3722 [00:38<00:08, 80.89it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3028/3722 [00:38<00:08, 80.68it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3037/3722 [00:39<00:08, 79.98it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3045/3722 [00:39<00:08, 78.53it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3053/3722 [00:39<00:08, 78.64it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3061/3722 [00:39<00:08, 78.06it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3071/3722 [00:39<00:08, 80.35it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3081/3722 [00:39<00:07, 83.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3090/3722 [00:39<00:08, 78.93it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3099/3722 [00:39<00:07, 78.67it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3107/3722 [00:40<00:10, 59.89it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 3115/3722 [00:40<00:09, 64.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3124/3722 [00:40<00:08, 68.81it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3134/3722 [00:40<00:07, 75.35it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3144/3722 [00:40<00:07, 80.91it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3154/3722 [00:40<00:06, 84.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3164/3722 [00:40<00:06, 86.34it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3175/3722 [00:40<00:06, 89.77it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3185/3722 [00:40<00:05, 90.53it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3196/3722 [00:41<00:05, 95.27it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3206/3722 [00:41<00:05, 92.22it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3216/3722 [00:41<00:05, 92.52it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3226/3722 [00:41<00:05, 94.52it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3237/3722 [00:41<00:05, 93.68it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3248/3722 [00:41<00:04, 97.35it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3258/3722 [00:41<00:05, 90.85it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3268/3722 [00:41<00:04, 92.80it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3278/3722 [00:41<00:05, 85.19it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3287/3722 [00:42<00:05, 83.20it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3296/3722 [00:42<00:05, 84.92it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3306/3722 [00:42<00:04, 87.12it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3315/3722 [00:42<00:04, 85.40it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3326/3722 [00:42<00:04, 90.15it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3336/3722 [00:42<00:04, 87.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3346/3722 [00:42<00:04, 89.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3357/3722 [00:42<00:03, 94.61it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3367/3722 [00:42<00:03, 94.41it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3377/3722 [00:43<00:03, 91.38it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3387/3722 [00:43<00:03, 92.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3398/3722 [00:43<00:03, 97.23it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3409/3722 [00:43<00:03, 95.73it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3420/3722 [00:43<00:03, 98.62it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3431/3722 [00:43<00:02, 99.06it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3441/3722 [00:43<00:02, 96.03it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3453/3722 [00:43<00:02, 99.20it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3464/3722 [00:43<00:02, 100.89it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3475/3722 [00:44<00:02, 97.83it/s]  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3486/3722 [00:44<00:02, 97.93it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3496/3722 [00:44<00:02, 97.80it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3507/3722 [00:44<00:02, 98.65it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3517/3722 [00:44<00:02, 98.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3527/3722 [00:44<00:01, 98.26it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3537/3722 [00:44<00:01, 98.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3547/3722 [00:44<00:01, 98.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3558/3722 [00:44<00:01, 99.69it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3568/3722 [00:45<00:01, 98.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3578/3722 [00:45<00:01, 96.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3590/3722 [00:45<00:01, 102.84it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3601/3722 [00:45<00:01, 101.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3612/3722 [00:45<00:01, 97.75it/s]  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3623/3722 [00:45<00:01, 97.87it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3633/3722 [00:45<00:00, 98.13it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3644/3722 [00:45<00:00, 100.32it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3655/3722 [00:45<00:00, 97.58it/s]  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3666/3722 [00:46<00:00, 94.77it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3678/3722 [00:46<00:00, 99.04it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3689/3722 [00:46<00:00, 98.99it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3699/3722 [00:46<00:00, 96.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3710/3722 [00:46<00:00, 100.34it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3722/3722 [00:46<00:00, 102.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3722/3722 [00:46<00:00, 79.94it/s] 
Start Training...
Epoch 1 Step 1 Train Loss: 0.6092
Epoch 1 Step 51 Train Loss: 0.3883
Epoch 1: Train Overall MSE: 0.3564 Validation Overall MSE: 0.4510. 
Train Top 20 DE MSE: 0.7891 Validation Top 20 DE MSE: 0.6119. 
Epoch 2 Step 1 Train Loss: 0.3673
Epoch 2 Step 51 Train Loss: 0.3634
Epoch 2: Train Overall MSE: 20.9175 Validation Overall MSE: 64.2914. 
Train Top 20 DE MSE: 51.6682 Validation Top 20 DE MSE: 155.6372. 
Epoch 3 Step 1 Train Loss: 0.3541
Epoch 3 Step 51 Train Loss: 0.5762
Epoch 3: Train Overall MSE: 126.6094 Validation Overall MSE: 341.7438. 
Train Top 20 DE MSE: 158.9929 Validation Top 20 DE MSE: 697.3464. 
Epoch 4 Step 1 Train Loss: 0.3769
Epoch 4 Step 51 Train Loss: 0.3596
Epoch 4: Train Overall MSE: 9.6630 Validation Overall MSE: 35.6507. 
Train Top 20 DE MSE: 13.4847 Validation Top 20 DE MSE: 66.4369. 
Epoch 5 Step 1 Train Loss: 0.4638
Epoch 5 Step 51 Train Loss: 0.3176
Epoch 5: Train Overall MSE: 1540.6530 Validation Overall MSE: 3551.2170. 
Train Top 20 DE MSE: 1771.8287 Validation Top 20 DE MSE: 8210.2188. 
Epoch 6 Step 1 Train Loss: 0.3227
Epoch 6 Step 51 Train Loss: 0.3338
Epoch 6: Train Overall MSE: 14.7348 Validation Overall MSE: 49.7748. 
Train Top 20 DE MSE: 18.3573 Validation Top 20 DE MSE: 109.5130. 
Epoch 7 Step 1 Train Loss: 0.3018
Epoch 7 Step 51 Train Loss: 0.3151
Epoch 7: Train Overall MSE: 2084.7922 Validation Overall MSE: 5020.1152. 
Train Top 20 DE MSE: 2392.7092 Validation Top 20 DE MSE: 12089.6465. 
Epoch 8 Step 1 Train Loss: 0.3764
Epoch 8 Step 51 Train Loss: 0.3536
Epoch 8: Train Overall MSE: 6.7191 Validation Overall MSE: 25.5400. 
Train Top 20 DE MSE: 8.3501 Validation Top 20 DE MSE: 57.6312. 
Epoch 9 Step 1 Train Loss: 0.3177
Epoch 9 Step 51 Train Loss: 0.3317
Epoch 9: Train Overall MSE: 36.8106 Validation Overall MSE: 115.1858. 
Train Top 20 DE MSE: 44.4219 Validation Top 20 DE MSE: 266.5658. 
Epoch 10 Step 1 Train Loss: 0.3052
Epoch 10 Step 51 Train Loss: 0.3272
Epoch 10: Train Overall MSE: 2642.8494 Validation Overall MSE: 6112.7026. 
Train Top 20 DE MSE: 2927.7295 Validation Top 20 DE MSE: 15209.7949. 
Epoch 11 Step 1 Train Loss: 0.3138
Epoch 11 Step 51 Train Loss: 0.3244
Epoch 11: Train Overall MSE: 49.3448 Validation Overall MSE: 144.8456. 
Train Top 20 DE MSE: 58.5470 Validation Top 20 DE MSE: 341.2605. 
Epoch 12 Step 1 Train Loss: 0.3400
Epoch 12 Step 51 Train Loss: 0.4842
Epoch 12: Train Overall MSE: 1427.9004 Validation Overall MSE: 3458.0315. 
Train Top 20 DE MSE: 1596.7200 Validation Top 20 DE MSE: 8426.5898. 
Epoch 13 Step 1 Train Loss: 0.4256
Epoch 13 Step 51 Train Loss: 0.3117
Epoch 13: Train Overall MSE: 15.6796 Validation Overall MSE: 53.2493. 
Train Top 20 DE MSE: 18.6880 Validation Top 20 DE MSE: 122.4777. 
Epoch 14 Step 1 Train Loss: 0.3513
Epoch 14 Step 51 Train Loss: 0.3394
Epoch 14: Train Overall MSE: 21.0191 Validation Overall MSE: 67.8347. 
Train Top 20 DE MSE: 24.5058 Validation Top 20 DE MSE: 157.3262. 
Epoch 15 Step 1 Train Loss: 0.3349
Epoch 15 Step 51 Train Loss: 0.3087
Epoch 15: Train Overall MSE: 2.2416 Validation Overall MSE: 10.3920. 
Train Top 20 DE MSE: 2.8230 Validation Top 20 DE MSE: 23.5285. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 25.4921
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 34.069176
test_combo_seen1_pearson: 0.11514392126440132
test_combo_seen1_mse_de: 44.584106
test_combo_seen1_pearson_de: -0.04401599773734711
test_combo_seen2_mse: 21.729519
test_combo_seen2_pearson: 0.14699229859442475
test_combo_seen2_mse_de: 31.634348
test_combo_seen2_pearson_de: 0.6488963047613898
test_unseen_single_mse: 1.2663502
test_unseen_single_pearson: 0.49601500452848823
test_unseen_single_mse_de: 3.329001
test_unseen_single_pearson_de: 0.30398232005930714
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: 0.00870016142149319
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.55
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.125
test_combo_seen1_mse_top20_de_non_dropout: 44.5841
test_combo_seen2_pearson_delta: 0.09143091274953612
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.2
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.09999999999999998
test_combo_seen2_mse_top20_de_non_dropout: 31.634348
test_unseen_single_pearson_delta: 0.009627104856072953
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.7
test_unseen_single_frac_sigma_below_1_non_dropout: 0.17500000000000004
test_unseen_single_mse_top20_de_non_dropout: 2.9220142
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.015 MB uploadedwandb: | 0.001 MB of 0.018 MB uploadedwandb: / 0.001 MB of 0.018 MB uploadedwandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–â–â–â–â–…â–â–‡â–â–â–ˆâ–â–…â–â–â–
wandb:                                             train_de_pearson â–…â–ƒâ–â–‡â–„â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–â–â–â–â–…â–â–‡â–â–â–ˆâ–â–…â–â–â–
wandb:                                                train_pearson â–ˆâ–â–â–†â–„â–†â–…â–†â–†â–…â–†â–…â–†â–†â–‡
wandb:                                                training_loss â–…â–†â–‚â–â–‚â–‚â–ƒâ–‚â–„â–‚â–†â–ˆâ–ƒâ–ƒâ–‚â–…â–„â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–ƒ
wandb:                                                   val_de_mse â–â–â–â–â–…â–â–‡â–â–â–ˆâ–â–…â–â–â–
wandb:                                               val_de_pearson â–ˆâ–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–ƒ
wandb:                                                      val_mse â–â–â–â–â–…â–â–‡â–â–â–ˆâ–â–…â–â–â–
wandb:                                                  val_pearson â–ˆâ–â–â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.55
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.125
wandb:                                         test_combo_seen1_mse 34.06918
wandb:                                      test_combo_seen1_mse_de 44.58411
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 44.5841
wandb:                                     test_combo_seen1_pearson 0.11514
wandb:                                  test_combo_seen1_pearson_de -0.04402
wandb:                               test_combo_seen1_pearson_delta 0.0087
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.2
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.1
wandb:                                         test_combo_seen2_mse 21.72952
wandb:                                      test_combo_seen2_mse_de 31.63435
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 31.63435
wandb:                                     test_combo_seen2_pearson 0.14699
wandb:                                  test_combo_seen2_pearson_de 0.6489
wandb:                               test_combo_seen2_pearson_delta 0.09143
wandb:                                                  test_de_mse 25.49211
wandb:                                              test_de_pearson 0.23377
wandb:               test_frac_opposite_direction_top20_non_dropout 0.54
wandb:                          test_frac_sigma_below_1_non_dropout 0.14
wandb:                                                     test_mse 18.48011
wandb:                                test_mse_top20_de_non_dropout 25.32931
wandb:                                                 test_pearson 0.27386
wandb:                                           test_pearson_delta 0.02562
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.7
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.175
wandb:                                       test_unseen_single_mse 1.26635
wandb:                                    test_unseen_single_mse_de 3.329
wandb:                  test_unseen_single_mse_top20_de_non_dropout 2.92201
wandb:                                   test_unseen_single_pearson 0.49602
wandb:                                test_unseen_single_pearson_de 0.30398
wandb:                             test_unseen_single_pearson_delta 0.00963
wandb:                                                 train_de_mse 2.82299
wandb:                                             train_de_pearson 0.49809
wandb:                                                    train_mse 2.24163
wandb:                                                train_pearson 0.75268
wandb:                                                training_loss 0.30909
wandb:                                                   val_de_mse 23.52846
wandb:                                               val_de_pearson -0.15649
wandb:                                                      val_mse 10.39195
wandb:                                                  val_pearson 0.08522
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406677_2_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/e3zuzl6v
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_013308-e3zuzl6v/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406677_2/splits/adamsonweissman2016_gsm2406677_2_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_013814-dw5txlqo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406677_2_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/dw5txlqo
Start Training...
Epoch 1 Step 1 Train Loss: 0.5241
Epoch 1 Step 51 Train Loss: 0.3813
Epoch 1: Train Overall MSE: 153.7152 Validation Overall MSE: 33.2240. 
Train Top 20 DE MSE: 94.3853 Validation Top 20 DE MSE: 71.1176. 
Epoch 2 Step 1 Train Loss: 0.3218
Epoch 2 Step 51 Train Loss: 0.4017
Epoch 2: Train Overall MSE: 10885.2891 Validation Overall MSE: 2389.4768. 
Train Top 20 DE MSE: 13477.2275 Validation Top 20 DE MSE: 4192.8198. 
Epoch 3 Step 1 Train Loss: 0.3678
Epoch 3 Step 51 Train Loss: 0.3099
Epoch 3: Train Overall MSE: 340.9964 Validation Overall MSE: 71.5836. 
Train Top 20 DE MSE: 379.3022 Validation Top 20 DE MSE: 144.0482. 
Epoch 4 Step 1 Train Loss: 0.5715
Epoch 4 Step 51 Train Loss: 0.3708
Epoch 4: Train Overall MSE: 1182.0819 Validation Overall MSE: 230.8673. 
Train Top 20 DE MSE: 1738.7369 Validation Top 20 DE MSE: 1059.4172. 
Epoch 5 Step 1 Train Loss: 0.3964
Epoch 5 Step 51 Train Loss: 0.3236
Epoch 5: Train Overall MSE: 303.7704 Validation Overall MSE: 64.9360. 
Train Top 20 DE MSE: 371.0124 Validation Top 20 DE MSE: 270.8377. 
Epoch 6 Step 1 Train Loss: 0.2910
Epoch 6 Step 51 Train Loss: 0.3614
Epoch 6: Train Overall MSE: 7572.5522 Validation Overall MSE: 1466.8431. 
Train Top 20 DE MSE: 8427.4697 Validation Top 20 DE MSE: 5767.1582. 
Epoch 7 Step 1 Train Loss: 0.2742
Epoch 7 Step 51 Train Loss: 0.2724
Epoch 7: Train Overall MSE: 9.2776 Validation Overall MSE: 2.3223. 
Train Top 20 DE MSE: 12.5321 Validation Top 20 DE MSE: 10.7847. 
Epoch 8 Step 1 Train Loss: 0.2940
Epoch 8 Step 51 Train Loss: 0.2774
Epoch 8: Train Overall MSE: 9.7340 Validation Overall MSE: 2.3773. 
Train Top 20 DE MSE: 12.8392 Validation Top 20 DE MSE: 11.1979. 
Epoch 9 Step 1 Train Loss: 0.3611
Epoch 9 Step 51 Train Loss: 0.2892
Epoch 9: Train Overall MSE: 308.8103 Validation Overall MSE: 63.3177. 
Train Top 20 DE MSE: 370.7520 Validation Top 20 DE MSE: 259.2535. 
Epoch 10 Step 1 Train Loss: 0.3374
Epoch 10 Step 51 Train Loss: 0.3065
Epoch 10: Train Overall MSE: 3189.4768 Validation Overall MSE: 620.9521. 
Train Top 20 DE MSE: 3591.1040 Validation Top 20 DE MSE: 2571.6028. 
Epoch 11 Step 1 Train Loss: 0.3844
Epoch 11 Step 51 Train Loss: 0.2787
Epoch 11: Train Overall MSE: 269.6611 Validation Overall MSE: 55.4793. 
Train Top 20 DE MSE: 331.1585 Validation Top 20 DE MSE: 237.6008. 
Epoch 12 Step 1 Train Loss: 0.2980
Epoch 12 Step 51 Train Loss: 0.3063
Epoch 12: Train Overall MSE: 5722.2808 Validation Overall MSE: 1126.8295. 
Train Top 20 DE MSE: 6535.5776 Validation Top 20 DE MSE: 4650.0630. 
Epoch 13 Step 1 Train Loss: 0.3198
Epoch 13 Step 51 Train Loss: 0.3372
Epoch 13: Train Overall MSE: 59.4505 Validation Overall MSE: 12.7300. 
Train Top 20 DE MSE: 72.8155 Validation Top 20 DE MSE: 54.5568. 
Epoch 14 Step 1 Train Loss: 0.2901
Epoch 14 Step 51 Train Loss: 0.2817
Epoch 14: Train Overall MSE: 62.2365 Validation Overall MSE: 13.3349. 
Train Top 20 DE MSE: 77.5944 Validation Top 20 DE MSE: 57.6791. 
Epoch 15 Step 1 Train Loss: 0.2952
Epoch 15 Step 51 Train Loss: 0.3665
Epoch 15: Train Overall MSE: 26244.7422 Validation Overall MSE: 4990.6514. 
Train Top 20 DE MSE: 29874.3301 Validation Top 20 DE MSE: 20899.2461. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 62.2624
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 21.493576
test_combo_seen1_pearson: 0.12189859288655885
test_combo_seen1_mse_de: 55.450226
test_combo_seen1_pearson_de: 0.03277840142213194
test_combo_seen2_mse: 42.981243
test_combo_seen2_pearson: 0.062497071068675934
test_combo_seen2_mse_de: 141.4979
test_combo_seen2_pearson_de: -0.4293259179490271
test_unseen_single_mse: 11.685414
test_unseen_single_pearson: 0.1856752377051729
test_unseen_single_mse_de: 29.456804
test_unseen_single_pearson_de: 0.11331604275838067
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: -0.05552317408184357
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.625
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.07500000000000001
test_combo_seen1_mse_top20_de_non_dropout: 55.450226
test_combo_seen2_pearson_delta: -0.08621866009484044
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.9
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.050000000000000044
test_combo_seen2_mse_top20_de_non_dropout: 141.49791
test_unseen_single_pearson_delta: -0.016030073106301636
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.675
test_unseen_single_frac_sigma_below_1_non_dropout: 0.025000000000000022
test_unseen_single_mse_top20_de_non_dropout: 34.154087
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.005 MB of 0.018 MB uploadedwandb: | 0.005 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–â–„â–â–â–â–ƒâ–â–â–â–‚â–â–ƒâ–â–â–ˆ
wandb:                                             train_de_pearson â–…â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–â–„â–â–â–â–ƒâ–â–â–â–‚â–â–ƒâ–â–â–ˆ
wandb:                                                train_pearson â–„â–â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                training_loss â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–ˆâ–ƒâ–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–„â–‚â–‚â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–â–â–‚
wandb:                                                   val_de_mse â–â–‚â–â–â–â–ƒâ–â–â–â–‚â–â–ƒâ–â–â–ˆ
wandb:                                               val_de_pearson â–‡â–ˆâ–†â–â–‚â–â–†â–†â–‚â–â–‚â–â–ƒâ–ƒâ–
wandb:                                                      val_mse â–â–„â–â–â–â–ƒâ–â–â–â–‚â–â–ƒâ–â–â–ˆ
wandb:                                                  val_pearson â–‚â–‚â–‚â–‚â–‚â–â–ˆâ–ˆâ–‚â–â–‚â–â–„â–„â–
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.625
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.075
wandb:                                         test_combo_seen1_mse 21.49358
wandb:                                      test_combo_seen1_mse_de 55.45023
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 55.45023
wandb:                                     test_combo_seen1_pearson 0.1219
wandb:                                  test_combo_seen1_pearson_de 0.03278
wandb:                               test_combo_seen1_pearson_delta -0.05552
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.9
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.05
wandb:                                         test_combo_seen2_mse 42.98124
wandb:                                      test_combo_seen2_mse_de 141.49789
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 141.49791
wandb:                                     test_combo_seen2_pearson 0.0625
wandb:                                  test_combo_seen2_pearson_de -0.42933
wandb:                               test_combo_seen2_pearson_delta -0.08622
wandb:                                                  test_de_mse 62.26239
wandb:                                              test_de_pearson -0.02743
wandb:               test_frac_opposite_direction_top20_non_dropout 0.7
wandb:                          test_frac_sigma_below_1_non_dropout 0.05
wandb:                                                     test_mse 21.86785
wandb:                                test_mse_top20_de_non_dropout 64.14131
wandb:                                                 test_pearson 0.13553
wandb:                                           test_pearson_delta -0.04587
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.675
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.025
wandb:                                       test_unseen_single_mse 11.68541
wandb:                                    test_unseen_single_mse_de 29.4568
wandb:                  test_unseen_single_mse_top20_de_non_dropout 34.15409
wandb:                                   test_unseen_single_pearson 0.18568
wandb:                                test_unseen_single_pearson_de 0.11332
wandb:                             test_unseen_single_pearson_delta -0.01603
wandb:                                                 train_de_mse 29874.33008
wandb:                                             train_de_pearson 0.47358
wandb:                                                    train_mse 26244.74219
wandb:                                                train_pearson 0.67219
wandb:                                                training_loss 0.35706
wandb:                                                   val_de_mse 20899.24609
wandb:                                               val_de_pearson -0.71641
wandb:                                                      val_mse 4990.65137
wandb:                                                  val_pearson 0.01951
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406677_2_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/dw5txlqo
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_013814-dw5txlqo/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406677_2/splits/adamsonweissman2016_gsm2406677_2_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:1
combo_seen1:2
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_014154-mxii3gf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406677_2_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/mxii3gf1
Start Training...
Epoch 1 Step 1 Train Loss: 0.6588
Epoch 1 Step 51 Train Loss: 0.3309
Epoch 1: Train Overall MSE: 53.8163 Validation Overall MSE: 204.7056. 
Train Top 20 DE MSE: 179.2224 Validation Top 20 DE MSE: 376.4676. 
Epoch 2 Step 1 Train Loss: 0.3479
Epoch 2 Step 51 Train Loss: 0.3924
Epoch 2: Train Overall MSE: 377.0084 Validation Overall MSE: 1504.3372. 
Train Top 20 DE MSE: 483.9392 Validation Top 20 DE MSE: 2540.3171. 
Epoch 3 Step 1 Train Loss: 0.2985
Epoch 3 Step 51 Train Loss: 0.3065
Epoch 3: Train Overall MSE: 74518.7891 Validation Overall MSE: 217640.8438. 
Train Top 20 DE MSE: 86527.9141 Validation Top 20 DE MSE: 291451.5625. 
Epoch 4 Step 1 Train Loss: 0.3153
Epoch 4 Step 51 Train Loss: 0.2677
Epoch 4: Train Overall MSE: 391877.1562 Validation Overall MSE: 1132327.3750. 
Train Top 20 DE MSE: 429302.2188 Validation Top 20 DE MSE: 1173101.8750. 
Epoch 5 Step 1 Train Loss: 0.2926
Epoch 5 Step 51 Train Loss: 0.2883
Epoch 5: Train Overall MSE: 148.0879 Validation Overall MSE: 458.7065. 
Train Top 20 DE MSE: 170.0468 Validation Top 20 DE MSE: 613.4899. 
Epoch 6 Step 1 Train Loss: 0.2997
Epoch 6 Step 51 Train Loss: 0.2350
Epoch 6: Train Overall MSE: 58900.7969 Validation Overall MSE: 168977.7188. 
Train Top 20 DE MSE: 61771.3086 Validation Top 20 DE MSE: 170633.8438. 
Epoch 7 Step 1 Train Loss: 0.4111
Epoch 7 Step 51 Train Loss: 0.3214
Epoch 7: Train Overall MSE: 263.1537 Validation Overall MSE: 812.2899. 
Train Top 20 DE MSE: 281.9807 Validation Top 20 DE MSE: 1120.0828. 
Epoch 8 Step 1 Train Loss: 0.3160
Epoch 8 Step 51 Train Loss: 0.3085
Epoch 8: Train Overall MSE: 955.9367 Validation Overall MSE: 2843.3474. 
Train Top 20 DE MSE: 1022.0521 Validation Top 20 DE MSE: 3449.4114. 
Epoch 9 Step 1 Train Loss: 0.3037
Epoch 9 Step 51 Train Loss: 0.2902
Epoch 9: Train Overall MSE: 365.7068 Validation Overall MSE: 1119.4653. 
Train Top 20 DE MSE: 414.3473 Validation Top 20 DE MSE: 1399.6606. 
Epoch 10 Step 1 Train Loss: 0.3235
Epoch 10 Step 51 Train Loss: 0.3073
Epoch 10: Train Overall MSE: 53.5629 Validation Overall MSE: 176.3725. 
Train Top 20 DE MSE: 60.9747 Validation Top 20 DE MSE: 254.3403. 
Epoch 11 Step 1 Train Loss: 0.2784
Epoch 11 Step 51 Train Loss: 0.3239
Epoch 11: Train Overall MSE: 883.7295 Validation Overall MSE: 2687.8569. 
Train Top 20 DE MSE: 916.1705 Validation Top 20 DE MSE: 3362.0181. 
Epoch 12 Step 1 Train Loss: 0.2868
Epoch 12 Step 51 Train Loss: 0.2590
Epoch 12: Train Overall MSE: 14.1166 Validation Overall MSE: 49.4648. 
Train Top 20 DE MSE: 16.3531 Validation Top 20 DE MSE: 75.5291. 
Epoch 13 Step 1 Train Loss: 0.3152
Epoch 13 Step 51 Train Loss: 0.3143
Epoch 13: Train Overall MSE: 38.5980 Validation Overall MSE: 127.6973. 
Train Top 20 DE MSE: 45.3033 Validation Top 20 DE MSE: 181.7522. 
Epoch 14 Step 1 Train Loss: 0.2952
Epoch 14 Step 51 Train Loss: 0.3187
Epoch 14: Train Overall MSE: 2291.3877 Validation Overall MSE: 6813.3950. 
Train Top 20 DE MSE: 2365.7114 Validation Top 20 DE MSE: 7979.3311. 
Epoch 15 Step 1 Train Loss: 0.2875
Epoch 15 Step 51 Train Loss: 0.2510
Epoch 15: Train Overall MSE: 261.0378 Validation Overall MSE: 825.8530. 
Train Top 20 DE MSE: 266.6522 Validation Top 20 DE MSE: 1145.8270. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 563.5802
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 87.4959
test_combo_seen0_pearson: -0.028793537510852663
test_combo_seen0_mse_de: 364.06177
test_combo_seen0_pearson_de: -0.04920903412170399
test_combo_seen1_mse: 121.88786
test_combo_seen1_pearson: -0.004877357009168198
test_combo_seen1_mse_de: 1186.0125
test_combo_seen1_pearson_de: 0.44272905096839693
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 20.35233
test_unseen_single_pearson: 0.06371529116453141
test_unseen_single_mse_de: 40.907135
test_unseen_single_pearson_de: -0.22048359360531822
test_combo_seen0_pearson_delta: 0.055388555790999015
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.15
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.09999999999999998
test_combo_seen0_mse_top20_de_non_dropout: 364.06177
test_combo_seen1_pearson_delta: 0.2678406966619391
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.07500000000000001
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.04999999999999999
test_combo_seen1_mse_top20_de_non_dropout: 1186.0126
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.03650665248004409
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.32499999999999996
test_unseen_single_frac_sigma_below_1_non_dropout: 0.07500000000000001
test_unseen_single_mse_top20_de_non_dropout: 40.907135
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.002 MB of 0.018 MB uploadedwandb: | 0.002 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen0_mse â–
wandb:                                      test_combo_seen0_mse_de â–
wandb:                    test_combo_seen0_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen0_pearson â–
wandb:                                  test_combo_seen0_pearson_de â–
wandb:                               test_combo_seen0_pearson_delta â–
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–â–â–‚â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–‡â–†â–‚â–â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–â–â–‚â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–‚â–ƒâ–â–‚â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:                                                training_loss â–ƒâ–‚â–‚â–‚â–ˆâ–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:                                                   val_de_mse â–â–â–ƒâ–ˆâ–â–‚â–â–â–â–â–â–â–â–â–
wandb:                                               val_de_pearson â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                      val_mse â–â–â–‚â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–ˆâ–ƒâ–â–ˆâ–ƒâ–…â–‚â–ƒâ–ƒâ–„â–‚â–‡â–„â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.15
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.1
wandb:                                         test_combo_seen0_mse 87.4959
wandb:                                      test_combo_seen0_mse_de 364.06177
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 364.06177
wandb:                                     test_combo_seen0_pearson -0.02879
wandb:                                  test_combo_seen0_pearson_de -0.04921
wandb:                               test_combo_seen0_pearson_delta 0.05539
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.075
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.05
wandb:                                         test_combo_seen1_mse 121.88786
wandb:                                      test_combo_seen1_mse_de 1186.01245
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 1186.01257
wandb:                                     test_combo_seen1_pearson -0.00488
wandb:                                  test_combo_seen1_pearson_de 0.44273
wandb:                               test_combo_seen1_pearson_delta 0.26784
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 563.5802
wandb:                                              test_de_pearson 0.07906
wandb:               test_frac_opposite_direction_top20_non_dropout 0.19
wandb:                          test_frac_sigma_below_1_non_dropout 0.07
wandb:                                                     test_mse 74.39526
wandb:                                test_mse_top20_de_non_dropout 563.5802
wandb:                                                 test_pearson 0.01778
wandb:                                           test_pearson_delta 0.13282
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.325
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.075
wandb:                                       test_unseen_single_mse 20.35233
wandb:                                    test_unseen_single_mse_de 40.90714
wandb:                  test_unseen_single_mse_top20_de_non_dropout 40.90714
wandb:                                   test_unseen_single_pearson 0.06372
wandb:                                test_unseen_single_pearson_de -0.22048
wandb:                             test_unseen_single_pearson_delta 0.03651
wandb:                                                 train_de_mse 266.65216
wandb:                                             train_de_pearson 0.49861
wandb:                                                    train_mse 261.03775
wandb:                                                train_pearson 0.69149
wandb:                                                training_loss 0.28806
wandb:                                                   val_de_mse 1145.82703
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 825.85303
wandb:                                                  val_pearson -0.04226
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406677_2_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/mxii3gf1
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_014154-mxii3gf1/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406677_2/splits/adamsonweissman2016_gsm2406677_2_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_014502-yp5sex9z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406677_2_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/yp5sex9z
Start Training...
Epoch 1 Step 1 Train Loss: 0.7744
Epoch 1 Step 51 Train Loss: 0.4499
Epoch 1 Step 101 Train Loss: 0.4267
Epoch 1: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0384. 
Train Top 20 DE MSE: 0.0948 Validation Top 20 DE MSE: 0.2999. 
Epoch 2 Step 1 Train Loss: 0.4403
Epoch 2 Step 51 Train Loss: 0.3958
Epoch 2 Step 101 Train Loss: 0.4084
Epoch 2: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0429 Validation Top 20 DE MSE: 0.2919. 
Epoch 3 Step 1 Train Loss: 0.4086
Epoch 3 Step 51 Train Loss: 0.4277
Epoch 3 Step 101 Train Loss: 0.3666
Epoch 3: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0381. 
Train Top 20 DE MSE: 0.0405 Validation Top 20 DE MSE: 0.2989. 
Epoch 4 Step 1 Train Loss: 0.3548
Epoch 4 Step 51 Train Loss: 0.4088
Epoch 4 Step 101 Train Loss: 0.3665
Epoch 4: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0378. 
Train Top 20 DE MSE: 0.0271 Validation Top 20 DE MSE: 0.2945. 
Epoch 5 Step 1 Train Loss: 0.4475
Epoch 5 Step 51 Train Loss: 0.3495
Epoch 5 Step 101 Train Loss: 0.3725
Epoch 5: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0379. 
Train Top 20 DE MSE: 0.0260 Validation Top 20 DE MSE: 0.2961. 
Epoch 6 Step 1 Train Loss: 0.4548
Epoch 6 Step 51 Train Loss: 0.3484
Epoch 6 Step 101 Train Loss: 0.3875
Epoch 6: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0144 Validation Top 20 DE MSE: 0.2949. 
Epoch 7 Step 1 Train Loss: 0.4088
Epoch 7 Step 51 Train Loss: 0.3819
Epoch 7 Step 101 Train Loss: 0.3766
Epoch 7: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0379. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.2967. 
Epoch 8 Step 1 Train Loss: 0.3875
Epoch 8 Step 51 Train Loss: 0.4092
Epoch 8 Step 101 Train Loss: 0.3701
Epoch 8: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0373. 
Train Top 20 DE MSE: 0.0127 Validation Top 20 DE MSE: 0.2890. 
Epoch 9 Step 1 Train Loss: 0.3686
Epoch 9 Step 51 Train Loss: 0.3947
Epoch 9 Step 101 Train Loss: 0.3591
Epoch 9: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0379. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.2967. 
Epoch 10 Step 1 Train Loss: 0.4220
Epoch 10 Step 51 Train Loss: 0.4362
Epoch 10 Step 101 Train Loss: 0.3331
Epoch 10: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.2938. 
Epoch 11 Step 1 Train Loss: 0.3772
Epoch 11 Step 51 Train Loss: 0.4014
Epoch 11 Step 101 Train Loss: 0.3895
Epoch 11: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0374. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.2889. 
Epoch 12 Step 1 Train Loss: 0.4290
Epoch 12 Step 51 Train Loss: 0.4008
Epoch 12 Step 101 Train Loss: 0.4182
Epoch 12: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0104 Validation Top 20 DE MSE: 0.2933. 
Epoch 13 Step 1 Train Loss: 0.3920
Epoch 13 Step 51 Train Loss: 0.4394
Epoch 13 Step 101 Train Loss: 0.3510
Epoch 13: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.2938. 
Epoch 14 Step 1 Train Loss: 0.3507
Epoch 14 Step 51 Train Loss: 0.4460
Epoch 14 Step 101 Train Loss: 0.3750
Epoch 14: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0379. 
Train Top 20 DE MSE: 0.0117 Validation Top 20 DE MSE: 0.2958. 
Epoch 15 Step 1 Train Loss: 0.3415
Epoch 15 Step 51 Train Loss: 0.3533
Epoch 15 Step 101 Train Loss: 0.3580
Epoch 15: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.2934. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1373
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 0.0022648037
test_combo_seen1_pearson: 0.9970449648567663
test_combo_seen1_mse_de: 0.1661027
test_combo_seen1_pearson_de: 0.9838268668080208
test_combo_seen2_mse: 0.00235723
test_combo_seen2_pearson: 0.9969232359196304
test_combo_seen2_mse_de: 0.18422654
test_combo_seen2_pearson_de: 0.9783303601715596
test_unseen_single_mse: 0.016278092
test_unseen_single_pearson: 0.9797965024110581
test_unseen_single_mse_de: 0.085140154
test_unseen_single_pearson_de: 0.49765884788136766
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: 0.6048208076857704
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.025
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.975
test_combo_seen1_mse_top20_de_non_dropout: 0.1661027
test_combo_seen2_pearson_delta: 0.7595777262530374
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.0
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.75
test_combo_seen2_mse_top20_de_non_dropout: 0.18422654
test_unseen_single_pearson_delta: 0.07754036340258762
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.55
test_unseen_single_frac_sigma_below_1_non_dropout: 0.575
test_unseen_single_mse_top20_de_non_dropout: 0.18769237
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.002 MB of 0.019 MB uploadedwandb: | 0.019 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–…â–…â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–‡â–ˆâ–„â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                train_pearson â–‚â–â–…â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                training_loss â–ˆâ–…â–ƒâ–„â–„â–„â–ƒâ–‚â–„â–â–…â–‡â–„â–…â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–â–„â–„â–ƒâ–„â–ƒâ–ƒâ–„â–â–‚â–ƒâ–ƒâ–‚
wandb:                                                   val_de_mse â–ˆâ–ƒâ–‡â–…â–†â–…â–†â–â–†â–„â–â–„â–„â–…â–„
wandb:                                               val_de_pearson â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                      val_mse â–ˆâ–ƒâ–†â–„â–…â–ƒâ–„â–â–…â–„â–â–ƒâ–„â–„â–ƒ
wandb:                                                  val_pearson â–â–†â–ƒâ–…â–„â–†â–„â–ˆâ–„â–…â–ˆâ–†â–…â–„â–†
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.025
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.975
wandb:                                         test_combo_seen1_mse 0.00226
wandb:                                      test_combo_seen1_mse_de 0.1661
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.1661
wandb:                                     test_combo_seen1_pearson 0.99704
wandb:                                  test_combo_seen1_pearson_de 0.98383
wandb:                               test_combo_seen1_pearson_delta 0.60482
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.0
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.75
wandb:                                         test_combo_seen2_mse 0.00236
wandb:                                      test_combo_seen2_mse_de 0.18423
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.18423
wandb:                                     test_combo_seen2_pearson 0.99692
wandb:                                  test_combo_seen2_pearson_de 0.97833
wandb:                               test_combo_seen2_pearson_delta 0.75958
wandb:                                                  test_de_mse 0.13734
wandb:                                              test_de_pearson 0.78826
wandb:               test_frac_opposite_direction_top20_non_dropout 0.23
wandb:                          test_frac_sigma_below_1_non_dropout 0.77
wandb:                                                     test_mse 0.00789
wandb:                                test_mse_top20_de_non_dropout 0.17836
wandb:                                                 test_pearson 0.99012
wandb:                                           test_pearson_delta 0.42486
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.55
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.575
wandb:                                       test_unseen_single_mse 0.01628
wandb:                                    test_unseen_single_mse_de 0.08514
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.18769
wandb:                                   test_unseen_single_pearson 0.9798
wandb:                                test_unseen_single_pearson_de 0.49766
wandb:                             test_unseen_single_pearson_delta 0.07754
wandb:                                                 train_de_mse 0.01252
wandb:                                             train_de_pearson 0.99794
wandb:                                                    train_mse 0.00141
wandb:                                                train_pearson 0.9982
wandb:                                                training_loss 0.34501
wandb:                                                   val_de_mse 0.29341
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.03769
wandb:                                                  val_pearson 0.95282
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406677_2_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/yp5sex9z
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_014502-yp5sex9z/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406677_2/splits/adamsonweissman2016_gsm2406677_2_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_014916-9agzn6c9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406677_2_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/9agzn6c9
Start Training...
Epoch 1 Step 1 Train Loss: 0.6816
Epoch 1 Step 51 Train Loss: 0.4461
Epoch 1 Step 101 Train Loss: 0.4160
Epoch 1: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0317. 
Train Top 20 DE MSE: 0.1422 Validation Top 20 DE MSE: 0.1206. 
Epoch 2 Step 1 Train Loss: 0.3836
Epoch 2 Step 51 Train Loss: 0.3819
Epoch 2 Step 101 Train Loss: 0.3808
Epoch 2: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.1012 Validation Top 20 DE MSE: 0.1204. 
Epoch 3 Step 1 Train Loss: 0.4509
Epoch 3 Step 51 Train Loss: 0.3948
Epoch 3 Step 101 Train Loss: 0.4269
Epoch 3: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0304. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.1202. 
Epoch 4 Step 1 Train Loss: 0.4448
Epoch 4 Step 51 Train Loss: 0.3646
Epoch 4 Step 101 Train Loss: 0.3684
Epoch 4: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.1178. 
Epoch 5 Step 1 Train Loss: 0.3768
Epoch 5 Step 51 Train Loss: 0.3648
Epoch 5 Step 101 Train Loss: 0.4277
Epoch 5: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0262 Validation Top 20 DE MSE: 0.1169. 
Epoch 6 Step 1 Train Loss: 0.3853
Epoch 6 Step 51 Train Loss: 0.3579
Epoch 6 Step 101 Train Loss: 0.4064
Epoch 6: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0184 Validation Top 20 DE MSE: 0.1160. 
Epoch 7 Step 1 Train Loss: 0.3875
Epoch 7 Step 51 Train Loss: 0.4086
Epoch 7 Step 101 Train Loss: 0.4317
Epoch 7: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0146 Validation Top 20 DE MSE: 0.1180. 
Epoch 8 Step 1 Train Loss: 0.4226
Epoch 8 Step 51 Train Loss: 0.3748
Epoch 8 Step 101 Train Loss: 0.4542
Epoch 8: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0087 Validation Top 20 DE MSE: 0.1177. 
Epoch 9 Step 1 Train Loss: 0.3859
Epoch 9 Step 51 Train Loss: 0.3697
Epoch 9 Step 101 Train Loss: 0.3565
Epoch 9: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0101 Validation Top 20 DE MSE: 0.1174. 
Epoch 10 Step 1 Train Loss: 0.4044
Epoch 10 Step 51 Train Loss: 0.3757
Epoch 10 Step 101 Train Loss: 0.4553
Epoch 10: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0116 Validation Top 20 DE MSE: 0.1171. 
Epoch 11 Step 1 Train Loss: 0.4148
Epoch 11 Step 51 Train Loss: 0.4107
Epoch 11 Step 101 Train Loss: 0.4059
Epoch 11: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0120 Validation Top 20 DE MSE: 0.1187. 
Epoch 12 Step 1 Train Loss: 0.4635
Epoch 12 Step 51 Train Loss: 0.4601
Epoch 12 Step 101 Train Loss: 0.4131
Epoch 12: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0123 Validation Top 20 DE MSE: 0.1179. 
Epoch 13 Step 1 Train Loss: 0.3754
Epoch 13 Step 51 Train Loss: 0.3540
Epoch 13 Step 101 Train Loss: 0.3738
Epoch 13: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0103 Validation Top 20 DE MSE: 0.1178. 
Epoch 14 Step 1 Train Loss: 0.3782
Epoch 14 Step 51 Train Loss: 0.4446
Epoch 14 Step 101 Train Loss: 0.3616
Epoch 14: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0126 Validation Top 20 DE MSE: 0.1175. 
Epoch 15 Step 1 Train Loss: 0.3476
Epoch 15 Step 51 Train Loss: 0.3852
Epoch 15 Step 101 Train Loss: 0.3338
Epoch 15: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0112 Validation Top 20 DE MSE: 0.1174. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.2148
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 0.0026787228
test_combo_seen1_pearson: 0.9965542672304748
test_combo_seen1_mse_de: 0.10911508
test_combo_seen1_pearson_de: 0.988678544238116
test_combo_seen2_mse: 0.00477212
test_combo_seen2_pearson: 0.9938824277423826
test_combo_seen2_mse_de: 0.46845227
test_combo_seen2_pearson_de: 0.9425601386812668
test_unseen_single_mse: 0.020503065
test_unseen_single_pearson: 0.9744522922835486
test_unseen_single_mse_de: 0.19371666
test_unseen_single_pearson_de: 0.49728196595878466
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: 0.6313922850097495
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.025
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.975
test_combo_seen1_mse_top20_de_non_dropout: 0.109115064
test_combo_seen2_pearson_delta: 0.4306797024363511
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.2
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.5
test_combo_seen2_mse_top20_de_non_dropout: 0.46845227
test_unseen_single_pearson_delta: -0.16760077058012796
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.65
test_unseen_single_frac_sigma_below_1_non_dropout: 0.525
test_unseen_single_mse_top20_de_non_dropout: 0.25624713
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.003 MB of 0.019 MB uploadedwandb: | 0.003 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen1_mse â–
wandb:                                      test_combo_seen1_mse_de â–
wandb:                    test_combo_seen1_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen1_pearson â–
wandb:                                  test_combo_seen1_pearson_de â–
wandb:                               test_combo_seen1_pearson_delta â–
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout â–
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout â–
wandb:                                         test_combo_seen2_mse â–
wandb:                                      test_combo_seen2_mse_de â–
wandb:                    test_combo_seen2_mse_top20_de_non_dropout â–
wandb:                                     test_combo_seen2_pearson â–
wandb:                                  test_combo_seen2_pearson_de â–
wandb:                               test_combo_seen2_pearson_delta â–
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–†â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–…â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–†â–„â–ƒâ–ˆâ–‚â–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–„â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–‚
wandb:                                                   val_de_mse â–ˆâ–ˆâ–‡â–„â–‚â–â–„â–„â–ƒâ–ƒâ–…â–„â–„â–ƒâ–ƒ
wandb:                                               val_de_pearson â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                      val_mse â–ˆâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                                  val_pearson â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.025
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.975
wandb:                                         test_combo_seen1_mse 0.00268
wandb:                                      test_combo_seen1_mse_de 0.10912
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.10912
wandb:                                     test_combo_seen1_pearson 0.99655
wandb:                                  test_combo_seen1_pearson_de 0.98868
wandb:                               test_combo_seen1_pearson_delta 0.63139
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.2
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.5
wandb:                                         test_combo_seen2_mse 0.00477
wandb:                                      test_combo_seen2_mse_de 0.46845
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.46845
wandb:                                     test_combo_seen2_pearson 0.99388
wandb:                                  test_combo_seen2_pearson_de 0.94256
wandb:                               test_combo_seen2_pearson_delta 0.43068
wandb:                                                  test_de_mse 0.21482
wandb:                                              test_de_pearson 0.7829
wandb:               test_frac_opposite_direction_top20_non_dropout 0.31
wandb:                          test_frac_sigma_below_1_non_dropout 0.7
wandb:                                                     test_mse 0.01023
wandb:                                test_mse_top20_de_non_dropout 0.23984
wandb:                                                 test_pearson 0.98718
wandb:                                           test_pearson_delta 0.27165
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.65
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.525
wandb:                                       test_unseen_single_mse 0.0205
wandb:                                    test_unseen_single_mse_de 0.19372
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.25625
wandb:                                   test_unseen_single_pearson 0.97445
wandb:                                test_unseen_single_pearson_de 0.49728
wandb:                             test_unseen_single_pearson_delta -0.1676
wandb:                                                 train_de_mse 0.0112
wandb:                                             train_de_pearson 0.99816
wandb:                                                    train_mse 0.00133
wandb:                                                train_pearson 0.99831
wandb:                                                training_loss 0.38773
wandb:                                                   val_de_mse 0.11739
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.03055
wandb:                                                  val_pearson 0.96181
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406677_2_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/9agzn6c9
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_014916-9agzn6c9/logs
Found local copy...
Creating pyg object for each cell in the data...
Creating dataset file...
  0%|          | 0/90 [00:00<?, ?it/s]  1%|          | 1/90 [00:00<01:18,  1.14it/s]  2%|â–         | 2/90 [01:33<1:20:29, 54.88s/it]  3%|â–Ž         | 3/90 [04:25<2:37:08, 108.38s/it]  4%|â–         | 4/90 [05:58<2:26:45, 102.39s/it]  6%|â–Œ         | 5/90 [07:26<2:17:41, 97.19s/it]   7%|â–‹         | 6/90 [10:52<3:07:32, 133.96s/it]  8%|â–Š         | 7/90 [11:35<2:24:10, 104.22s/it]  9%|â–‰         | 8/90 [13:10<2:18:23, 101.26s/it] 10%|â–ˆ         | 9/90 [15:03<2:22:02, 105.21s/it] 11%|â–ˆ         | 10/90 [16:57<2:23:38, 107.73s/it] 12%|â–ˆâ–        | 11/90 [18:35<2:17:49, 104.67s/it] 13%|â–ˆâ–Ž        | 12/90 [20:22<2:17:19, 105.64s/it] 14%|â–ˆâ–        | 13/90 [24:41<3:14:53, 151.86s/it] 16%|â–ˆâ–Œ        | 14/90 [26:14<2:49:55, 134.16s/it] 17%|â–ˆâ–‹        | 15/90 [29:57<3:21:02, 160.83s/it] 18%|â–ˆâ–Š        | 16/90 [31:38<2:56:29, 143.10s/it] 19%|â–ˆâ–‰        | 17/90 [34:16<2:59:30, 147.54s/it] 20%|â–ˆâ–ˆ        | 18/90 [35:08<2:22:32, 118.79s/it] 21%|â–ˆâ–ˆ        | 19/90 [36:13<2:01:17, 102.50s/it] 22%|â–ˆâ–ˆâ–       | 20/90 [37:38<1:53:29, 97.28s/it]  23%|â–ˆâ–ˆâ–Ž       | 21/90 [39:06<1:48:41, 94.52s/it] 24%|â–ˆâ–ˆâ–       | 22/90 [40:17<1:39:18, 87.62s/it] 26%|â–ˆâ–ˆâ–Œ       | 23/90 [42:17<1:48:28, 97.15s/it] 27%|â–ˆâ–ˆâ–‹       | 24/90 [46:17<2:33:59, 139.99s/it] 28%|â–ˆâ–ˆâ–Š       | 25/90 [47:26<2:08:46, 118.87s/it] 29%|â–ˆâ–ˆâ–‰       | 26/90 [48:52<1:56:02, 108.79s/it] 30%|â–ˆâ–ˆâ–ˆ       | 27/90 [50:15<1:46:16, 101.21s/it] 31%|â–ˆâ–ˆâ–ˆ       | 28/90 [51:18<1:32:49, 89.82s/it]  32%|â–ˆâ–ˆâ–ˆâ–      | 29/90 [52:46<1:30:40, 89.19s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 30/90 [54:08<1:26:52, 86.88s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 31/90 [55:31<1:24:28, 85.90s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 32/90 [56:37<1:17:18, 79.97s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 33/90 [58:10<1:19:38, 83.82s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 34/90 [59:08<1:11:01, 76.09s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 35/90 [59:31<55:05, 60.10s/it]   40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 36/90 [1:00:40<56:37, 62.91s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 37/90 [1:02:08<1:02:12, 70.42s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 38/90 [1:04:12<1:14:53, 86.42s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 39/90 [1:05:23<1:09:31, 81.80s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/90 [1:06:38<1:06:18, 79.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 41/90 [1:07:19<55:36, 68.10s/it]   47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 42/90 [1:08:26<54:08, 67.67s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 43/90 [1:09:37<53:48, 68.70s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 44/90 [1:11:11<58:31, 76.34s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 45/90 [1:12:38<59:38, 79.52s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 46/90 [1:14:14<1:01:54, 84.42s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 47/90 [1:15:27<58:09, 81.15s/it]   53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 48/90 [1:17:20<1:03:32, 90.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/90 [1:19:00<1:03:48, 93.37s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/90 [1:20:35<1:02:39, 93.98s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 51/90 [1:22:52<1:09:23, 106.75s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 52/90 [1:24:31<1:06:10, 104.48s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 53/90 [1:26:03<1:02:02, 100.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 54/90 [1:27:23<56:47, 94.65s/it]    61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 55/90 [1:28:39<51:58, 89.10s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 56/90 [1:29:39<45:27, 80.22s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 57/90 [1:31:13<46:20, 84.25s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 58/90 [1:32:39<45:18, 84.96s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 59/90 [1:33:43<40:41, 78.76s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 60/90 [1:34:58<38:43, 77.45s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 61/90 [1:36:52<42:46, 88.51s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 62/90 [1:39:40<52:23, 112.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 63/90 [1:41:20<48:52, 108.60s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 64/90 [1:42:35<42:39, 98.43s/it]  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 65/90 [1:45:32<50:53, 122.14s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 66/90 [1:46:45<42:57, 107.39s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 67/90 [1:48:09<38:28, 100.38s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 68/90 [1:49:26<34:13, 93.34s/it]  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 69/90 [1:50:21<28:37, 81.80s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 70/90 [1:51:52<28:09, 84.49s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 71/90 [1:53:26<27:40, 87.40s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 72/90 [1:55:07<27:26, 91.46s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 73/90 [1:56:37<25:49, 91.13s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 74/90 [1:57:28<21:02, 78.92s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 75/90 [1:58:26<18:12, 72.83s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 76/90 [1:58:58<14:09, 60.65s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 77/90 [2:00:08<13:45, 63.48s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 78/90 [2:01:12<12:41, 63.47s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 79/90 [2:02:51<13:35, 74.14s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 80/90 [2:03:25<10:20, 62.09s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 81/90 [2:04:36<09:44, 64.89s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 82/90 [2:05:16<07:38, 57.27s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 83/90 [2:05:28<05:05, 43.70s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 84/90 [2:06:41<05:14, 52.41s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 85/90 [2:08:15<05:25, 65.07s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 86/90 [2:08:24<03:12, 48.18s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 87/90 [2:08:26<01:42, 34.30s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 88/90 [2:08:30<00:50, 25.26s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 89/90 [2:08:39<00:20, 20.34s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [2:08:40<00:00, 14.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [2:08:40<00:00, 85.78s/it]
Done!
Saving new dataset pyg object at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406681_3/data_pyg/cell_graphs.pkl
Done!
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406681_3/splits/adamsonweissman2016_gsm2406681_3_simulation_1_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:23
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_040350-ctqvuedm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406681_3_split1
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/ctqvuedm
  0%|          | 0/3555 [00:00<?, ?it/s]  0%|          | 6/3555 [00:00<01:04, 55.04it/s]  0%|          | 14/3555 [00:00<00:52, 67.88it/s]  1%|          | 22/3555 [00:00<00:48, 73.12it/s]  1%|          | 32/3555 [00:00<00:44, 79.66it/s]  1%|          | 42/3555 [00:00<00:43, 81.43it/s]  1%|â–         | 53/3555 [00:00<00:38, 90.47it/s]  2%|â–         | 63/3555 [00:00<00:40, 86.44it/s]  2%|â–         | 73/3555 [00:00<00:38, 90.00it/s]  2%|â–         | 83/3555 [00:00<00:38, 90.71it/s]  3%|â–Ž         | 93/3555 [00:01<00:39, 88.59it/s]  3%|â–Ž         | 102/3555 [00:02<03:41, 15.59it/s]  3%|â–Ž         | 109/3555 [00:03<03:07, 18.42it/s]  3%|â–Ž         | 115/3555 [00:03<02:39, 21.50it/s]  3%|â–Ž         | 121/3555 [00:03<02:16, 25.19it/s]  5%|â–Œ         | 187/3555 [00:03<00:31, 105.93it/s]  6%|â–Œ         | 210/3555 [00:03<00:35, 94.15it/s]   6%|â–‹         | 229/3555 [00:03<00:38, 87.48it/s]  7%|â–‹         | 244/3555 [00:04<00:39, 84.15it/s]  7%|â–‹         | 257/3555 [00:04<00:38, 85.04it/s]  8%|â–Š         | 269/3555 [00:04<00:38, 85.68it/s]  8%|â–Š         | 280/3555 [00:04<00:37, 87.82it/s]  8%|â–Š         | 291/3555 [00:04<00:36, 89.55it/s]  8%|â–Š         | 302/3555 [00:04<00:36, 88.20it/s]  9%|â–‰         | 312/3555 [00:04<00:36, 88.49it/s]  9%|â–‰         | 322/3555 [00:05<00:36, 88.24it/s]  9%|â–‰         | 332/3555 [00:05<00:36, 89.43it/s] 10%|â–‰         | 342/3555 [00:05<00:36, 86.96it/s] 10%|â–‰         | 351/3555 [00:05<00:37, 85.75it/s] 10%|â–ˆ         | 361/3555 [00:05<00:36, 87.85it/s] 10%|â–ˆ         | 370/3555 [00:05<00:36, 86.71it/s] 11%|â–ˆ         | 379/3555 [00:05<00:36, 87.01it/s] 11%|â–ˆ         | 388/3555 [00:05<00:36, 85.76it/s] 11%|â–ˆ         | 397/3555 [00:05<00:36, 85.58it/s] 11%|â–ˆâ–        | 406/3555 [00:05<00:36, 86.62it/s] 12%|â–ˆâ–        | 415/3555 [00:06<00:36, 85.68it/s] 12%|â–ˆâ–        | 424/3555 [00:06<00:37, 83.43it/s] 12%|â–ˆâ–        | 434/3555 [00:06<00:35, 87.69it/s] 12%|â–ˆâ–        | 443/3555 [00:06<00:35, 87.23it/s] 13%|â–ˆâ–Ž        | 452/3555 [00:06<00:36, 84.79it/s] 13%|â–ˆâ–Ž        | 461/3555 [00:06<00:36, 85.10it/s] 13%|â–ˆâ–Ž        | 471/3555 [00:06<00:35, 87.84it/s] 14%|â–ˆâ–Ž        | 480/3555 [00:06<00:36, 85.25it/s] 14%|â–ˆâ–        | 489/3555 [00:06<00:36, 84.54it/s] 14%|â–ˆâ–        | 498/3555 [00:07<00:35, 85.21it/s] 14%|â–ˆâ–        | 507/3555 [00:07<00:35, 86.19it/s] 15%|â–ˆâ–        | 517/3555 [00:07<00:34, 88.41it/s] 15%|â–ˆâ–        | 526/3555 [00:07<00:35, 84.93it/s] 15%|â–ˆâ–Œ        | 536/3555 [00:07<00:34, 87.27it/s] 15%|â–ˆâ–Œ        | 545/3555 [00:07<00:34, 87.19it/s] 16%|â–ˆâ–Œ        | 554/3555 [00:07<00:35, 85.03it/s] 16%|â–ˆâ–Œ        | 564/3555 [00:07<00:35, 84.82it/s] 16%|â–ˆâ–Œ        | 573/3555 [00:07<00:34, 85.50it/s] 16%|â–ˆâ–‹        | 583/3555 [00:08<00:33, 89.44it/s] 17%|â–ˆâ–‹        | 592/3555 [00:08<00:33, 87.67it/s] 17%|â–ˆâ–‹        | 601/3555 [00:08<00:34, 84.57it/s] 17%|â–ˆâ–‹        | 611/3555 [00:08<00:34, 84.48it/s] 17%|â–ˆâ–‹        | 621/3555 [00:08<00:33, 86.71it/s] 18%|â–ˆâ–Š        | 630/3555 [00:08<00:34, 84.80it/s] 18%|â–ˆâ–Š        | 640/3555 [00:08<00:34, 84.24it/s] 18%|â–ˆâ–Š        | 649/3555 [00:08<00:34, 83.95it/s] 19%|â–ˆâ–Š        | 658/3555 [00:08<00:34, 84.82it/s] 19%|â–ˆâ–‰        | 667/3555 [00:09<00:34, 84.23it/s] 19%|â–ˆâ–‰        | 676/3555 [00:09<00:33, 85.11it/s] 19%|â–ˆâ–‰        | 685/3555 [00:09<00:33, 85.28it/s] 20%|â–ˆâ–‰        | 695/3555 [00:09<00:32, 87.20it/s] 20%|â–ˆâ–‰        | 704/3555 [00:09<00:32, 87.69it/s] 20%|â–ˆâ–ˆ        | 713/3555 [00:09<00:34, 83.55it/s] 20%|â–ˆâ–ˆ        | 723/3555 [00:09<00:33, 83.89it/s] 21%|â–ˆâ–ˆ        | 733/3555 [00:09<00:32, 88.19it/s] 21%|â–ˆâ–ˆ        | 742/3555 [00:09<00:33, 83.82it/s] 21%|â–ˆâ–ˆ        | 751/3555 [00:10<00:33, 84.61it/s] 21%|â–ˆâ–ˆâ–       | 761/3555 [00:10<00:32, 84.84it/s] 22%|â–ˆâ–ˆâ–       | 771/3555 [00:10<00:32, 86.63it/s] 22%|â–ˆâ–ˆâ–       | 780/3555 [00:10<00:32, 84.19it/s] 22%|â–ˆâ–ˆâ–       | 789/3555 [00:10<00:33, 83.58it/s] 22%|â–ˆâ–ˆâ–       | 799/3555 [00:10<00:32, 83.79it/s] 23%|â–ˆâ–ˆâ–Ž       | 809/3555 [00:10<00:31, 87.61it/s] 23%|â–ˆâ–ˆâ–Ž       | 818/3555 [00:10<00:31, 86.19it/s] 23%|â–ˆâ–ˆâ–Ž       | 827/3555 [00:10<00:32, 83.34it/s] 24%|â–ˆâ–ˆâ–Ž       | 836/3555 [00:11<00:32, 83.72it/s] 24%|â–ˆâ–ˆâ–       | 845/3555 [00:11<00:32, 83.82it/s] 24%|â–ˆâ–ˆâ–       | 854/3555 [00:11<00:31, 85.15it/s] 24%|â–ˆâ–ˆâ–       | 863/3555 [00:11<00:31, 84.96it/s] 25%|â–ˆâ–ˆâ–       | 872/3555 [00:11<00:31, 84.60it/s] 25%|â–ˆâ–ˆâ–       | 882/3555 [00:11<00:30, 88.24it/s] 25%|â–ˆâ–ˆâ–Œ       | 891/3555 [00:11<00:30, 86.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 900/3555 [00:11<00:31, 85.30it/s] 26%|â–ˆâ–ˆâ–Œ       | 909/3555 [00:11<00:30, 86.29it/s] 26%|â–ˆâ–ˆâ–Œ       | 918/3555 [00:11<00:30, 85.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 927/3555 [00:12<00:31, 83.15it/s] 26%|â–ˆâ–ˆâ–‹       | 936/3555 [00:12<00:31, 83.74it/s] 27%|â–ˆâ–ˆâ–‹       | 945/3555 [00:12<00:31, 83.81it/s] 27%|â–ˆâ–ˆâ–‹       | 954/3555 [00:12<00:30, 85.03it/s] 27%|â–ˆâ–ˆâ–‹       | 963/3555 [00:12<00:30, 84.81it/s] 27%|â–ˆâ–ˆâ–‹       | 972/3555 [00:12<00:30, 85.15it/s] 28%|â–ˆâ–ˆâ–Š       | 982/3555 [00:12<00:29, 86.54it/s] 28%|â–ˆâ–ˆâ–Š       | 992/3555 [00:12<00:29, 85.56it/s] 28%|â–ˆâ–ˆâ–Š       | 1001/3555 [00:12<00:29, 86.19it/s] 28%|â–ˆâ–ˆâ–Š       | 1011/3555 [00:13<00:28, 89.07it/s] 29%|â–ˆâ–ˆâ–Š       | 1020/3555 [00:13<00:28, 88.12it/s] 29%|â–ˆâ–ˆâ–‰       | 1029/3555 [00:13<00:28, 87.68it/s] 29%|â–ˆâ–ˆâ–‰       | 1038/3555 [00:13<00:28, 87.02it/s] 29%|â–ˆâ–ˆâ–‰       | 1047/3555 [00:13<00:29, 86.45it/s] 30%|â–ˆâ–ˆâ–‰       | 1056/3555 [00:13<00:29, 84.98it/s] 30%|â–ˆâ–ˆâ–‰       | 1065/3555 [00:13<00:29, 84.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1074/3555 [00:13<00:29, 84.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 1084/3555 [00:13<00:27, 88.39it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1093/3555 [00:14<00:28, 86.35it/s] 31%|â–ˆâ–ˆâ–ˆ       | 1102/3555 [00:14<00:28, 86.81it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 1111/3555 [00:14<00:29, 83.42it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1121/3555 [00:14<00:28, 86.68it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1130/3555 [00:14<00:28, 84.41it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1140/3555 [00:14<00:27, 86.79it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 1149/3555 [00:14<00:27, 86.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1158/3555 [00:14<00:28, 84.96it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1167/3555 [00:14<00:28, 84.83it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1176/3555 [00:14<00:27, 85.32it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1186/3555 [00:15<00:26, 88.29it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1195/3555 [00:15<00:26, 87.66it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1204/3555 [00:15<00:27, 84.79it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1214/3555 [00:15<00:26, 87.08it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 1223/3555 [00:15<00:27, 86.19it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1232/3555 [00:15<00:27, 83.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 1242/3555 [00:15<00:27, 83.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1251/3555 [00:15<00:27, 83.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1260/3555 [00:15<00:27, 84.22it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1269/3555 [00:16<00:26, 84.72it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1278/3555 [00:16<00:26, 85.72it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1288/3555 [00:16<00:26, 85.56it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 1297/3555 [00:16<00:26, 85.37it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1308/3555 [00:16<00:26, 86.41it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1317/3555 [00:16<00:25, 86.29it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1326/3555 [00:16<00:25, 86.33it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1336/3555 [00:16<00:24, 89.27it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1345/3555 [00:16<00:25, 88.09it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1354/3555 [00:17<00:24, 88.15it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1363/3555 [00:17<00:25, 87.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 1372/3555 [00:17<00:26, 83.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1381/3555 [00:17<00:25, 85.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1390/3555 [00:17<00:25, 84.51it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1399/3555 [00:17<00:25, 84.93it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1409/3555 [00:17<00:25, 85.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 1419/3555 [00:17<00:25, 85.26it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1428/3555 [00:17<00:24, 85.22it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1438/3555 [00:18<00:24, 87.68it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1447/3555 [00:18<00:25, 84.29it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1456/3555 [00:18<00:24, 85.79it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1466/3555 [00:18<00:23, 87.90it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1475/3555 [00:18<00:23, 87.43it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1484/3555 [00:18<00:23, 87.73it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1493/3555 [00:18<00:23, 87.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1502/3555 [00:18<00:24, 84.00it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1511/3555 [00:18<00:24, 85.04it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1521/3555 [00:19<00:24, 84.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1530/3555 [00:19<00:23, 85.80it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1540/3555 [00:19<00:22, 88.24it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1549/3555 [00:19<00:23, 84.26it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1558/3555 [00:19<00:23, 85.20it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1567/3555 [00:19<00:23, 85.32it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1576/3555 [00:19<00:23, 85.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1585/3555 [00:19<00:22, 85.99it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1595/3555 [00:19<00:22, 88.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1604/3555 [00:19<00:22, 85.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1613/3555 [00:20<00:22, 84.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1622/3555 [00:20<00:24, 80.11it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1631/3555 [00:20<00:24, 78.29it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1639/3555 [00:20<00:25, 76.61it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1648/3555 [00:20<00:24, 79.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1658/3555 [00:20<00:22, 84.23it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1667/3555 [00:20<00:22, 84.71it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1676/3555 [00:20<00:22, 82.90it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1685/3555 [00:20<00:22, 84.56it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1695/3555 [00:21<00:21, 87.66it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1704/3555 [00:21<00:21, 86.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1713/3555 [00:21<00:21, 86.02it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1722/3555 [00:21<00:21, 86.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1731/3555 [00:21<00:21, 84.82it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1742/3555 [00:21<00:20, 87.88it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1751/3555 [00:21<00:20, 88.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1761/3555 [00:21<00:19, 90.37it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1771/3555 [00:21<00:20, 88.10it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1780/3555 [00:22<00:20, 85.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1790/3555 [00:22<00:20, 86.89it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1799/3555 [00:22<00:20, 83.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1809/3555 [00:22<00:20, 84.01it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1819/3555 [00:22<00:20, 86.20it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1828/3555 [00:22<00:20, 84.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1838/3555 [00:22<00:20, 84.65it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1847/3555 [00:22<00:20, 84.10it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1856/3555 [00:22<00:20, 84.39it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1866/3555 [00:23<00:20, 83.88it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1875/3555 [00:23<00:19, 84.66it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1885/3555 [00:23<00:19, 87.73it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1894/3555 [00:23<00:19, 86.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1903/3555 [00:23<00:19, 86.87it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1912/3555 [00:23<00:19, 85.94it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1921/3555 [00:23<00:19, 85.22it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1930/3555 [00:23<00:19, 83.83it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1940/3555 [00:23<00:18, 86.14it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1949/3555 [00:24<00:19, 83.47it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1959/3555 [00:24<00:18, 87.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1968/3555 [00:24<00:19, 83.23it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1977/3555 [00:24<00:18, 84.04it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1987/3555 [00:24<00:18, 86.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1996/3555 [00:24<00:18, 85.56it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2005/3555 [00:24<00:18, 83.78it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2014/3555 [00:24<00:18, 83.78it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2023/3555 [00:24<00:18, 84.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2032/3555 [00:25<00:18, 84.35it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2041/3555 [00:25<00:17, 84.37it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2050/3555 [00:25<00:17, 85.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2060/3555 [00:25<00:17, 85.67it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2070/3555 [00:25<00:16, 87.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2079/3555 [00:25<00:17, 85.53it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2088/3555 [00:25<00:17, 84.84it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2097/3555 [00:25<00:17, 84.68it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2106/3555 [00:25<00:16, 85.53it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2116/3555 [00:25<00:16, 86.97it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2125/3555 [00:26<00:16, 84.35it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2135/3555 [00:26<00:16, 87.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2144/3555 [00:26<00:16, 83.47it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2154/3555 [00:26<00:16, 84.30it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2163/3555 [00:26<00:16, 84.52it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2173/3555 [00:26<00:15, 87.62it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2182/3555 [00:26<00:15, 87.88it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2191/3555 [00:26<00:15, 86.55it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2200/3555 [00:26<00:16, 84.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2209/3555 [00:27<00:15, 84.54it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2219/3555 [00:27<00:15, 87.80it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2228/3555 [00:27<00:15, 85.92it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2237/3555 [00:27<00:15, 85.93it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2247/3555 [00:27<00:14, 88.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2256/3555 [00:27<00:15, 86.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 2266/3555 [00:27<00:14, 86.06it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2275/3555 [00:27<00:14, 86.01it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2285/3555 [00:27<00:14, 88.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2294/3555 [00:28<00:14, 87.21it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2303/3555 [00:28<00:14, 85.38it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2313/3555 [00:28<00:14, 85.49it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2322/3555 [00:28<00:14, 84.98it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2333/3555 [00:28<00:14, 85.92it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2342/3555 [00:28<00:14, 86.08it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2352/3555 [00:28<00:13, 87.90it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2361/3555 [00:28<00:13, 87.84it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2371/3555 [00:28<00:13, 89.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2380/3555 [00:29<00:13, 88.78it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2390/3555 [00:29<00:13, 89.54it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2399/3555 [00:29<00:13, 86.51it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2409/3555 [00:29<00:12, 89.97it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2419/3555 [00:29<00:13, 85.56it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2429/3555 [00:29<00:12, 87.26it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2439/3555 [00:29<00:12, 89.54it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2449/3555 [00:29<00:12, 85.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2458/3555 [00:29<00:12, 85.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2468/3555 [00:30<00:12, 88.34it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2477/3555 [00:30<00:12, 88.19it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2486/3555 [00:30<00:12, 87.46it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2495/3555 [00:30<00:12, 86.30it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2504/3555 [00:30<00:12, 84.50it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2513/3555 [00:30<00:12, 84.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2522/3555 [00:30<00:12, 84.39it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2531/3555 [00:30<00:11, 85.79it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2541/3555 [00:30<00:11, 87.87it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2550/3555 [00:31<00:11, 84.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2560/3555 [00:31<00:11, 87.46it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2569/3555 [00:31<00:11, 82.97it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2578/3555 [00:31<00:12, 80.93it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2587/3555 [00:31<00:12, 78.42it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2597/3555 [00:31<00:11, 80.54it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2607/3555 [00:31<00:11, 83.51it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2616/3555 [00:31<00:11, 83.88it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2625/3555 [00:31<00:10, 85.04it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2634/3555 [00:32<00:10, 85.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2643/3555 [00:32<00:10, 85.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2653/3555 [00:32<00:10, 87.38it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2663/3555 [00:32<00:10, 86.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2673/3555 [00:32<00:10, 87.03it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2682/3555 [00:33<00:22, 39.26it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2691/3555 [00:33<00:18, 46.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2700/3555 [00:33<00:15, 54.09it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2709/3555 [00:33<00:13, 60.96it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2718/3555 [00:33<00:12, 66.23it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2728/3555 [00:33<00:11, 74.00it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2737/3555 [00:33<00:10, 77.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2746/3555 [00:33<00:10, 77.33it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2755/3555 [00:33<00:09, 80.26it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2764/3555 [00:33<00:09, 81.15it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2773/3555 [00:34<00:09, 83.02it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2783/3555 [00:34<00:08, 87.46it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2792/3555 [00:34<00:08, 86.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2801/3555 [00:34<00:08, 85.01it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2811/3555 [00:34<00:08, 87.91it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2820/3555 [00:34<00:08, 86.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2829/3555 [00:34<00:08, 83.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2838/3555 [00:34<00:08, 80.19it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2847/3555 [00:34<00:08, 78.91it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2855/3555 [00:35<00:08, 78.15it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2863/3555 [00:35<00:08, 78.04it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2871/3555 [00:35<00:08, 76.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2879/3555 [00:35<00:08, 76.42it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2888/3555 [00:35<00:08, 78.87it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2897/3555 [00:35<00:08, 80.76it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2906/3555 [00:35<00:08, 80.85it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2916/3555 [00:35<00:07, 82.00it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2925/3555 [00:35<00:07, 83.62it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2934/3555 [00:36<00:07, 84.31it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2944/3555 [00:36<00:07, 87.22it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2953/3555 [00:36<00:07, 85.44it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2962/3555 [00:36<00:06, 85.66it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2972/3555 [00:36<00:06, 85.01it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2982/3555 [00:36<00:06, 86.92it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2992/3555 [00:36<00:06, 85.73it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3001/3555 [00:36<00:06, 86.22it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3010/3555 [00:36<00:06, 86.71it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3020/3555 [00:37<00:06, 87.93it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3029/3555 [00:37<00:06, 86.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3038/3555 [00:37<00:06, 85.88it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3047/3555 [00:37<00:05, 85.08it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3057/3555 [00:37<00:05, 86.66it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3067/3555 [00:37<00:05, 86.54it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3077/3555 [00:37<00:05, 87.81it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3087/3555 [00:37<00:05, 90.54it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3097/3555 [00:37<00:05, 89.21it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3106/3555 [00:37<00:05, 87.93it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3115/3555 [00:38<00:05, 87.01it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3124/3555 [00:38<00:04, 87.50it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3135/3555 [00:38<00:04, 91.22it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3145/3555 [00:38<00:04, 89.39it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3154/3555 [00:38<00:04, 87.28it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3163/3555 [00:38<00:04, 87.07it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3172/3555 [00:38<00:04, 87.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3183/3555 [00:38<00:04, 91.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3193/3555 [00:38<00:04, 86.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3203/3555 [00:39<00:03, 88.54it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3213/3555 [00:39<00:03, 88.22it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3223/3555 [00:39<00:03, 88.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3234/3555 [00:39<00:03, 91.56it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3244/3555 [00:39<00:03, 90.38it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3254/3555 [00:39<00:03, 88.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3264/3555 [00:39<00:03, 89.37it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3273/3555 [00:39<00:03, 85.07it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3282/3555 [00:40<00:03, 82.28it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3291/3555 [00:40<00:03, 80.00it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3300/3555 [00:40<00:03, 78.60it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3308/3555 [00:40<00:03, 77.72it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3316/3555 [00:40<00:03, 74.60it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 3325/3555 [00:40<00:02, 77.21it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3333/3555 [00:40<00:02, 76.96it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3341/3555 [00:40<00:02, 76.36it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3349/3555 [00:40<00:02, 75.86it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3357/3555 [00:41<00:02, 75.88it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3365/3555 [00:41<00:02, 75.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3373/3555 [00:41<00:02, 75.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3381/3555 [00:41<00:02, 75.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3389/3555 [00:41<00:02, 75.24it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3397/3555 [00:41<00:02, 75.52it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3405/3555 [00:41<00:01, 75.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3413/3555 [00:41<00:01, 74.86it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3421/3555 [00:41<00:01, 74.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3429/3555 [00:41<00:01, 74.19it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3437/3555 [00:42<00:01, 74.28it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3445/3555 [00:42<00:01, 74.05it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3453/3555 [00:42<00:01, 74.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3461/3555 [00:42<00:01, 74.73it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3469/3555 [00:42<00:01, 74.61it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3477/3555 [00:42<00:01, 74.55it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3485/3555 [00:42<00:00, 74.36it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3493/3555 [00:42<00:00, 74.36it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3501/3555 [00:42<00:00, 74.77it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3509/3555 [00:43<00:00, 74.79it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3517/3555 [00:43<00:00, 74.40it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3525/3555 [00:43<00:00, 74.53it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3533/3555 [00:43<00:00, 73.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3541/3555 [00:43<00:00, 73.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3549/3555 [00:43<00:00, 74.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3555/3555 [00:43<00:00, 81.43it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 1.5119
Epoch 1 Step 51 Train Loss: 0.7208
Epoch 1 Step 101 Train Loss: 0.6639
Epoch 1 Step 151 Train Loss: 0.6282
Epoch 1 Step 201 Train Loss: 0.6793
Epoch 1 Step 251 Train Loss: 0.5539
Epoch 1 Step 301 Train Loss: 0.6789
Epoch 1 Step 351 Train Loss: 0.6391
Epoch 1 Step 401 Train Loss: 0.5393
Epoch 1 Step 451 Train Loss: 0.6673
Epoch 1 Step 501 Train Loss: 0.5855
Epoch 1 Step 551 Train Loss: 0.6280
Epoch 1 Step 601 Train Loss: 0.5987
Epoch 1: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0652 Validation Top 20 DE MSE: 0.2068. 
Epoch 2 Step 1 Train Loss: 0.5839
Epoch 2 Step 51 Train Loss: 0.6406
Epoch 2 Step 101 Train Loss: 0.5626
Epoch 2 Step 151 Train Loss: 0.5420
Epoch 2 Step 201 Train Loss: 0.5325
Epoch 2 Step 251 Train Loss: 0.5456
Epoch 2 Step 301 Train Loss: 0.5766
Epoch 2 Step 351 Train Loss: 0.5381
Epoch 2 Step 401 Train Loss: 0.5203
Epoch 2 Step 451 Train Loss: 0.5672
Epoch 2 Step 501 Train Loss: 0.5839
Epoch 2 Step 551 Train Loss: 0.5654
Epoch 2 Step 601 Train Loss: 0.6165
Epoch 2: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0577 Validation Top 20 DE MSE: 0.2141. 
Epoch 3 Step 1 Train Loss: 0.5718
Epoch 3 Step 51 Train Loss: 0.5454
Epoch 3 Step 101 Train Loss: 0.5375
Epoch 3 Step 151 Train Loss: 0.6234
Epoch 3 Step 201 Train Loss: 0.5451
Epoch 3 Step 251 Train Loss: 0.6064
Epoch 3 Step 301 Train Loss: 0.5480
Epoch 3 Step 351 Train Loss: 0.5262
Epoch 3 Step 401 Train Loss: 0.5242
Epoch 3 Step 451 Train Loss: 0.5533
Epoch 3 Step 501 Train Loss: 0.5929
Epoch 3 Step 551 Train Loss: 0.6484
Epoch 3 Step 601 Train Loss: 0.5717
Epoch 3: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0546 Validation Top 20 DE MSE: 0.1932. 
Epoch 4 Step 1 Train Loss: 0.5673
Epoch 4 Step 51 Train Loss: 0.5735
Epoch 4 Step 101 Train Loss: 0.4933
Epoch 4 Step 151 Train Loss: 0.5777
Epoch 4 Step 201 Train Loss: 0.5964
Epoch 4 Step 251 Train Loss: 0.5813
Epoch 4 Step 301 Train Loss: 0.5456
Epoch 4 Step 351 Train Loss: 0.5060
Epoch 4 Step 401 Train Loss: 0.5724
Epoch 4 Step 451 Train Loss: 0.5662
Epoch 4 Step 501 Train Loss: 0.5456
Epoch 4 Step 551 Train Loss: 0.6179
Epoch 4 Step 601 Train Loss: 0.4985
Epoch 4: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0508 Validation Top 20 DE MSE: 0.1990. 
Epoch 5 Step 1 Train Loss: 0.5747
Epoch 5 Step 51 Train Loss: 0.5497
Epoch 5 Step 101 Train Loss: 0.6108
Epoch 5 Step 151 Train Loss: 0.5748
Epoch 5 Step 201 Train Loss: 0.6439
Epoch 5 Step 251 Train Loss: 0.6140
Epoch 5 Step 301 Train Loss: 0.5398
Epoch 5 Step 351 Train Loss: 0.6213
Epoch 5 Step 401 Train Loss: 0.5700
Epoch 5 Step 451 Train Loss: 0.5435
Epoch 5 Step 501 Train Loss: 0.4946
Epoch 5 Step 551 Train Loss: 0.5494
Epoch 5 Step 601 Train Loss: 0.6198
Epoch 5: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0457 Validation Top 20 DE MSE: 0.2054. 
Epoch 6 Step 1 Train Loss: 0.6307
Epoch 6 Step 51 Train Loss: 0.5412
Epoch 6 Step 101 Train Loss: 0.5368
Epoch 6 Step 151 Train Loss: 0.5250
Epoch 6 Step 201 Train Loss: 0.6800
Epoch 6 Step 251 Train Loss: 0.5558
Epoch 6 Step 301 Train Loss: 0.5804
Epoch 6 Step 351 Train Loss: 0.6140
Epoch 6 Step 401 Train Loss: 0.5536
Epoch 6 Step 451 Train Loss: 0.5709
Epoch 6 Step 501 Train Loss: 0.5370
Epoch 6 Step 551 Train Loss: 0.5443
Epoch 6 Step 601 Train Loss: 0.5635
Epoch 6: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0454 Validation Top 20 DE MSE: 0.1935. 
Epoch 7 Step 1 Train Loss: 0.6205
Epoch 7 Step 51 Train Loss: 0.5727
Epoch 7 Step 101 Train Loss: 0.5672
Epoch 7 Step 151 Train Loss: 0.5377
Epoch 7 Step 201 Train Loss: 0.6093
Epoch 7 Step 251 Train Loss: 0.6236
Epoch 7 Step 301 Train Loss: 0.5278
Epoch 7 Step 351 Train Loss: 0.5362
Epoch 7 Step 401 Train Loss: 0.5592
Epoch 7 Step 451 Train Loss: 0.5292
Epoch 7 Step 501 Train Loss: 0.5576
Epoch 7 Step 551 Train Loss: 0.5427
Epoch 7 Step 601 Train Loss: 0.5969
Epoch 7: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0444 Validation Top 20 DE MSE: 0.1979. 
Epoch 8 Step 1 Train Loss: 0.5632
Epoch 8 Step 51 Train Loss: 0.6112
Epoch 8 Step 101 Train Loss: 0.5915
Epoch 8 Step 151 Train Loss: 0.5197
Epoch 8 Step 201 Train Loss: 0.5881
Epoch 8 Step 251 Train Loss: 0.5617
Epoch 8 Step 301 Train Loss: 0.5830
Epoch 8 Step 351 Train Loss: 0.5529
Epoch 8 Step 401 Train Loss: 0.5350
Epoch 8 Step 451 Train Loss: 0.5816
Epoch 8 Step 501 Train Loss: 0.4970
Epoch 8 Step 551 Train Loss: 0.5734
Epoch 8 Step 601 Train Loss: 0.5718
Epoch 8: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0463 Validation Top 20 DE MSE: 0.1996. 
Epoch 9 Step 1 Train Loss: 0.5928
Epoch 9 Step 51 Train Loss: 0.5869
Epoch 9 Step 101 Train Loss: 0.5031
Epoch 9 Step 151 Train Loss: 0.6077
Epoch 9 Step 201 Train Loss: 0.5078
Epoch 9 Step 251 Train Loss: 0.5479
Epoch 9 Step 301 Train Loss: 0.5536
Epoch 9 Step 351 Train Loss: 0.5688
Epoch 9 Step 401 Train Loss: 0.6102
Epoch 9 Step 451 Train Loss: 0.5482
Epoch 9 Step 501 Train Loss: 0.5288
Epoch 9 Step 551 Train Loss: 0.5401
Epoch 9 Step 601 Train Loss: 0.5795
Epoch 9: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0455 Validation Top 20 DE MSE: 0.2022. 
Epoch 10 Step 1 Train Loss: 0.5039
Epoch 10 Step 51 Train Loss: 0.5481
Epoch 10 Step 101 Train Loss: 0.5352
Epoch 10 Step 151 Train Loss: 0.5888
Epoch 10 Step 201 Train Loss: 0.5839
Epoch 10 Step 251 Train Loss: 0.5217
Epoch 10 Step 301 Train Loss: 0.5517
Epoch 10 Step 351 Train Loss: 0.5641
Epoch 10 Step 401 Train Loss: 0.5229
Epoch 10 Step 451 Train Loss: 0.5517
Epoch 10 Step 501 Train Loss: 0.5585
Epoch 10 Step 551 Train Loss: 0.5958
Epoch 10 Step 601 Train Loss: 0.6078
Epoch 10: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0440 Validation Top 20 DE MSE: 0.2004. 
Epoch 11 Step 1 Train Loss: 0.4911
Epoch 11 Step 51 Train Loss: 0.6281
Epoch 11 Step 101 Train Loss: 0.5485
Epoch 11 Step 151 Train Loss: 0.5332
Epoch 11 Step 201 Train Loss: 0.5590
Epoch 11 Step 251 Train Loss: 0.4959
Epoch 11 Step 301 Train Loss: 0.5808
Epoch 11 Step 351 Train Loss: 0.5453
Epoch 11 Step 401 Train Loss: 0.5275
Epoch 11 Step 451 Train Loss: 0.5785
Epoch 11 Step 501 Train Loss: 0.6400
Epoch 11 Step 551 Train Loss: 0.5590
Epoch 11 Step 601 Train Loss: 0.5523
Epoch 11: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0468 Validation Top 20 DE MSE: 0.2035. 
Epoch 12 Step 1 Train Loss: 0.5951
Epoch 12 Step 51 Train Loss: 0.5513
Epoch 12 Step 101 Train Loss: 0.5988
Epoch 12 Step 151 Train Loss: 0.5788
Epoch 12 Step 201 Train Loss: 0.5240
Epoch 12 Step 251 Train Loss: 0.5280
Epoch 12 Step 301 Train Loss: 0.5534
Epoch 12 Step 351 Train Loss: 0.5327
Epoch 12 Step 401 Train Loss: 0.5678
Epoch 12 Step 451 Train Loss: 0.5783
Epoch 12 Step 501 Train Loss: 0.5931
Epoch 12 Step 551 Train Loss: 0.5428
Epoch 12 Step 601 Train Loss: 0.5955
Epoch 12: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0440 Validation Top 20 DE MSE: 0.2006. 
Epoch 13 Step 1 Train Loss: 0.5253
Epoch 13 Step 51 Train Loss: 0.5452
Epoch 13 Step 101 Train Loss: 0.5679
Epoch 13 Step 151 Train Loss: 0.5644
Epoch 13 Step 201 Train Loss: 0.5655
Epoch 13 Step 251 Train Loss: 0.5916
Epoch 13 Step 301 Train Loss: 0.5797
Epoch 13 Step 351 Train Loss: 0.5297
Epoch 13 Step 401 Train Loss: 0.5497
Epoch 13 Step 451 Train Loss: 0.6038
Epoch 13 Step 501 Train Loss: 0.5762
Epoch 13 Step 551 Train Loss: 0.5900
Epoch 13 Step 601 Train Loss: 0.5514
Epoch 13: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0462 Validation Top 20 DE MSE: 0.2036. 
Epoch 14 Step 1 Train Loss: 0.5597
Epoch 14 Step 51 Train Loss: 0.5319
Epoch 14 Step 101 Train Loss: 0.5652
Epoch 14 Step 151 Train Loss: 0.5290
Epoch 14 Step 201 Train Loss: 0.5523
Epoch 14 Step 251 Train Loss: 0.5764
Epoch 14 Step 301 Train Loss: 0.6266
Epoch 14 Step 351 Train Loss: 0.5667
Epoch 14 Step 401 Train Loss: 0.5262
Epoch 14 Step 451 Train Loss: 0.6199
Epoch 14 Step 501 Train Loss: 0.4855
Epoch 14 Step 551 Train Loss: 0.6014
Epoch 14 Step 601 Train Loss: 0.5981
Epoch 14: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0437 Validation Top 20 DE MSE: 0.1954. 
Epoch 15 Step 1 Train Loss: 0.5193
Epoch 15 Step 51 Train Loss: 0.5931
Epoch 15 Step 101 Train Loss: 0.5348
Epoch 15 Step 151 Train Loss: 0.5685
Epoch 15 Step 201 Train Loss: 0.5035
Epoch 15 Step 251 Train Loss: 0.6106
Epoch 15 Step 301 Train Loss: 0.5614
Epoch 15 Step 351 Train Loss: 0.5436
Epoch 15 Step 401 Train Loss: 0.5309
Epoch 15 Step 451 Train Loss: 0.6401
Epoch 15 Step 501 Train Loss: 0.5695
Epoch 15 Step 551 Train Loss: 0.5540
Epoch 15 Step 601 Train Loss: 0.5549
Epoch 15: Train Overall MSE: 0.0013 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0428 Validation Top 20 DE MSE: 0.1953. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1444
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0036318859
test_unseen_single_pearson: 0.9931099084644984
test_unseen_single_mse_de: 0.14442159
test_unseen_single_pearson_de: 0.8817128055282227
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3938971735552909
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.21956521739130438
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8499999999999998
test_unseen_single_mse_top20_de_non_dropout: 0.15075518
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.003 MB of 0.024 MB uploadedwandb: | 0.018 MB of 0.024 MB uploadedwandb: / 0.018 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–†â–…â–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–
wandb:                                             train_de_pearson â–â–„â–…â–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–„â–ƒâ–â–‚â–„â–‡â–‚â–†â–‚â–„â–ƒâ–‚â–„â–„â–‚â–ƒâ–„â–‚â–‡â–„â–‚â–†â–‚â–…â–ƒâ–„â–‚â–‚â–ƒâ–…â–†â–ƒâ–‚â–†â–ƒâ–ˆâ–„â–„â–‚
wandb:                                                   val_de_mse â–†â–ˆâ–â–ƒâ–…â–â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–‚â–‚
wandb:                                               val_de_pearson â–‚â–â–ˆâ–…â–‚â–†â–„â–„â–ƒâ–„â–‚â–‚â–‚â–…â–…
wandb:                                                      val_mse â–ˆâ–…â–‚â–‚â–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚
wandb:                                                  val_pearson â–â–ƒâ–ˆâ–‡â–†â–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.14442
wandb:                                              test_de_pearson 0.88171
wandb:               test_frac_opposite_direction_top20_non_dropout 0.21957
wandb:                          test_frac_sigma_below_1_non_dropout 0.85
wandb:                                                     test_mse 0.00363
wandb:                                test_mse_top20_de_non_dropout 0.15076
wandb:                                                 test_pearson 0.99311
wandb:                                           test_pearson_delta 0.3939
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.21957
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.85
wandb:                                       test_unseen_single_mse 0.00363
wandb:                                    test_unseen_single_mse_de 0.14442
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15076
wandb:                                   test_unseen_single_pearson 0.99311
wandb:                                test_unseen_single_pearson_de 0.88171
wandb:                             test_unseen_single_pearson_delta 0.3939
wandb:                                                 train_de_mse 0.04278
wandb:                                             train_de_pearson 0.89448
wandb:                                                    train_mse 0.00133
wandb:                                                train_pearson 0.99756
wandb:                                                training_loss 0.5583
wandb:                                                   val_de_mse 0.1953
wandb:                                               val_de_pearson 0.96981
wandb:                                                      val_mse 0.00296
wandb:                                                  val_pearson 0.99436
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406681_3_split1 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/ctqvuedm
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_040350-ctqvuedm/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406681_3/splits/adamsonweissman2016_gsm2406681_3_simulation_2_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:23
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_043922-2kkl06mw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406681_3_split2
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/2kkl06mw
Start Training...
Epoch 1 Step 1 Train Loss: 1.3089
Epoch 1 Step 51 Train Loss: 0.7822
Epoch 1 Step 101 Train Loss: 0.7245
Epoch 1 Step 151 Train Loss: 0.5735
Epoch 1 Step 201 Train Loss: 0.6885
Epoch 1 Step 251 Train Loss: 0.6227
Epoch 1 Step 301 Train Loss: 0.6224
Epoch 1 Step 351 Train Loss: 0.6315
Epoch 1 Step 401 Train Loss: 0.5257
Epoch 1 Step 451 Train Loss: 0.6141
Epoch 1 Step 501 Train Loss: 0.5645
Epoch 1 Step 551 Train Loss: 0.6169
Epoch 1 Step 601 Train Loss: 0.6195
Epoch 1: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0987 Validation Top 20 DE MSE: 0.1565. 
Epoch 2 Step 1 Train Loss: 0.5840
Epoch 2 Step 51 Train Loss: 0.6214
Epoch 2 Step 101 Train Loss: 0.6390
Epoch 2 Step 151 Train Loss: 0.6306
Epoch 2 Step 201 Train Loss: 0.5548
Epoch 2 Step 251 Train Loss: 0.5581
Epoch 2 Step 301 Train Loss: 0.5428
Epoch 2 Step 351 Train Loss: 0.5544
Epoch 2 Step 401 Train Loss: 0.5096
Epoch 2 Step 451 Train Loss: 0.5515
Epoch 2 Step 501 Train Loss: 0.5416
Epoch 2 Step 551 Train Loss: 0.4923
Epoch 2 Step 601 Train Loss: 0.5422
Epoch 2: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0765 Validation Top 20 DE MSE: 0.1112. 
Epoch 3 Step 1 Train Loss: 0.5328
Epoch 3 Step 51 Train Loss: 0.5797
Epoch 3 Step 101 Train Loss: 0.6146
Epoch 3 Step 151 Train Loss: 0.5302
Epoch 3 Step 201 Train Loss: 0.5901
Epoch 3 Step 251 Train Loss: 0.5333
Epoch 3 Step 301 Train Loss: 0.5887
Epoch 3 Step 351 Train Loss: 0.5818
Epoch 3 Step 401 Train Loss: 0.5552
Epoch 3 Step 451 Train Loss: 0.5497
Epoch 3 Step 501 Train Loss: 0.5177
Epoch 3 Step 551 Train Loss: 0.5980
Epoch 3 Step 601 Train Loss: 0.5534
Epoch 3: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0688 Validation Top 20 DE MSE: 0.1255. 
Epoch 4 Step 1 Train Loss: 0.6075
Epoch 4 Step 51 Train Loss: 0.5779
Epoch 4 Step 101 Train Loss: 0.5498
Epoch 4 Step 151 Train Loss: 0.5565
Epoch 4 Step 201 Train Loss: 0.5197
Epoch 4 Step 251 Train Loss: 0.6041
Epoch 4 Step 301 Train Loss: 0.5588
Epoch 4 Step 351 Train Loss: 0.5760
Epoch 4 Step 401 Train Loss: 0.5807
Epoch 4 Step 451 Train Loss: 0.5208
Epoch 4 Step 501 Train Loss: 0.5466
Epoch 4 Step 551 Train Loss: 0.5618
Epoch 4 Step 601 Train Loss: 0.5780
Epoch 4: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0554 Validation Top 20 DE MSE: 0.1089. 
Epoch 5 Step 1 Train Loss: 0.5129
Epoch 5 Step 51 Train Loss: 0.5576
Epoch 5 Step 101 Train Loss: 0.5556
Epoch 5 Step 151 Train Loss: 0.6006
Epoch 5 Step 201 Train Loss: 0.5108
Epoch 5 Step 251 Train Loss: 0.5937
Epoch 5 Step 301 Train Loss: 0.5529
Epoch 5 Step 351 Train Loss: 0.5317
Epoch 5 Step 401 Train Loss: 0.6112
Epoch 5 Step 451 Train Loss: 0.5211
Epoch 5 Step 501 Train Loss: 0.5460
Epoch 5 Step 551 Train Loss: 0.5795
Epoch 5 Step 601 Train Loss: 0.5902
Epoch 5: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0476 Validation Top 20 DE MSE: 0.1041. 
Epoch 6 Step 1 Train Loss: 0.5118
Epoch 6 Step 51 Train Loss: 0.5677
Epoch 6 Step 101 Train Loss: 0.5320
Epoch 6 Step 151 Train Loss: 0.5721
Epoch 6 Step 201 Train Loss: 0.5178
Epoch 6 Step 251 Train Loss: 0.5363
Epoch 6 Step 301 Train Loss: 0.5635
Epoch 6 Step 351 Train Loss: 0.5625
Epoch 6 Step 401 Train Loss: 0.5410
Epoch 6 Step 451 Train Loss: 0.5641
Epoch 6 Step 501 Train Loss: 0.5772
Epoch 6 Step 551 Train Loss: 0.5707
Epoch 6 Step 601 Train Loss: 0.5691
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0495 Validation Top 20 DE MSE: 0.1108. 
Epoch 7 Step 1 Train Loss: 0.5978
Epoch 7 Step 51 Train Loss: 0.5421
Epoch 7 Step 101 Train Loss: 0.5678
Epoch 7 Step 151 Train Loss: 0.5232
Epoch 7 Step 201 Train Loss: 0.5231
Epoch 7 Step 251 Train Loss: 0.6328
Epoch 7 Step 301 Train Loss: 0.6090
Epoch 7 Step 351 Train Loss: 0.5962
Epoch 7 Step 401 Train Loss: 0.5467
Epoch 7 Step 451 Train Loss: 0.5827
Epoch 7 Step 501 Train Loss: 0.6227
Epoch 7 Step 551 Train Loss: 0.5059
Epoch 7 Step 601 Train Loss: 0.5554
Epoch 7: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0500 Validation Top 20 DE MSE: 0.1109. 
Epoch 8 Step 1 Train Loss: 0.6061
Epoch 8 Step 51 Train Loss: 0.5520
Epoch 8 Step 101 Train Loss: 0.6247
Epoch 8 Step 151 Train Loss: 0.5528
Epoch 8 Step 201 Train Loss: 0.5548
Epoch 8 Step 251 Train Loss: 0.6087
Epoch 8 Step 301 Train Loss: 0.5927
Epoch 8 Step 351 Train Loss: 0.5981
Epoch 8 Step 401 Train Loss: 0.5844
Epoch 8 Step 451 Train Loss: 0.5303
Epoch 8 Step 501 Train Loss: 0.5204
Epoch 8 Step 551 Train Loss: 0.5111
Epoch 8 Step 601 Train Loss: 0.5240
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0515 Validation Top 20 DE MSE: 0.1136. 
Epoch 9 Step 1 Train Loss: 0.5717
Epoch 9 Step 51 Train Loss: 0.5749
Epoch 9 Step 101 Train Loss: 0.5756
Epoch 9 Step 151 Train Loss: 0.5123
Epoch 9 Step 201 Train Loss: 0.5164
Epoch 9 Step 251 Train Loss: 0.5260
Epoch 9 Step 301 Train Loss: 0.5588
Epoch 9 Step 351 Train Loss: 0.5988
Epoch 9 Step 401 Train Loss: 0.5807
Epoch 9 Step 451 Train Loss: 0.6259
Epoch 9 Step 501 Train Loss: 0.5243
Epoch 9 Step 551 Train Loss: 0.5043
Epoch 9 Step 601 Train Loss: 0.5121
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0499 Validation Top 20 DE MSE: 0.1097. 
Epoch 10 Step 1 Train Loss: 0.5685
Epoch 10 Step 51 Train Loss: 0.6366
Epoch 10 Step 101 Train Loss: 0.5285
Epoch 10 Step 151 Train Loss: 0.5628
Epoch 10 Step 201 Train Loss: 0.4857
Epoch 10 Step 251 Train Loss: 0.4980
Epoch 10 Step 301 Train Loss: 0.5432
Epoch 10 Step 351 Train Loss: 0.5233
Epoch 10 Step 401 Train Loss: 0.5860
Epoch 10 Step 451 Train Loss: 0.5613
Epoch 10 Step 501 Train Loss: 0.5922
Epoch 10 Step 551 Train Loss: 0.5609
Epoch 10 Step 601 Train Loss: 0.5750
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0522 Validation Top 20 DE MSE: 0.1169. 
Epoch 11 Step 1 Train Loss: 0.5478
Epoch 11 Step 51 Train Loss: 0.5664
Epoch 11 Step 101 Train Loss: 0.5529
Epoch 11 Step 151 Train Loss: 0.5621
Epoch 11 Step 201 Train Loss: 0.5737
Epoch 11 Step 251 Train Loss: 0.5331
Epoch 11 Step 301 Train Loss: 0.6628
Epoch 11 Step 351 Train Loss: 0.5841
Epoch 11 Step 401 Train Loss: 0.5610
Epoch 11 Step 451 Train Loss: 0.5402
Epoch 11 Step 501 Train Loss: 0.5452
Epoch 11 Step 551 Train Loss: 0.5056
Epoch 11 Step 601 Train Loss: 0.5256
Epoch 11: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0482 Validation Top 20 DE MSE: 0.1079. 
Epoch 12 Step 1 Train Loss: 0.6197
Epoch 12 Step 51 Train Loss: 0.5529
Epoch 12 Step 101 Train Loss: 0.4960
Epoch 12 Step 151 Train Loss: 0.5548
Epoch 12 Step 201 Train Loss: 0.5836
Epoch 12 Step 251 Train Loss: 0.6032
Epoch 12 Step 301 Train Loss: 0.5211
Epoch 12 Step 351 Train Loss: 0.5124
Epoch 12 Step 401 Train Loss: 0.5448
Epoch 12 Step 451 Train Loss: 0.5355
Epoch 12 Step 501 Train Loss: 0.5139
Epoch 12 Step 551 Train Loss: 0.5389
Epoch 12 Step 601 Train Loss: 0.5397
Epoch 12: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0496 Validation Top 20 DE MSE: 0.1097. 
Epoch 13 Step 1 Train Loss: 0.5288
Epoch 13 Step 51 Train Loss: 0.6454
Epoch 13 Step 101 Train Loss: 0.6145
Epoch 13 Step 151 Train Loss: 0.5636
Epoch 13 Step 201 Train Loss: 0.5983
Epoch 13 Step 251 Train Loss: 0.5986
Epoch 13 Step 301 Train Loss: 0.5526
Epoch 13 Step 351 Train Loss: 0.6002
Epoch 13 Step 401 Train Loss: 0.5185
Epoch 13 Step 451 Train Loss: 0.4961
Epoch 13 Step 501 Train Loss: 0.5736
Epoch 13 Step 551 Train Loss: 0.5740
Epoch 13 Step 601 Train Loss: 0.5264
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0499 Validation Top 20 DE MSE: 0.1121. 
Epoch 14 Step 1 Train Loss: 0.5269
Epoch 14 Step 51 Train Loss: 0.5768
Epoch 14 Step 101 Train Loss: 0.5361
Epoch 14 Step 151 Train Loss: 0.5463
Epoch 14 Step 201 Train Loss: 0.6250
Epoch 14 Step 251 Train Loss: 0.5683
Epoch 14 Step 301 Train Loss: 0.4925
Epoch 14 Step 351 Train Loss: 0.5895
Epoch 14 Step 401 Train Loss: 0.5169
Epoch 14 Step 451 Train Loss: 0.5848
Epoch 14 Step 501 Train Loss: 0.5316
Epoch 14 Step 551 Train Loss: 0.5572
Epoch 14 Step 601 Train Loss: 0.5378
Epoch 14: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0505 Validation Top 20 DE MSE: 0.1147. 
Epoch 15 Step 1 Train Loss: 0.5691
Epoch 15 Step 51 Train Loss: 0.5424
Epoch 15 Step 101 Train Loss: 0.5585
Epoch 15 Step 151 Train Loss: 0.5206
Epoch 15 Step 201 Train Loss: 0.5142
Epoch 15 Step 251 Train Loss: 0.5937
Epoch 15 Step 301 Train Loss: 0.6041
Epoch 15 Step 351 Train Loss: 0.5494
Epoch 15 Step 401 Train Loss: 0.5401
Epoch 15 Step 451 Train Loss: 0.5283
Epoch 15 Step 501 Train Loss: 0.5061
Epoch 15 Step 551 Train Loss: 0.5754
Epoch 15 Step 601 Train Loss: 0.5980
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0499 Validation Top 20 DE MSE: 0.1123. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1062
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0025685788
test_unseen_single_pearson: 0.9951985699723559
test_unseen_single_mse_de: 0.10622986
test_unseen_single_pearson_de: 0.8809252706363118
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.37018859834617407
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25869565217391305
test_unseen_single_frac_sigma_below_1_non_dropout: 0.893478260869565
test_unseen_single_mse_top20_de_non_dropout: 0.1121034
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.022 MB of 0.024 MB uploadedwandb: / 0.022 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–…â–„â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–
wandb:                                             train_de_pearson â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–‡â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‚â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–‡â–„â–…â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–…â–ƒâ–‚â–ƒâ–ˆâ–â–ƒâ–‚â–‚â–„â–ƒâ–â–‚â–…â–„â–ƒâ–ƒâ–†â–‚â–ƒâ–‚
wandb:                                                   val_de_mse â–ˆâ–‚â–„â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:                                               val_de_pearson â–â–ˆâ–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡
wandb:                                                      val_mse â–ˆâ–ƒâ–„â–â–â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:                                                  val_pearson â–â–†â–…â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.10623
wandb:                                              test_de_pearson 0.88093
wandb:               test_frac_opposite_direction_top20_non_dropout 0.2587
wandb:                          test_frac_sigma_below_1_non_dropout 0.89348
wandb:                                                     test_mse 0.00257
wandb:                                test_mse_top20_de_non_dropout 0.1121
wandb:                                                 test_pearson 0.9952
wandb:                                           test_pearson_delta 0.37019
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.2587
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.89348
wandb:                                       test_unseen_single_mse 0.00257
wandb:                                    test_unseen_single_mse_de 0.10623
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.1121
wandb:                                   test_unseen_single_pearson 0.9952
wandb:                                test_unseen_single_pearson_de 0.88093
wandb:                             test_unseen_single_pearson_delta 0.37019
wandb:                                                 train_de_mse 0.04989
wandb:                                             train_de_pearson 0.89901
wandb:                                                    train_mse 0.00166
wandb:                                                train_pearson 0.997
wandb:                                                training_loss 0.58559
wandb:                                                   val_de_mse 0.11226
wandb:                                               val_de_pearson 0.96464
wandb:                                                      val_mse 0.00177
wandb:                                                  val_pearson 0.99664
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406681_3_split2 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/2kkl06mw
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_043922-2kkl06mw/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406681_3/splits/adamsonweissman2016_gsm2406681_3_simulation_3_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:23
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_051259-2fnyr6o9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406681_3_split3
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/2fnyr6o9
Start Training...
Epoch 1 Step 1 Train Loss: 1.3758
Epoch 1 Step 51 Train Loss: 0.6696
Epoch 1 Step 101 Train Loss: 0.5863
Epoch 1 Step 151 Train Loss: 0.6290
Epoch 1 Step 201 Train Loss: 0.5517
Epoch 1 Step 251 Train Loss: 0.5670
Epoch 1 Step 301 Train Loss: 0.6449
Epoch 1 Step 351 Train Loss: 0.5605
Epoch 1 Step 401 Train Loss: 0.5894
Epoch 1 Step 451 Train Loss: 0.5844
Epoch 1 Step 501 Train Loss: 0.6085
Epoch 1 Step 551 Train Loss: 0.6047
Epoch 1 Step 601 Train Loss: 0.5822
Epoch 1: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0812 Validation Top 20 DE MSE: 0.0844. 
Epoch 2 Step 1 Train Loss: 0.5807
Epoch 2 Step 51 Train Loss: 0.6227
Epoch 2 Step 101 Train Loss: 0.6055
Epoch 2 Step 151 Train Loss: 0.5697
Epoch 2 Step 201 Train Loss: 0.5863
Epoch 2 Step 251 Train Loss: 0.5799
Epoch 2 Step 301 Train Loss: 0.5935
Epoch 2 Step 351 Train Loss: 0.5682
Epoch 2 Step 401 Train Loss: 0.6042
Epoch 2 Step 451 Train Loss: 0.6122
Epoch 2 Step 501 Train Loss: 0.5678
Epoch 2 Step 551 Train Loss: 0.5689
Epoch 2 Step 601 Train Loss: 0.5678
Epoch 2: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0042. 
Train Top 20 DE MSE: 0.0561 Validation Top 20 DE MSE: 0.0535. 
Epoch 3 Step 1 Train Loss: 0.5419
Epoch 3 Step 51 Train Loss: 0.5377
Epoch 3 Step 101 Train Loss: 0.5419
Epoch 3 Step 151 Train Loss: 0.5406
Epoch 3 Step 201 Train Loss: 0.5509
Epoch 3 Step 251 Train Loss: 0.5856
Epoch 3 Step 301 Train Loss: 0.5688
Epoch 3 Step 351 Train Loss: 0.5227
Epoch 3 Step 401 Train Loss: 0.5632
Epoch 3 Step 451 Train Loss: 0.5393
Epoch 3 Step 501 Train Loss: 0.5653
Epoch 3 Step 551 Train Loss: 0.5404
Epoch 3 Step 601 Train Loss: 0.5395
Epoch 3: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0590 Validation Top 20 DE MSE: 0.0779. 
Epoch 4 Step 1 Train Loss: 0.5824
Epoch 4 Step 51 Train Loss: 0.5384
Epoch 4 Step 101 Train Loss: 0.5368
Epoch 4 Step 151 Train Loss: 0.5711
Epoch 4 Step 201 Train Loss: 0.5516
Epoch 4 Step 251 Train Loss: 0.5496
Epoch 4 Step 301 Train Loss: 0.5419
Epoch 4 Step 351 Train Loss: 0.5889
Epoch 4 Step 401 Train Loss: 0.5408
Epoch 4 Step 451 Train Loss: 0.5315
Epoch 4 Step 501 Train Loss: 0.5338
Epoch 4 Step 551 Train Loss: 0.5580
Epoch 4 Step 601 Train Loss: 0.5979
Epoch 4: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0583 Validation Top 20 DE MSE: 0.0825. 
Epoch 5 Step 1 Train Loss: 0.5850
Epoch 5 Step 51 Train Loss: 0.5392
Epoch 5 Step 101 Train Loss: 0.6249
Epoch 5 Step 151 Train Loss: 0.5811
Epoch 5 Step 201 Train Loss: 0.5543
Epoch 5 Step 251 Train Loss: 0.5510
Epoch 5 Step 301 Train Loss: 0.5241
Epoch 5 Step 351 Train Loss: 0.5099
Epoch 5 Step 401 Train Loss: 0.5517
Epoch 5 Step 451 Train Loss: 0.5935
Epoch 5 Step 501 Train Loss: 0.5200
Epoch 5 Step 551 Train Loss: 0.5149
Epoch 5 Step 601 Train Loss: 0.5563
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0497 Validation Top 20 DE MSE: 0.0647. 
Epoch 6 Step 1 Train Loss: 0.5405
Epoch 6 Step 51 Train Loss: 0.5658
Epoch 6 Step 101 Train Loss: 0.5847
Epoch 6 Step 151 Train Loss: 0.5691
Epoch 6 Step 201 Train Loss: 0.5327
Epoch 6 Step 251 Train Loss: 0.5473
Epoch 6 Step 301 Train Loss: 0.5266
Epoch 6 Step 351 Train Loss: 0.5889
Epoch 6 Step 401 Train Loss: 0.5499
Epoch 6 Step 451 Train Loss: 0.5357
Epoch 6 Step 501 Train Loss: 0.5657
Epoch 6 Step 551 Train Loss: 0.5683
Epoch 6 Step 601 Train Loss: 0.5824
Epoch 6: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0535 Validation Top 20 DE MSE: 0.0681. 
Epoch 7 Step 1 Train Loss: 0.6495
Epoch 7 Step 51 Train Loss: 0.6053
Epoch 7 Step 101 Train Loss: 0.7047
Epoch 7 Step 151 Train Loss: 0.5155
Epoch 7 Step 201 Train Loss: 0.5512
Epoch 7 Step 251 Train Loss: 0.5261
Epoch 7 Step 301 Train Loss: 0.5667
Epoch 7 Step 351 Train Loss: 0.4877
Epoch 7 Step 401 Train Loss: 0.5333
Epoch 7 Step 451 Train Loss: 0.5559
Epoch 7 Step 501 Train Loss: 0.5670
Epoch 7 Step 551 Train Loss: 0.5792
Epoch 7 Step 601 Train Loss: 0.5721
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0491 Validation Top 20 DE MSE: 0.0630. 
Epoch 8 Step 1 Train Loss: 0.6039
Epoch 8 Step 51 Train Loss: 0.5112
Epoch 8 Step 101 Train Loss: 0.6135
Epoch 8 Step 151 Train Loss: 0.5514
Epoch 8 Step 201 Train Loss: 0.5777
Epoch 8 Step 251 Train Loss: 0.5679
Epoch 8 Step 301 Train Loss: 0.5608
Epoch 8 Step 351 Train Loss: 0.5214
Epoch 8 Step 401 Train Loss: 0.5252
Epoch 8 Step 451 Train Loss: 0.5465
Epoch 8 Step 501 Train Loss: 0.5570
Epoch 8 Step 551 Train Loss: 0.6125
Epoch 8 Step 601 Train Loss: 0.5687
Epoch 8: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0467 Validation Top 20 DE MSE: 0.0604. 
Epoch 9 Step 1 Train Loss: 0.5579
Epoch 9 Step 51 Train Loss: 0.6083
Epoch 9 Step 101 Train Loss: 0.5365
Epoch 9 Step 151 Train Loss: 0.5967
Epoch 9 Step 201 Train Loss: 0.5654
Epoch 9 Step 251 Train Loss: 0.5273
Epoch 9 Step 301 Train Loss: 0.5738
Epoch 9 Step 351 Train Loss: 0.5309
Epoch 9 Step 401 Train Loss: 0.5606
Epoch 9 Step 451 Train Loss: 0.7104
Epoch 9 Step 501 Train Loss: 0.5791
Epoch 9 Step 551 Train Loss: 0.5239
Epoch 9 Step 601 Train Loss: 0.5994
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0460 Validation Top 20 DE MSE: 0.0582. 
Epoch 10 Step 1 Train Loss: 0.6148
Epoch 10 Step 51 Train Loss: 0.5794
Epoch 10 Step 101 Train Loss: 0.5300
Epoch 10 Step 151 Train Loss: 0.5419
Epoch 10 Step 201 Train Loss: 0.5599
Epoch 10 Step 251 Train Loss: 0.5500
Epoch 10 Step 301 Train Loss: 0.4873
Epoch 10 Step 351 Train Loss: 0.5969
Epoch 10 Step 401 Train Loss: 0.5105
Epoch 10 Step 451 Train Loss: 0.5527
Epoch 10 Step 501 Train Loss: 0.5647
Epoch 10 Step 551 Train Loss: 0.5924
Epoch 10 Step 601 Train Loss: 0.6098
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0480 Validation Top 20 DE MSE: 0.0609. 
Epoch 11 Step 1 Train Loss: 0.6009
Epoch 11 Step 51 Train Loss: 0.5054
Epoch 11 Step 101 Train Loss: 0.5316
Epoch 11 Step 151 Train Loss: 0.5625
Epoch 11 Step 201 Train Loss: 0.5878
Epoch 11 Step 251 Train Loss: 0.4802
Epoch 11 Step 301 Train Loss: 0.5811
Epoch 11 Step 351 Train Loss: 0.5894
Epoch 11 Step 401 Train Loss: 0.5418
Epoch 11 Step 451 Train Loss: 0.5990
Epoch 11 Step 501 Train Loss: 0.5483
Epoch 11 Step 551 Train Loss: 0.5439
Epoch 11 Step 601 Train Loss: 0.5298
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0441 Validation Top 20 DE MSE: 0.0569. 
Epoch 12 Step 1 Train Loss: 0.6061
Epoch 12 Step 51 Train Loss: 0.5602
Epoch 12 Step 101 Train Loss: 0.5934
Epoch 12 Step 151 Train Loss: 0.6103
Epoch 12 Step 201 Train Loss: 0.5159
Epoch 12 Step 251 Train Loss: 0.5988
Epoch 12 Step 301 Train Loss: 0.6048
Epoch 12 Step 351 Train Loss: 0.4951
Epoch 12 Step 401 Train Loss: 0.5594
Epoch 12 Step 451 Train Loss: 0.5620
Epoch 12 Step 501 Train Loss: 0.5784
Epoch 12 Step 551 Train Loss: 0.5512
Epoch 12 Step 601 Train Loss: 0.5744
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0505 Validation Top 20 DE MSE: 0.0633. 
Epoch 13 Step 1 Train Loss: 0.6420
Epoch 13 Step 51 Train Loss: 0.6395
Epoch 13 Step 101 Train Loss: 0.5017
Epoch 13 Step 151 Train Loss: 0.6449
Epoch 13 Step 201 Train Loss: 0.5909
Epoch 13 Step 251 Train Loss: 0.5733
Epoch 13 Step 301 Train Loss: 0.5529
Epoch 13 Step 351 Train Loss: 0.5678
Epoch 13 Step 401 Train Loss: 0.5665
Epoch 13 Step 451 Train Loss: 0.4715
Epoch 13 Step 501 Train Loss: 0.5094
Epoch 13 Step 551 Train Loss: 0.6627
Epoch 13 Step 601 Train Loss: 0.5922
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0460 Validation Top 20 DE MSE: 0.0603. 
Epoch 14 Step 1 Train Loss: 0.5889
Epoch 14 Step 51 Train Loss: 0.5506
Epoch 14 Step 101 Train Loss: 0.5461
Epoch 14 Step 151 Train Loss: 0.5367
Epoch 14 Step 201 Train Loss: 0.6819
Epoch 14 Step 251 Train Loss: 0.5163
Epoch 14 Step 301 Train Loss: 0.5580
Epoch 14 Step 351 Train Loss: 0.5337
Epoch 14 Step 401 Train Loss: 0.5303
Epoch 14 Step 451 Train Loss: 0.5936
Epoch 14 Step 501 Train Loss: 0.5649
Epoch 14 Step 551 Train Loss: 0.5510
Epoch 14 Step 601 Train Loss: 0.5016
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0482 Validation Top 20 DE MSE: 0.0619. 
Epoch 15 Step 1 Train Loss: 0.6338
Epoch 15 Step 51 Train Loss: 0.5261
Epoch 15 Step 101 Train Loss: 0.5209
Epoch 15 Step 151 Train Loss: 0.5679
Epoch 15 Step 201 Train Loss: 0.5421
Epoch 15 Step 251 Train Loss: 0.6315
Epoch 15 Step 301 Train Loss: 0.6238
Epoch 15 Step 351 Train Loss: 0.5303
Epoch 15 Step 401 Train Loss: 0.5637
Epoch 15 Step 451 Train Loss: 0.5765
Epoch 15 Step 501 Train Loss: 0.5639
Epoch 15 Step 551 Train Loss: 0.5535
Epoch 15 Step 601 Train Loss: 0.5243
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0475 Validation Top 20 DE MSE: 0.0614. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1126
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0028338558
test_unseen_single_pearson: 0.9947632170687551
test_unseen_single_mse_de: 0.11261055
test_unseen_single_pearson_de: 0.8893257679025292
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3832434466837024
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2478260869565217
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8695652173913042
test_unseen_single_mse_top20_de_non_dropout: 0.115731955
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.001 MB of 0.024 MB uploadedwandb: / 0.013 MB of 0.024 MB uploadedwandb: - 0.013 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–ƒâ–„â–„â–‚â–ƒâ–‚â–â–â–‚â–â–‚â–â–‚â–‚
wandb:                                             train_de_pearson â–â–†â–…â–…â–‡â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡
wandb:                                                    train_mse â–ˆâ–‡â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–‚â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–ƒâ–†â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–â–…â–â–„â–ƒâ–ƒâ–ƒâ–‡â–‚â–…â–ƒâ–â–„â–†â–†â–„â–ƒâ–…â–‚â–ƒâ–ƒâ–„â–…â–‚â–…â–…â–ƒâ–ƒâ–‚â–‚â–„
wandb:                                                   val_de_mse â–ˆâ–â–‡â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb:                                               val_de_pearson â–â–ƒâ–†â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–ˆâ–†â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–ƒâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.11261
wandb:                                              test_de_pearson 0.88933
wandb:               test_frac_opposite_direction_top20_non_dropout 0.24783
wandb:                          test_frac_sigma_below_1_non_dropout 0.86957
wandb:                                                     test_mse 0.00283
wandb:                                test_mse_top20_de_non_dropout 0.11573
wandb:                                                 test_pearson 0.99476
wandb:                                           test_pearson_delta 0.38324
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.24783
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.86957
wandb:                                       test_unseen_single_mse 0.00283
wandb:                                    test_unseen_single_mse_de 0.11261
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.11573
wandb:                                   test_unseen_single_pearson 0.99476
wandb:                                test_unseen_single_pearson_de 0.88933
wandb:                             test_unseen_single_pearson_delta 0.38324
wandb:                                                 train_de_mse 0.04754
wandb:                                             train_de_pearson 0.91715
wandb:                                                    train_mse 0.0016
wandb:                                                train_pearson 0.99711
wandb:                                                training_loss 0.5456
wandb:                                                   val_de_mse 0.06145
wandb:                                               val_de_pearson 0.79598
wandb:                                                      val_mse 0.00324
wandb:                                                  val_pearson 0.9938
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406681_3_split3 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/2fnyr6o9
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_051259-2fnyr6o9/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406681_3/splits/adamsonweissman2016_gsm2406681_3_simulation_4_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:23
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_054824-11ci73as
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406681_3_split4
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/11ci73as
Start Training...
Epoch 1 Step 1 Train Loss: 1.6422
Epoch 1 Step 51 Train Loss: 0.6407
Epoch 1 Step 101 Train Loss: 0.6375
Epoch 1 Step 151 Train Loss: 0.6810
Epoch 1 Step 201 Train Loss: 0.6834
Epoch 1 Step 251 Train Loss: 0.7046
Epoch 1 Step 301 Train Loss: 0.5788
Epoch 1 Step 351 Train Loss: 0.5810
Epoch 1 Step 401 Train Loss: 0.5752
Epoch 1 Step 451 Train Loss: 0.6141
Epoch 1 Step 501 Train Loss: 0.5838
Epoch 1 Step 551 Train Loss: 0.6722
Epoch 1 Step 601 Train Loss: 0.5226
Epoch 1: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0878 Validation Top 20 DE MSE: 0.1002. 
Epoch 2 Step 1 Train Loss: 0.5927
Epoch 2 Step 51 Train Loss: 0.5269
Epoch 2 Step 101 Train Loss: 0.6016
Epoch 2 Step 151 Train Loss: 0.6037
Epoch 2 Step 201 Train Loss: 0.6165
Epoch 2 Step 251 Train Loss: 0.5559
Epoch 2 Step 301 Train Loss: 0.5579
Epoch 2 Step 351 Train Loss: 0.5902
Epoch 2 Step 401 Train Loss: 0.5965
Epoch 2 Step 451 Train Loss: 0.6115
Epoch 2 Step 501 Train Loss: 0.6101
Epoch 2 Step 551 Train Loss: 0.5518
Epoch 2 Step 601 Train Loss: 0.5638
Epoch 2: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0611 Validation Top 20 DE MSE: 0.0923. 
Epoch 3 Step 1 Train Loss: 0.5527
Epoch 3 Step 51 Train Loss: 0.5060
Epoch 3 Step 101 Train Loss: 0.5260
Epoch 3 Step 151 Train Loss: 0.5265
Epoch 3 Step 201 Train Loss: 0.5300
Epoch 3 Step 251 Train Loss: 0.5755
Epoch 3 Step 301 Train Loss: 0.5525
Epoch 3 Step 351 Train Loss: 0.5820
Epoch 3 Step 401 Train Loss: 0.5657
Epoch 3 Step 451 Train Loss: 0.5527
Epoch 3 Step 501 Train Loss: 0.5695
Epoch 3 Step 551 Train Loss: 0.5608
Epoch 3 Step 601 Train Loss: 0.5872
Epoch 3: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0566 Validation Top 20 DE MSE: 0.0935. 
Epoch 4 Step 1 Train Loss: 0.5999
Epoch 4 Step 51 Train Loss: 0.5541
Epoch 4 Step 101 Train Loss: 0.5796
Epoch 4 Step 151 Train Loss: 0.6174
Epoch 4 Step 201 Train Loss: 0.5062
Epoch 4 Step 251 Train Loss: 0.5024
Epoch 4 Step 301 Train Loss: 0.5849
Epoch 4 Step 351 Train Loss: 0.5836
Epoch 4 Step 401 Train Loss: 0.5821
Epoch 4 Step 451 Train Loss: 0.6048
Epoch 4 Step 501 Train Loss: 0.5346
Epoch 4 Step 551 Train Loss: 0.5131
Epoch 4 Step 601 Train Loss: 0.5402
Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0514 Validation Top 20 DE MSE: 0.0931. 
Epoch 5 Step 1 Train Loss: 0.5629
Epoch 5 Step 51 Train Loss: 0.4968
Epoch 5 Step 101 Train Loss: 0.5853
Epoch 5 Step 151 Train Loss: 0.5644
Epoch 5 Step 201 Train Loss: 0.6096
Epoch 5 Step 251 Train Loss: 0.6069
Epoch 5 Step 301 Train Loss: 0.5805
Epoch 5 Step 351 Train Loss: 0.5349
Epoch 5 Step 401 Train Loss: 0.5877
Epoch 5 Step 451 Train Loss: 0.5523
Epoch 5 Step 501 Train Loss: 0.5138
Epoch 5 Step 551 Train Loss: 0.6382
Epoch 5 Step 601 Train Loss: 0.5727
Epoch 5: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0496 Validation Top 20 DE MSE: 0.0905. 
Epoch 6 Step 1 Train Loss: 0.6403
Epoch 6 Step 51 Train Loss: 0.5681
Epoch 6 Step 101 Train Loss: 0.5910
Epoch 6 Step 151 Train Loss: 0.6458
Epoch 6 Step 201 Train Loss: 0.5758
Epoch 6 Step 251 Train Loss: 0.5144
Epoch 6 Step 301 Train Loss: 0.6157
Epoch 6 Step 351 Train Loss: 0.5405
Epoch 6 Step 401 Train Loss: 0.6047
Epoch 6 Step 451 Train Loss: 0.5623
Epoch 6 Step 501 Train Loss: 0.5305
Epoch 6 Step 551 Train Loss: 0.6638
Epoch 6 Step 601 Train Loss: 0.5746
Epoch 6: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0502 Validation Top 20 DE MSE: 0.0886. 
Epoch 7 Step 1 Train Loss: 0.5314
Epoch 7 Step 51 Train Loss: 0.5776
Epoch 7 Step 101 Train Loss: 0.5766
Epoch 7 Step 151 Train Loss: 0.5648
Epoch 7 Step 201 Train Loss: 0.5602
Epoch 7 Step 251 Train Loss: 0.5463
Epoch 7 Step 301 Train Loss: 0.5407
Epoch 7 Step 351 Train Loss: 0.5232
Epoch 7 Step 401 Train Loss: 0.5234
Epoch 7 Step 451 Train Loss: 0.5561
Epoch 7 Step 501 Train Loss: 0.5568
Epoch 7 Step 551 Train Loss: 0.5014
Epoch 7 Step 601 Train Loss: 0.5984
Epoch 7: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0479 Validation Top 20 DE MSE: 0.0894. 
Epoch 8 Step 1 Train Loss: 0.5697
Epoch 8 Step 51 Train Loss: 0.5156
Epoch 8 Step 101 Train Loss: 0.5972
Epoch 8 Step 151 Train Loss: 0.5768
Epoch 8 Step 201 Train Loss: 0.5685
Epoch 8 Step 251 Train Loss: 0.6124
Epoch 8 Step 301 Train Loss: 0.5669
Epoch 8 Step 351 Train Loss: 0.5786
Epoch 8 Step 401 Train Loss: 0.6002
Epoch 8 Step 451 Train Loss: 0.6020
Epoch 8 Step 501 Train Loss: 0.5197
Epoch 8 Step 551 Train Loss: 0.5224
Epoch 8 Step 601 Train Loss: 0.5750
Epoch 8: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0467 Validation Top 20 DE MSE: 0.0890. 
Epoch 9 Step 1 Train Loss: 0.5282
Epoch 9 Step 51 Train Loss: 0.5662
Epoch 9 Step 101 Train Loss: 0.5747
Epoch 9 Step 151 Train Loss: 0.5088
Epoch 9 Step 201 Train Loss: 0.5175
Epoch 9 Step 251 Train Loss: 0.5845
Epoch 9 Step 301 Train Loss: 0.5699
Epoch 9 Step 351 Train Loss: 0.5934
Epoch 9 Step 401 Train Loss: 0.5622
Epoch 9 Step 451 Train Loss: 0.4987
Epoch 9 Step 501 Train Loss: 0.4966
Epoch 9 Step 551 Train Loss: 0.5288
Epoch 9 Step 601 Train Loss: 0.6653
Epoch 9: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0467 Validation Top 20 DE MSE: 0.0896. 
Epoch 10 Step 1 Train Loss: 0.5870
Epoch 10 Step 51 Train Loss: 0.5674
Epoch 10 Step 101 Train Loss: 0.7164
Epoch 10 Step 151 Train Loss: 0.5005
Epoch 10 Step 201 Train Loss: 0.6109
Epoch 10 Step 251 Train Loss: 0.5113
Epoch 10 Step 301 Train Loss: 0.5201
Epoch 10 Step 351 Train Loss: 0.6211
Epoch 10 Step 401 Train Loss: 0.6061
Epoch 10 Step 451 Train Loss: 0.5496
Epoch 10 Step 501 Train Loss: 0.5329
Epoch 10 Step 551 Train Loss: 0.5136
Epoch 10 Step 601 Train Loss: 0.5754
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0468 Validation Top 20 DE MSE: 0.0890. 
Epoch 11 Step 1 Train Loss: 0.5175
Epoch 11 Step 51 Train Loss: 0.5148
Epoch 11 Step 101 Train Loss: 0.6146
Epoch 11 Step 151 Train Loss: 0.5431
Epoch 11 Step 201 Train Loss: 0.5821
Epoch 11 Step 251 Train Loss: 0.5343
Epoch 11 Step 301 Train Loss: 0.6002
Epoch 11 Step 351 Train Loss: 0.5526
Epoch 11 Step 401 Train Loss: 0.5542
Epoch 11 Step 451 Train Loss: 0.5529
Epoch 11 Step 501 Train Loss: 0.5811
Epoch 11 Step 551 Train Loss: 0.5596
Epoch 11 Step 601 Train Loss: 0.5483
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0457 Validation Top 20 DE MSE: 0.0890. 
Epoch 12 Step 1 Train Loss: 0.5622
Epoch 12 Step 51 Train Loss: 0.5586
Epoch 12 Step 101 Train Loss: 0.5441
Epoch 12 Step 151 Train Loss: 0.6650
Epoch 12 Step 201 Train Loss: 0.5508
Epoch 12 Step 251 Train Loss: 0.5276
Epoch 12 Step 301 Train Loss: 0.6255
Epoch 12 Step 351 Train Loss: 0.5799
Epoch 12 Step 401 Train Loss: 0.6108
Epoch 12 Step 451 Train Loss: 0.5403
Epoch 12 Step 501 Train Loss: 0.5760
Epoch 12 Step 551 Train Loss: 0.5894
Epoch 12 Step 601 Train Loss: 0.5444
Epoch 12: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0469 Validation Top 20 DE MSE: 0.0892. 
Epoch 13 Step 1 Train Loss: 0.5880
Epoch 13 Step 51 Train Loss: 0.6207
Epoch 13 Step 101 Train Loss: 0.5731
Epoch 13 Step 151 Train Loss: 0.5389
Epoch 13 Step 201 Train Loss: 0.6203
Epoch 13 Step 251 Train Loss: 0.5449
Epoch 13 Step 301 Train Loss: 0.5450
Epoch 13 Step 351 Train Loss: 0.6178
Epoch 13 Step 401 Train Loss: 0.6679
Epoch 13 Step 451 Train Loss: 0.6153
Epoch 13 Step 501 Train Loss: 0.6065
Epoch 13 Step 551 Train Loss: 0.5678
Epoch 13 Step 601 Train Loss: 0.5639
Epoch 13: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0482 Validation Top 20 DE MSE: 0.0890. 
Epoch 14 Step 1 Train Loss: 0.5483
Epoch 14 Step 51 Train Loss: 0.5486
Epoch 14 Step 101 Train Loss: 0.4881
Epoch 14 Step 151 Train Loss: 0.5560
Epoch 14 Step 201 Train Loss: 0.5941
Epoch 14 Step 251 Train Loss: 0.5901
Epoch 14 Step 301 Train Loss: 0.6253
Epoch 14 Step 351 Train Loss: 0.5850
Epoch 14 Step 401 Train Loss: 0.5869
Epoch 14 Step 451 Train Loss: 0.5962
Epoch 14 Step 501 Train Loss: 0.5167
Epoch 14 Step 551 Train Loss: 0.5561
Epoch 14 Step 601 Train Loss: 0.6654
Epoch 14: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0491 Validation Top 20 DE MSE: 0.0895. 
Epoch 15 Step 1 Train Loss: 0.5646
Epoch 15 Step 51 Train Loss: 0.5606
Epoch 15 Step 101 Train Loss: 0.5154
Epoch 15 Step 151 Train Loss: 0.5662
Epoch 15 Step 201 Train Loss: 0.5423
Epoch 15 Step 251 Train Loss: 0.5491
Epoch 15 Step 301 Train Loss: 0.5352
Epoch 15 Step 351 Train Loss: 0.5941
Epoch 15 Step 401 Train Loss: 0.5240
Epoch 15 Step 451 Train Loss: 0.5600
Epoch 15 Step 501 Train Loss: 0.5754
Epoch 15 Step 551 Train Loss: 0.5500
Epoch 15 Step 601 Train Loss: 0.5303
Epoch 15: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0455 Validation Top 20 DE MSE: 0.0895. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1383
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002867399
test_unseen_single_pearson: 0.9944779727466851
test_unseen_single_mse_de: 0.1382501
test_unseen_single_pearson_de: 0.8421136720806927
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3235458453861375
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2956521739130435
test_unseen_single_frac_sigma_below_1_non_dropout: 0.876086956521739
test_unseen_single_mse_top20_de_non_dropout: 0.14360029
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.003 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–
wandb:                                             train_de_pearson â–â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:                                                    train_mse â–ˆâ–„â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–„â–‚â–„â–â–‚â–ƒâ–‚â–‚â–‚â–…â–ƒâ–â–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–…â–„â–ƒâ–ƒâ–ƒ
wandb:                                                   val_de_mse â–ˆâ–ƒâ–„â–„â–‚â–â–â–â–‚â–â–â–â–â–‚â–‚
wandb:                                               val_de_pearson â–â–…â–‡â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                      val_mse â–ˆâ–ƒâ–†â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb:                                                  val_pearson â–â–†â–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.13825
wandb:                                              test_de_pearson 0.84211
wandb:               test_frac_opposite_direction_top20_non_dropout 0.29565
wandb:                          test_frac_sigma_below_1_non_dropout 0.87609
wandb:                                                     test_mse 0.00287
wandb:                                test_mse_top20_de_non_dropout 0.1436
wandb:                                                 test_pearson 0.99448
wandb:                                           test_pearson_delta 0.32355
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.29565
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.87609
wandb:                                       test_unseen_single_mse 0.00287
wandb:                                    test_unseen_single_mse_de 0.13825
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.1436
wandb:                                   test_unseen_single_pearson 0.99448
wandb:                                test_unseen_single_pearson_de 0.84211
wandb:                             test_unseen_single_pearson_delta 0.32355
wandb:                                                 train_de_mse 0.04552
wandb:                                             train_de_pearson 0.91671
wandb:                                                    train_mse 0.00162
wandb:                                                train_pearson 0.99707
wandb:                                                training_loss 0.57139
wandb:                                                   val_de_mse 0.08952
wandb:                                               val_de_pearson 0.94716
wandb:                                                      val_mse 0.00192
wandb:                                                  val_pearson 0.99631
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406681_3_split4 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/11ci73as
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_054824-11ci73as/logs
Creating new splits....
Saving new splits at /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/adamsonweissman2016_gsm2406681_3/splits/adamsonweissman2016_gsm2406681_3_simulation_5_0.75.pkl
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:23
Done!
Creating dataloaders....
Done!
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/benchmark_data/01A_total_re/03final/wandb/run-20240724_062229-91ihjz9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run AdamsonWeissman2016_GSM2406681_3_split5
wandb: â­ï¸ View project at https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: ðŸš€ View run at https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/91ihjz9j
Start Training...
Epoch 1 Step 1 Train Loss: 1.2885
Epoch 1 Step 51 Train Loss: 0.6460
Epoch 1 Step 101 Train Loss: 0.6034
Epoch 1 Step 151 Train Loss: 0.6495
Epoch 1 Step 201 Train Loss: 0.5630
Epoch 1 Step 251 Train Loss: 0.6362
Epoch 1 Step 301 Train Loss: 0.5960
Epoch 1 Step 351 Train Loss: 0.6288
Epoch 1 Step 401 Train Loss: 0.5989
Epoch 1 Step 451 Train Loss: 0.6021
Epoch 1 Step 501 Train Loss: 0.5856
Epoch 1 Step 551 Train Loss: 0.6334
Epoch 1 Step 601 Train Loss: 0.5486
Epoch 1 Step 651 Train Loss: 0.5798
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0925 Validation Top 20 DE MSE: 0.1001. 
Epoch 2 Step 1 Train Loss: 0.5619
Epoch 2 Step 51 Train Loss: 0.5419
Epoch 2 Step 101 Train Loss: 0.6791
Epoch 2 Step 151 Train Loss: 0.5493
Epoch 2 Step 201 Train Loss: 0.5861
Epoch 2 Step 251 Train Loss: 0.6258
Epoch 2 Step 301 Train Loss: 0.6331
Epoch 2 Step 351 Train Loss: 0.6126
Epoch 2 Step 401 Train Loss: 0.4949
Epoch 2 Step 451 Train Loss: 0.5452
Epoch 2 Step 501 Train Loss: 0.5152
Epoch 2 Step 551 Train Loss: 0.6058
Epoch 2 Step 601 Train Loss: 0.5831
Epoch 2 Step 651 Train Loss: 0.5213
Epoch 2: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0804 Validation Top 20 DE MSE: 0.1034. 
Epoch 3 Step 1 Train Loss: 0.5590
Epoch 3 Step 51 Train Loss: 0.5842
Epoch 3 Step 101 Train Loss: 0.5857
Epoch 3 Step 151 Train Loss: 0.5577
Epoch 3 Step 201 Train Loss: 0.6039
Epoch 3 Step 251 Train Loss: 0.5612
Epoch 3 Step 301 Train Loss: 0.5247
Epoch 3 Step 351 Train Loss: 0.5338
Epoch 3 Step 401 Train Loss: 0.5941
Epoch 3 Step 451 Train Loss: 0.5219
Epoch 3 Step 501 Train Loss: 0.5108
Epoch 3 Step 551 Train Loss: 0.5513
Epoch 3 Step 601 Train Loss: 0.5829
Epoch 3 Step 651 Train Loss: 0.5661
Epoch 3: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0496 Validation Top 20 DE MSE: 0.0774. 
Epoch 4 Step 1 Train Loss: 0.6014
Epoch 4 Step 51 Train Loss: 0.5131
Epoch 4 Step 101 Train Loss: 0.5604
Epoch 4 Step 151 Train Loss: 0.5885
Epoch 4 Step 201 Train Loss: 0.5803
Epoch 4 Step 251 Train Loss: 0.5387
Epoch 4 Step 301 Train Loss: 0.5571
Epoch 4 Step 351 Train Loss: 0.5823
Epoch 4 Step 401 Train Loss: 0.5899
Epoch 4 Step 451 Train Loss: 0.5301
Epoch 4 Step 501 Train Loss: 0.5671
Epoch 4 Step 551 Train Loss: 0.5643
Epoch 4 Step 601 Train Loss: 0.6211
Epoch 4 Step 651 Train Loss: 0.5960
Epoch 4: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0479 Validation Top 20 DE MSE: 0.0690. 
Epoch 5 Step 1 Train Loss: 0.6446
Epoch 5 Step 51 Train Loss: 0.5608
Epoch 5 Step 101 Train Loss: 0.5801
Epoch 5 Step 151 Train Loss: 0.5584
Epoch 5 Step 201 Train Loss: 0.5581
Epoch 5 Step 251 Train Loss: 0.5408
Epoch 5 Step 301 Train Loss: 0.5785
Epoch 5 Step 351 Train Loss: 0.5742
Epoch 5 Step 401 Train Loss: 0.5857
Epoch 5 Step 451 Train Loss: 0.5867
Epoch 5 Step 501 Train Loss: 0.5695
Epoch 5 Step 551 Train Loss: 0.5388
Epoch 5 Step 601 Train Loss: 0.5298
Epoch 5 Step 651 Train Loss: 0.5792
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0501 Validation Top 20 DE MSE: 0.0718. 
Epoch 6 Step 1 Train Loss: 0.5334
Epoch 6 Step 51 Train Loss: 0.5642
Epoch 6 Step 101 Train Loss: 0.5834
Epoch 6 Step 151 Train Loss: 0.5906
Epoch 6 Step 201 Train Loss: 0.5380
Epoch 6 Step 251 Train Loss: 0.5650
Epoch 6 Step 301 Train Loss: 0.5637
Epoch 6 Step 351 Train Loss: 0.5642
Epoch 6 Step 401 Train Loss: 0.5539
Epoch 6 Step 451 Train Loss: 0.5785
Epoch 6 Step 501 Train Loss: 0.5545
Epoch 6 Step 551 Train Loss: 0.5721
Epoch 6 Step 601 Train Loss: 0.5788
Epoch 6 Step 651 Train Loss: 0.6082
Epoch 6: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0479 Validation Top 20 DE MSE: 0.0716. 
Epoch 7 Step 1 Train Loss: 0.5552
Epoch 7 Step 51 Train Loss: 0.5894
Epoch 7 Step 101 Train Loss: 0.5733
Epoch 7 Step 151 Train Loss: 0.5795
Epoch 7 Step 201 Train Loss: 0.5440
Epoch 7 Step 251 Train Loss: 0.5433
Epoch 7 Step 301 Train Loss: 0.5685
Epoch 7 Step 351 Train Loss: 0.5258
Epoch 7 Step 401 Train Loss: 0.5554
Epoch 7 Step 451 Train Loss: 0.5717
Epoch 7 Step 501 Train Loss: 0.6087
Epoch 7 Step 551 Train Loss: 0.5944
Epoch 7 Step 601 Train Loss: 0.6009
Epoch 7 Step 651 Train Loss: 0.5556
Epoch 7: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0475 Validation Top 20 DE MSE: 0.0755. 
Epoch 8 Step 1 Train Loss: 0.5672
Epoch 8 Step 51 Train Loss: 0.5621
Epoch 8 Step 101 Train Loss: 0.5763
Epoch 8 Step 151 Train Loss: 0.6061
Epoch 8 Step 201 Train Loss: 0.6381
Epoch 8 Step 251 Train Loss: 0.5388
Epoch 8 Step 301 Train Loss: 0.5733
Epoch 8 Step 351 Train Loss: 0.5179
Epoch 8 Step 401 Train Loss: 0.5260
Epoch 8 Step 451 Train Loss: 0.6014
Epoch 8 Step 501 Train Loss: 0.5864
Epoch 8 Step 551 Train Loss: 0.5364
Epoch 8 Step 601 Train Loss: 0.5429
Epoch 8 Step 651 Train Loss: 0.5538
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0464 Validation Top 20 DE MSE: 0.0725. 
Epoch 9 Step 1 Train Loss: 0.5510
Epoch 9 Step 51 Train Loss: 0.5844
Epoch 9 Step 101 Train Loss: 0.6009
Epoch 9 Step 151 Train Loss: 0.5333
Epoch 9 Step 201 Train Loss: 0.6335
Epoch 9 Step 251 Train Loss: 0.5378
Epoch 9 Step 301 Train Loss: 0.5593
Epoch 9 Step 351 Train Loss: 0.5781
Epoch 9 Step 401 Train Loss: 0.5201
Epoch 9 Step 451 Train Loss: 0.5285
Epoch 9 Step 501 Train Loss: 0.6337
Epoch 9 Step 551 Train Loss: 0.4748
Epoch 9 Step 601 Train Loss: 0.5209
Epoch 9 Step 651 Train Loss: 0.5433
Epoch 9: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0461 Validation Top 20 DE MSE: 0.0746. 
Epoch 10 Step 1 Train Loss: 0.5547
Epoch 10 Step 51 Train Loss: 0.6234
Epoch 10 Step 101 Train Loss: 0.5043
Epoch 10 Step 151 Train Loss: 0.5721
Epoch 10 Step 201 Train Loss: 0.5656
Epoch 10 Step 251 Train Loss: 0.6253
Epoch 10 Step 301 Train Loss: 0.5948
Epoch 10 Step 351 Train Loss: 0.5348
Epoch 10 Step 401 Train Loss: 0.5714
Epoch 10 Step 451 Train Loss: 0.5375
Epoch 10 Step 501 Train Loss: 0.5477
Epoch 10 Step 551 Train Loss: 0.5571
Epoch 10 Step 601 Train Loss: 0.5424
Epoch 10 Step 651 Train Loss: 0.5739
Epoch 10: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0445 Validation Top 20 DE MSE: 0.0712. 
Epoch 11 Step 1 Train Loss: 0.6422
Epoch 11 Step 51 Train Loss: 0.5865
Epoch 11 Step 101 Train Loss: 0.4965
Epoch 11 Step 151 Train Loss: 0.5340
Epoch 11 Step 201 Train Loss: 0.5295
Epoch 11 Step 251 Train Loss: 0.6286
Epoch 11 Step 301 Train Loss: 0.5560
Epoch 11 Step 351 Train Loss: 0.5158
Epoch 11 Step 401 Train Loss: 0.5168
Epoch 11 Step 451 Train Loss: 0.5603
Epoch 11 Step 501 Train Loss: 0.5538
Epoch 11 Step 551 Train Loss: 0.5971
Epoch 11 Step 601 Train Loss: 0.6007
Epoch 11 Step 651 Train Loss: 0.5564
Epoch 11: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0477 Validation Top 20 DE MSE: 0.0731. 
Epoch 12 Step 1 Train Loss: 0.5309
Epoch 12 Step 51 Train Loss: 0.5480
Epoch 12 Step 101 Train Loss: 0.5876
Epoch 12 Step 151 Train Loss: 0.5900
Epoch 12 Step 201 Train Loss: 0.5756
Epoch 12 Step 251 Train Loss: 0.5423
Epoch 12 Step 301 Train Loss: 0.5072
Epoch 12 Step 351 Train Loss: 0.5603
Epoch 12 Step 401 Train Loss: 0.5566
Epoch 12 Step 451 Train Loss: 0.5644
Epoch 12 Step 501 Train Loss: 0.5710
Epoch 12 Step 551 Train Loss: 0.5062
Epoch 12 Step 601 Train Loss: 0.5699
Epoch 12 Step 651 Train Loss: 0.5401
Epoch 12: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0471 Validation Top 20 DE MSE: 0.0736. 
Epoch 13 Step 1 Train Loss: 0.5504
Epoch 13 Step 51 Train Loss: 0.5645
Epoch 13 Step 101 Train Loss: 0.6283
Epoch 13 Step 151 Train Loss: 0.5451
Epoch 13 Step 201 Train Loss: 0.6128
Epoch 13 Step 251 Train Loss: 0.5689
Epoch 13 Step 301 Train Loss: 0.5586
Epoch 13 Step 351 Train Loss: 0.5230
Epoch 13 Step 401 Train Loss: 0.5757
Epoch 13 Step 451 Train Loss: 0.5618
Epoch 13 Step 501 Train Loss: 0.5141
Epoch 13 Step 551 Train Loss: 0.5827
Epoch 13 Step 601 Train Loss: 0.5524
Epoch 13 Step 651 Train Loss: 0.5420
Epoch 13: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0462 Validation Top 20 DE MSE: 0.0736. 
Epoch 14 Step 1 Train Loss: 0.6114
Epoch 14 Step 51 Train Loss: 0.5792
Epoch 14 Step 101 Train Loss: 0.5865
Epoch 14 Step 151 Train Loss: 0.5966
Epoch 14 Step 201 Train Loss: 0.5643
Epoch 14 Step 251 Train Loss: 0.5482
Epoch 14 Step 301 Train Loss: 0.5900
Epoch 14 Step 351 Train Loss: 0.5326
Epoch 14 Step 401 Train Loss: 0.5778
Epoch 14 Step 451 Train Loss: 0.5956
Epoch 14 Step 501 Train Loss: 0.5618
Epoch 14 Step 551 Train Loss: 0.5941
Epoch 14 Step 601 Train Loss: 0.5584
Epoch 14 Step 651 Train Loss: 0.6064
Epoch 14: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0471 Validation Top 20 DE MSE: 0.0762. 
Epoch 15 Step 1 Train Loss: 0.6913
Epoch 15 Step 51 Train Loss: 0.5585
Epoch 15 Step 101 Train Loss: 0.5564
Epoch 15 Step 151 Train Loss: 0.5427
Epoch 15 Step 201 Train Loss: 0.4894
Epoch 15 Step 251 Train Loss: 0.6069
Epoch 15 Step 301 Train Loss: 0.5645
Epoch 15 Step 351 Train Loss: 0.5289
Epoch 15 Step 401 Train Loss: 0.5942
Epoch 15 Step 451 Train Loss: 0.6861
Epoch 15 Step 501 Train Loss: 0.6095
Epoch 15 Step 551 Train Loss: 0.5395
Epoch 15 Step 601 Train Loss: 0.5293
Epoch 15 Step 651 Train Loss: 0.5428
Epoch 15: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0454 Validation Top 20 DE MSE: 0.0716. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1195
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0028606437
test_unseen_single_pearson: 0.9945283968021665
test_unseen_single_mse_de: 0.11950146
test_unseen_single_pearson_de: 0.9294683329389954
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3700593291739345
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8717391304347827
test_unseen_single_mse_top20_de_non_dropout: 0.123010844
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.024 MB uploadedwandb: | 0.001 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse â–
wandb:                                              test_de_pearson â–
wandb:               test_frac_opposite_direction_top20_non_dropout â–
wandb:                          test_frac_sigma_below_1_non_dropout â–
wandb:                                                     test_mse â–
wandb:                                test_mse_top20_de_non_dropout â–
wandb:                                                 test_pearson â–
wandb:                                           test_pearson_delta â–
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout â–
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout â–
wandb:                                       test_unseen_single_mse â–
wandb:                                    test_unseen_single_mse_de â–
wandb:                  test_unseen_single_mse_top20_de_non_dropout â–
wandb:                                   test_unseen_single_pearson â–
wandb:                                test_unseen_single_pearson_de â–
wandb:                             test_unseen_single_pearson_delta â–
wandb:                                                 train_de_mse â–ˆâ–†â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                             train_de_pearson â–â–„â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                    train_mse â–ˆâ–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                                                train_pearson â–â–ƒâ–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                                                training_loss â–ˆâ–…â–‡â–†â–„â–‚â–…â–ƒâ–ƒâ–…â–…â–…â–ƒâ–†â–…â–‚â–„â–†â–†â–†â–…â–ƒâ–‡â–…â–†â–„â–„â–„â–…â–…â–„â–…â–„â–…â–„â–†â–„â–ƒâ–„â–
wandb:                                                   val_de_mse â–‡â–ˆâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:                                               val_de_pearson â–‚â–â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:                                                      val_mse â–†â–ˆâ–„â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–
wandb:                                                  val_pearson â–ƒâ–â–…â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.1195
wandb:                                              test_de_pearson 0.92947
wandb:               test_frac_opposite_direction_top20_non_dropout 0.25
wandb:                          test_frac_sigma_below_1_non_dropout 0.87174
wandb:                                                     test_mse 0.00286
wandb:                                test_mse_top20_de_non_dropout 0.12301
wandb:                                                 test_pearson 0.99453
wandb:                                           test_pearson_delta 0.37006
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.87174
wandb:                                       test_unseen_single_mse 0.00286
wandb:                                    test_unseen_single_mse_de 0.1195
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.12301
wandb:                                   test_unseen_single_pearson 0.99453
wandb:                                test_unseen_single_pearson_de 0.92947
wandb:                             test_unseen_single_pearson_delta 0.37006
wandb:                                                 train_de_mse 0.04542
wandb:                                             train_de_pearson 0.89645
wandb:                                                    train_mse 0.00155
wandb:                                                train_pearson 0.99719
wandb:                                                training_loss 0.62092
wandb:                                                   val_de_mse 0.0716
wandb:                                               val_de_pearson 0.83339
wandb:                                                      val_mse 0.00185
wandb:                                                  val_pearson 0.9965
wandb: 
wandb: ðŸš€ View run AdamsonWeissman2016_GSM2406681_3_split5 at: https://wandb.ai/zhoumin1130/01_dataset_all_gears/runs/91ihjz9j
wandb: â­ï¸ View project at: https://wandb.ai/zhoumin1130/01_dataset_all_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_062229-91ihjz9j/logs
