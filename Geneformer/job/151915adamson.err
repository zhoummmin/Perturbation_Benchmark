Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_221750-3l69y7zm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406675_1_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/3l69y7zm
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
  0%|                                                  | 0/3895 [00:00<?, ?it/s]  0%|                                          | 7/3895 [00:00<00:58, 66.56it/s]  0%|‚ñè                                        | 15/3895 [00:00<00:52, 73.63it/s]  1%|‚ñè                                        | 23/3895 [00:00<00:52, 73.76it/s]  1%|‚ñé                                        | 34/3895 [00:00<00:46, 82.78it/s]  1%|‚ñç                                        | 44/3895 [00:00<00:45, 85.41it/s]  1%|‚ñå                                        | 53/3895 [00:00<00:44, 86.43it/s]  2%|‚ñã                                        | 63/3895 [00:00<00:43, 87.98it/s]  2%|‚ñä                                        | 74/3895 [00:00<00:42, 89.07it/s]  2%|‚ñâ                                        | 84/3895 [00:00<00:41, 91.47it/s]  2%|‚ñâ                                        | 94/3895 [00:01<00:41, 90.99it/s]  3%|‚ñà                                       | 104/3895 [00:01<00:43, 88.15it/s]  3%|‚ñà‚ñè                                      | 114/3895 [00:01<00:42, 89.61it/s]  3%|‚ñà‚ñé                                      | 125/3895 [00:01<00:40, 92.50it/s]  3%|‚ñà‚ñç                                      | 135/3895 [00:01<00:41, 91.48it/s]  4%|‚ñà‚ñç                                      | 145/3895 [00:01<00:42, 88.84it/s]  4%|‚ñà‚ñå                                      | 154/3895 [00:01<00:42, 88.92it/s]  4%|‚ñà‚ñã                                      | 165/3895 [00:01<00:41, 89.99it/s]  4%|‚ñà‚ñä                                      | 175/3895 [00:01<00:41, 89.93it/s]  5%|‚ñà‚ñâ                                      | 186/3895 [00:02<00:39, 93.50it/s]  5%|‚ñà‚ñà                                      | 196/3895 [00:02<00:39, 92.76it/s]  5%|‚ñà‚ñà                                      | 206/3895 [00:02<00:40, 91.69it/s]  6%|‚ñà‚ñà‚ñè                                     | 216/3895 [00:02<00:41, 88.16it/s]  6%|‚ñà‚ñà‚ñé                                     | 227/3895 [00:02<00:40, 91.44it/s]  6%|‚ñà‚ñà‚ñç                                     | 237/3895 [00:02<00:41, 88.53it/s]  6%|‚ñà‚ñà‚ñå                                     | 248/3895 [00:02<00:39, 91.65it/s]  7%|‚ñà‚ñà‚ñã                                     | 258/3895 [00:02<00:40, 90.13it/s]  7%|‚ñà‚ñà‚ñä                                     | 268/3895 [00:03<00:40, 89.29it/s]  7%|‚ñà‚ñà‚ñä                                     | 277/3895 [00:03<00:41, 86.36it/s]  7%|‚ñà‚ñà‚ñâ                                     | 287/3895 [00:03<00:41, 87.50it/s]  8%|‚ñà‚ñà‚ñà                                     | 296/3895 [00:03<00:40, 87.80it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 305/3895 [00:03<00:42, 84.96it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 314/3895 [00:03<00:43, 82.89it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 323/3895 [00:03<00:45, 78.96it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 332/3895 [00:03<00:43, 81.52it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 341/3895 [00:03<00:43, 81.08it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 350/3895 [00:04<00:43, 80.67it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 359/3895 [00:04<00:45, 77.48it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 368/3895 [00:04<00:43, 80.63it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 377/3895 [00:04<00:43, 80.11it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 386/3895 [00:04<00:43, 80.03it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 395/3895 [00:04<00:43, 79.89it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 404/3895 [00:04<00:43, 79.65it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 412/3895 [00:04<00:43, 79.69it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 420/3895 [00:04<00:45, 76.92it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 428/3895 [00:05<00:44, 77.62it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 437/3895 [00:05<00:44, 77.82it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 446/3895 [00:05<00:42, 80.97it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 455/3895 [00:05<00:42, 80.57it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 464/3895 [00:05<00:42, 80.47it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 473/3895 [00:05<00:44, 77.70it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 482/3895 [00:05<00:43, 78.64it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 490/3895 [00:05<00:46, 73.34it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 498/3895 [00:05<00:48, 69.90it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 506/3895 [00:06<00:50, 67.44it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 513/3895 [00:06<00:51, 65.87it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 520/3895 [00:06<00:51, 66.13it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 527/3895 [00:06<00:51, 64.98it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 534/3895 [00:06<00:51, 64.75it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 541/3895 [00:06<00:52, 64.45it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 548/3895 [00:06<00:51, 64.60it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 555/3895 [00:06<00:52, 64.18it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 562/3895 [00:06<00:53, 62.77it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 569/3895 [00:07<00:55, 59.42it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 577/3895 [00:07<00:53, 62.28it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 584/3895 [00:07<00:52, 63.23it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 591/3895 [00:07<00:51, 64.18it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 598/3895 [00:07<00:51, 63.88it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 605/3895 [00:07<00:51, 63.41it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 612/3895 [00:07<00:55, 59.55it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 619/3895 [00:07<00:52, 62.23it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 626/3895 [00:08<00:53, 61.50it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 633/3895 [00:08<00:52, 62.32it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 640/3895 [00:08<00:51, 62.80it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 647/3895 [00:08<00:51, 63.07it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 654/3895 [00:08<00:51, 63.23it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 661/3895 [00:08<00:51, 63.38it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 668/3895 [00:08<00:51, 63.08it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 675/3895 [00:08<00:51, 62.81it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 682/3895 [00:08<00:51, 62.79it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 689/3895 [00:09<00:50, 63.02it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 696/3895 [00:09<00:50, 63.06it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 703/3895 [00:09<00:50, 63.44it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 710/3895 [00:09<00:50, 63.06it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 717/3895 [00:09<00:50, 63.13it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 724/3895 [00:09<00:50, 62.72it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 731/3895 [00:09<00:51, 62.04it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 738/3895 [00:09<00:50, 62.14it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 745/3895 [00:09<00:50, 62.50it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 752/3895 [00:10<00:50, 62.74it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 759/3895 [00:10<00:49, 62.83it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 766/3895 [00:10<00:49, 62.67it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 773/3895 [00:10<00:49, 63.00it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 780/3895 [00:10<00:49, 62.93it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 787/3895 [00:10<00:49, 63.13it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 794/3895 [00:10<00:48, 63.61it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 801/3895 [00:10<00:48, 63.86it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 808/3895 [00:10<00:48, 63.18it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 815/3895 [00:11<00:48, 63.25it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 822/3895 [00:11<00:49, 62.50it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 829/3895 [00:11<00:48, 62.70it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 836/3895 [00:11<00:48, 62.96it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 843/3895 [00:11<00:48, 63.12it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 850/3895 [00:11<00:48, 63.44it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 857/3895 [00:11<00:48, 62.33it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 864/3895 [00:11<00:50, 59.99it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 872/3895 [00:11<00:50, 60.36it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 880/3895 [00:12<00:47, 63.58it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 887/3895 [00:12<00:47, 63.78it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 894/3895 [00:12<00:47, 63.84it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 901/3895 [00:12<00:48, 62.01it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 908/3895 [00:12<00:46, 64.14it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 915/3895 [00:12<00:47, 62.77it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 922/3895 [00:12<00:47, 62.39it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 929/3895 [00:12<00:47, 62.27it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 936/3895 [00:12<00:46, 63.04it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 943/3895 [00:13<00:46, 63.43it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 950/3895 [00:13<00:46, 63.23it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 957/3895 [00:13<00:46, 63.17it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 964/3895 [00:13<00:46, 63.21it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 971/3895 [00:13<00:45, 63.61it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 978/3895 [00:13<00:46, 63.24it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 985/3895 [00:13<00:46, 63.19it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 992/3895 [00:13<00:45, 63.35it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 999/3895 [00:13<00:47, 60.62it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1007/3895 [00:14<00:45, 63.79it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1014/3895 [00:14<00:45, 63.48it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1021/3895 [00:14<00:45, 63.59it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1028/3895 [00:14<00:45, 62.67it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1035/3895 [00:14<00:46, 62.02it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1042/3895 [00:14<00:46, 61.97it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1049/3895 [00:14<00:45, 62.43it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1056/3895 [00:14<00:45, 62.24it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1063/3895 [00:14<00:45, 62.03it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1070/3895 [00:15<00:45, 62.17it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1077/3895 [00:15<00:45, 62.24it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1084/3895 [00:15<00:44, 63.14it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1091/3895 [00:15<00:44, 63.37it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1098/3895 [00:15<00:44, 63.41it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1105/3895 [00:15<00:44, 62.80it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1112/3895 [00:15<00:44, 62.41it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1119/3895 [00:15<00:45, 60.50it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1127/3895 [00:15<00:44, 61.59it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1135/3895 [00:16<00:42, 64.56it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1142/3895 [00:16<00:42, 64.04it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1149/3895 [00:16<00:42, 63.89it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1156/3895 [00:16<00:43, 63.59it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1163/3895 [00:16<00:43, 63.31it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1170/3895 [00:16<00:43, 62.17it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1177/3895 [00:16<00:43, 62.00it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1184/3895 [00:16<00:43, 62.77it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1191/3895 [00:16<00:42, 63.39it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1198/3895 [00:17<00:42, 63.82it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1205/3895 [00:17<00:42, 63.75it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1212/3895 [00:17<00:42, 62.90it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1219/3895 [00:17<00:43, 62.06it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1226/3895 [00:17<00:43, 61.91it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1233/3895 [00:17<00:42, 62.68it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1240/3895 [00:17<00:43, 60.63it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1248/3895 [00:17<00:41, 64.35it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1255/3895 [00:18<00:41, 63.95it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1262/3895 [00:18<00:41, 64.15it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1269/3895 [00:18<00:41, 63.55it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1276/3895 [00:18<00:41, 63.24it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1283/3895 [00:18<00:41, 63.52it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1290/3895 [00:18<00:40, 64.18it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1297/3895 [00:18<00:40, 64.09it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1304/3895 [00:18<00:40, 63.89it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1311/3895 [00:18<00:40, 63.27it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1318/3895 [00:18<00:40, 63.15it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1325/3895 [00:19<00:40, 63.05it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1332/3895 [00:19<00:40, 62.62it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1339/3895 [00:19<00:40, 62.71it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1346/3895 [00:19<00:40, 62.51it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1353/3895 [00:19<00:40, 62.44it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1360/3895 [00:19<00:40, 62.93it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1367/3895 [00:19<00:40, 62.75it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1374/3895 [00:19<00:39, 63.18it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1381/3895 [00:19<00:39, 63.59it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1388/3895 [00:20<00:39, 63.65it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1395/3895 [00:20<00:39, 63.68it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1402/3895 [00:20<00:38, 63.97it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1409/3895 [00:20<00:38, 63.86it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1416/3895 [00:20<00:38, 64.28it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1423/3895 [00:20<00:38, 64.00it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1430/3895 [00:20<00:38, 64.18it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1437/3895 [00:20<00:38, 64.27it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1444/3895 [00:20<00:38, 64.31it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1451/3895 [00:21<00:39, 61.92it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1458/3895 [00:21<00:39, 61.91it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1466/3895 [00:21<00:37, 64.27it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1473/3895 [00:21<00:38, 63.14it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1480/3895 [00:21<00:37, 63.61it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1487/3895 [00:21<00:37, 63.70it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1494/3895 [00:21<00:37, 63.89it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1501/3895 [00:21<00:38, 61.39it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1508/3895 [00:22<00:38, 61.74it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1516/3895 [00:22<00:38, 61.25it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1524/3895 [00:22<00:38, 61.84it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1532/3895 [00:22<00:36, 65.27it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1539/3895 [00:22<00:36, 65.01it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1546/3895 [00:22<00:36, 64.71it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1553/3895 [00:22<00:36, 64.90it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1560/3895 [00:22<00:36, 64.50it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1567/3895 [00:22<00:36, 63.81it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1574/3895 [00:23<00:37, 61.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1582/3895 [00:23<00:35, 64.97it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1589/3895 [00:23<00:35, 64.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1596/3895 [00:23<00:35, 64.53it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1603/3895 [00:23<00:35, 64.23it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1610/3895 [00:23<00:35, 64.07it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1617/3895 [00:23<00:35, 63.71it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1624/3895 [00:23<00:35, 63.33it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1631/3895 [00:23<00:35, 63.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1638/3895 [00:24<00:36, 62.50it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1645/3895 [00:24<00:35, 62.72it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1652/3895 [00:24<00:35, 62.76it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1659/3895 [00:24<00:35, 63.04it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1666/3895 [00:24<00:35, 62.99it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1673/3895 [00:24<00:35, 62.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1680/3895 [00:24<00:35, 63.25it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1687/3895 [00:24<00:34, 63.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1694/3895 [00:24<00:34, 63.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1701/3895 [00:25<00:35, 61.67it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1709/3895 [00:25<00:33, 65.28it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1716/3895 [00:25<00:33, 64.44it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1723/3895 [00:25<00:33, 64.51it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1730/3895 [00:25<00:33, 64.35it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1737/3895 [00:25<00:33, 64.77it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1744/3895 [00:25<00:33, 64.69it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1751/3895 [00:25<00:33, 64.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1758/3895 [00:25<00:34, 61.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1766/3895 [00:26<00:33, 64.27it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1773/3895 [00:26<00:33, 63.82it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1780/3895 [00:26<00:33, 63.69it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1787/3895 [00:26<00:32, 64.13it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1794/3895 [00:26<00:32, 64.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1801/3895 [00:26<00:32, 64.86it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1808/3895 [00:26<00:33, 61.90it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1815/3895 [00:26<00:32, 64.01it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1822/3895 [00:26<00:32, 63.49it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1829/3895 [00:27<00:32, 63.72it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1836/3895 [00:27<00:32, 63.85it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1843/3895 [00:27<00:32, 63.90it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1850/3895 [00:27<00:31, 64.13it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1857/3895 [00:27<00:31, 64.01it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1864/3895 [00:27<00:31, 64.04it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1871/3895 [00:27<00:31, 64.09it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1878/3895 [00:27<00:32, 61.48it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1886/3895 [00:27<00:30, 64.83it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1893/3895 [00:28<00:32, 62.44it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1901/3895 [00:28<00:30, 65.59it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1908/3895 [00:28<00:30, 65.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1915/3895 [00:28<00:30, 65.09it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1922/3895 [00:28<00:30, 64.80it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1929/3895 [00:28<00:30, 64.44it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1936/3895 [00:28<00:30, 63.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1943/3895 [00:28<00:30, 63.49it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1950/3895 [00:28<00:30, 63.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1957/3895 [00:29<00:30, 63.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1964/3895 [00:29<00:30, 63.67it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1971/3895 [00:29<00:30, 63.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1978/3895 [00:29<00:31, 60.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1986/3895 [00:29<00:29, 64.20it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1993/3895 [00:29<00:29, 64.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2000/3895 [00:29<00:29, 63.86it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2007/3895 [00:29<00:29, 64.10it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2014/3895 [00:29<00:30, 61.47it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2022/3895 [00:30<00:28, 65.21it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2029/3895 [00:30<00:28, 64.80it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2036/3895 [00:30<00:28, 64.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2043/3895 [00:30<00:28, 64.87it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2050/3895 [00:30<00:28, 64.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2057/3895 [00:30<00:28, 64.96it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2064/3895 [00:30<00:28, 64.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2071/3895 [00:30<00:28, 64.15it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2078/3895 [00:30<00:29, 61.65it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2086/3895 [00:31<00:27, 65.12it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2093/3895 [00:31<00:27, 64.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2100/3895 [00:31<00:27, 64.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2107/3895 [00:31<00:27, 64.65it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2114/3895 [00:31<00:29, 60.94it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2121/3895 [00:31<00:28, 62.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2128/3895 [00:31<00:27, 63.18it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2136/3895 [00:31<00:26, 66.02it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2143/3895 [00:31<00:26, 65.98it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2150/3895 [00:32<00:26, 65.61it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2157/3895 [00:32<00:26, 65.31it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2164/3895 [00:32<00:27, 62.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2172/3895 [00:32<00:26, 66.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2179/3895 [00:32<00:25, 66.07it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2186/3895 [00:32<00:26, 65.68it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2193/3895 [00:32<00:26, 65.33it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2200/3895 [00:32<00:26, 65.00it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2207/3895 [00:32<00:26, 64.51it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2214/3895 [00:33<00:25, 64.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2221/3895 [00:33<00:25, 64.70it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2228/3895 [00:33<00:25, 64.72it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2235/3895 [00:33<00:25, 64.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2242/3895 [00:33<00:25, 64.18it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2249/3895 [00:33<00:25, 64.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2256/3895 [00:33<00:25, 63.79it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2263/3895 [00:33<00:25, 64.06it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2270/3895 [00:33<00:25, 63.93it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2277/3895 [00:34<00:25, 64.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2284/3895 [00:34<00:24, 65.10it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2291/3895 [00:34<00:24, 64.95it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2298/3895 [00:34<00:24, 65.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2305/3895 [00:34<00:25, 62.44it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2313/3895 [00:34<00:23, 66.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2320/3895 [00:34<00:23, 66.11it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2327/3895 [00:34<00:23, 66.11it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2334/3895 [00:34<00:23, 66.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2341/3895 [00:34<00:23, 66.11it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2348/3895 [00:35<00:23, 65.77it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2355/3895 [00:35<00:24, 62.44it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2363/3895 [00:35<00:23, 65.67it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2370/3895 [00:35<00:23, 64.98it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2377/3895 [00:35<00:23, 65.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2384/3895 [00:35<00:23, 64.99it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2391/3895 [00:35<00:23, 65.34it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2398/3895 [00:35<00:23, 62.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2406/3895 [00:35<00:22, 66.28it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2413/3895 [00:36<00:22, 65.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2420/3895 [00:36<00:23, 63.98it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2427/3895 [00:36<00:22, 64.80it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2434/3895 [00:36<00:22, 64.42it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2441/3895 [00:36<00:22, 64.30it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2448/3895 [00:36<00:22, 64.43it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2455/3895 [00:36<00:23, 61.66it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2463/3895 [00:36<00:22, 64.76it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2470/3895 [00:36<00:21, 66.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2477/3895 [00:37<00:21, 65.66it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2484/3895 [00:37<00:21, 64.97it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2492/3895 [00:37<00:20, 67.21it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2500/3895 [00:37<00:20, 69.05it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2507/3895 [00:37<00:20, 67.66it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2516/3895 [00:37<00:18, 72.74it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2524/3895 [00:37<00:18, 73.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2532/3895 [00:37<00:18, 73.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2540/3895 [00:37<00:18, 73.93it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2548/3895 [00:38<00:18, 74.36it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2556/3895 [00:38<00:17, 74.41it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2564/3895 [00:38<00:18, 72.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2573/3895 [00:38<00:17, 75.97it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2581/3895 [00:38<00:17, 75.74it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2589/3895 [00:38<00:17, 76.09it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2597/3895 [00:38<00:17, 73.05it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2606/3895 [00:38<00:16, 76.38it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2614/3895 [00:38<00:16, 76.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2622/3895 [00:39<00:16, 76.24it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2630/3895 [00:39<00:16, 76.52it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2638/3895 [00:39<00:16, 76.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2646/3895 [00:39<00:16, 76.00it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2654/3895 [00:39<00:16, 76.10it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2662/3895 [00:39<00:16, 75.67it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2670/3895 [00:39<00:16, 75.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2678/3895 [00:39<00:16, 74.30it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2688/3895 [00:39<00:14, 80.70it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2697/3895 [00:40<00:15, 78.71it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2706/3895 [00:40<00:15, 77.44it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2714/3895 [00:40<00:15, 75.96it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2723/3895 [00:40<00:15, 76.71it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2732/3895 [00:40<00:14, 79.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2740/3895 [00:40<00:15, 73.70it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2748/3895 [00:40<00:16, 70.21it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2756/3895 [00:40<00:16, 70.26it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2764/3895 [00:40<00:17, 65.91it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2772/3895 [00:41<00:16, 67.28it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2782/3895 [00:41<00:15, 72.30it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2791/3895 [00:41<00:14, 75.21it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2801/3895 [00:41<00:14, 76.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2812/3895 [00:41<00:12, 84.09it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2821/3895 [00:41<00:12, 84.40it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2830/3895 [00:41<00:12, 82.01it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2840/3895 [00:41<00:12, 83.15it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2849/3895 [00:41<00:12, 84.38it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2859/3895 [00:42<00:12, 85.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2869/3895 [00:42<00:11, 87.67it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2879/3895 [00:42<00:11, 90.33it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2889/3895 [00:42<00:11, 86.67it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2898/3895 [00:42<00:11, 87.01it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2908/3895 [00:42<00:11, 89.13it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2917/3895 [00:42<00:11, 88.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2926/3895 [00:42<00:11, 84.96it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2936/3895 [00:42<00:11, 84.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2946/3895 [00:43<00:10, 87.43it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2955/3895 [00:43<00:11, 84.97it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2965/3895 [00:43<00:10, 85.89it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2975/3895 [00:43<00:10, 88.28it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2984/3895 [00:43<00:10, 87.36it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2993/3895 [00:43<00:10, 87.38it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3002/3895 [00:43<00:10, 84.53it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3011/3895 [00:43<00:10, 84.90it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3021/3895 [00:43<00:10, 82.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3032/3895 [00:44<00:09, 86.99it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3042/3895 [00:44<00:09, 88.96it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3051/3895 [00:44<00:09, 87.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3060/3895 [00:44<00:09, 85.44it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3070/3895 [00:44<00:09, 88.45it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3079/3895 [00:44<00:09, 85.07it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3088/3895 [00:44<00:09, 85.78it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3097/3895 [00:44<00:09, 86.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3106/3895 [00:44<00:09, 86.67it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3116/3895 [00:45<00:08, 89.00it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3125/3895 [00:45<00:08, 88.18it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3134/3895 [00:45<00:08, 88.00it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3143/3895 [00:45<00:08, 86.85it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3152/3895 [00:45<00:08, 86.83it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3161/3895 [00:45<00:09, 81.50it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3171/3895 [00:45<00:08, 85.43it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3181/3895 [00:45<00:08, 88.80it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3190/3895 [00:45<00:07, 88.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3199/3895 [00:46<00:08, 85.94it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3208/3895 [00:46<00:07, 86.71it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3217/3895 [00:46<00:07, 87.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3227/3895 [00:46<00:07, 90.09it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3237/3895 [00:46<00:07, 86.74it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3246/3895 [00:46<00:07, 86.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3256/3895 [00:46<00:07, 89.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3265/3895 [00:46<00:07, 89.01it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3274/3895 [00:46<00:07, 86.52it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3284/3895 [00:46<00:06, 89.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3294/3895 [00:47<00:06, 87.35it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3304/3895 [00:47<00:06, 89.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3314/3895 [00:47<00:06, 86.37it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3323/3895 [00:47<00:06, 87.36it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3332/3895 [00:47<00:06, 87.47it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3342/3895 [00:47<00:06, 90.51it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3352/3895 [00:47<00:06, 86.36it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3362/3895 [00:47<00:06, 86.55it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3372/3895 [00:47<00:06, 87.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3381/3895 [00:48<00:05, 86.87it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3391/3895 [00:48<00:05, 87.07it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3401/3895 [00:48<00:05, 90.60it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3411/3895 [00:48<00:05, 89.12it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3420/3895 [00:48<00:05, 86.17it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3429/3895 [00:48<00:05, 86.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3438/3895 [00:48<00:05, 87.23it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3448/3895 [00:48<00:05, 88.36it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3458/3895 [00:48<00:04, 89.05it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3468/3895 [00:49<00:04, 90.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3478/3895 [00:49<00:04, 90.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3488/3895 [00:49<00:04, 90.36it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3498/3895 [00:49<00:04, 92.77it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3508/3895 [00:49<00:04, 89.52it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3518/3895 [00:49<00:04, 92.40it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3528/3895 [00:49<00:04, 89.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3538/3895 [00:49<00:03, 91.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3548/3895 [00:49<00:03, 89.05it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3558/3895 [00:50<00:03, 89.10it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3567/3895 [00:50<00:03, 87.21it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3577/3895 [00:50<00:03, 90.40it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3587/3895 [00:50<00:03, 92.95it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3597/3895 [00:50<00:03, 92.28it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3607/3895 [00:50<00:03, 91.34it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3617/3895 [00:50<00:03, 85.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3628/3895 [00:50<00:02, 89.43it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3638/3895 [00:50<00:02, 89.45it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3648/3895 [00:51<00:02, 90.51it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3658/3895 [00:51<00:02, 90.48it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3668/3895 [00:51<00:02, 91.44it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3679/3895 [00:51<00:02, 94.70it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3689/3895 [00:51<00:02, 90.56it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3699/3895 [00:51<00:02, 92.87it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3709/3895 [00:51<00:02, 91.72it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3719/3895 [00:51<00:01, 92.00it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3729/3895 [00:51<00:01, 88.52it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3738/3895 [00:52<00:01, 86.94it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3749/3895 [00:52<00:01, 91.59it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3759/3895 [00:52<00:01, 91.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3770/3895 [00:52<00:01, 94.25it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3780/3895 [00:52<00:01, 93.85it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3790/3895 [00:52<00:01, 93.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3800/3895 [00:52<00:01, 90.75it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3810/3895 [00:52<00:00, 90.42it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3820/3895 [00:52<00:00, 92.20it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3830/3895 [00:53<00:00, 92.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3841/3895 [00:53<00:00, 95.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3851/3895 [00:53<00:00, 87.68it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3864/3895 [00:53<00:00, 94.22it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3875/3895 [00:53<00:00, 94.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3885/3895 [00:53<00:00, 93.15it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3895/3895 [00:53<00:00, 72.50it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.2525
Epoch 1 Step 51 Train Loss: 0.2638
Epoch 1 Step 101 Train Loss: 0.2618
Epoch 1: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0181 Validation Top 20 DE MSE: 0.0573. 
Epoch 2 Step 1 Train Loss: 0.2705
Epoch 2 Step 51 Train Loss: 0.2583
Epoch 2 Step 101 Train Loss: 0.2655
Epoch 2: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0159 Validation Top 20 DE MSE: 0.0539. 
Epoch 3 Step 1 Train Loss: 0.2665
Epoch 3 Step 51 Train Loss: 0.2579
Epoch 3 Step 101 Train Loss: 0.2589
Epoch 3: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.0696. 
Epoch 4 Step 1 Train Loss: 0.2622
Epoch 4 Step 51 Train Loss: 0.2542
Epoch 4 Step 101 Train Loss: 0.2548
Epoch 4: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0627. 
Epoch 5 Step 1 Train Loss: 0.2332
Epoch 5 Step 51 Train Loss: 0.2561
Epoch 5 Step 101 Train Loss: 0.2643
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0575. 
Epoch 6 Step 1 Train Loss: 0.2646
Epoch 6 Step 51 Train Loss: 0.2673
Epoch 6 Step 101 Train Loss: 0.2559
Epoch 6: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0051 Validation Top 20 DE MSE: 0.0619. 
Epoch 7 Step 1 Train Loss: 0.2594
Epoch 7 Step 51 Train Loss: 0.2403
Epoch 7 Step 101 Train Loss: 0.2512
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0535. 
Epoch 8 Step 1 Train Loss: 0.2615
Epoch 8 Step 51 Train Loss: 0.2507
Epoch 8 Step 101 Train Loss: 0.2604
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0529. 
Epoch 9 Step 1 Train Loss: 0.2690
Epoch 9 Step 51 Train Loss: 0.2680
Epoch 9 Step 101 Train Loss: 0.2541
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0538. 
Epoch 10 Step 1 Train Loss: 0.2481
Epoch 10 Step 51 Train Loss: 0.2496
Epoch 10 Step 101 Train Loss: 0.2824
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0561. 
Epoch 11 Step 1 Train Loss: 0.2511
Epoch 11 Step 51 Train Loss: 0.2481
Epoch 11 Step 101 Train Loss: 0.2538
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0565. 
Epoch 12 Step 1 Train Loss: 0.2431
Epoch 12 Step 51 Train Loss: 0.2401
Epoch 12 Step 101 Train Loss: 0.2640
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0572. 
Epoch 13 Step 1 Train Loss: 0.2507
Epoch 13 Step 51 Train Loss: 0.2553
Epoch 13 Step 101 Train Loss: 0.2575
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0554. 
Epoch 14 Step 1 Train Loss: 0.2500
Epoch 14 Step 51 Train Loss: 0.2499
Epoch 14 Step 101 Train Loss: 0.2623
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0560. 
Epoch 15 Step 1 Train Loss: 0.2556
Epoch 15 Step 51 Train Loss: 0.2575
Epoch 15 Step 101 Train Loss: 0.2641
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0548. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1286
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0015413172
test_unseen_single_pearson: 0.9954886819808353
test_unseen_single_mse_de: 0.12864856
test_unseen_single_pearson_de: 0.9507445319404366
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.13272986734165565
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.42500000000000004
test_unseen_single_frac_sigma_below_1_non_dropout: 0.85
test_unseen_single_mse_top20_de_non_dropout: 0.12864858
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.003 MB of 0.020 MB uploadedwandb: / 0.010 MB of 0.020 MB uploadedwandb: - 0.014 MB of 0.020 MB uploadedwandb: \ 0.014 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ
wandb:                                                   val_de_mse ‚ñÉ‚ñÅ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÑ‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.12865
wandb:                                              test_de_pearson 0.95074
wandb:               test_frac_opposite_direction_top20_non_dropout 0.425
wandb:                          test_frac_sigma_below_1_non_dropout 0.85
wandb:                                                     test_mse 0.00154
wandb:                                test_mse_top20_de_non_dropout 0.12865
wandb:                                                 test_pearson 0.99549
wandb:                                           test_pearson_delta 0.13273
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.425
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.85
wandb:                                       test_unseen_single_mse 0.00154
wandb:                                    test_unseen_single_mse_de 0.12865
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.12865
wandb:                                   test_unseen_single_pearson 0.99549
wandb:                                test_unseen_single_pearson_de 0.95074
wandb:                             test_unseen_single_pearson_delta 0.13273
wandb:                                                 train_de_mse 0.00389
wandb:                                             train_de_pearson 0.99903
wandb:                                                    train_mse 0.00056
wandb:                                                train_pearson 0.99837
wandb:                                                training_loss 0.26113
wandb:                                                   val_de_mse 0.05483
wandb:                                               val_de_pearson 0.97833
wandb:                                                      val_mse 0.00123
wandb:                                                  val_pearson 0.99639
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406675_1_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/3l69y7zm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_221750-3l69y7zm/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222155-iipglplf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406675_1_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/iipglplf
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2563
Epoch 1 Step 51 Train Loss: 0.2588
Epoch 1 Step 101 Train Loss: 0.2613
Epoch 1: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0304 Validation Top 20 DE MSE: 0.0080. 
Epoch 2 Step 1 Train Loss: 0.2525
Epoch 2 Step 51 Train Loss: 0.2661
Epoch 2 Step 101 Train Loss: 0.2579
Epoch 2: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0164 Validation Top 20 DE MSE: 0.0094. 
Epoch 3 Step 1 Train Loss: 0.2506
Epoch 3 Step 51 Train Loss: 0.2519
Epoch 3 Step 101 Train Loss: 0.2562
Epoch 3: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0068. 
Epoch 4 Step 1 Train Loss: 0.2553
Epoch 4 Step 51 Train Loss: 0.2495
Epoch 4 Step 101 Train Loss: 0.2755
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0131 Validation Top 20 DE MSE: 0.0058. 
Epoch 5 Step 1 Train Loss: 0.2536
Epoch 5 Step 51 Train Loss: 0.2644
Epoch 5 Step 101 Train Loss: 0.2569
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0064. 
Epoch 6 Step 1 Train Loss: 0.2604
Epoch 6 Step 51 Train Loss: 0.2551
Epoch 6 Step 101 Train Loss: 0.2732
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0064. 
Epoch 7 Step 1 Train Loss: 0.2757
Epoch 7 Step 51 Train Loss: 0.2597
Epoch 7 Step 101 Train Loss: 0.2648
Epoch 7: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0047 Validation Top 20 DE MSE: 0.0057. 
Epoch 8 Step 1 Train Loss: 0.2573
Epoch 8 Step 51 Train Loss: 0.2552
Epoch 8 Step 101 Train Loss: 0.2558
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0068. 
Epoch 9 Step 1 Train Loss: 0.2516
Epoch 9 Step 51 Train Loss: 0.2470
Epoch 9 Step 101 Train Loss: 0.2580
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0080. 
Epoch 10 Step 1 Train Loss: 0.2565
Epoch 10 Step 51 Train Loss: 0.2482
Epoch 10 Step 101 Train Loss: 0.2573
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0065. 
Epoch 11 Step 1 Train Loss: 0.2529
Epoch 11 Step 51 Train Loss: 0.2547
Epoch 11 Step 101 Train Loss: 0.2833
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0037 Validation Top 20 DE MSE: 0.0065. 
Epoch 12 Step 1 Train Loss: 0.2548
Epoch 12 Step 51 Train Loss: 0.2637
Epoch 12 Step 101 Train Loss: 0.2469
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0036 Validation Top 20 DE MSE: 0.0073. 
Epoch 13 Step 1 Train Loss: 0.2558
Epoch 13 Step 51 Train Loss: 0.2568
Epoch 13 Step 101 Train Loss: 0.2603
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0062. 
Epoch 14 Step 1 Train Loss: 0.2544
Epoch 14 Step 51 Train Loss: 0.2583
Epoch 14 Step 101 Train Loss: 0.2644
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0067. 
Epoch 15 Step 1 Train Loss: 0.2657
Epoch 15 Step 51 Train Loss: 0.2435
Epoch 15 Step 101 Train Loss: 0.2598
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0070. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1527
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0017377838
test_unseen_single_pearson: 0.9949051375476005
test_unseen_single_mse_de: 0.15265755
test_unseen_single_pearson_de: 0.9366926175730089
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.11943888545970732
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.375
test_unseen_single_frac_sigma_below_1_non_dropout: 0.825
test_unseen_single_mse_top20_de_non_dropout: 0.15265754
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.001 MB of 0.020 MB uploadedwandb: / 0.009 MB of 0.020 MB uploadedwandb: - 0.014 MB of 0.020 MB uploadedwandb: \ 0.014 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ
wandb:                                               val_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:                                                      val_mse ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÖ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.15266
wandb:                                              test_de_pearson 0.93669
wandb:               test_frac_opposite_direction_top20_non_dropout 0.375
wandb:                          test_frac_sigma_below_1_non_dropout 0.825
wandb:                                                     test_mse 0.00174
wandb:                                test_mse_top20_de_non_dropout 0.15266
wandb:                                                 test_pearson 0.99491
wandb:                                           test_pearson_delta 0.11944
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.825
wandb:                                       test_unseen_single_mse 0.00174
wandb:                                    test_unseen_single_mse_de 0.15266
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15266
wandb:                                   test_unseen_single_pearson 0.99491
wandb:                                test_unseen_single_pearson_de 0.93669
wandb:                             test_unseen_single_pearson_delta 0.11944
wandb:                                                 train_de_mse 0.00354
wandb:                                             train_de_pearson 0.99883
wandb:                                                    train_mse 0.00052
wandb:                                                train_pearson 0.99849
wandb:                                                training_loss 0.25207
wandb:                                                   val_de_mse 0.00701
wandb:                                               val_de_pearson 0.99835
wandb:                                                      val_mse 0.00074
wandb:                                                  val_pearson 0.99783
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406675_1_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/iipglplf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222155-iipglplf/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222415-xetsmnfk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406675_1_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/xetsmnfk
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2586
Epoch 1 Step 51 Train Loss: 0.2604
Epoch 1 Step 101 Train Loss: 0.2583
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0205 Validation Top 20 DE MSE: 0.0512. 
Epoch 2 Step 1 Train Loss: 0.2602
Epoch 2 Step 51 Train Loss: 0.2487
Epoch 2 Step 101 Train Loss: 0.2789
Epoch 2: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0311 Validation Top 20 DE MSE: 0.0501. 
Epoch 3 Step 1 Train Loss: 0.2468
Epoch 3 Step 51 Train Loss: 0.2757
Epoch 3 Step 101 Train Loss: 0.2427
Epoch 3: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0158 Validation Top 20 DE MSE: 0.0513. 
Epoch 4 Step 1 Train Loss: 0.2819
Epoch 4 Step 51 Train Loss: 0.2650
Epoch 4 Step 101 Train Loss: 0.2551
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0047 Validation Top 20 DE MSE: 0.0558. 
Epoch 5 Step 1 Train Loss: 0.2877
Epoch 5 Step 51 Train Loss: 0.2519
Epoch 5 Step 101 Train Loss: 0.2642
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0481. 
Epoch 6 Step 1 Train Loss: 0.2751
Epoch 6 Step 51 Train Loss: 0.2464
Epoch 6 Step 101 Train Loss: 0.2694
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0078 Validation Top 20 DE MSE: 0.0496. 
Epoch 7 Step 1 Train Loss: 0.2407
Epoch 7 Step 51 Train Loss: 0.2392
Epoch 7 Step 101 Train Loss: 0.2357
Epoch 7: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0484. 
Epoch 8 Step 1 Train Loss: 0.2481
Epoch 8 Step 51 Train Loss: 0.2616
Epoch 8 Step 101 Train Loss: 0.2577
Epoch 8: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0052 Validation Top 20 DE MSE: 0.0493. 
Epoch 9 Step 1 Train Loss: 0.2463
Epoch 9 Step 51 Train Loss: 0.2550
Epoch 9 Step 101 Train Loss: 0.2529
Epoch 9: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0484. 
Epoch 10 Step 1 Train Loss: 0.2397
Epoch 10 Step 51 Train Loss: 0.2555
Epoch 10 Step 101 Train Loss: 0.2353
Epoch 10: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0491. 
Epoch 11 Step 1 Train Loss: 0.2783
Epoch 11 Step 51 Train Loss: 0.2743
Epoch 11 Step 101 Train Loss: 0.2524
Epoch 11: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0047 Validation Top 20 DE MSE: 0.0499. 
Epoch 12 Step 1 Train Loss: 0.2603
Epoch 12 Step 51 Train Loss: 0.2712
Epoch 12 Step 101 Train Loss: 0.2513
Epoch 12: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0478. 
Epoch 13 Step 1 Train Loss: 0.2513
Epoch 13 Step 51 Train Loss: 0.2402
Epoch 13 Step 101 Train Loss: 0.2546
Epoch 13: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0485. 
Epoch 14 Step 1 Train Loss: 0.2634
Epoch 14 Step 51 Train Loss: 0.2604
Epoch 14 Step 101 Train Loss: 0.2516
Epoch 14: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0489. 
Epoch 15 Step 1 Train Loss: 0.2663
Epoch 15 Step 51 Train Loss: 0.2665
Epoch 15 Step 101 Train Loss: 0.2441
Epoch 15: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0489. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0452
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.00093328813
test_unseen_single_pearson: 0.9972820019806731
test_unseen_single_mse_de: 0.04516421
test_unseen_single_pearson_de: 0.9867603206987318
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.23770050312701657
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.22499999999999998
test_unseen_single_frac_sigma_below_1_non_dropout: 0.925
test_unseen_single_mse_top20_de_non_dropout: 0.045222055
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.003 MB of 0.020 MB uploadedwandb: / 0.009 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb: | 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÖ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.04516
wandb:                                              test_de_pearson 0.98676
wandb:               test_frac_opposite_direction_top20_non_dropout 0.225
wandb:                          test_frac_sigma_below_1_non_dropout 0.925
wandb:                                                     test_mse 0.00093
wandb:                                test_mse_top20_de_non_dropout 0.04522
wandb:                                                 test_pearson 0.99728
wandb:                                           test_pearson_delta 0.2377
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.225
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.925
wandb:                                       test_unseen_single_mse 0.00093
wandb:                                    test_unseen_single_mse_de 0.04516
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.04522
wandb:                                   test_unseen_single_pearson 0.99728
wandb:                                test_unseen_single_pearson_de 0.98676
wandb:                             test_unseen_single_pearson_delta 0.2377
wandb:                                                 train_de_mse 0.00406
wandb:                                             train_de_pearson 0.99846
wandb:                                                    train_mse 0.0005
wandb:                                                train_pearson 0.99857
wandb:                                                training_loss 0.26153
wandb:                                                   val_de_mse 0.04895
wandb:                                               val_de_pearson 0.98009
wandb:                                                      val_mse 0.00094
wandb:                                                  val_pearson 0.99724
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406675_1_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/xetsmnfk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222415-xetsmnfk/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222639-6gvi4bak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406675_1_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/6gvi4bak
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2520
Epoch 1 Step 51 Train Loss: 0.2556
Epoch 1 Step 101 Train Loss: 0.2513
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0413 Validation Top 20 DE MSE: 0.0437. 
Epoch 2 Step 1 Train Loss: 0.2597
Epoch 2 Step 51 Train Loss: 0.2472
Epoch 2 Step 101 Train Loss: 0.2526
Epoch 2: Train Overall MSE: 0.0060 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0567 Validation Top 20 DE MSE: 0.0448. 
Epoch 3 Step 1 Train Loss: 0.2477
Epoch 3 Step 51 Train Loss: 0.2513
Epoch 3 Step 101 Train Loss: 0.2524
Epoch 3: Train Overall MSE: 0.0012 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.0539. 
Epoch 4 Step 1 Train Loss: 0.2453
Epoch 4 Step 51 Train Loss: 0.2593
Epoch 4 Step 101 Train Loss: 0.2762
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0059 Validation Top 20 DE MSE: 0.0474. 
Epoch 5 Step 1 Train Loss: 0.2475
Epoch 5 Step 51 Train Loss: 0.2496
Epoch 5 Step 101 Train Loss: 0.2464
Epoch 5: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.0386. 
Epoch 6 Step 1 Train Loss: 0.2574
Epoch 6 Step 51 Train Loss: 0.2455
Epoch 6 Step 101 Train Loss: 0.2613
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0041 Validation Top 20 DE MSE: 0.0443. 
Epoch 7 Step 1 Train Loss: 0.2513
Epoch 7 Step 51 Train Loss: 0.2637
Epoch 7 Step 101 Train Loss: 0.2345
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0399. 
Epoch 8 Step 1 Train Loss: 0.2524
Epoch 8 Step 51 Train Loss: 0.2740
Epoch 8 Step 101 Train Loss: 0.2571
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0471. 
Epoch 9 Step 1 Train Loss: 0.2390
Epoch 9 Step 51 Train Loss: 0.2623
Epoch 9 Step 101 Train Loss: 0.2651
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0433. 
Epoch 10 Step 1 Train Loss: 0.2585
Epoch 10 Step 51 Train Loss: 0.2276
Epoch 10 Step 101 Train Loss: 0.2728
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0007. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0400. 
Epoch 11 Step 1 Train Loss: 0.2589
Epoch 11 Step 51 Train Loss: 0.2474
Epoch 11 Step 101 Train Loss: 0.2595
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0006. 
Train Top 20 DE MSE: 0.0044 Validation Top 20 DE MSE: 0.0407. 
Epoch 12 Step 1 Train Loss: 0.3115
Epoch 12 Step 51 Train Loss: 0.2606
Epoch 12 Step 101 Train Loss: 0.2614
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0040 Validation Top 20 DE MSE: 0.0428. 
Epoch 13 Step 1 Train Loss: 0.2523
Epoch 13 Step 51 Train Loss: 0.2908
Epoch 13 Step 101 Train Loss: 0.2716
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0008. 
Train Top 20 DE MSE: 0.0043 Validation Top 20 DE MSE: 0.0425. 
Epoch 14 Step 1 Train Loss: 0.2536
Epoch 14 Step 51 Train Loss: 0.2540
Epoch 14 Step 101 Train Loss: 0.2481
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0444. 
Epoch 15 Step 1 Train Loss: 0.2465
Epoch 15 Step 51 Train Loss: 0.2785
Epoch 15 Step 101 Train Loss: 0.2725
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0441. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1021
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014189961
test_unseen_single_pearson: 0.9958685922607273
test_unseen_single_mse_de: 0.10211178
test_unseen_single_pearson_de: 0.9542986794260778
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.24133073866736882
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.275
test_unseen_single_frac_sigma_below_1_non_dropout: 0.75
test_unseen_single_mse_top20_de_non_dropout: 0.10218862
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.010 MB of 0.020 MB uploadedwandb: / 0.010 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÖ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñá‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb:                                               val_de_pearson ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:                                                      val_mse ‚ñà‚ñà‚ñá‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                                                  val_pearson ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.10211
wandb:                                              test_de_pearson 0.9543
wandb:               test_frac_opposite_direction_top20_non_dropout 0.275
wandb:                          test_frac_sigma_below_1_non_dropout 0.75
wandb:                                                     test_mse 0.00142
wandb:                                test_mse_top20_de_non_dropout 0.10219
wandb:                                                 test_pearson 0.99587
wandb:                                           test_pearson_delta 0.24133
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.275
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.75
wandb:                                       test_unseen_single_mse 0.00142
wandb:                                    test_unseen_single_mse_de 0.10211
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.10219
wandb:                                   test_unseen_single_pearson 0.99587
wandb:                                test_unseen_single_pearson_de 0.9543
wandb:                             test_unseen_single_pearson_delta 0.24133
wandb:                                                 train_de_mse 0.00346
wandb:                                             train_de_pearson 0.99898
wandb:                                                    train_mse 0.00059
wandb:                                                train_pearson 0.99827
wandb:                                                training_loss 0.25708
wandb:                                                   val_de_mse 0.04411
wandb:                                               val_de_pearson 0.99203
wandb:                                                      val_mse 0.00089
wandb:                                                  val_pearson 0.99743
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406675_1_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/6gvi4bak
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222639-6gvi4bak/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222856-00ti3l5l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406675_1_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/00ti3l5l
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2527
Epoch 1 Step 51 Train Loss: 0.2723
Epoch 1 Step 101 Train Loss: 0.2680
Epoch 1: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0280 Validation Top 20 DE MSE: 0.0611. 
Epoch 2 Step 1 Train Loss: 0.2548
Epoch 2 Step 51 Train Loss: 0.2827
Epoch 2 Step 101 Train Loss: 0.2591
Epoch 2: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0155 Validation Top 20 DE MSE: 0.0745. 
Epoch 3 Step 1 Train Loss: 0.2502
Epoch 3 Step 51 Train Loss: 0.2573
Epoch 3 Step 101 Train Loss: 0.2518
Epoch 3: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0194 Validation Top 20 DE MSE: 0.0662. 
Epoch 4 Step 1 Train Loss: 0.2579
Epoch 4 Step 51 Train Loss: 0.2609
Epoch 4 Step 101 Train Loss: 0.2637
Epoch 4: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0052 Validation Top 20 DE MSE: 0.0723. 
Epoch 5 Step 1 Train Loss: 0.2446
Epoch 5 Step 51 Train Loss: 0.2625
Epoch 5 Step 101 Train Loss: 0.2528
Epoch 5: Train Overall MSE: 0.0007 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0676. 
Epoch 6 Step 1 Train Loss: 0.2541
Epoch 6 Step 51 Train Loss: 0.2437
Epoch 6 Step 101 Train Loss: 0.2438
Epoch 6: Train Overall MSE: 0.0005 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0668. 
Epoch 7 Step 1 Train Loss: 0.2459
Epoch 7 Step 51 Train Loss: 0.2470
Epoch 7 Step 101 Train Loss: 0.2539
Epoch 7: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0039 Validation Top 20 DE MSE: 0.0744. 
Epoch 8 Step 1 Train Loss: 0.2645
Epoch 8 Step 51 Train Loss: 0.2643
Epoch 8 Step 101 Train Loss: 0.2578
Epoch 8: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0742. 
Epoch 9 Step 1 Train Loss: 0.2378
Epoch 9 Step 51 Train Loss: 0.2515
Epoch 9 Step 101 Train Loss: 0.2588
Epoch 9: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0760. 
Epoch 10 Step 1 Train Loss: 0.2657
Epoch 10 Step 51 Train Loss: 0.2563
Epoch 10 Step 101 Train Loss: 0.2472
Epoch 10: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0033 Validation Top 20 DE MSE: 0.0768. 
Epoch 11 Step 1 Train Loss: 0.2659
Epoch 11 Step 51 Train Loss: 0.2412
Epoch 11 Step 101 Train Loss: 0.2703
Epoch 11: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0031 Validation Top 20 DE MSE: 0.0795. 
Epoch 12 Step 1 Train Loss: 0.2674
Epoch 12 Step 51 Train Loss: 0.2465
Epoch 12 Step 101 Train Loss: 0.2591
Epoch 12: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0034 Validation Top 20 DE MSE: 0.0713. 
Epoch 13 Step 1 Train Loss: 0.2570
Epoch 13 Step 51 Train Loss: 0.2491
Epoch 13 Step 101 Train Loss: 0.2566
Epoch 13: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0032 Validation Top 20 DE MSE: 0.0713. 
Epoch 14 Step 1 Train Loss: 0.2461
Epoch 14 Step 51 Train Loss: 0.2639
Epoch 14 Step 101 Train Loss: 0.2641
Epoch 14: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0035 Validation Top 20 DE MSE: 0.0718. 
Epoch 15 Step 1 Train Loss: 0.2502
Epoch 15 Step 51 Train Loss: 0.2482
Epoch 15 Step 101 Train Loss: 0.2661
Epoch 15: Train Overall MSE: 0.0006 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0038 Validation Top 20 DE MSE: 0.0737. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1209
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0016210512
test_unseen_single_pearson: 0.9952496564264169
test_unseen_single_mse_de: 0.12091288
test_unseen_single_pearson_de: 0.9536128577377186
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.11340078947866643
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.475
test_unseen_single_frac_sigma_below_1_non_dropout: 0.825
test_unseen_single_mse_top20_de_non_dropout: 0.120912895
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.020 MB uploadedwandb: | 0.007 MB of 0.020 MB uploadedwandb: / 0.014 MB of 0.020 MB uploadedwandb: - 0.014 MB of 0.020 MB uploadedwandb: \ 0.014 MB of 0.020 MB uploadedwandb: | 0.014 MB of 0.020 MB uploadedwandb: / 0.014 MB of 0.020 MB uploadedwandb: - 0.014 MB of 0.020 MB uploadedwandb: \ 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÜ
wandb:                                               val_de_pearson ‚ñà‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb:                                                      val_mse ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                  val_pearson ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.12091
wandb:                                              test_de_pearson 0.95361
wandb:               test_frac_opposite_direction_top20_non_dropout 0.475
wandb:                          test_frac_sigma_below_1_non_dropout 0.825
wandb:                                                     test_mse 0.00162
wandb:                                test_mse_top20_de_non_dropout 0.12091
wandb:                                                 test_pearson 0.99525
wandb:                                           test_pearson_delta 0.1134
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.475
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.825
wandb:                                       test_unseen_single_mse 0.00162
wandb:                                    test_unseen_single_mse_de 0.12091
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.12091
wandb:                                   test_unseen_single_pearson 0.99525
wandb:                                test_unseen_single_pearson_de 0.95361
wandb:                             test_unseen_single_pearson_delta 0.1134
wandb:                                                 train_de_mse 0.00381
wandb:                                             train_de_pearson 0.99895
wandb:                                                    train_mse 0.00058
wandb:                                                train_pearson 0.99832
wandb:                                                training_loss 0.24461
wandb:                                                   val_de_mse 0.0737
wandb:                                               val_de_pearson 0.9691
wandb:                                                      val_mse 0.00134
wandb:                                                  val_pearson 0.99606
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406675_1_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/00ti3l5l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222856-00ti3l5l/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_223152-uzduflew
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406677_2_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/uzduflew
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
  0%|                                                  | 0/3722 [00:00<?, ?it/s]  0%|                                          | 7/3722 [00:00<01:05, 56.59it/s]  0%|‚ñè                                        | 15/3722 [00:00<00:56, 65.51it/s]  1%|‚ñè                                        | 22/3722 [00:00<00:55, 66.78it/s]  1%|‚ñé                                        | 31/3722 [00:00<00:49, 74.11it/s]  1%|‚ñç                                        | 40/3722 [00:00<00:46, 78.36it/s]  1%|‚ñå                                        | 48/3722 [00:00<00:47, 78.10it/s]  2%|‚ñå                                        | 56/3722 [00:00<00:47, 77.85it/s]  2%|‚ñã                                        | 64/3722 [00:00<00:47, 76.97it/s]  2%|‚ñä                                        | 72/3722 [00:00<00:47, 77.21it/s]  2%|‚ñâ                                        | 80/3722 [00:01<00:47, 77.12it/s]  2%|‚ñâ                                        | 88/3722 [00:01<00:47, 76.21it/s]  3%|‚ñà                                        | 96/3722 [00:01<00:47, 76.61it/s]  3%|‚ñà                                       | 104/3722 [00:01<00:47, 76.61it/s]  3%|‚ñà‚ñè                                      | 112/3722 [00:01<00:47, 76.57it/s]  3%|‚ñà‚ñé                                      | 120/3722 [00:01<00:46, 76.96it/s]  3%|‚ñà‚ñç                                      | 128/3722 [00:01<00:47, 76.35it/s]  4%|‚ñà‚ñç                                      | 136/3722 [00:01<00:49, 72.40it/s]  4%|‚ñà‚ñå                                      | 145/3722 [00:01<00:47, 75.85it/s]  4%|‚ñà‚ñã                                      | 153/3722 [00:02<00:46, 76.38it/s]  4%|‚ñà‚ñã                                      | 161/3722 [00:02<00:46, 76.91it/s]  5%|‚ñà‚ñä                                      | 169/3722 [00:02<00:46, 77.02it/s]  5%|‚ñà‚ñâ                                      | 177/3722 [00:02<00:45, 77.10it/s]  5%|‚ñà‚ñâ                                      | 186/3722 [00:02<00:45, 78.19it/s]  5%|‚ñà‚ñà                                      | 194/3722 [00:02<00:45, 78.09it/s]  5%|‚ñà‚ñà‚ñè                                     | 202/3722 [00:02<00:45, 77.74it/s]  6%|‚ñà‚ñà‚ñé                                     | 210/3722 [00:02<00:45, 77.38it/s]  6%|‚ñà‚ñà‚ñé                                     | 218/3722 [00:02<00:45, 77.20it/s]  6%|‚ñà‚ñà‚ñç                                     | 226/3722 [00:02<00:45, 77.14it/s]  6%|‚ñà‚ñà‚ñå                                     | 234/3722 [00:03<00:44, 77.90it/s]  7%|‚ñà‚ñà‚ñå                                     | 242/3722 [00:03<00:44, 77.78it/s]  7%|‚ñà‚ñà‚ñã                                     | 250/3722 [00:03<00:44, 77.84it/s]  7%|‚ñà‚ñà‚ñä                                     | 258/3722 [00:03<00:44, 78.23it/s]  7%|‚ñà‚ñà‚ñä                                     | 266/3722 [00:03<00:45, 75.62it/s]  7%|‚ñà‚ñà‚ñâ                                     | 275/3722 [00:03<00:43, 79.41it/s]  8%|‚ñà‚ñà‚ñà                                     | 284/3722 [00:03<00:43, 79.75it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 293/3722 [00:03<00:42, 79.92it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 302/3722 [00:03<00:42, 80.21it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 311/3722 [00:04<00:42, 80.74it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 320/3722 [00:04<00:48, 70.87it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 328/3722 [00:04<00:51, 66.00it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 335/3722 [00:04<00:54, 62.67it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 342/3722 [00:04<00:55, 60.88it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 349/3722 [00:04<00:56, 59.37it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 355/3722 [00:04<00:57, 58.42it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 361/3722 [00:04<00:57, 58.01it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 367/3722 [00:05<00:58, 57.53it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 373/3722 [00:05<00:58, 56.89it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 379/3722 [00:05<00:58, 57.05it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 385/3722 [00:05<00:58, 56.88it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 391/3722 [00:05<00:59, 56.36it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 397/3722 [00:05<00:59, 56.27it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 403/3722 [00:05<00:58, 56.57it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 409/3722 [00:05<00:58, 56.18it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 415/3722 [00:05<00:59, 55.85it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 421/3722 [00:06<00:59, 55.63it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 427/3722 [00:06<00:59, 55.72it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 433/3722 [00:06<00:59, 55.38it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 439/3722 [00:06<00:59, 54.97it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 445/3722 [00:06<00:59, 54.76it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 451/3722 [00:06<01:00, 54.27it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 457/3722 [00:06<00:59, 54.91it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 463/3722 [00:06<00:58, 55.37it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 469/3722 [00:06<00:58, 55.72it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 475/3722 [00:06<00:58, 55.54it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 481/3722 [00:07<00:59, 54.93it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 487/3722 [00:07<01:00, 53.30it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 493/3722 [00:07<01:01, 52.20it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 499/3722 [00:07<01:02, 51.37it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 505/3722 [00:07<01:03, 50.99it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 511/3722 [00:08<02:29, 21.42it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 516/3722 [00:08<02:16, 23.42it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 521/3722 [00:08<02:01, 26.34it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 525/3722 [00:08<02:19, 22.85it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 536/3722 [00:08<01:37, 32.81it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 541/3722 [00:09<01:35, 33.16it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 546/3722 [00:09<01:37, 32.57it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 552/3722 [00:09<01:32, 34.23it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 556/3722 [00:09<01:43, 30.54it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 585/3722 [00:09<00:39, 78.71it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 595/3722 [00:09<00:45, 68.39it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 604/3722 [00:10<00:49, 62.90it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 612/3722 [00:10<00:51, 60.48it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 619/3722 [00:10<00:50, 61.55it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 629/3722 [00:10<00:44, 69.54it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 637/3722 [00:10<00:43, 71.49it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 647/3722 [00:10<00:41, 73.92it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 658/3722 [00:10<00:37, 82.52it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 667/3722 [00:10<00:36, 84.21it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 676/3722 [00:10<00:35, 85.37it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 686/3722 [00:11<00:34, 86.77it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 696/3722 [00:11<00:34, 86.92it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 705/3722 [00:11<00:34, 87.11it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 714/3722 [00:11<00:34, 86.39it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 724/3722 [00:11<00:35, 85.00it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 733/3722 [00:11<00:35, 84.14it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 742/3722 [00:11<00:36, 81.80it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 751/3722 [00:11<00:37, 79.47it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 759/3722 [00:11<00:37, 79.24it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 769/3722 [00:12<00:35, 82.70it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 778/3722 [00:12<00:36, 81.48it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 787/3722 [00:12<00:36, 79.34it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 796/3722 [00:12<00:37, 77.51it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 807/3722 [00:12<00:35, 81.38it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 816/3722 [00:12<00:35, 81.69it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 826/3722 [00:12<00:35, 81.23it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 835/3722 [00:12<00:35, 81.59it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 845/3722 [00:13<00:34, 84.37it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 854/3722 [00:13<00:34, 83.90it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 863/3722 [00:13<00:34, 82.40it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 873/3722 [00:13<00:33, 84.75it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 882/3722 [00:13<00:34, 82.36it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 892/3722 [00:13<00:33, 83.56it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 901/3722 [00:13<00:33, 83.14it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 910/3722 [00:13<00:33, 83.61it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 920/3722 [00:13<00:33, 84.11it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 929/3722 [00:14<00:34, 81.88it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 938/3722 [00:14<00:33, 82.61it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 948/3722 [00:14<00:31, 87.02it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 957/3722 [00:14<00:33, 81.67it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 967/3722 [00:14<00:32, 86.04it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 977/3722 [00:14<00:32, 85.53it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 986/3722 [00:14<00:32, 83.47it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 997/3722 [00:14<00:30, 90.15it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1007/3722 [00:14<00:31, 86.11it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1016/3722 [00:15<00:31, 86.67it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1025/3722 [00:15<00:32, 83.08it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1035/3722 [00:15<00:31, 86.48it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1045/3722 [00:15<00:30, 88.41it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1054/3722 [00:15<00:32, 82.22it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1065/3722 [00:15<00:31, 83.34it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1076/3722 [00:15<00:30, 87.75it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1085/3722 [00:15<00:30, 86.66it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1094/3722 [00:15<00:31, 83.74it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1104/3722 [00:16<00:31, 84.04it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1113/3722 [00:16<00:30, 84.28it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1123/3722 [00:16<00:30, 84.15it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1132/3722 [00:16<00:31, 83.10it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1141/3722 [00:16<00:31, 82.48it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1150/3722 [00:16<00:31, 82.83it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1159/3722 [00:16<00:31, 80.89it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1170/3722 [00:16<00:29, 87.24it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1179/3722 [00:16<00:29, 86.60it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1188/3722 [00:17<00:30, 83.82it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1198/3722 [00:17<00:29, 86.76it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1207/3722 [00:17<00:30, 81.48it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1218/3722 [00:17<00:28, 87.85it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1227/3722 [00:17<00:28, 87.07it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1236/3722 [00:17<00:29, 84.29it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1245/3722 [00:17<00:29, 84.55it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1254/3722 [00:17<00:29, 84.30it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1265/3722 [00:17<00:27, 88.62it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1274/3722 [00:18<00:28, 85.17it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1283/3722 [00:18<00:28, 85.40it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1292/3722 [00:18<00:28, 85.66it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1302/3722 [00:18<00:27, 88.43it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1311/3722 [00:18<00:27, 87.56it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1320/3722 [00:18<00:27, 86.83it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1329/3722 [00:18<00:28, 83.48it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1339/3722 [00:18<00:27, 86.41it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1348/3722 [00:18<00:27, 86.15it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1357/3722 [00:19<00:30, 78.20it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1369/3722 [00:19<00:26, 88.79it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1379/3722 [00:19<00:26, 88.09it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1388/3722 [00:19<00:27, 85.04it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1398/3722 [00:19<00:26, 87.62it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1407/3722 [00:19<00:27, 84.76it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1417/3722 [00:19<00:27, 85.15it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1426/3722 [00:19<00:27, 84.65it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1436/3722 [00:19<00:26, 87.52it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1445/3722 [00:20<00:26, 86.94it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1454/3722 [00:20<00:26, 86.06it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1463/3722 [00:20<00:26, 86.29it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1472/3722 [00:20<00:27, 83.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1481/3722 [00:20<00:27, 81.36it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1492/3722 [00:20<00:26, 85.07it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1501/3722 [00:20<00:26, 84.55it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1511/3722 [00:20<00:25, 87.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1520/3722 [00:20<00:25, 87.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1529/3722 [00:21<00:25, 86.45it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1538/3722 [00:21<00:26, 83.40it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1548/3722 [00:21<00:25, 86.46it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1557/3722 [00:21<00:25, 83.50it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1566/3722 [00:21<00:25, 83.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1575/3722 [00:21<00:25, 84.08it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1584/3722 [00:21<00:25, 84.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1593/3722 [00:21<00:25, 83.24it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1603/3722 [00:21<00:24, 86.60it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1612/3722 [00:22<00:26, 79.51it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1623/3722 [00:22<00:25, 83.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1633/3722 [00:22<00:24, 84.10it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1643/3722 [00:22<00:24, 86.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1652/3722 [00:22<00:24, 85.38it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1661/3722 [00:22<00:24, 82.87it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1670/3722 [00:22<00:24, 84.76it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1679/3722 [00:22<00:26, 77.30it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1687/3722 [00:23<00:27, 72.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1697/3722 [00:23<00:26, 76.19it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1706/3722 [00:23<00:25, 78.19it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1714/3722 [00:23<00:26, 75.94it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1722/3722 [00:23<00:26, 75.63it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1730/3722 [00:23<00:26, 74.55it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1739/3722 [00:23<00:25, 78.67it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1748/3722 [00:23<00:24, 79.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1756/3722 [00:23<00:24, 79.48it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1764/3722 [00:23<00:24, 79.15it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1772/3722 [00:24<00:25, 75.96it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1780/3722 [00:24<00:25, 76.55it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1789/3722 [00:24<00:24, 79.90it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1798/3722 [00:24<00:24, 79.51it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1806/3722 [00:24<00:24, 78.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1814/3722 [00:24<00:24, 78.36it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1822/3722 [00:24<00:25, 75.62it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1831/3722 [00:24<00:24, 78.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1839/3722 [00:24<00:24, 78.16it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1847/3722 [00:25<00:23, 78.51it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1856/3722 [00:25<00:23, 78.93it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1864/3722 [00:25<00:23, 79.00it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1872/3722 [00:25<00:23, 79.07it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1880/3722 [00:25<00:24, 75.90it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1888/3722 [00:25<00:23, 76.57it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1897/3722 [00:25<00:22, 80.05it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1906/3722 [00:25<00:22, 79.41it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1914/3722 [00:25<00:22, 78.83it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1923/3722 [00:26<00:22, 80.39it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1932/3722 [00:26<00:23, 76.15it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1941/3722 [00:26<00:22, 78.27it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1952/3722 [00:26<00:20, 84.90it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1961/3722 [00:26<00:22, 78.74it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1969/3722 [00:26<00:24, 72.92it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1977/3722 [00:26<00:25, 68.05it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1984/3722 [00:26<00:26, 66.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1991/3722 [00:27<00:27, 63.97it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1998/3722 [00:27<00:26, 63.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2006/3722 [00:27<00:25, 66.80it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2013/3722 [00:27<00:25, 66.25it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2021/3722 [00:27<00:24, 69.71it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2030/3722 [00:27<00:22, 74.02it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2039/3722 [00:27<00:21, 76.99it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2047/3722 [00:27<00:22, 74.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2056/3722 [00:27<00:21, 77.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2064/3722 [00:27<00:22, 73.86it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2072/3722 [00:28<00:22, 74.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2080/3722 [00:28<00:22, 71.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2090/3722 [00:28<00:21, 75.18it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2098/3722 [00:28<00:21, 76.27it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2107/3722 [00:28<00:21, 75.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2115/3722 [00:28<00:21, 75.95it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2124/3722 [00:28<00:20, 76.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2133/3722 [00:28<00:20, 76.92it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2142/3722 [00:29<00:19, 79.17it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2150/3722 [00:29<00:20, 78.46it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2158/3722 [00:29<00:19, 78.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2167/3722 [00:29<00:19, 80.19it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2176/3722 [00:29<00:19, 77.35it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2184/3722 [00:29<00:20, 75.55it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2193/3722 [00:29<00:19, 77.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2201/3722 [00:29<00:20, 73.77it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2209/3722 [00:29<00:20, 74.23it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2217/3722 [00:29<00:20, 74.67it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2226/3722 [00:30<00:19, 77.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2234/3722 [00:30<00:19, 77.58it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2243/3722 [00:30<00:18, 79.24it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2251/3722 [00:30<00:20, 72.85it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2261/3722 [00:30<00:18, 77.90it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2270/3722 [00:30<00:18, 80.09it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2279/3722 [00:30<00:18, 79.63it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2288/3722 [00:30<00:18, 76.95it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2296/3722 [00:31<00:18, 77.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2304/3722 [00:31<00:18, 77.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2312/3722 [00:31<00:18, 76.98it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2320/3722 [00:31<00:18, 74.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2330/3722 [00:31<00:18, 76.20it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2338/3722 [00:31<00:18, 73.55it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2347/3722 [00:31<00:17, 77.31it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2355/3722 [00:31<00:17, 76.30it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2363/3722 [00:31<00:18, 72.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2373/3722 [00:32<00:17, 78.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2382/3722 [00:32<00:17, 77.92it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2392/3722 [00:32<00:16, 82.60it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2402/3722 [00:32<00:15, 83.04it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2411/3722 [00:32<00:15, 83.35it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2420/3722 [00:32<00:15, 82.77it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2430/3722 [00:32<00:16, 80.60it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2440/3722 [00:32<00:15, 84.97it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2449/3722 [00:32<00:15, 84.47it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2458/3722 [00:33<00:14, 84.89it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2467/3722 [00:33<00:14, 85.01it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2476/3722 [00:33<00:14, 83.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2485/3722 [00:33<00:14, 84.19it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2494/3722 [00:33<00:14, 84.15it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2503/3722 [00:33<00:14, 84.20it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2512/3722 [00:33<00:14, 84.60it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2522/3722 [00:33<00:13, 87.49it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2531/3722 [00:33<00:13, 86.18it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2540/3722 [00:33<00:14, 83.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2550/3722 [00:34<00:13, 85.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2559/3722 [00:34<00:13, 84.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2568/3722 [00:34<00:13, 84.19it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2577/3722 [00:34<00:13, 83.32it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2586/3722 [00:34<00:13, 83.86it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2595/3722 [00:34<00:13, 84.16it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2604/3722 [00:34<00:13, 84.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2613/3722 [00:34<00:13, 80.04it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2624/3722 [00:34<00:13, 84.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2634/3722 [00:35<00:12, 87.15it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2643/3722 [00:35<00:12, 86.61it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2652/3722 [00:35<00:12, 86.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2661/3722 [00:35<00:12, 86.35it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2670/3722 [00:35<00:13, 80.91it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2680/3722 [00:35<00:12, 84.59it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2689/3722 [00:35<00:12, 84.70it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2698/3722 [00:35<00:12, 84.63it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2707/3722 [00:35<00:11, 84.87it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2717/3722 [00:36<00:11, 84.67it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2727/3722 [00:36<00:11, 84.63it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2737/3722 [00:36<00:11, 85.09it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2747/3722 [00:36<00:11, 85.12it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2756/3722 [00:36<00:11, 86.03it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2766/3722 [00:36<00:11, 86.30it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2775/3722 [00:36<00:10, 86.29it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2785/3722 [00:36<00:10, 86.82it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2794/3722 [00:36<00:10, 87.12it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2803/3722 [00:37<00:10, 85.95it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2812/3722 [00:37<00:11, 82.22it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2821/3722 [00:37<00:11, 79.82it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2830/3722 [00:37<00:11, 77.64it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2838/3722 [00:37<00:11, 76.01it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2846/3722 [00:37<00:11, 74.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2854/3722 [00:37<00:11, 74.04it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2862/3722 [00:37<00:11, 73.07it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2870/3722 [00:38<00:11, 72.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2880/3722 [00:38<00:10, 77.43it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2888/3722 [00:38<00:10, 76.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2896/3722 [00:38<00:10, 76.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2905/3722 [00:38<00:10, 77.58it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2913/3722 [00:38<00:10, 76.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2921/3722 [00:38<00:10, 74.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2929/3722 [00:38<00:11, 71.81it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2938/3722 [00:38<00:10, 75.19it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2946/3722 [00:39<00:11, 69.75it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2956/3722 [00:39<00:10, 76.27it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2964/3722 [00:39<00:10, 72.92it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2973/3722 [00:39<00:10, 73.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2982/3722 [00:39<00:09, 75.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2990/3722 [00:39<00:09, 75.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2998/3722 [00:39<00:10, 72.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3007/3722 [00:39<00:09, 72.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3015/3722 [00:39<00:09, 72.68it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3023/3722 [00:40<00:09, 73.28it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3032/3722 [00:40<00:09, 75.80it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3040/3722 [00:40<00:09, 72.82it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3049/3722 [00:40<00:08, 74.98it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3057/3722 [00:40<00:08, 74.69it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3065/3722 [00:40<00:08, 74.57it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 3073/3722 [00:40<00:09, 71.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3082/3722 [00:40<00:08, 75.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3090/3722 [00:40<00:08, 71.80it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3099/3722 [00:41<00:08, 72.44it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3108/3722 [00:41<00:08, 75.83it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3116/3722 [00:41<00:07, 76.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 3125/3722 [00:41<00:07, 77.70it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3134/3722 [00:41<00:07, 79.27it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3142/3722 [00:41<00:07, 77.88it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3152/3722 [00:41<00:07, 79.31it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3162/3722 [00:41<00:06, 82.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3171/3722 [00:41<00:06, 80.69it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3181/3722 [00:42<00:06, 84.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3190/3722 [00:42<00:06, 83.86it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3199/3722 [00:42<00:06, 84.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3208/3722 [00:42<00:06, 83.46it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3217/3722 [00:42<00:06, 81.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3227/3722 [00:42<00:05, 84.43it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3236/3722 [00:42<00:05, 83.93it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3245/3722 [00:42<00:05, 84.16it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3254/3722 [00:42<00:05, 83.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3263/3722 [00:43<00:05, 80.68it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3273/3722 [00:43<00:05, 84.59it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3282/3722 [00:43<00:05, 84.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3291/3722 [00:43<00:05, 83.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3300/3722 [00:43<00:05, 83.73it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3309/3722 [00:43<00:05, 79.18it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3319/3722 [00:43<00:04, 84.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3328/3722 [00:43<00:04, 84.79it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3337/3722 [00:43<00:04, 85.02it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3346/3722 [00:44<00:04, 85.85it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3356/3722 [00:44<00:04, 86.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3366/3722 [00:44<00:03, 89.72it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3376/3722 [00:44<00:03, 89.45it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3385/3722 [00:44<00:03, 88.44it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3394/3722 [00:44<00:03, 83.37it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3404/3722 [00:44<00:03, 86.82it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3414/3722 [00:44<00:03, 87.00it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3423/3722 [00:44<00:03, 84.37it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3434/3722 [00:45<00:03, 89.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3443/3722 [00:45<00:03, 89.11it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3452/3722 [00:45<00:03, 82.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3463/3722 [00:45<00:02, 88.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3472/3722 [00:45<00:02, 85.91it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3482/3722 [00:45<00:02, 88.85it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3491/3722 [00:45<00:02, 88.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3500/3722 [00:45<00:02, 87.75it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3509/3722 [00:45<00:02, 86.95it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3518/3722 [00:46<00:02, 84.58it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3527/3722 [00:46<00:02, 84.95it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3536/3722 [00:46<00:02, 85.54it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3546/3722 [00:46<00:01, 89.67it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3555/3722 [00:46<00:01, 88.11it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3564/3722 [00:46<00:01, 82.40it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3575/3722 [00:46<00:01, 89.60it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3585/3722 [00:46<00:01, 89.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3594/3722 [00:46<00:01, 88.55it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3603/3722 [00:46<00:01, 87.87it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3612/3722 [00:47<00:01, 87.60it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3621/3722 [00:47<00:01, 88.13it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3630/3722 [00:47<00:01, 87.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3639/3722 [00:47<00:00, 86.76it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3648/3722 [00:47<00:00, 87.39it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3657/3722 [00:47<00:00, 86.90it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3666/3722 [00:47<00:00, 86.78it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3675/3722 [00:47<00:00, 87.27it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3684/3722 [00:47<00:00, 83.62it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3694/3722 [00:48<00:00, 88.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3703/3722 [00:48<00:00, 87.22it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3712/3722 [00:48<00:00, 87.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3721/3722 [00:48<00:00, 86.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3722/3722 [00:48<00:00, 76.92it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.3094
Epoch 1 Step 51 Train Loss: 0.2898
Epoch 1 Step 101 Train Loss: 0.3132
Epoch 1 Step 151 Train Loss: 0.2582
Epoch 1: Train Overall MSE: 1096.5209 Validation Overall MSE: 734.4812. 
Train Top 20 DE MSE: 1269.7823 Validation Top 20 DE MSE: 1185.4670. 
Epoch 2 Step 1 Train Loss: 0.2655
Epoch 2 Step 51 Train Loss: 0.3157
Epoch 2 Step 101 Train Loss: 0.2850
Epoch 2 Step 151 Train Loss: 0.2826
Epoch 2: Train Overall MSE: 126.7970 Validation Overall MSE: 58.6099. 
Train Top 20 DE MSE: 287.4031 Validation Top 20 DE MSE: 113.5334. 
Epoch 3 Step 1 Train Loss: 0.2772
Epoch 3 Step 51 Train Loss: 0.2993
Epoch 3 Step 101 Train Loss: 0.2491
Epoch 3 Step 151 Train Loss: 0.2908
Epoch 3: Train Overall MSE: 18.8059 Validation Overall MSE: 15.3792. 
Train Top 20 DE MSE: 37.1463 Validation Top 20 DE MSE: 40.9283. 
Epoch 4 Step 1 Train Loss: 0.2667
Epoch 4 Step 51 Train Loss: 0.2453
Epoch 4 Step 101 Train Loss: 0.2381
Epoch 4 Step 151 Train Loss: 0.3450
Epoch 4: Train Overall MSE: 2.7884 Validation Overall MSE: 2.7039. 
Train Top 20 DE MSE: 5.1997 Validation Top 20 DE MSE: 7.3167. 
Epoch 5 Step 1 Train Loss: 0.3456
Epoch 5 Step 51 Train Loss: 0.2535
Epoch 5 Step 101 Train Loss: 0.3161
Epoch 5 Step 151 Train Loss: 0.2743
Epoch 5: Train Overall MSE: 23892.3770 Validation Overall MSE: 13062.3848. 
Train Top 20 DE MSE: 36129.1484 Validation Top 20 DE MSE: 38930.6172. 
Epoch 6 Step 1 Train Loss: 0.2470
Epoch 6 Step 51 Train Loss: 0.2883
Epoch 6 Step 101 Train Loss: 0.2729
Epoch 6 Step 151 Train Loss: 0.3223
Epoch 6: Train Overall MSE: 79393.2734 Validation Overall MSE: 45949.5117. 
Train Top 20 DE MSE: 118202.5469 Validation Top 20 DE MSE: 142224.1875. 
Epoch 7 Step 1 Train Loss: 0.3165
Epoch 7 Step 51 Train Loss: 0.3285
Epoch 7 Step 101 Train Loss: 0.3468
Epoch 7 Step 151 Train Loss: 0.2521
Epoch 7: Train Overall MSE: 7.3803 Validation Overall MSE: 7.4866. 
Train Top 20 DE MSE: 12.5755 Validation Top 20 DE MSE: 24.5345. 
Epoch 8 Step 1 Train Loss: 0.2875
Epoch 8 Step 51 Train Loss: 0.2665
Epoch 8 Step 101 Train Loss: 0.2727
Epoch 8 Step 151 Train Loss: 0.3182
Epoch 8: Train Overall MSE: 3.2128 Validation Overall MSE: 3.3650. 
Train Top 20 DE MSE: 5.2144 Validation Top 20 DE MSE: 11.3674. 
Epoch 9 Step 1 Train Loss: 0.2940
Epoch 9 Step 51 Train Loss: 0.2602
Epoch 9 Step 101 Train Loss: 0.2513
Epoch 9 Step 151 Train Loss: 0.3156
Epoch 9: Train Overall MSE: 84.4990 Validation Overall MSE: 62.3313. 
Train Top 20 DE MSE: 124.3979 Validation Top 20 DE MSE: 198.9818. 
Epoch 10 Step 1 Train Loss: 0.2256
Epoch 10 Step 51 Train Loss: 0.2688
Epoch 10 Step 101 Train Loss: 0.2328
Epoch 10 Step 151 Train Loss: 0.3500
Epoch 10: Train Overall MSE: 24.8343 Validation Overall MSE: 20.3086. 
Train Top 20 DE MSE: 36.3428 Validation Top 20 DE MSE: 65.5337. 
Epoch 11 Step 1 Train Loss: 0.2550
Epoch 11 Step 51 Train Loss: 0.3069
Epoch 11 Step 101 Train Loss: 0.2305
Epoch 11 Step 151 Train Loss: 0.2150
Epoch 11: Train Overall MSE: 42.8050 Validation Overall MSE: 32.5434. 
Train Top 20 DE MSE: 60.8693 Validation Top 20 DE MSE: 104.2429. 
Epoch 12 Step 1 Train Loss: 0.2635
Epoch 12 Step 51 Train Loss: 0.4722
Epoch 12 Step 101 Train Loss: 0.2555
Epoch 12 Step 151 Train Loss: 0.2829
Epoch 12: Train Overall MSE: 1174.0621 Validation Overall MSE: 760.9677. 
Train Top 20 DE MSE: 1611.4935 Validation Top 20 DE MSE: 2400.5864. 
Epoch 13 Step 1 Train Loss: 0.2937
Epoch 13 Step 51 Train Loss: 0.2997
Epoch 13 Step 101 Train Loss: 0.2459
Epoch 13 Step 151 Train Loss: 0.2813
Epoch 13: Train Overall MSE: 289.6424 Validation Overall MSE: 194.1965. 
Train Top 20 DE MSE: 409.3886 Validation Top 20 DE MSE: 616.2928. 
Epoch 14 Step 1 Train Loss: 0.3635
Epoch 14 Step 51 Train Loss: 0.3320
Epoch 14 Step 101 Train Loss: 0.2380
Epoch 14 Step 151 Train Loss: 0.2364
Epoch 14: Train Overall MSE: 44.3004 Validation Overall MSE: 33.8849. 
Train Top 20 DE MSE: 63.9641 Validation Top 20 DE MSE: 108.9861. 
Epoch 15 Step 1 Train Loss: 0.2477
Epoch 15 Step 51 Train Loss: 0.2445
Epoch 15 Step 101 Train Loss: 0.4196
Epoch 15 Step 151 Train Loss: 0.2916
Epoch 15: Train Overall MSE: 44.8693 Validation Overall MSE: 34.1867. 
Train Top 20 DE MSE: 56.7573 Validation Top 20 DE MSE: 107.7422. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 10.1186
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 1.020472
test_combo_seen1_pearson: 0.6725370488677713
test_combo_seen1_mse_de: 3.1111197
test_combo_seen1_pearson_de: 0.5385402790120701
test_combo_seen2_mse: 0.0073853713
test_combo_seen2_pearson: 0.9905923938727623
test_combo_seen2_mse_de: 0.42904943
test_combo_seen2_pearson_de: 0.9449015483659392
test_unseen_single_mse: 7.226098
test_unseen_single_pearson: 0.15531728405789103
test_unseen_single_mse_de: 21.970877
test_unseen_single_pearson_de: 0.09061725812557811
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: 0.2553641901523919
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.375
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.45
test_combo_seen1_mse_top20_de_non_dropout: 3.1111202
test_combo_seen2_pearson_delta: -0.050074163914063656
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.45
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.55
test_combo_seen2_mse_top20_de_non_dropout: 0.4290494
test_unseen_single_pearson_delta: -0.005662546647470755
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.625
test_unseen_single_frac_sigma_below_1_non_dropout: 0.10000000000000003
test_unseen_single_mse_top20_de_non_dropout: 23.35495
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.021 MB uploadedwandb: | 0.001 MB of 0.021 MB uploadedwandb: / 0.015 MB of 0.021 MB uploadedwandb: - 0.021 MB of 0.021 MB uploadedwandb: \ 0.021 MB of 0.021 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb: / 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÑ‚ñÉ‚ñá‚ñà‚ñÇ‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                training_loss ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:                                                      val_mse ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.375
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.45
wandb:                                         test_combo_seen1_mse 1.02047
wandb:                                      test_combo_seen1_mse_de 3.11112
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 3.11112
wandb:                                     test_combo_seen1_pearson 0.67254
wandb:                                  test_combo_seen1_pearson_de 0.53854
wandb:                               test_combo_seen1_pearson_delta 0.25536
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.45
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.55
wandb:                                         test_combo_seen2_mse 0.00739
wandb:                                      test_combo_seen2_mse_de 0.42905
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.42905
wandb:                                     test_combo_seen2_pearson 0.99059
wandb:                                  test_combo_seen2_pearson_de 0.9449
wandb:                               test_combo_seen2_pearson_delta -0.05007
wandb:                                                  test_de_mse 10.11861
wandb:                                              test_de_pearson 0.44064
wandb:               test_frac_opposite_direction_top20_non_dropout 0.49
wandb:                          test_frac_sigma_below_1_non_dropout 0.33
wandb:                                                     test_mse 3.30011
wandb:                                test_mse_top20_de_non_dropout 10.67224
wandb:                                                 test_pearson 0.52926
wandb:                                           test_pearson_delta 0.08987
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.625
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.1
wandb:                                       test_unseen_single_mse 7.2261
wandb:                                    test_unseen_single_mse_de 21.97088
wandb:                  test_unseen_single_mse_top20_de_non_dropout 23.35495
wandb:                                   test_unseen_single_pearson 0.15532
wandb:                                test_unseen_single_pearson_de 0.09062
wandb:                             test_unseen_single_pearson_delta -0.00566
wandb:                                                 train_de_mse 56.75734
wandb:                                             train_de_pearson 0.49668
wandb:                                                    train_mse 44.86934
wandb:                                                train_pearson 0.69298
wandb:                                                training_loss 0.25638
wandb:                                                   val_de_mse 107.7422
wandb:                                               val_de_pearson -0.23276
wandb:                                                      val_mse 34.1867
wandb:                                                  val_pearson 0.03454
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406677_2_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/uzduflew
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_223152-uzduflew/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_223654-pv17f1xr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406677_2_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/pv17f1xr
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2700
Epoch 1 Step 51 Train Loss: 0.5700
Epoch 1 Step 101 Train Loss: 0.2972
Epoch 1 Step 151 Train Loss: 0.2528
Epoch 1: Train Overall MSE: 26123174.0000 Validation Overall MSE: 8672045.0000. 
Train Top 20 DE MSE: 48973616.0000 Validation Top 20 DE MSE: 15242824.0000. 
Epoch 2 Step 1 Train Loss: 0.3243
Epoch 2 Step 51 Train Loss: 0.6108
Epoch 2 Step 101 Train Loss: 0.2423
Epoch 2 Step 151 Train Loss: 0.2993
Epoch 2: Train Overall MSE: 73633.6641 Validation Overall MSE: 28295.7559. 
Train Top 20 DE MSE: 74948.0000 Validation Top 20 DE MSE: 106485.1016. 
Epoch 3 Step 1 Train Loss: 0.8924
Epoch 3 Step 51 Train Loss: 0.2788
Epoch 3 Step 101 Train Loss: 0.3348
Epoch 3 Step 151 Train Loss: 0.2439
Epoch 3: Train Overall MSE: 4429.3286 Validation Overall MSE: 1844.8890. 
Train Top 20 DE MSE: 4944.7676 Validation Top 20 DE MSE: 10799.9297. 
Epoch 4 Step 1 Train Loss: 0.2941
Epoch 4 Step 51 Train Loss: 0.4871
Epoch 4 Step 101 Train Loss: 0.2951
Epoch 4 Step 151 Train Loss: 0.2754
Epoch 4: Train Overall MSE: 3165.3369 Validation Overall MSE: 1353.0044. 
Train Top 20 DE MSE: 1553.0813 Validation Top 20 DE MSE: 6416.1929. 
Epoch 5 Step 1 Train Loss: 0.2536
Epoch 5 Step 51 Train Loss: 0.3037
Epoch 5 Step 101 Train Loss: 0.3568
Epoch 5 Step 151 Train Loss: 0.2373
Epoch 5: Train Overall MSE: 1377.0651 Validation Overall MSE: 590.9121. 
Train Top 20 DE MSE: 598.2430 Validation Top 20 DE MSE: 2789.2563. 
Epoch 6 Step 1 Train Loss: 0.2694
Epoch 6 Step 51 Train Loss: 0.2748
Epoch 6 Step 101 Train Loss: 0.3251
Epoch 6 Step 151 Train Loss: 0.3097
Epoch 6: Train Overall MSE: 319.5202 Validation Overall MSE: 136.3017. 
Train Top 20 DE MSE: 107.5495 Validation Top 20 DE MSE: 670.1508. 
Epoch 7 Step 1 Train Loss: 0.3009
Epoch 7 Step 51 Train Loss: 0.2884
Epoch 7 Step 101 Train Loss: 0.3053
Epoch 7 Step 151 Train Loss: 0.2700
Epoch 7: Train Overall MSE: 89.4127 Validation Overall MSE: 36.8959. 
Train Top 20 DE MSE: 27.9104 Validation Top 20 DE MSE: 182.6606. 
Epoch 8 Step 1 Train Loss: 0.2546
Epoch 8 Step 51 Train Loss: 0.2979
Epoch 8 Step 101 Train Loss: 0.2858
Epoch 8 Step 151 Train Loss: 0.2609
Epoch 8: Train Overall MSE: 327.5304 Validation Overall MSE: 139.8643. 
Train Top 20 DE MSE: 116.6304 Validation Top 20 DE MSE: 670.8517. 
Epoch 9 Step 1 Train Loss: 0.2542
Epoch 9 Step 51 Train Loss: 0.3871
Epoch 9 Step 101 Train Loss: 0.2714
Epoch 9 Step 151 Train Loss: 0.2827
Epoch 9: Train Overall MSE: 255.2192 Validation Overall MSE: 108.6771. 
Train Top 20 DE MSE: 90.3927 Validation Top 20 DE MSE: 470.6081. 
Epoch 10 Step 1 Train Loss: 0.3851
Epoch 10 Step 51 Train Loss: 0.2996
Epoch 10 Step 101 Train Loss: 0.3835
Epoch 10 Step 151 Train Loss: 0.2586
Epoch 10: Train Overall MSE: 395.1055 Validation Overall MSE: 170.9042. 
Train Top 20 DE MSE: 132.7381 Validation Top 20 DE MSE: 774.4306. 
Epoch 11 Step 1 Train Loss: 0.2785
Epoch 11 Step 51 Train Loss: 0.3156
Epoch 11 Step 101 Train Loss: 0.2259
Epoch 11 Step 151 Train Loss: 0.2490
Epoch 11: Train Overall MSE: 74.3470 Validation Overall MSE: 30.5881. 
Train Top 20 DE MSE: 23.6289 Validation Top 20 DE MSE: 145.2966. 
Epoch 12 Step 1 Train Loss: 0.2745
Epoch 12 Step 51 Train Loss: 0.2943
Epoch 12 Step 101 Train Loss: 0.2710
Epoch 12 Step 151 Train Loss: 0.2383
Epoch 12: Train Overall MSE: 16416.3613 Validation Overall MSE: 7294.7764. 
Train Top 20 DE MSE: 5884.9536 Validation Top 20 DE MSE: 33359.5547. 
Epoch 13 Step 1 Train Loss: 0.3308
Epoch 13 Step 51 Train Loss: 0.3488
Epoch 13 Step 101 Train Loss: 0.2672
Epoch 13 Step 151 Train Loss: 0.3182
Epoch 13: Train Overall MSE: 2079.0972 Validation Overall MSE: 911.8784. 
Train Top 20 DE MSE: 725.3740 Validation Top 20 DE MSE: 3994.4961. 
Epoch 14 Step 1 Train Loss: 0.2576
Epoch 14 Step 51 Train Loss: 0.2464
Epoch 14 Step 101 Train Loss: 0.3514
Epoch 14 Step 151 Train Loss: 0.2358
Epoch 14: Train Overall MSE: 250.0264 Validation Overall MSE: 106.5000. 
Train Top 20 DE MSE: 86.7666 Validation Top 20 DE MSE: 453.8637. 
Epoch 15 Step 1 Train Loss: 0.2404
Epoch 15 Step 51 Train Loss: 0.2553
Epoch 15 Step 101 Train Loss: 0.2512
Epoch 15 Step 151 Train Loss: 0.2290
Epoch 15: Train Overall MSE: 573.6647 Validation Overall MSE: 248.1720. 
Train Top 20 DE MSE: 177.1834 Validation Top 20 DE MSE: 1130.7859. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 208.1061
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 12.800338
test_combo_seen1_pearson: 0.26342969638399166
test_combo_seen1_mse_de: 61.94867
test_combo_seen1_pearson_de: -0.13074228542280572
test_combo_seen2_mse: 0.90867335
test_combo_seen2_pearson: 0.5071771694611129
test_combo_seen2_mse_de: 5.7517495
test_combo_seen2_pearson_de: -0.13927671086726603
test_unseen_single_mse: 117.03497
test_unseen_single_pearson: 0.006926395832489178
test_unseen_single_mse_de: 455.4408
test_unseen_single_pearson_de: 0.061502058479424304
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: -0.13841085414256676
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.925
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.04999999999999999
test_combo_seen1_mse_top20_de_non_dropout: 61.948666
test_combo_seen2_pearson_delta: -0.21589989999983059
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.9
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.0
test_combo_seen2_mse_top20_de_non_dropout: 5.75175
test_unseen_single_pearson_delta: -0.026717395256558404
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.675
test_unseen_single_frac_sigma_below_1_non_dropout: 0.0
test_unseen_single_mse_top20_de_non_dropout: 459.37274
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.017 MB uploadedwandb: | 0.009 MB of 0.021 MB uploadedwandb: / 0.012 MB of 0.021 MB uploadedwandb: - 0.015 MB of 0.021 MB uploadedwandb: \ 0.015 MB of 0.021 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÖ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.925
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.05
wandb:                                         test_combo_seen1_mse 12.80034
wandb:                                      test_combo_seen1_mse_de 61.94867
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 61.94867
wandb:                                     test_combo_seen1_pearson 0.26343
wandb:                                  test_combo_seen1_pearson_de -0.13074
wandb:                               test_combo_seen1_pearson_delta -0.13841
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.9
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.0
wandb:                                         test_combo_seen2_mse 0.90867
wandb:                                      test_combo_seen2_mse_de 5.75175
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 5.75175
wandb:                                     test_combo_seen2_pearson 0.50718
wandb:                                  test_combo_seen2_pearson_de -0.13928
wandb:                               test_combo_seen2_pearson_delta -0.2159
wandb:                                                  test_de_mse 208.10612
wandb:                                              test_de_pearson -0.05555
wandb:               test_frac_opposite_direction_top20_non_dropout 0.82
wandb:                          test_frac_sigma_below_1_non_dropout 0.02
wandb:                                                     test_mse 52.11586
wandb:                                test_mse_top20_de_non_dropout 209.67891
wandb:                                                 test_pearson 0.20958
wandb:                                           test_pearson_delta -0.10923
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.675
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.0
wandb:                                       test_unseen_single_mse 117.03497
wandb:                                    test_unseen_single_mse_de 455.4408
wandb:                  test_unseen_single_mse_top20_de_non_dropout 459.37274
wandb:                                   test_unseen_single_pearson 0.00693
wandb:                                test_unseen_single_pearson_de 0.0615
wandb:                             test_unseen_single_pearson_delta -0.02672
wandb:                                                 train_de_mse 177.18344
wandb:                                             train_de_pearson 0.49886
wandb:                                                    train_mse 573.66473
wandb:                                                train_pearson 0.6914
wandb:                                                training_loss 0.30143
wandb:                                                   val_de_mse 1130.78589
wandb:                                               val_de_pearson -0.69141
wandb:                                                      val_mse 248.17197
wandb:                                                  val_pearson -0.02904
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406677_2_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/pv17f1xr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_223654-pv17f1xr/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:1
combo_seen1:2
combo_seen2:0
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_224014-mcut5lm7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406677_2_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/mcut5lm7
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2850
Epoch 1 Step 51 Train Loss: 0.2456
Epoch 1 Step 101 Train Loss: 0.2579
Epoch 1 Step 151 Train Loss: 0.2387
Epoch 1: Train Overall MSE: 6716421.5000 Validation Overall MSE: 14022062.0000. 
Train Top 20 DE MSE: 4829605.5000 Validation Top 20 DE MSE: 29093252.0000. 
Epoch 2 Step 1 Train Loss: 0.3227
Epoch 2 Step 51 Train Loss: 0.2573
Epoch 2 Step 101 Train Loss: 0.3114
Epoch 2 Step 151 Train Loss: 0.3347
Epoch 2: Train Overall MSE: 1819.9994 Validation Overall MSE: 4016.3848. 
Train Top 20 DE MSE: 1886.3444 Validation Top 20 DE MSE: 4457.5977. 
Epoch 3 Step 1 Train Loss: 0.2508
Epoch 3 Step 51 Train Loss: 0.2633
Epoch 3 Step 101 Train Loss: 0.3024
Epoch 3 Step 151 Train Loss: 0.3930
Epoch 3: Train Overall MSE: 4.6341 Validation Overall MSE: 10.6519. 
Train Top 20 DE MSE: 4.4606 Validation Top 20 DE MSE: 28.1024. 
Epoch 4 Step 1 Train Loss: 0.3105
Epoch 4 Step 51 Train Loss: 0.3892
Epoch 4 Step 101 Train Loss: 0.2769
Epoch 4 Step 151 Train Loss: 0.3591
Epoch 4: Train Overall MSE: 3768.9783 Validation Overall MSE: 8004.0503. 
Train Top 20 DE MSE: 2646.1848 Validation Top 20 DE MSE: 16980.8945. 
Epoch 5 Step 1 Train Loss: 0.2933
Epoch 5 Step 51 Train Loss: 0.2515
Epoch 5 Step 101 Train Loss: 0.2527
Epoch 5 Step 151 Train Loss: 0.2923
Epoch 5: Train Overall MSE: 790534.5625 Validation Overall MSE: 1698407.3750. 
Train Top 20 DE MSE: 591101.6875 Validation Top 20 DE MSE: 3576859.5000. 
Epoch 6 Step 1 Train Loss: 0.2567
Epoch 6 Step 51 Train Loss: 0.2665
Epoch 6 Step 101 Train Loss: 0.2530
Epoch 6 Step 151 Train Loss: 0.2857
Epoch 6: Train Overall MSE: 5325.7808 Validation Overall MSE: 11189.9688. 
Train Top 20 DE MSE: 3599.4629 Validation Top 20 DE MSE: 21853.5957. 
Epoch 7 Step 1 Train Loss: 0.3004
Epoch 7 Step 51 Train Loss: 0.2527
Epoch 7 Step 101 Train Loss: 0.2595
Epoch 7 Step 151 Train Loss: 0.2896
Epoch 7: Train Overall MSE: 392521.4688 Validation Overall MSE: 827255.5000. 
Train Top 20 DE MSE: 269848.9688 Validation Top 20 DE MSE: 1616390.2500. 
Epoch 8 Step 1 Train Loss: 0.2397
Epoch 8 Step 51 Train Loss: 0.3190
Epoch 8 Step 101 Train Loss: 0.2778
Epoch 8 Step 151 Train Loss: 0.2601
Epoch 8: Train Overall MSE: 19783.7168 Validation Overall MSE: 41449.0234. 
Train Top 20 DE MSE: 13315.9619 Validation Top 20 DE MSE: 77088.3281. 
Epoch 9 Step 1 Train Loss: 0.2506
Epoch 9 Step 51 Train Loss: 0.2618
Epoch 9 Step 101 Train Loss: 0.3062
Epoch 9 Step 151 Train Loss: 0.3358
Epoch 9: Train Overall MSE: 23.0996 Validation Overall MSE: 51.7016. 
Train Top 20 DE MSE: 15.7954 Validation Top 20 DE MSE: 106.7547. 
Epoch 10 Step 1 Train Loss: 0.2833
Epoch 10 Step 51 Train Loss: 0.2950
Epoch 10 Step 101 Train Loss: 0.2572
Epoch 10 Step 151 Train Loss: 0.2653
Epoch 10: Train Overall MSE: 81294.3750 Validation Overall MSE: 170028.4375. 
Train Top 20 DE MSE: 53647.6641 Validation Top 20 DE MSE: 330861.4375. 
Epoch 11 Step 1 Train Loss: 0.2520
Epoch 11 Step 51 Train Loss: 0.2796
Epoch 11 Step 101 Train Loss: 0.2786
Epoch 11 Step 151 Train Loss: 0.2938
Epoch 11: Train Overall MSE: 253718.9844 Validation Overall MSE: 532008.5625. 
Train Top 20 DE MSE: 173718.3438 Validation Top 20 DE MSE: 1065359.7500. 
Epoch 12 Step 1 Train Loss: 0.2353
Epoch 12 Step 51 Train Loss: 0.3311
Epoch 12 Step 101 Train Loss: 0.2850
Epoch 12 Step 151 Train Loss: 0.2765
Epoch 12: Train Overall MSE: 15.6574 Validation Overall MSE: 35.2908. 
Train Top 20 DE MSE: 10.6116 Validation Top 20 DE MSE: 71.9291. 
Epoch 13 Step 1 Train Loss: 0.2546
Epoch 13 Step 51 Train Loss: 0.2629
Epoch 13 Step 101 Train Loss: 0.3548
Epoch 13 Step 151 Train Loss: 0.2441
Epoch 13: Train Overall MSE: 52.4287 Validation Overall MSE: 115.2399. 
Train Top 20 DE MSE: 34.9076 Validation Top 20 DE MSE: 233.6009. 
Epoch 14 Step 1 Train Loss: 0.3143
Epoch 14 Step 51 Train Loss: 0.2629
Epoch 14 Step 101 Train Loss: 0.2388
Epoch 14 Step 151 Train Loss: 0.2581
Epoch 14: Train Overall MSE: 124103.0000 Validation Overall MSE: 260292.1094. 
Train Top 20 DE MSE: 84440.9609 Validation Top 20 DE MSE: 509805.6562. 
Epoch 15 Step 1 Train Loss: 0.2517
Epoch 15 Step 51 Train Loss: 0.2566
Epoch 15 Step 101 Train Loss: 0.2575
Epoch 15 Step 151 Train Loss: 0.2550
Epoch 15: Train Overall MSE: 134.1488 Validation Overall MSE: 290.4956. 
Train Top 20 DE MSE: 86.5617 Validation Top 20 DE MSE: 590.3676. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 4.7191
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 3.8949206
test_combo_seen0_pearson: 0.24066413179321763
test_combo_seen0_mse_de: 8.859385
test_combo_seen0_pearson_de: -0.07232405871124092
test_combo_seen1_mse: 0.3054116
test_combo_seen1_pearson: 0.7601397681511571
test_combo_seen1_mse_de: 0.74423516
test_combo_seen1_pearson_de: 0.8103308586245608
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 2.830912
test_unseen_single_pearson: 0.3161411767584466
test_unseen_single_mse_de: 6.6237507
test_unseen_single_pearson_de: -0.07315012861819842
test_combo_seen0_pearson_delta: 0.029992965158486956
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.4
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.15000000000000002
test_combo_seen0_mse_top20_de_non_dropout: 8.859385
test_combo_seen1_pearson_delta: 0.07338951694628676
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.325
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.5249999999999999
test_combo_seen1_mse_top20_de_non_dropout: 0.7442351
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.028448765364693093
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.35
test_unseen_single_frac_sigma_below_1_non_dropout: 0.125
test_unseen_single_mse_top20_de_non_dropout: 6.6237507
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.021 MB uploadedwandb: | 0.009 MB of 0.021 MB uploadedwandb: / 0.021 MB of 0.021 MB uploadedwandb: - 0.021 MB of 0.021 MB uploadedwandb: \ 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÇ‚ñà‚ñÜ‚ñÉ‚ñá‚ñÑ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá
wandb:                                                training_loss ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÉ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.4
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.15
wandb:                                         test_combo_seen0_mse 3.89492
wandb:                                      test_combo_seen0_mse_de 8.85938
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 8.85938
wandb:                                     test_combo_seen0_pearson 0.24066
wandb:                                  test_combo_seen0_pearson_de -0.07232
wandb:                               test_combo_seen0_pearson_delta 0.02999
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.325
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.525
wandb:                                         test_combo_seen1_mse 0.30541
wandb:                                      test_combo_seen1_mse_de 0.74424
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.74424
wandb:                                     test_combo_seen1_pearson 0.76014
wandb:                                  test_combo_seen1_pearson_de 0.81033
wandb:                               test_combo_seen1_pearson_delta 0.07339
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 4.71907
wandb:                                              test_de_pearson 0.28041
wandb:               test_frac_opposite_direction_top20_non_dropout 0.35
wandb:                          test_frac_sigma_below_1_non_dropout 0.29
wandb:                                                     test_mse 2.03351
wandb:                                test_mse_top20_de_non_dropout 4.71907
wandb:                                                 test_pearson 0.47865
wandb:                                           test_pearson_delta 0.04673
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.35
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.125
wandb:                                       test_unseen_single_mse 2.83091
wandb:                                    test_unseen_single_mse_de 6.62375
wandb:                  test_unseen_single_mse_top20_de_non_dropout 6.62375
wandb:                                   test_unseen_single_pearson 0.31614
wandb:                                test_unseen_single_pearson_de -0.07315
wandb:                             test_unseen_single_pearson_delta 0.02845
wandb:                                                 train_de_mse 86.56168
wandb:                                             train_de_pearson 0.49793
wandb:                                                    train_mse 134.14879
wandb:                                                train_pearson 0.69388
wandb:                                                training_loss 0.25007
wandb:                                                   val_de_mse 590.36755
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 290.49557
wandb:                                                  val_pearson -0.04622
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406677_2_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/mcut5lm7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_224014-mcut5lm7/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_224316-na4g3wfa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406677_2_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/na4g3wfa
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3646
Epoch 1 Step 51 Train Loss: 0.3541
Epoch 1 Step 101 Train Loss: 0.4584
Epoch 1 Step 151 Train Loss: 0.4786
Epoch 1 Step 201 Train Loss: 0.3730
Epoch 1: Train Overall MSE: 0.0036 Validation Overall MSE: 0.0377. 
Train Top 20 DE MSE: 0.0418 Validation Top 20 DE MSE: 0.2692. 
Epoch 2 Step 1 Train Loss: 0.3994
Epoch 2 Step 51 Train Loss: 0.4139
Epoch 2 Step 101 Train Loss: 0.4150
Epoch 2 Step 151 Train Loss: 0.3176
Epoch 2 Step 201 Train Loss: 0.3364
Epoch 2: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0368. 
Train Top 20 DE MSE: 0.0394 Validation Top 20 DE MSE: 0.2922. 
Epoch 3 Step 1 Train Loss: 0.3545
Epoch 3 Step 51 Train Loss: 0.2982
Epoch 3 Step 101 Train Loss: 0.3387
Epoch 3 Step 151 Train Loss: 0.3947
Epoch 3 Step 201 Train Loss: 0.3660
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0364. 
Train Top 20 DE MSE: 0.0187 Validation Top 20 DE MSE: 0.2783. 
Epoch 4 Step 1 Train Loss: 0.3843
Epoch 4 Step 51 Train Loss: 0.3158
Epoch 4 Step 101 Train Loss: 0.4275
Epoch 4 Step 151 Train Loss: 0.4337
Epoch 4 Step 201 Train Loss: 0.3321
Epoch 4: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0363. 
Train Top 20 DE MSE: 0.0246 Validation Top 20 DE MSE: 0.2802. 
Epoch 5 Step 1 Train Loss: 0.4709
Epoch 5 Step 51 Train Loss: 0.3506
Epoch 5 Step 101 Train Loss: 0.3577
Epoch 5 Step 151 Train Loss: 0.3762
Epoch 5 Step 201 Train Loss: 0.3506
Epoch 5: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0362. 
Train Top 20 DE MSE: 0.0353 Validation Top 20 DE MSE: 0.2791. 
Epoch 6 Step 1 Train Loss: 0.3459
Epoch 6 Step 51 Train Loss: 0.5848
Epoch 6 Step 101 Train Loss: 0.3346
Epoch 6 Step 151 Train Loss: 0.3654
Epoch 6 Step 201 Train Loss: 0.3166
Epoch 6: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0366. 
Train Top 20 DE MSE: 0.0153 Validation Top 20 DE MSE: 0.2848. 
Epoch 7 Step 1 Train Loss: 0.3809
Epoch 7 Step 51 Train Loss: 0.3701
Epoch 7 Step 101 Train Loss: 0.3215
Epoch 7 Step 151 Train Loss: 0.4008
Epoch 7 Step 201 Train Loss: 0.3453
Epoch 7: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0362. 
Train Top 20 DE MSE: 0.0142 Validation Top 20 DE MSE: 0.2780. 
Epoch 8 Step 1 Train Loss: 0.3196
Epoch 8 Step 51 Train Loss: 0.3508
Epoch 8 Step 101 Train Loss: 0.3557
Epoch 8 Step 151 Train Loss: 0.3501
Epoch 8 Step 201 Train Loss: 0.3678
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0363. 
Train Top 20 DE MSE: 0.0162 Validation Top 20 DE MSE: 0.2791. 
Epoch 9 Step 1 Train Loss: 0.4023
Epoch 9 Step 51 Train Loss: 0.4124
Epoch 9 Step 101 Train Loss: 0.3302
Epoch 9 Step 151 Train Loss: 0.3746
Epoch 9 Step 201 Train Loss: 0.3434
Epoch 9: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0364. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.2810. 
Epoch 10 Step 1 Train Loss: 0.3654
Epoch 10 Step 51 Train Loss: 0.3561
Epoch 10 Step 101 Train Loss: 0.3673
Epoch 10 Step 151 Train Loss: 0.3270
Epoch 10 Step 201 Train Loss: 0.3513
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0368. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.2858. 
Epoch 11 Step 1 Train Loss: 0.3720
Epoch 11 Step 51 Train Loss: 0.3920
Epoch 11 Step 101 Train Loss: 0.3533
Epoch 11 Step 151 Train Loss: 0.3064
Epoch 11 Step 201 Train Loss: 0.3332
Epoch 11: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0366. 
Train Top 20 DE MSE: 0.0160 Validation Top 20 DE MSE: 0.2839. 
Epoch 12 Step 1 Train Loss: 0.3238
Epoch 12 Step 51 Train Loss: 0.3336
Epoch 12 Step 101 Train Loss: 0.3533
Epoch 12 Step 151 Train Loss: 0.3110
Epoch 12 Step 201 Train Loss: 0.3342
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0366. 
Train Top 20 DE MSE: 0.0163 Validation Top 20 DE MSE: 0.2827. 
Epoch 13 Step 1 Train Loss: 0.3295
Epoch 13 Step 51 Train Loss: 0.3841
Epoch 13 Step 101 Train Loss: 0.3856
Epoch 13 Step 151 Train Loss: 0.3289
Epoch 13 Step 201 Train Loss: 0.3139
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0365. 
Train Top 20 DE MSE: 0.0164 Validation Top 20 DE MSE: 0.2823. 
Epoch 14 Step 1 Train Loss: 0.3793
Epoch 14 Step 51 Train Loss: 0.3800
Epoch 14 Step 101 Train Loss: 0.3965
Epoch 14 Step 151 Train Loss: 0.3711
Epoch 14 Step 201 Train Loss: 0.3158
Epoch 14: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0370. 
Train Top 20 DE MSE: 0.0196 Validation Top 20 DE MSE: 0.2872. 
Epoch 15 Step 1 Train Loss: 0.3454
Epoch 15 Step 51 Train Loss: 0.3487
Epoch 15 Step 101 Train Loss: 0.3252
Epoch 15 Step 151 Train Loss: 0.3819
Epoch 15 Step 201 Train Loss: 0.3085
Epoch 15: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0365. 
Train Top 20 DE MSE: 0.0142 Validation Top 20 DE MSE: 0.2822. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1928
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 0.004197024
test_combo_seen1_pearson: 0.9946239477907555
test_combo_seen1_mse_de: 0.106516965
test_combo_seen1_pearson_de: 0.9897180173253595
test_combo_seen2_mse: 0.0068822545
test_combo_seen2_pearson: 0.9911956711973047
test_combo_seen2_mse_de: 0.58467275
test_combo_seen2_pearson_de: 0.918620575608836
test_unseen_single_mse: 0.021223534
test_unseen_single_pearson: 0.9738339931590565
test_unseen_single_mse_de: 0.083226986
test_unseen_single_pearson_de: 0.4947349218900251
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: 0.43741283935443104
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.05
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.975
test_combo_seen1_mse_top20_de_non_dropout: 0.106516965
test_combo_seen2_pearson_delta: -0.08972012237498428
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.65
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.35
test_combo_seen2_mse_top20_de_non_dropout: 0.5846727
test_unseen_single_pearson_delta: 0.12746255759365394
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.525
test_unseen_single_frac_sigma_below_1_non_dropout: 0.575
test_unseen_single_mse_top20_de_non_dropout: 0.17748702
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.021 MB uploadedwandb: | 0.003 MB of 0.021 MB uploadedwandb: / 0.009 MB of 0.021 MB uploadedwandb: - 0.015 MB of 0.021 MB uploadedwandb: \ 0.015 MB of 0.021 MB uploadedwandb: | 0.021 MB of 0.021 MB uploadedwandb: / 0.021 MB of 0.021 MB uploadedwandb: - 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÅ‚ñÜ‚ñÖ‚ñÇ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ
wandb:                                                   val_de_mse ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.05
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.975
wandb:                                         test_combo_seen1_mse 0.0042
wandb:                                      test_combo_seen1_mse_de 0.10652
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.10652
wandb:                                     test_combo_seen1_pearson 0.99462
wandb:                                  test_combo_seen1_pearson_de 0.98972
wandb:                               test_combo_seen1_pearson_delta 0.43741
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.65
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.35
wandb:                                         test_combo_seen2_mse 0.00688
wandb:                                      test_combo_seen2_mse_de 0.58467
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.58467
wandb:                                     test_combo_seen2_pearson 0.9912
wandb:                                  test_combo_seen2_pearson_de 0.91862
wandb:                               test_combo_seen2_pearson_delta -0.08972
wandb:                                                  test_de_mse 0.19283
wandb:                                              test_de_pearson 0.77751
wandb:               test_frac_opposite_direction_top20_non_dropout 0.36
wandb:                          test_frac_sigma_below_1_non_dropout 0.69
wandb:                                                     test_mse 0.01154
wandb:                                test_mse_top20_de_non_dropout 0.23054
wandb:                                                 test_pearson 0.98562
wandb:                                           test_pearson_delta 0.20801
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.525
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.575
wandb:                                       test_unseen_single_mse 0.02122
wandb:                                    test_unseen_single_mse_de 0.08323
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.17749
wandb:                                   test_unseen_single_pearson 0.97383
wandb:                                test_unseen_single_pearson_de 0.49473
wandb:                             test_unseen_single_pearson_delta 0.12746
wandb:                                                 train_de_mse 0.01415
wandb:                                             train_de_pearson 0.99781
wandb:                                                    train_mse 0.0009
wandb:                                                train_pearson 0.99886
wandb:                                                training_loss 0.36237
wandb:                                                   val_de_mse 0.28224
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.03652
wandb:                                                  val_pearson 0.954
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406677_2_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/na4g3wfa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_224316-na4g3wfa/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:2
combo_seen2:1
unseen_single:2
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_224711-stmlq0x5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406677_2_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/stmlq0x5
wandb: WARNING Serializing object of type ndarray that is 20484224 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3231
Epoch 1 Step 51 Train Loss: 0.4093
Epoch 1 Step 101 Train Loss: 0.3804
Epoch 1 Step 151 Train Loss: 0.4189
Epoch 1 Step 201 Train Loss: 0.3853
Epoch 1: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0323. 
Train Top 20 DE MSE: 0.1388 Validation Top 20 DE MSE: 0.1064. 
Epoch 2 Step 1 Train Loss: 0.3855
Epoch 2 Step 51 Train Loss: 0.3490
Epoch 2 Step 101 Train Loss: 0.3452
Epoch 2 Step 151 Train Loss: 0.3588
Epoch 2 Step 201 Train Loss: 0.3687
Epoch 2: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0308. 
Train Top 20 DE MSE: 0.1144 Validation Top 20 DE MSE: 0.1251. 
Epoch 3 Step 1 Train Loss: 0.3434
Epoch 3 Step 51 Train Loss: 0.3567
Epoch 3 Step 101 Train Loss: 0.3613
Epoch 3 Step 151 Train Loss: 0.3625
Epoch 3 Step 201 Train Loss: 0.3884
Epoch 3: Train Overall MSE: 0.0014 Validation Overall MSE: 0.0307. 
Train Top 20 DE MSE: 0.0640 Validation Top 20 DE MSE: 0.1217. 
Epoch 4 Step 1 Train Loss: 0.3831
Epoch 4 Step 51 Train Loss: 0.3232
Epoch 4 Step 101 Train Loss: 0.3733
Epoch 4 Step 151 Train Loss: 0.3909
Epoch 4 Step 201 Train Loss: 0.2981
Epoch 4: Train Overall MSE: 0.0008 Validation Overall MSE: 0.0308. 
Train Top 20 DE MSE: 0.0392 Validation Top 20 DE MSE: 0.1150. 
Epoch 5 Step 1 Train Loss: 0.4359
Epoch 5 Step 51 Train Loss: 0.3587
Epoch 5 Step 101 Train Loss: 0.3094
Epoch 5 Step 151 Train Loss: 0.3425
Epoch 5 Step 201 Train Loss: 0.3042
Epoch 5: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0308. 
Train Top 20 DE MSE: 0.0136 Validation Top 20 DE MSE: 0.1226. 
Epoch 6 Step 1 Train Loss: 0.4052
Epoch 6 Step 51 Train Loss: 0.4343
Epoch 6 Step 101 Train Loss: 0.3780
Epoch 6 Step 151 Train Loss: 0.3332
Epoch 6 Step 201 Train Loss: 0.3632
Epoch 6: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0307. 
Train Top 20 DE MSE: 0.0083 Validation Top 20 DE MSE: 0.1212. 
Epoch 7 Step 1 Train Loss: 0.3220
Epoch 7 Step 51 Train Loss: 0.4377
Epoch 7 Step 101 Train Loss: 0.3831
Epoch 7 Step 151 Train Loss: 0.3314
Epoch 7 Step 201 Train Loss: 0.3555
Epoch 7: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0146 Validation Top 20 DE MSE: 0.1210. 
Epoch 8 Step 1 Train Loss: 0.4014
Epoch 8 Step 51 Train Loss: 0.3398
Epoch 8 Step 101 Train Loss: 0.3838
Epoch 8 Step 151 Train Loss: 0.3890
Epoch 8 Step 201 Train Loss: 0.3054
Epoch 8: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0308. 
Train Top 20 DE MSE: 0.0097 Validation Top 20 DE MSE: 0.1210. 
Epoch 9 Step 1 Train Loss: 0.3996
Epoch 9 Step 51 Train Loss: 0.3390
Epoch 9 Step 101 Train Loss: 0.3882
Epoch 9 Step 151 Train Loss: 0.3032
Epoch 9 Step 201 Train Loss: 0.3773
Epoch 9: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0111 Validation Top 20 DE MSE: 0.1206. 
Epoch 10 Step 1 Train Loss: 0.3151
Epoch 10 Step 51 Train Loss: 0.3197
Epoch 10 Step 101 Train Loss: 0.3655
Epoch 10 Step 151 Train Loss: 0.3859
Epoch 10 Step 201 Train Loss: 0.3432
Epoch 10: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0325. 
Train Top 20 DE MSE: 0.0118 Validation Top 20 DE MSE: 0.1294. 
Epoch 11 Step 1 Train Loss: 0.3450
Epoch 11 Step 51 Train Loss: 0.3838
Epoch 11 Step 101 Train Loss: 0.3537
Epoch 11 Step 151 Train Loss: 0.3825
Epoch 11 Step 201 Train Loss: 0.3790
Epoch 11: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.1208. 
Epoch 12 Step 1 Train Loss: 0.3673
Epoch 12 Step 51 Train Loss: 0.3733
Epoch 12 Step 101 Train Loss: 0.3395
Epoch 12 Step 151 Train Loss: 0.3307
Epoch 12 Step 201 Train Loss: 0.3442
Epoch 12: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0309. 
Train Top 20 DE MSE: 0.0129 Validation Top 20 DE MSE: 0.1221. 
Epoch 13 Step 1 Train Loss: 0.4242
Epoch 13 Step 51 Train Loss: 0.3185
Epoch 13 Step 101 Train Loss: 0.3760
Epoch 13 Step 151 Train Loss: 0.3465
Epoch 13 Step 201 Train Loss: 0.4613
Epoch 13: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0306. 
Train Top 20 DE MSE: 0.0098 Validation Top 20 DE MSE: 0.1163. 
Epoch 14 Step 1 Train Loss: 0.3146
Epoch 14 Step 51 Train Loss: 0.3599
Epoch 14 Step 101 Train Loss: 0.3068
Epoch 14 Step 151 Train Loss: 0.3189
Epoch 14 Step 201 Train Loss: 0.4055
Epoch 14: Train Overall MSE: 0.0009 Validation Overall MSE: 0.0316. 
Train Top 20 DE MSE: 0.0107 Validation Top 20 DE MSE: 0.1263. 
Epoch 15 Step 1 Train Loss: 0.3411
Epoch 15 Step 51 Train Loss: 0.3747
Epoch 15 Step 101 Train Loss: 0.3620
Epoch 15 Step 151 Train Loss: 0.3780
Epoch 15 Step 201 Train Loss: 0.3422
Epoch 15: Train Overall MSE: 0.0010 Validation Overall MSE: 0.0305. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.1197. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.3044
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: 0.005421362
test_combo_seen1_pearson: 0.9929795568478458
test_combo_seen1_mse_de: 0.26706126
test_combo_seen1_pearson_de: 0.9711014130947142
test_combo_seen2_mse: 0.0075039878
test_combo_seen2_pearson: 0.9904718780596882
test_combo_seen2_mse_de: 0.6534263
test_combo_seen2_pearson_de: 0.910972691139089
test_unseen_single_mse: 0.022182463
test_unseen_single_pearson: 0.9723777561814443
test_unseen_single_mse_de: 0.16715686
test_unseen_single_pearson_de: 0.49622554057607876
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: 0.20988784366201246
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.425
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.7
test_combo_seen1_mse_top20_de_non_dropout: 0.26706126
test_combo_seen2_pearson_delta: -0.10665389826739739
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.7
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.35
test_combo_seen2_mse_top20_de_non_dropout: 0.65342635
test_unseen_single_pearson_delta: 0.02503462963826733
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.3
test_unseen_single_frac_sigma_below_1_non_dropout: 0.525
test_unseen_single_mse_top20_de_non_dropout: 0.23239516
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.021 MB uploadedwandb: | 0.003 MB of 0.021 MB uploadedwandb: / 0.017 MB of 0.021 MB uploadedwandb: - 0.017 MB of 0.021 MB uploadedwandb: \ 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:                                                   val_de_mse ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÖ
wandb:                                               val_de_pearson ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                      val_mse ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ
wandb:                                                  val_pearson ‚ñÉ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÅ‚ñà‚ñá‚ñà‚ñÑ‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.425
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.7
wandb:                                         test_combo_seen1_mse 0.00542
wandb:                                      test_combo_seen1_mse_de 0.26706
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.26706
wandb:                                     test_combo_seen1_pearson 0.99298
wandb:                                  test_combo_seen1_pearson_de 0.9711
wandb:                               test_combo_seen1_pearson_delta 0.20989
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.7
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.35
wandb:                                         test_combo_seen2_mse 0.0075
wandb:                                      test_combo_seen2_mse_de 0.65343
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.65343
wandb:                                     test_combo_seen2_pearson 0.99047
wandb:                                  test_combo_seen2_pearson_de 0.91097
wandb:                               test_combo_seen2_pearson_delta -0.10665
wandb:                                                  test_de_mse 0.30437
wandb:                                              test_de_pearson 0.76913
wandb:               test_frac_opposite_direction_top20_non_dropout 0.43
wandb:                          test_frac_sigma_below_1_non_dropout 0.56
wandb:                                                     test_mse 0.01254
wandb:                                test_mse_top20_de_non_dropout 0.33047
wandb:                                                 test_pearson 0.98424
wandb:                                           test_pearson_delta 0.07264
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.3
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.525
wandb:                                       test_unseen_single_mse 0.02218
wandb:                                    test_unseen_single_mse_de 0.16716
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.2324
wandb:                                   test_unseen_single_pearson 0.97238
wandb:                                test_unseen_single_pearson_de 0.49623
wandb:                             test_unseen_single_pearson_delta 0.02503
wandb:                                                 train_de_mse 0.01063
wandb:                                             train_de_pearson 0.99822
wandb:                                                    train_mse 0.00099
wandb:                                                train_pearson 0.99875
wandb:                                                training_loss 0.30501
wandb:                                                   val_de_mse 0.11966
wandb:                                               val_de_pearson 0.0
wandb:                                                      val_mse 0.03049
wandb:                                                  val_pearson 0.96187
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406677_2_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/stmlq0x5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_224711-stmlq0x5/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:22
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_225147-zy8b6gud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406681_3_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zy8b6gud
wandb: WARNING Serializing object of type ndarray that is 20738176 bytes
  0%|                                                  | 0/3555 [00:00<?, ?it/s]  0%|                                          | 6/3555 [00:00<01:01, 57.94it/s]  0%|‚ñè                                        | 13/3555 [00:00<00:56, 63.07it/s]  1%|‚ñè                                        | 20/3555 [00:00<00:54, 65.13it/s]  1%|‚ñé                                        | 27/3555 [00:00<01:09, 50.90it/s]  1%|‚ñç                                        | 41/3555 [00:00<00:45, 76.47it/s]  1%|‚ñå                                        | 50/3555 [00:00<00:45, 76.38it/s]  2%|‚ñã                                        | 59/3555 [00:00<00:47, 74.37it/s]  2%|‚ñä                                        | 68/3555 [00:00<00:45, 77.27it/s]  2%|‚ñâ                                        | 76/3555 [00:01<01:00, 57.67it/s]  2%|‚ñâ                                        | 86/3555 [00:01<00:52, 65.69it/s]  3%|‚ñà                                        | 94/3555 [00:01<00:51, 67.46it/s]  3%|‚ñà‚ñè                                      | 102/3555 [00:01<00:49, 69.39it/s]  3%|‚ñà‚ñè                                      | 110/3555 [00:01<00:53, 64.02it/s]  3%|‚ñà‚ñé                                      | 121/3555 [00:01<00:48, 71.05it/s]  4%|‚ñà‚ñç                                      | 129/3555 [00:01<00:53, 63.73it/s]  4%|‚ñà‚ñå                                      | 140/3555 [00:02<00:46, 74.04it/s]  4%|‚ñà‚ñã                                      | 148/3555 [00:02<00:47, 71.44it/s]  4%|‚ñà‚ñä                                      | 156/3555 [00:02<00:49, 68.36it/s]  5%|‚ñà‚ñä                                      | 164/3555 [00:02<00:47, 70.73it/s]  5%|‚ñà‚ñâ                                      | 172/3555 [00:02<00:48, 69.99it/s]  5%|‚ñà‚ñà                                      | 180/3555 [00:02<00:48, 69.26it/s]  5%|‚ñà‚ñà                                      | 188/3555 [00:02<00:49, 68.59it/s]  5%|‚ñà‚ñà‚ñè                                     | 195/3555 [00:02<00:48, 68.61it/s]  6%|‚ñà‚ñà‚ñé                                     | 202/3555 [00:02<00:48, 68.83it/s]  6%|‚ñà‚ñà‚ñé                                     | 209/3555 [00:03<00:48, 68.35it/s]  6%|‚ñà‚ñà‚ñç                                     | 216/3555 [00:03<00:51, 64.90it/s]  6%|‚ñà‚ñà‚ñå                                     | 223/3555 [00:03<01:02, 53.33it/s]  7%|‚ñà‚ñà‚ñå                                     | 232/3555 [00:03<00:55, 59.72it/s]  7%|‚ñà‚ñà‚ñã                                     | 239/3555 [00:03<00:59, 56.04it/s]  7%|‚ñà‚ñà‚ñä                                     | 245/3555 [00:03<01:00, 55.00it/s]  7%|‚ñà‚ñà‚ñä                                     | 251/3555 [00:03<01:05, 50.42it/s]  7%|‚ñà‚ñà‚ñâ                                     | 257/3555 [00:04<01:04, 50.90it/s]  7%|‚ñà‚ñà‚ñâ                                     | 263/3555 [00:04<01:03, 52.01it/s]  8%|‚ñà‚ñà‚ñà                                     | 270/3555 [00:04<00:59, 55.41it/s]  8%|‚ñà‚ñà‚ñà                                     | 277/3555 [00:04<00:56, 58.34it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 285/3555 [00:04<00:51, 63.88it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 293/3555 [00:04<00:50, 64.97it/s]  8%|‚ñà‚ñà‚ñà‚ñç                                    | 300/3555 [00:04<00:54, 59.94it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 307/3555 [00:04<00:58, 55.25it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 313/3555 [00:04<01:02, 51.84it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 319/3555 [00:05<01:02, 51.40it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 325/3555 [00:05<01:20, 40.31it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 337/3555 [00:05<00:57, 55.90it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 344/3555 [00:05<01:06, 47.95it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 353/3555 [00:05<00:58, 54.28it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 360/3555 [00:05<01:01, 51.80it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 366/3555 [00:06<01:00, 52.59it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 372/3555 [00:06<01:07, 47.36it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 380/3555 [00:06<00:59, 53.71it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 386/3555 [00:06<00:58, 54.08it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 392/3555 [00:06<00:58, 54.07it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 398/3555 [00:06<01:05, 48.24it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 406/3555 [00:06<00:57, 54.42it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 412/3555 [00:06<00:58, 53.83it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 418/3555 [00:07<00:59, 52.35it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 425/3555 [00:07<00:56, 55.69it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 431/3555 [00:07<01:06, 46.82it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 441/3555 [00:07<00:53, 57.76it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 448/3555 [00:07<00:54, 57.21it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 454/3555 [00:07<01:05, 47.33it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 461/3555 [00:07<01:01, 50.46it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 467/3555 [00:07<01:01, 50.03it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 473/3555 [00:08<01:13, 41.79it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 483/3555 [00:08<01:09, 44.34it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 491/3555 [00:08<01:00, 50.82it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 497/3555 [00:08<01:08, 44.52it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 505/3555 [00:08<01:02, 49.08it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 520/3555 [00:08<00:44, 67.76it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 528/3555 [00:09<00:52, 57.85it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 540/3555 [00:09<00:43, 69.76it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 548/3555 [00:09<00:43, 68.43it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 556/3555 [00:09<00:51, 57.80it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 565/3555 [00:09<00:46, 64.59it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 574/3555 [00:09<00:42, 70.05it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 582/3555 [00:09<00:52, 56.29it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 590/3555 [00:10<00:48, 61.06it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 597/3555 [00:10<00:46, 62.96it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 604/3555 [00:10<00:45, 64.32it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 611/3555 [00:10<00:45, 64.93it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 618/3555 [00:10<00:44, 65.47it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 625/3555 [00:10<00:44, 65.78it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 632/3555 [00:10<00:43, 66.65it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 639/3555 [00:10<00:43, 67.24it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 647/3555 [00:10<00:42, 68.02it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 654/3555 [00:11<00:42, 68.58it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 661/3555 [00:11<00:42, 68.59it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 668/3555 [00:11<00:42, 67.96it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 675/3555 [00:11<00:45, 63.02it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 685/3555 [00:11<00:40, 70.02it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 693/3555 [00:11<00:39, 72.15it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 701/3555 [00:11<00:41, 68.44it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 708/3555 [00:11<00:41, 68.40it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 715/3555 [00:11<00:41, 68.67it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 723/3555 [00:12<00:41, 68.74it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 731/3555 [00:12<00:39, 71.47it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 739/3555 [00:12<00:39, 70.45it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 747/3555 [00:12<00:40, 69.79it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 755/3555 [00:12<00:41, 66.90it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 763/3555 [00:12<00:41, 67.40it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 770/3555 [00:12<00:41, 67.04it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 778/3555 [00:12<00:40, 68.36it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 786/3555 [00:12<00:39, 70.59it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 794/3555 [00:13<00:38, 71.80it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 802/3555 [00:13<00:37, 72.63it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 811/3555 [00:13<00:35, 76.77it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 819/3555 [00:13<00:35, 76.50it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 827/3555 [00:13<00:37, 73.67it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 836/3555 [00:13<00:35, 77.13it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 844/3555 [00:13<00:36, 74.18it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 852/3555 [00:13<00:36, 74.15it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 860/3555 [00:13<00:36, 74.74it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 868/3555 [00:14<00:35, 75.53it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 876/3555 [00:14<00:36, 74.10it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 884/3555 [00:14<00:37, 71.34it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 892/3555 [00:14<00:38, 69.97it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 900/3555 [00:14<00:39, 66.80it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 908/3555 [00:14<00:38, 68.55it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 915/3555 [00:14<00:38, 68.01it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 924/3555 [00:14<00:36, 71.72it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 932/3555 [00:14<00:37, 70.13it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 941/3555 [00:15<00:34, 74.90it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 949/3555 [00:15<00:34, 74.88it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 957/3555 [00:15<00:34, 74.92it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 966/3555 [00:15<00:35, 73.21it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 975/3555 [00:15<00:34, 75.85it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 984/3555 [00:15<00:33, 77.45it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 992/3555 [00:15<00:34, 74.57it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1001/3555 [00:15<00:34, 74.32it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1010/3555 [00:15<00:33, 76.71it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1018/3555 [00:16<00:34, 73.78it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1026/3555 [00:16<00:35, 70.69it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1035/3555 [00:16<00:35, 70.19it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1044/3555 [00:16<00:33, 74.86it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1052/3555 [00:16<00:35, 70.21it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1060/3555 [00:16<00:34, 71.97it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1068/3555 [00:16<00:36, 68.53it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1075/3555 [00:16<00:36, 68.55it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1082/3555 [00:17<00:36, 68.69it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1089/3555 [00:17<00:35, 68.59it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1097/3555 [00:17<00:35, 69.90it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1105/3555 [00:17<00:37, 66.10it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1112/3555 [00:17<00:37, 64.93it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1119/3555 [00:17<00:40, 59.64it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1126/3555 [00:17<00:44, 54.92it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1134/3555 [00:17<00:41, 58.48it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1140/3555 [00:18<00:43, 55.42it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1146/3555 [00:18<00:43, 55.16it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1153/3555 [00:18<00:41, 57.40it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1161/3555 [00:18<00:38, 62.24it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1169/3555 [00:18<00:35, 66.61it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1177/3555 [00:18<00:34, 69.57it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1185/3555 [00:18<00:33, 71.43it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1193/3555 [00:18<00:32, 73.47it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1201/3555 [00:18<00:31, 74.52it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1209/3555 [00:18<00:35, 66.35it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1220/3555 [00:19<00:30, 77.17it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1228/3555 [00:19<00:31, 74.10it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1237/3555 [00:19<00:30, 77.14it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1246/3555 [00:19<00:28, 79.94it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1255/3555 [00:19<00:31, 73.23it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1265/3555 [00:19<00:28, 79.32it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1274/3555 [00:19<00:28, 79.12it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1283/3555 [00:19<00:30, 75.51it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1291/3555 [00:20<00:29, 76.01it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1300/3555 [00:20<00:29, 76.05it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1309/3555 [00:20<00:28, 78.53it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1317/3555 [00:20<00:29, 75.35it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1325/3555 [00:20<00:29, 75.53it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1334/3555 [00:20<00:28, 77.48it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1342/3555 [00:20<00:28, 77.29it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1350/3555 [00:20<00:29, 74.72it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1359/3555 [00:20<00:28, 77.51it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1367/3555 [00:21<00:28, 77.77it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1375/3555 [00:21<00:29, 74.14it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1384/3555 [00:21<00:28, 76.95it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1392/3555 [00:21<00:29, 74.34it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1400/3555 [00:21<00:28, 75.14it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1408/3555 [00:21<00:29, 72.65it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1416/3555 [00:21<00:28, 74.09it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1425/3555 [00:21<00:28, 75.25it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1434/3555 [00:21<00:27, 78.16it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1442/3555 [00:22<00:27, 77.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1450/3555 [00:22<00:28, 74.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1458/3555 [00:22<00:27, 75.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1467/3555 [00:22<00:28, 73.16it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1476/3555 [00:22<00:26, 77.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1484/3555 [00:22<00:26, 76.97it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1492/3555 [00:22<00:26, 77.42it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1500/3555 [00:22<00:26, 77.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1509/3555 [00:22<00:25, 79.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1517/3555 [00:22<00:26, 77.62it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1525/3555 [00:23<00:27, 74.06it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1534/3555 [00:23<00:27, 74.51it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1542/3555 [00:23<00:26, 75.93it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1550/3555 [00:23<00:26, 76.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1559/3555 [00:23<00:25, 79.02it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1567/3555 [00:23<00:25, 78.71it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1575/3555 [00:23<00:27, 73.18it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1584/3555 [00:23<00:25, 76.95it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1593/3555 [00:24<00:27, 72.50it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1601/3555 [00:24<00:29, 66.63it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1615/3555 [00:24<00:23, 81.56it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1624/3555 [00:24<00:24, 77.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1633/3555 [00:24<00:23, 80.19it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1642/3555 [00:24<00:23, 82.15it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1651/3555 [00:24<00:24, 77.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1660/3555 [00:24<00:24, 77.99it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1669/3555 [00:24<00:23, 80.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1678/3555 [00:25<00:24, 77.11it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1686/3555 [00:25<00:24, 77.22it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1695/3555 [00:25<00:23, 77.86it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1703/3555 [00:25<00:27, 67.78it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1711/3555 [00:25<00:26, 70.29it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1720/3555 [00:25<00:25, 72.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1728/3555 [00:25<00:25, 71.56it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1737/3555 [00:25<00:24, 75.11it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1745/3555 [00:26<00:24, 75.19it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1753/3555 [00:26<00:23, 75.22it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1761/3555 [00:26<00:23, 74.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1770/3555 [00:26<00:23, 77.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1778/3555 [00:26<00:24, 73.87it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1786/3555 [00:26<00:23, 73.97it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1795/3555 [00:26<00:22, 77.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1803/3555 [00:26<00:23, 73.83it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1811/3555 [00:26<00:23, 74.26it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1820/3555 [00:27<00:22, 75.72it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1828/3555 [00:27<00:22, 76.16it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1836/3555 [00:27<00:22, 76.70it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1844/3555 [00:27<00:23, 71.45it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1853/3555 [00:27<00:22, 76.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1861/3555 [00:27<00:22, 76.32it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1870/3555 [00:27<00:21, 79.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1879/3555 [00:27<00:21, 79.18it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1887/3555 [00:27<00:21, 78.59it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1895/3555 [00:27<00:21, 78.93it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1903/3555 [00:28<00:20, 78.80it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1911/3555 [00:28<00:21, 77.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1919/3555 [00:28<00:20, 78.19it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1928/3555 [00:28<00:20, 78.81it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1936/3555 [00:28<00:20, 78.56it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1944/3555 [00:28<00:20, 78.82it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1953/3555 [00:28<00:20, 78.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1961/3555 [00:28<00:20, 76.89it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1969/3555 [00:28<00:23, 68.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1981/3555 [00:29<00:19, 80.36it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 1990/3555 [00:29<00:19, 78.57it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1999/3555 [00:29<00:20, 75.56it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2008/3555 [00:29<00:19, 77.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2016/3555 [00:29<00:20, 76.60it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2024/3555 [00:29<00:20, 76.12it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2032/3555 [00:29<00:20, 75.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2040/3555 [00:29<00:20, 74.93it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2048/3555 [00:29<00:20, 75.10it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2056/3555 [00:30<00:20, 74.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2064/3555 [00:30<00:21, 69.40it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2074/3555 [00:30<00:19, 75.96it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2082/3555 [00:30<00:19, 75.45it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2090/3555 [00:30<00:20, 72.73it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2098/3555 [00:30<00:19, 73.16it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2106/3555 [00:30<00:19, 73.74it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2114/3555 [00:30<00:20, 70.45it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2123/3555 [00:31<00:19, 74.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2131/3555 [00:31<00:19, 73.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2139/3555 [00:31<00:20, 70.74it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2148/3555 [00:31<00:19, 73.93it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2156/3555 [00:31<00:18, 73.65it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2164/3555 [00:31<00:19, 70.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2172/3555 [00:31<00:19, 71.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2180/3555 [00:31<00:18, 72.67it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2189/3555 [00:31<00:17, 76.22it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2197/3555 [00:32<00:18, 73.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2205/3555 [00:32<00:18, 73.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2214/3555 [00:32<00:17, 76.66it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2222/3555 [00:32<00:17, 76.33it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2230/3555 [00:32<00:17, 75.59it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2238/3555 [00:32<00:17, 75.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2246/3555 [00:32<00:17, 73.04it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2255/3555 [00:32<00:16, 77.07it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2263/3555 [00:32<00:16, 76.72it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2271/3555 [00:32<00:16, 77.01it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2279/3555 [00:33<00:16, 77.38it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2287/3555 [00:33<00:16, 76.62it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2295/3555 [00:33<00:16, 76.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2303/3555 [00:33<00:17, 73.51it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2312/3555 [00:33<00:16, 76.60it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2320/3555 [00:33<00:18, 66.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2328/3555 [00:33<00:17, 68.68it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2336/3555 [00:33<00:17, 70.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2344/3555 [00:34<00:17, 69.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2352/3555 [00:34<00:16, 70.99it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2361/3555 [00:34<00:16, 74.36it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2369/3555 [00:34<00:15, 74.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2377/3555 [00:34<00:16, 71.63it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2385/3555 [00:34<00:16, 72.38it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2393/3555 [00:34<00:15, 73.14it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2402/3555 [00:34<00:14, 77.46it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2410/3555 [00:34<00:14, 76.80it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2418/3555 [00:34<00:15, 74.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2426/3555 [00:35<00:14, 75.85it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2434/3555 [00:35<00:14, 76.32it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2443/3555 [00:35<00:13, 79.47it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2451/3555 [00:35<00:14, 76.28it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2459/3555 [00:35<00:14, 76.34it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2468/3555 [00:35<00:13, 79.25it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2476/3555 [00:35<00:14, 75.65it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2484/3555 [00:35<00:14, 76.33it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2493/3555 [00:35<00:13, 79.57it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2501/3555 [00:36<00:13, 79.11it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2509/3555 [00:36<00:13, 75.26it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2518/3555 [00:36<00:13, 79.04it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2526/3555 [00:36<00:13, 76.33it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2534/3555 [00:36<00:13, 76.85it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2542/3555 [00:36<00:13, 76.89it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2551/3555 [00:36<00:12, 80.37it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2560/3555 [00:36<00:12, 77.09it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2569/3555 [00:36<00:12, 80.33it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2578/3555 [00:37<00:12, 77.07it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2587/3555 [00:37<00:12, 77.10it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2596/3555 [00:37<00:11, 79.99it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2605/3555 [00:37<00:13, 72.03it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2613/3555 [00:37<00:12, 73.59it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2621/3555 [00:37<00:12, 75.29it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2631/3555 [00:37<00:11, 79.45it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2640/3555 [00:37<00:11, 78.54it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2648/3555 [00:37<00:11, 78.72it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2657/3555 [00:38<00:11, 76.26it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2665/3555 [00:38<00:12, 73.84it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2674/3555 [00:38<00:11, 77.82it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2683/3555 [00:38<00:11, 77.58it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2691/3555 [00:38<00:12, 69.67it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2703/3555 [00:38<00:10, 81.49it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2712/3555 [00:38<00:10, 80.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2721/3555 [00:38<00:10, 77.15it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2729/3555 [00:39<00:10, 77.83it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2738/3555 [00:39<00:10, 78.68it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2746/3555 [00:39<00:10, 77.79it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2754/3555 [00:39<00:10, 77.50it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2762/3555 [00:39<00:10, 76.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2770/3555 [00:39<00:10, 76.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2778/3555 [00:39<00:10, 76.94it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2786/3555 [00:39<00:09, 77.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2794/3555 [00:39<00:10, 69.28it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2802/3555 [00:40<00:11, 67.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2811/3555 [00:40<00:10, 70.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2820/3555 [00:40<00:10, 72.83it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2828/3555 [00:40<00:09, 74.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2837/3555 [00:40<00:09, 77.84it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2845/3555 [00:40<00:09, 77.97it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2853/3555 [00:40<00:08, 78.15it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2861/3555 [00:40<00:08, 77.69it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2869/3555 [00:40<00:08, 76.89it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2877/3555 [00:40<00:08, 77.14it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2885/3555 [00:41<00:08, 76.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2893/3555 [00:41<00:08, 76.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2901/3555 [00:41<00:08, 76.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2909/3555 [00:41<00:08, 73.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2918/3555 [00:41<00:08, 76.97it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2926/3555 [00:41<00:08, 73.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2935/3555 [00:41<00:08, 76.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2943/3555 [00:41<00:08, 75.97it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2951/3555 [00:41<00:07, 75.85it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2959/3555 [00:42<00:08, 73.10it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2968/3555 [00:42<00:07, 76.05it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2976/3555 [00:42<00:07, 75.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2984/3555 [00:42<00:07, 75.78it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2992/3555 [00:42<00:07, 75.64it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3000/3555 [00:42<00:07, 75.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3008/3555 [00:42<00:07, 73.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3017/3555 [00:42<00:07, 76.62it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3025/3555 [00:42<00:06, 76.45it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3033/3555 [00:43<00:06, 76.02it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 3041/3555 [00:43<00:06, 74.48it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3049/3555 [00:43<00:06, 74.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3057/3555 [00:43<00:06, 74.31it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 3065/3555 [00:43<00:06, 73.87it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3073/3555 [00:43<00:06, 74.36it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 3081/3555 [00:43<00:06, 73.20it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3089/3555 [00:43<00:07, 66.11it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3096/3555 [00:43<00:07, 62.69it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3103/3555 [00:44<00:08, 55.28it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 3109/3555 [00:44<00:08, 53.93it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3115/3555 [00:44<00:08, 54.58it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3121/3555 [00:44<00:08, 53.01it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3127/3555 [00:44<00:08, 52.39it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 3133/3555 [00:44<00:08, 51.25it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3140/3555 [00:44<00:08, 50.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3146/3555 [00:45<00:08, 48.69it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3151/3555 [00:45<00:08, 47.38it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3156/3555 [00:45<00:08, 46.33it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3161/3555 [00:45<00:10, 37.77it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3171/3555 [00:45<00:07, 50.66it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3177/3555 [00:45<00:07, 47.99it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3183/3555 [00:45<00:07, 48.99it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3190/3555 [00:45<00:06, 53.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3199/3555 [00:46<00:05, 62.43it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3207/3555 [00:46<00:05, 65.97it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3214/3555 [00:46<00:05, 61.84it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3225/3555 [00:46<00:04, 74.05it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3233/3555 [00:46<00:04, 72.63it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3241/3555 [00:46<00:04, 67.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3249/3555 [00:46<00:04, 69.24it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3257/3555 [00:46<00:04, 68.70it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3266/3555 [00:46<00:03, 73.30it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3274/3555 [00:47<00:03, 73.87it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3282/3555 [00:47<00:03, 73.98it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3290/3555 [00:47<00:03, 73.76it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3298/3555 [00:47<00:03, 74.02it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3306/3555 [00:47<00:03, 74.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3314/3555 [00:47<00:03, 74.35it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3322/3555 [00:47<00:03, 74.44it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3330/3555 [00:47<00:02, 75.04it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3338/3555 [00:47<00:02, 75.17it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3346/3555 [00:48<00:02, 75.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3354/3555 [00:48<00:02, 76.30it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3362/3555 [00:48<00:02, 73.03it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3370/3555 [00:48<00:02, 74.33it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3379/3555 [00:48<00:02, 78.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3387/3555 [00:48<00:02, 77.55it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3395/3555 [00:48<00:02, 77.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3403/3555 [00:48<00:01, 77.60it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3411/3555 [00:48<00:02, 71.25it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3419/3555 [00:49<00:02, 61.78it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3426/3555 [00:49<00:02, 61.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3433/3555 [00:49<00:02, 59.51it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3441/3555 [00:49<00:01, 63.56it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3448/3555 [00:49<00:01, 63.98it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3455/3555 [00:49<00:01, 57.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3468/3555 [00:49<00:01, 74.71it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3476/3555 [00:49<00:01, 74.73it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3484/3555 [00:49<00:00, 72.36it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3493/3555 [00:50<00:00, 75.57it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3501/3555 [00:50<00:00, 75.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3509/3555 [00:50<00:00, 72.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3518/3555 [00:50<00:00, 75.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3526/3555 [00:50<00:00, 75.72it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3534/3555 [00:50<00:00, 72.35it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3542/3555 [00:50<00:00, 64.32it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3555/3555 [00:50<00:00, 80.48it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3555/3555 [00:50<00:00, 69.81it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.6888
Epoch 1 Step 51 Train Loss: 0.6664
Epoch 1 Step 101 Train Loss: 0.5163
Epoch 1 Step 151 Train Loss: 0.5051
Epoch 1 Step 201 Train Loss: 0.5304
Epoch 1 Step 251 Train Loss: 0.5617
Epoch 1 Step 301 Train Loss: 0.4904
Epoch 1 Step 351 Train Loss: 0.5120
Epoch 1 Step 401 Train Loss: 0.5454
Epoch 1 Step 451 Train Loss: 0.5038
Epoch 1 Step 501 Train Loss: 0.5617
Epoch 1 Step 551 Train Loss: 0.5479
Epoch 1 Step 601 Train Loss: 0.4523
Epoch 1 Step 651 Train Loss: 0.5105
Epoch 1 Step 701 Train Loss: 0.4975
Epoch 1 Step 751 Train Loss: 0.5522
Epoch 1 Step 801 Train Loss: 0.5203
Epoch 1 Step 851 Train Loss: 0.5275
Epoch 1 Step 901 Train Loss: 0.5345
Epoch 1 Step 951 Train Loss: 0.5409
Epoch 1 Step 1001 Train Loss: 0.4449
Epoch 1 Step 1051 Train Loss: 0.4961
Epoch 1 Step 1101 Train Loss: 0.4393
Epoch 1 Step 1151 Train Loss: 0.4581
Epoch 1 Step 1201 Train Loss: 0.4552
Epoch 1 Step 1251 Train Loss: 0.4883
Epoch 1 Step 1301 Train Loss: 0.4761
Epoch 1 Step 1351 Train Loss: 0.4497
Epoch 1: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0040. 
Train Top 20 DE MSE: 0.1165 Validation Top 20 DE MSE: 0.2032. 
Epoch 2 Step 1 Train Loss: 0.4076
Epoch 2 Step 51 Train Loss: 0.4910
Epoch 2 Step 101 Train Loss: 0.4832
Epoch 2 Step 151 Train Loss: 0.5821
Epoch 2 Step 201 Train Loss: 0.5477
Epoch 2 Step 251 Train Loss: 0.5184
Epoch 2 Step 301 Train Loss: 0.5135
Epoch 2 Step 351 Train Loss: 0.4777
Epoch 2 Step 401 Train Loss: 0.4910
Epoch 2 Step 451 Train Loss: 0.5309
Epoch 2 Step 501 Train Loss: 0.5356
Epoch 2 Step 551 Train Loss: 0.5808
Epoch 2 Step 601 Train Loss: 0.5375
Epoch 2 Step 651 Train Loss: 0.4906
Epoch 2 Step 701 Train Loss: 0.5081
Epoch 2 Step 751 Train Loss: 0.5515
Epoch 2 Step 801 Train Loss: 0.4154
Epoch 2 Step 851 Train Loss: 0.4631
Epoch 2 Step 901 Train Loss: 0.4802
Epoch 2 Step 951 Train Loss: 0.5053
Epoch 2 Step 1001 Train Loss: 0.4846
Epoch 2 Step 1051 Train Loss: 0.5336
Epoch 2 Step 1101 Train Loss: 0.5361
Epoch 2 Step 1151 Train Loss: 0.5158
Epoch 2 Step 1201 Train Loss: 0.4767
Epoch 2 Step 1251 Train Loss: 0.4982
Epoch 2 Step 1301 Train Loss: 0.5213
Epoch 2 Step 1351 Train Loss: 0.5372
Epoch 2: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.1280 Validation Top 20 DE MSE: 0.2741. 
Epoch 3 Step 1 Train Loss: 0.4788
Epoch 3 Step 51 Train Loss: 0.4944
Epoch 3 Step 101 Train Loss: 0.4593
Epoch 3 Step 151 Train Loss: 0.5575
Epoch 3 Step 201 Train Loss: 0.4950
Epoch 3 Step 251 Train Loss: 0.4661
Epoch 3 Step 301 Train Loss: 0.4866
Epoch 3 Step 351 Train Loss: 0.4980
Epoch 3 Step 401 Train Loss: 0.5452
Epoch 3 Step 451 Train Loss: 0.4939
Epoch 3 Step 501 Train Loss: 0.5324
Epoch 3 Step 551 Train Loss: 0.5634
Epoch 3 Step 601 Train Loss: 0.4741
Epoch 3 Step 651 Train Loss: 0.5538
Epoch 3 Step 701 Train Loss: 0.5359
Epoch 3 Step 751 Train Loss: 0.4687
Epoch 3 Step 801 Train Loss: 0.4269
Epoch 3 Step 851 Train Loss: 0.4506
Epoch 3 Step 901 Train Loss: 0.4934
Epoch 3 Step 951 Train Loss: 0.4419
Epoch 3 Step 1001 Train Loss: 0.5105
Epoch 3 Step 1051 Train Loss: 0.4666
Epoch 3 Step 1101 Train Loss: 0.4485
Epoch 3 Step 1151 Train Loss: 0.5174
Epoch 3 Step 1201 Train Loss: 0.5106
Epoch 3 Step 1251 Train Loss: 0.5450
Epoch 3 Step 1301 Train Loss: 0.4063
Epoch 3 Step 1351 Train Loss: 0.4764
Epoch 3: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.0985 Validation Top 20 DE MSE: 0.1995. 
Epoch 4 Step 1 Train Loss: 0.5028
Epoch 4 Step 51 Train Loss: 0.5436
Epoch 4 Step 101 Train Loss: 0.5240
Epoch 4 Step 151 Train Loss: 0.5070
Epoch 4 Step 201 Train Loss: 0.4456
Epoch 4 Step 251 Train Loss: 0.4880
Epoch 4 Step 301 Train Loss: 0.5475
Epoch 4 Step 351 Train Loss: 0.5264
Epoch 4 Step 401 Train Loss: 0.5283
Epoch 4 Step 451 Train Loss: 0.5652
Epoch 4 Step 501 Train Loss: 0.4894
Epoch 4 Step 551 Train Loss: 0.5189
Epoch 4 Step 601 Train Loss: 0.5343
Epoch 4 Step 651 Train Loss: 0.4819
Epoch 4 Step 701 Train Loss: 0.5308
Epoch 4 Step 751 Train Loss: 0.4668
Epoch 4 Step 801 Train Loss: 0.4379
Epoch 4 Step 851 Train Loss: 0.5472
Epoch 4 Step 901 Train Loss: 0.5831
Epoch 4 Step 951 Train Loss: 0.4917
Epoch 4 Step 1001 Train Loss: 0.5780
Epoch 4 Step 1051 Train Loss: 0.4444
Epoch 4 Step 1101 Train Loss: 0.5809
Epoch 4 Step 1151 Train Loss: 0.4897
Epoch 4 Step 1201 Train Loss: 0.5537
Epoch 4 Step 1251 Train Loss: 0.5626
Epoch 4 Step 1301 Train Loss: 0.6421
Epoch 4 Step 1351 Train Loss: 0.4708
Epoch 4: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0795 Validation Top 20 DE MSE: 0.1528. 
Epoch 5 Step 1 Train Loss: 0.4461
Epoch 5 Step 51 Train Loss: 0.4673
Epoch 5 Step 101 Train Loss: 0.5791
Epoch 5 Step 151 Train Loss: 0.5375
Epoch 5 Step 201 Train Loss: 0.4793
Epoch 5 Step 251 Train Loss: 0.4527
Epoch 5 Step 301 Train Loss: 0.4774
Epoch 5 Step 351 Train Loss: 0.4435
Epoch 5 Step 401 Train Loss: 0.4585
Epoch 5 Step 451 Train Loss: 0.4813
Epoch 5 Step 501 Train Loss: 0.4561
Epoch 5 Step 551 Train Loss: 0.6181
Epoch 5 Step 601 Train Loss: 0.4785
Epoch 5 Step 651 Train Loss: 0.5054
Epoch 5 Step 701 Train Loss: 0.5636
Epoch 5 Step 751 Train Loss: 0.5565
Epoch 5 Step 801 Train Loss: 0.4957
Epoch 5 Step 851 Train Loss: 0.5420
Epoch 5 Step 901 Train Loss: 0.5311
Epoch 5 Step 951 Train Loss: 0.5462
Epoch 5 Step 1001 Train Loss: 0.5262
Epoch 5 Step 1051 Train Loss: 0.5302
Epoch 5 Step 1101 Train Loss: 0.5190
Epoch 5 Step 1151 Train Loss: 0.4757
Epoch 5 Step 1201 Train Loss: 0.5535
Epoch 5 Step 1251 Train Loss: 0.5239
Epoch 5 Step 1301 Train Loss: 0.4864
Epoch 5 Step 1351 Train Loss: 0.4946
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0752 Validation Top 20 DE MSE: 0.1377. 
Epoch 6 Step 1 Train Loss: 0.4729
Epoch 6 Step 51 Train Loss: 0.5406
Epoch 6 Step 101 Train Loss: 0.5268
Epoch 6 Step 151 Train Loss: 0.4957
Epoch 6 Step 201 Train Loss: 0.5350
Epoch 6 Step 251 Train Loss: 0.5090
Epoch 6 Step 301 Train Loss: 0.5034
Epoch 6 Step 351 Train Loss: 0.4724
Epoch 6 Step 401 Train Loss: 0.5188
Epoch 6 Step 451 Train Loss: 0.4921
Epoch 6 Step 501 Train Loss: 0.5544
Epoch 6 Step 551 Train Loss: 0.4850
Epoch 6 Step 601 Train Loss: 0.4451
Epoch 6 Step 651 Train Loss: 0.4943
Epoch 6 Step 701 Train Loss: 0.5799
Epoch 6 Step 751 Train Loss: 0.5270
Epoch 6 Step 801 Train Loss: 0.5137
Epoch 6 Step 851 Train Loss: 0.5797
Epoch 6 Step 901 Train Loss: 0.5307
Epoch 6 Step 951 Train Loss: 0.4868
Epoch 6 Step 1001 Train Loss: 0.5218
Epoch 6 Step 1051 Train Loss: 0.4502
Epoch 6 Step 1101 Train Loss: 0.5188
Epoch 6 Step 1151 Train Loss: 0.4931
Epoch 6 Step 1201 Train Loss: 0.5281
Epoch 6 Step 1251 Train Loss: 0.5437
Epoch 6 Step 1301 Train Loss: 0.5258
Epoch 6 Step 1351 Train Loss: 0.5099
Epoch 6: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0797 Validation Top 20 DE MSE: 0.1610. 
Epoch 7 Step 1 Train Loss: 0.5663
Epoch 7 Step 51 Train Loss: 0.5007
Epoch 7 Step 101 Train Loss: 0.5395
Epoch 7 Step 151 Train Loss: 0.6054
Epoch 7 Step 201 Train Loss: 0.5503
Epoch 7 Step 251 Train Loss: 0.4698
Epoch 7 Step 301 Train Loss: 0.5326
Epoch 7 Step 351 Train Loss: 0.5099
Epoch 7 Step 401 Train Loss: 0.5203
Epoch 7 Step 451 Train Loss: 0.5220
Epoch 7 Step 501 Train Loss: 0.5526
Epoch 7 Step 551 Train Loss: 0.5356
Epoch 7 Step 601 Train Loss: 0.4857
Epoch 7 Step 651 Train Loss: 0.6092
Epoch 7 Step 701 Train Loss: 0.5233
Epoch 7 Step 751 Train Loss: 0.6036
Epoch 7 Step 801 Train Loss: 0.5344
Epoch 7 Step 851 Train Loss: 0.6553
Epoch 7 Step 901 Train Loss: 0.4680
Epoch 7 Step 951 Train Loss: 0.5388
Epoch 7 Step 1001 Train Loss: 0.5595
Epoch 7 Step 1051 Train Loss: 0.4663
Epoch 7 Step 1101 Train Loss: 0.5103
Epoch 7 Step 1151 Train Loss: 0.5604
Epoch 7 Step 1201 Train Loss: 0.5331
Epoch 7 Step 1251 Train Loss: 0.5102
Epoch 7 Step 1301 Train Loss: 0.4515
Epoch 7 Step 1351 Train Loss: 0.5038
Epoch 7: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0782 Validation Top 20 DE MSE: 0.1531. 
Epoch 8 Step 1 Train Loss: 0.4915
Epoch 8 Step 51 Train Loss: 0.4268
Epoch 8 Step 101 Train Loss: 0.5294
Epoch 8 Step 151 Train Loss: 0.5580
Epoch 8 Step 201 Train Loss: 0.4642
Epoch 8 Step 251 Train Loss: 0.5004
Epoch 8 Step 301 Train Loss: 0.5109
Epoch 8 Step 351 Train Loss: 0.5224
Epoch 8 Step 401 Train Loss: 0.6291
Epoch 8 Step 451 Train Loss: 0.5299
Epoch 8 Step 501 Train Loss: 0.4739
Epoch 8 Step 551 Train Loss: 0.5375
Epoch 8 Step 601 Train Loss: 0.4915
Epoch 8 Step 651 Train Loss: 0.4632
Epoch 8 Step 701 Train Loss: 0.4980
Epoch 8 Step 751 Train Loss: 0.5120
Epoch 8 Step 801 Train Loss: 0.5058
Epoch 8 Step 851 Train Loss: 0.4587
Epoch 8 Step 901 Train Loss: 0.5614
Epoch 8 Step 951 Train Loss: 0.5566
Epoch 8 Step 1001 Train Loss: 0.5165
Epoch 8 Step 1051 Train Loss: 0.4501
Epoch 8 Step 1101 Train Loss: 0.5142
Epoch 8 Step 1151 Train Loss: 0.5125
Epoch 8 Step 1201 Train Loss: 0.4949
Epoch 8 Step 1251 Train Loss: 0.4791
Epoch 8 Step 1301 Train Loss: 0.5055
Epoch 8 Step 1351 Train Loss: 0.4834
Epoch 8: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0757 Validation Top 20 DE MSE: 0.1469. 
Epoch 9 Step 1 Train Loss: 0.4889
Epoch 9 Step 51 Train Loss: 0.5162
Epoch 9 Step 101 Train Loss: 0.4446
Epoch 9 Step 151 Train Loss: 0.4480
Epoch 9 Step 201 Train Loss: 0.5186
Epoch 9 Step 251 Train Loss: 0.5625
Epoch 9 Step 301 Train Loss: 0.5645
Epoch 9 Step 351 Train Loss: 0.4954
Epoch 9 Step 401 Train Loss: 0.4863
Epoch 9 Step 451 Train Loss: 0.4860
Epoch 9 Step 501 Train Loss: 0.5138
Epoch 9 Step 551 Train Loss: 0.4771
Epoch 9 Step 601 Train Loss: 0.5681
Epoch 9 Step 651 Train Loss: 0.6058
Epoch 9 Step 701 Train Loss: 0.5011
Epoch 9 Step 751 Train Loss: 0.5994
Epoch 9 Step 801 Train Loss: 0.5556
Epoch 9 Step 851 Train Loss: 0.5521
Epoch 9 Step 901 Train Loss: 0.5411
Epoch 9 Step 951 Train Loss: 0.5370
Epoch 9 Step 1001 Train Loss: 0.5794
Epoch 9 Step 1051 Train Loss: 0.4963
Epoch 9 Step 1101 Train Loss: 0.4675
Epoch 9 Step 1151 Train Loss: 0.4805
Epoch 9 Step 1201 Train Loss: 0.5028
Epoch 9 Step 1251 Train Loss: 0.5758
Epoch 9 Step 1301 Train Loss: 0.5449
Epoch 9 Step 1351 Train Loss: 0.5811
Epoch 9: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0750 Validation Top 20 DE MSE: 0.1476. 
Epoch 10 Step 1 Train Loss: 0.5176
Epoch 10 Step 51 Train Loss: 0.4848
Epoch 10 Step 101 Train Loss: 0.5769
Epoch 10 Step 151 Train Loss: 0.5071
Epoch 10 Step 201 Train Loss: 0.4938
Epoch 10 Step 251 Train Loss: 0.5576
Epoch 10 Step 301 Train Loss: 0.6091
Epoch 10 Step 351 Train Loss: 0.5124
Epoch 10 Step 401 Train Loss: 0.5484
Epoch 10 Step 451 Train Loss: 0.5079
Epoch 10 Step 501 Train Loss: 0.4916
Epoch 10 Step 551 Train Loss: 0.5394
Epoch 10 Step 601 Train Loss: 0.5753
Epoch 10 Step 651 Train Loss: 0.4897
Epoch 10 Step 701 Train Loss: 0.4856
Epoch 10 Step 751 Train Loss: 0.6067
Epoch 10 Step 801 Train Loss: 0.4429
Epoch 10 Step 851 Train Loss: 0.5623
Epoch 10 Step 901 Train Loss: 0.4986
Epoch 10 Step 951 Train Loss: 0.5615
Epoch 10 Step 1001 Train Loss: 0.4880
Epoch 10 Step 1051 Train Loss: 0.5277
Epoch 10 Step 1101 Train Loss: 0.5652
Epoch 10 Step 1151 Train Loss: 0.5512
Epoch 10 Step 1201 Train Loss: 0.4284
Epoch 10 Step 1251 Train Loss: 0.5388
Epoch 10 Step 1301 Train Loss: 0.5284
Epoch 10 Step 1351 Train Loss: 0.5092
Epoch 10: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0714 Validation Top 20 DE MSE: 0.1362. 
Epoch 11 Step 1 Train Loss: 0.5097
Epoch 11 Step 51 Train Loss: 0.5189
Epoch 11 Step 101 Train Loss: 0.5589
Epoch 11 Step 151 Train Loss: 0.5596
Epoch 11 Step 201 Train Loss: 0.5432
Epoch 11 Step 251 Train Loss: 0.6213
Epoch 11 Step 301 Train Loss: 0.4934
Epoch 11 Step 351 Train Loss: 0.4812
Epoch 11 Step 401 Train Loss: 0.5369
Epoch 11 Step 451 Train Loss: 0.5438
Epoch 11 Step 501 Train Loss: 0.5320
Epoch 11 Step 551 Train Loss: 0.5081
Epoch 11 Step 601 Train Loss: 0.5532
Epoch 11 Step 651 Train Loss: 0.5251
Epoch 11 Step 701 Train Loss: 0.5252
Epoch 11 Step 751 Train Loss: 0.5725
Epoch 11 Step 801 Train Loss: 0.5498
Epoch 11 Step 851 Train Loss: 0.5079
Epoch 11 Step 901 Train Loss: 0.5201
Epoch 11 Step 951 Train Loss: 0.7263
Epoch 11 Step 1001 Train Loss: 0.5208
Epoch 11 Step 1051 Train Loss: 0.5435
Epoch 11 Step 1101 Train Loss: 0.5359
Epoch 11 Step 1151 Train Loss: 0.5257
Epoch 11 Step 1201 Train Loss: 0.6068
Epoch 11 Step 1251 Train Loss: 0.5054
Epoch 11 Step 1301 Train Loss: 0.5334
Epoch 11 Step 1351 Train Loss: 0.5666
Epoch 11: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0755 Validation Top 20 DE MSE: 0.1436. 
Epoch 12 Step 1 Train Loss: 0.5373
Epoch 12 Step 51 Train Loss: 0.5247
Epoch 12 Step 101 Train Loss: 0.6157
Epoch 12 Step 151 Train Loss: 0.4998
Epoch 12 Step 201 Train Loss: 0.5063
Epoch 12 Step 251 Train Loss: 0.5049
Epoch 12 Step 301 Train Loss: 0.5240
Epoch 12 Step 351 Train Loss: 0.5213
Epoch 12 Step 401 Train Loss: 0.5248
Epoch 12 Step 451 Train Loss: 0.5287
Epoch 12 Step 501 Train Loss: 0.5442
Epoch 12 Step 551 Train Loss: 0.5040
Epoch 12 Step 601 Train Loss: 0.6090
Epoch 12 Step 651 Train Loss: 0.5694
Epoch 12 Step 701 Train Loss: 0.4957
Epoch 12 Step 751 Train Loss: 0.5675
Epoch 12 Step 801 Train Loss: 0.5183
Epoch 12 Step 851 Train Loss: 0.5686
Epoch 12 Step 901 Train Loss: 0.4947
Epoch 12 Step 951 Train Loss: 0.4847
Epoch 12 Step 1001 Train Loss: 0.4100
Epoch 12 Step 1051 Train Loss: 0.5327
Epoch 12 Step 1101 Train Loss: 0.4837
Epoch 12 Step 1151 Train Loss: 0.4748
Epoch 12 Step 1201 Train Loss: 0.5354
Epoch 12 Step 1251 Train Loss: 0.5303
Epoch 12 Step 1301 Train Loss: 0.4173
Epoch 12 Step 1351 Train Loss: 0.5488
Epoch 12: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0718 Validation Top 20 DE MSE: 0.1437. 
Epoch 13 Step 1 Train Loss: 0.5416
Epoch 13 Step 51 Train Loss: 0.4662
Epoch 13 Step 101 Train Loss: 0.4760
Epoch 13 Step 151 Train Loss: 0.5085
Epoch 13 Step 201 Train Loss: 0.4696
Epoch 13 Step 251 Train Loss: 0.4997
Epoch 13 Step 301 Train Loss: 0.5720
Epoch 13 Step 351 Train Loss: 0.4527
Epoch 13 Step 401 Train Loss: 0.5506
Epoch 13 Step 451 Train Loss: 0.4254
Epoch 13 Step 501 Train Loss: 0.4521
Epoch 13 Step 551 Train Loss: 0.4648
Epoch 13 Step 601 Train Loss: 0.4514
Epoch 13 Step 651 Train Loss: 0.5133
Epoch 13 Step 701 Train Loss: 0.5512
Epoch 13 Step 751 Train Loss: 0.6105
Epoch 13 Step 801 Train Loss: 0.5731
Epoch 13 Step 851 Train Loss: 0.5636
Epoch 13 Step 901 Train Loss: 0.4233
Epoch 13 Step 951 Train Loss: 0.6160
Epoch 13 Step 1001 Train Loss: 0.4577
Epoch 13 Step 1051 Train Loss: 0.5409
Epoch 13 Step 1101 Train Loss: 0.4873
Epoch 13 Step 1151 Train Loss: 0.4987
Epoch 13 Step 1201 Train Loss: 0.5287
Epoch 13 Step 1251 Train Loss: 0.4869
Epoch 13 Step 1301 Train Loss: 0.5454
Epoch 13 Step 1351 Train Loss: 0.4641
Epoch 13: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0742 Validation Top 20 DE MSE: 0.1497. 
Epoch 14 Step 1 Train Loss: 0.5305
Epoch 14 Step 51 Train Loss: 0.6114
Epoch 14 Step 101 Train Loss: 0.6454
Epoch 14 Step 151 Train Loss: 0.5011
Epoch 14 Step 201 Train Loss: 0.4626
Epoch 14 Step 251 Train Loss: 0.5477
Epoch 14 Step 301 Train Loss: 0.5157
Epoch 14 Step 351 Train Loss: 0.5752
Epoch 14 Step 401 Train Loss: 0.4935
Epoch 14 Step 451 Train Loss: 0.5140
Epoch 14 Step 501 Train Loss: 0.5977
Epoch 14 Step 551 Train Loss: 0.4946
Epoch 14 Step 601 Train Loss: 0.5044
Epoch 14 Step 651 Train Loss: 0.4587
Epoch 14 Step 701 Train Loss: 0.4884
Epoch 14 Step 751 Train Loss: 0.4544
Epoch 14 Step 801 Train Loss: 0.5307
Epoch 14 Step 851 Train Loss: 0.5233
Epoch 14 Step 901 Train Loss: 0.7312
Epoch 14 Step 951 Train Loss: 0.5274
Epoch 14 Step 1001 Train Loss: 0.5129
Epoch 14 Step 1051 Train Loss: 0.4912
Epoch 14 Step 1101 Train Loss: 0.5155
Epoch 14 Step 1151 Train Loss: 0.5576
Epoch 14 Step 1201 Train Loss: 0.5283
Epoch 14 Step 1251 Train Loss: 0.5391
Epoch 14 Step 1301 Train Loss: 0.4923
Epoch 14 Step 1351 Train Loss: 0.5432
Epoch 14: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0724 Validation Top 20 DE MSE: 0.1425. 
Epoch 15 Step 1 Train Loss: 0.4852
Epoch 15 Step 51 Train Loss: 0.6918
Epoch 15 Step 101 Train Loss: 0.4645
Epoch 15 Step 151 Train Loss: 0.5077
Epoch 15 Step 201 Train Loss: 0.5460
Epoch 15 Step 251 Train Loss: 0.5312
Epoch 15 Step 301 Train Loss: 0.5798
Epoch 15 Step 351 Train Loss: 0.4735
Epoch 15 Step 401 Train Loss: 0.4561
Epoch 15 Step 451 Train Loss: 0.4869
Epoch 15 Step 501 Train Loss: 0.5481
Epoch 15 Step 551 Train Loss: 0.4465
Epoch 15 Step 601 Train Loss: 0.5002
Epoch 15 Step 651 Train Loss: 0.5789
Epoch 15 Step 701 Train Loss: 0.4842
Epoch 15 Step 751 Train Loss: 0.4811
Epoch 15 Step 801 Train Loss: 0.5571
Epoch 15 Step 851 Train Loss: 0.5488
Epoch 15 Step 901 Train Loss: 0.5300
Epoch 15 Step 951 Train Loss: 0.5610
Epoch 15 Step 1001 Train Loss: 0.5734
Epoch 15 Step 1051 Train Loss: 0.4383
Epoch 15 Step 1101 Train Loss: 0.4901
Epoch 15 Step 1151 Train Loss: 0.5482
Epoch 15 Step 1201 Train Loss: 0.4843
Epoch 15 Step 1251 Train Loss: 0.6204
Epoch 15 Step 1301 Train Loss: 0.4974
Epoch 15 Step 1351 Train Loss: 0.5175
Epoch 15: Train Overall MSE: 0.0015 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0748 Validation Top 20 DE MSE: 0.1456. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0984
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0027200875
test_unseen_single_pearson: 0.9949033818761758
test_unseen_single_mse_de: 0.09839296
test_unseen_single_pearson_de: 0.8017906857521208
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.31658922311683235
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25227272727272726
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8999999999999997
test_unseen_single_mse_top20_de_non_dropout: 0.10834191
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.033 MB uploadedwandb: | 0.011 MB of 0.033 MB uploadedwandb: / 0.029 MB of 0.033 MB uploadedwandb: - 0.033 MB of 0.033 MB uploadedwandb: \ 0.033 MB of 0.033 MB uploadedwandb: | 0.033 MB of 0.033 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÉ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñá‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÇ
wandb:                                                   val_de_mse ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                               val_de_pearson ‚ñÖ‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:                                                      val_mse ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:                                                  val_pearson ‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.09839
wandb:                                              test_de_pearson 0.80179
wandb:               test_frac_opposite_direction_top20_non_dropout 0.25227
wandb:                          test_frac_sigma_below_1_non_dropout 0.9
wandb:                                                     test_mse 0.00272
wandb:                                test_mse_top20_de_non_dropout 0.10834
wandb:                                                 test_pearson 0.9949
wandb:                                           test_pearson_delta 0.31659
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25227
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9
wandb:                                       test_unseen_single_mse 0.00272
wandb:                                    test_unseen_single_mse_de 0.09839
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.10834
wandb:                                   test_unseen_single_pearson 0.9949
wandb:                                test_unseen_single_pearson_de 0.80179
wandb:                             test_unseen_single_pearson_delta 0.31659
wandb:                                                 train_de_mse 0.07479
wandb:                                             train_de_pearson 0.93874
wandb:                                                    train_mse 0.00152
wandb:                                                train_pearson 0.99713
wandb:                                                training_loss 0.57975
wandb:                                                   val_de_mse 0.14561
wandb:                                               val_de_pearson 0.96692
wandb:                                                      val_mse 0.00298
wandb:                                                  val_pearson 0.99423
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406681_3_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/zy8b6gud
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_225147-zy8b6gud/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:22
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_232738-bwwhka7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406681_3_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/bwwhka7l
wandb: WARNING Serializing object of type ndarray that is 20738176 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5798
Epoch 1 Step 51 Train Loss: 0.5561
Epoch 1 Step 101 Train Loss: 0.4973
Epoch 1 Step 151 Train Loss: 0.5042
Epoch 1 Step 201 Train Loss: 0.4487
Epoch 1 Step 251 Train Loss: 0.6015
Epoch 1 Step 301 Train Loss: 0.5698
Epoch 1 Step 351 Train Loss: 0.5701
Epoch 1 Step 401 Train Loss: 0.4722
Epoch 1 Step 451 Train Loss: 0.5212
Epoch 1 Step 501 Train Loss: 0.4925
Epoch 1 Step 551 Train Loss: 0.5252
Epoch 1 Step 601 Train Loss: 0.4916
Epoch 1 Step 651 Train Loss: 0.5047
Epoch 1 Step 701 Train Loss: 0.5824
Epoch 1 Step 751 Train Loss: 0.5626
Epoch 1 Step 801 Train Loss: 0.5556
Epoch 1 Step 851 Train Loss: 0.5122
Epoch 1 Step 901 Train Loss: 0.5241
Epoch 1 Step 951 Train Loss: 0.5142
Epoch 1 Step 1001 Train Loss: 0.4905
Epoch 1 Step 1051 Train Loss: 0.4942
Epoch 1 Step 1101 Train Loss: 0.4371
Epoch 1 Step 1151 Train Loss: 0.4991
Epoch 1 Step 1201 Train Loss: 0.5185
Epoch 1 Step 1251 Train Loss: 0.5133
Epoch 1 Step 1301 Train Loss: 0.5655
Epoch 1: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0842 Validation Top 20 DE MSE: 0.1718. 
Epoch 2 Step 1 Train Loss: 0.5739
Epoch 2 Step 51 Train Loss: 0.5513
Epoch 2 Step 101 Train Loss: 0.5110
Epoch 2 Step 151 Train Loss: 0.5157
Epoch 2 Step 201 Train Loss: 0.4821
Epoch 2 Step 251 Train Loss: 0.4943
Epoch 2 Step 301 Train Loss: 0.5292
Epoch 2 Step 351 Train Loss: 0.4629
Epoch 2 Step 401 Train Loss: 0.4172
Epoch 2 Step 451 Train Loss: 0.6503
Epoch 2 Step 501 Train Loss: 0.4495
Epoch 2 Step 551 Train Loss: 0.5067
Epoch 2 Step 601 Train Loss: 0.4946
Epoch 2 Step 651 Train Loss: 0.4938
Epoch 2 Step 701 Train Loss: 0.5438
Epoch 2 Step 751 Train Loss: 0.5658
Epoch 2 Step 801 Train Loss: 0.5605
Epoch 2 Step 851 Train Loss: 0.4517
Epoch 2 Step 901 Train Loss: 0.5286
Epoch 2 Step 951 Train Loss: 0.4952
Epoch 2 Step 1001 Train Loss: 0.5493
Epoch 2 Step 1051 Train Loss: 0.5381
Epoch 2 Step 1101 Train Loss: 0.4990
Epoch 2 Step 1151 Train Loss: 0.6593
Epoch 2 Step 1201 Train Loss: 0.5460
Epoch 2 Step 1251 Train Loss: 0.4908
Epoch 2 Step 1301 Train Loss: 0.4507
Epoch 2: Train Overall MSE: 0.0042 Validation Overall MSE: 0.0043. 
Train Top 20 DE MSE: 0.0837 Validation Top 20 DE MSE: 0.1590. 
Epoch 3 Step 1 Train Loss: 0.5319
Epoch 3 Step 51 Train Loss: 0.4473
Epoch 3 Step 101 Train Loss: 0.5240
Epoch 3 Step 151 Train Loss: 0.5001
Epoch 3 Step 201 Train Loss: 0.5132
Epoch 3 Step 251 Train Loss: 0.4937
Epoch 3 Step 301 Train Loss: 0.5397
Epoch 3 Step 351 Train Loss: 0.5583
Epoch 3 Step 401 Train Loss: 0.5717
Epoch 3 Step 451 Train Loss: 0.5815
Epoch 3 Step 501 Train Loss: 0.5264
Epoch 3 Step 551 Train Loss: 0.4848
Epoch 3 Step 601 Train Loss: 0.5228
Epoch 3 Step 651 Train Loss: 0.5804
Epoch 3 Step 701 Train Loss: 0.4744
Epoch 3 Step 751 Train Loss: 0.4768
Epoch 3 Step 801 Train Loss: 0.5908
Epoch 3 Step 851 Train Loss: 0.4848
Epoch 3 Step 901 Train Loss: 0.4643
Epoch 3 Step 951 Train Loss: 0.4516
Epoch 3 Step 1001 Train Loss: 0.5009
Epoch 3 Step 1051 Train Loss: 0.5060
Epoch 3 Step 1101 Train Loss: 0.5495
Epoch 3 Step 1151 Train Loss: 0.4871
Epoch 3 Step 1201 Train Loss: 0.5145
Epoch 3 Step 1251 Train Loss: 0.5149
Epoch 3 Step 1301 Train Loss: 0.5452
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0751 Validation Top 20 DE MSE: 0.1591. 
Epoch 4 Step 1 Train Loss: 0.5241
Epoch 4 Step 51 Train Loss: 0.5257
Epoch 4 Step 101 Train Loss: 0.4987
Epoch 4 Step 151 Train Loss: 0.5568
Epoch 4 Step 201 Train Loss: 0.4470
Epoch 4 Step 251 Train Loss: 0.4898
Epoch 4 Step 301 Train Loss: 0.4115
Epoch 4 Step 351 Train Loss: 0.5564
Epoch 4 Step 401 Train Loss: 0.4771
Epoch 4 Step 451 Train Loss: 0.5611
Epoch 4 Step 501 Train Loss: 0.4836
Epoch 4 Step 551 Train Loss: 0.5196
Epoch 4 Step 601 Train Loss: 0.4909
Epoch 4 Step 651 Train Loss: 0.4619
Epoch 4 Step 701 Train Loss: 0.4866
Epoch 4 Step 751 Train Loss: 0.5332
Epoch 4 Step 801 Train Loss: 0.4750
Epoch 4 Step 851 Train Loss: 0.5162
Epoch 4 Step 901 Train Loss: 0.5004
Epoch 4 Step 951 Train Loss: 0.5029
Epoch 4 Step 1001 Train Loss: 0.5139
Epoch 4 Step 1051 Train Loss: 0.4857
Epoch 4 Step 1101 Train Loss: 0.4646
Epoch 4 Step 1151 Train Loss: 0.5737
Epoch 4 Step 1201 Train Loss: 0.4978
Epoch 4 Step 1251 Train Loss: 0.4483
Epoch 4 Step 1301 Train Loss: 0.4673
Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0661 Validation Top 20 DE MSE: 0.1561. 
Epoch 5 Step 1 Train Loss: 0.4289
Epoch 5 Step 51 Train Loss: 0.5833
Epoch 5 Step 101 Train Loss: 0.6016
Epoch 5 Step 151 Train Loss: 0.4326
Epoch 5 Step 201 Train Loss: 0.4728
Epoch 5 Step 251 Train Loss: 0.5633
Epoch 5 Step 301 Train Loss: 0.4862
Epoch 5 Step 351 Train Loss: 0.6269
Epoch 5 Step 401 Train Loss: 0.4911
Epoch 5 Step 451 Train Loss: 0.5020
Epoch 5 Step 501 Train Loss: 0.5120
Epoch 5 Step 551 Train Loss: 0.4528
Epoch 5 Step 601 Train Loss: 0.5123
Epoch 5 Step 651 Train Loss: 0.4808
Epoch 5 Step 701 Train Loss: 0.6375
Epoch 5 Step 751 Train Loss: 0.6894
Epoch 5 Step 801 Train Loss: 0.4924
Epoch 5 Step 851 Train Loss: 0.5119
Epoch 5 Step 901 Train Loss: 0.5627
Epoch 5 Step 951 Train Loss: 0.4431
Epoch 5 Step 1001 Train Loss: 0.5443
Epoch 5 Step 1051 Train Loss: 0.5590
Epoch 5 Step 1101 Train Loss: 0.5197
Epoch 5 Step 1151 Train Loss: 0.5397
Epoch 5 Step 1201 Train Loss: 0.4512
Epoch 5 Step 1251 Train Loss: 0.5388
Epoch 5 Step 1301 Train Loss: 0.5231
Epoch 5: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0022. 
Train Top 20 DE MSE: 0.0660 Validation Top 20 DE MSE: 0.1564. 
Epoch 6 Step 1 Train Loss: 0.5167
Epoch 6 Step 51 Train Loss: 0.5358
Epoch 6 Step 101 Train Loss: 0.5508
Epoch 6 Step 151 Train Loss: 0.5036
Epoch 6 Step 201 Train Loss: 0.4639
Epoch 6 Step 251 Train Loss: 0.5786
Epoch 6 Step 301 Train Loss: 0.5260
Epoch 6 Step 351 Train Loss: 0.5731
Epoch 6 Step 401 Train Loss: 0.4505
Epoch 6 Step 451 Train Loss: 0.4816
Epoch 6 Step 501 Train Loss: 0.5389
Epoch 6 Step 551 Train Loss: 0.4664
Epoch 6 Step 601 Train Loss: 0.4763
Epoch 6 Step 651 Train Loss: 0.5139
Epoch 6 Step 701 Train Loss: 0.4931
Epoch 6 Step 751 Train Loss: 0.5592
Epoch 6 Step 801 Train Loss: 0.4896
Epoch 6 Step 851 Train Loss: 0.4486
Epoch 6 Step 901 Train Loss: 0.4896
Epoch 6 Step 951 Train Loss: 0.4864
Epoch 6 Step 1001 Train Loss: 0.5319
Epoch 6 Step 1051 Train Loss: 0.4875
Epoch 6 Step 1101 Train Loss: 0.5051
Epoch 6 Step 1151 Train Loss: 0.6084
Epoch 6 Step 1201 Train Loss: 0.4893
Epoch 6 Step 1251 Train Loss: 0.6200
Epoch 6 Step 1301 Train Loss: 0.5981
Epoch 6: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0643 Validation Top 20 DE MSE: 0.1579. 
Epoch 7 Step 1 Train Loss: 0.4427
Epoch 7 Step 51 Train Loss: 0.5148
Epoch 7 Step 101 Train Loss: 0.4735
Epoch 7 Step 151 Train Loss: 0.5493
Epoch 7 Step 201 Train Loss: 0.4972
Epoch 7 Step 251 Train Loss: 0.5013
Epoch 7 Step 301 Train Loss: 0.5289
Epoch 7 Step 351 Train Loss: 0.5308
Epoch 7 Step 401 Train Loss: 0.4915
Epoch 7 Step 451 Train Loss: 0.5237
Epoch 7 Step 501 Train Loss: 0.6605
Epoch 7 Step 551 Train Loss: 0.5992
Epoch 7 Step 601 Train Loss: 0.5160
Epoch 7 Step 651 Train Loss: 0.5108
Epoch 7 Step 701 Train Loss: 0.5327
Epoch 7 Step 751 Train Loss: 0.4857
Epoch 7 Step 801 Train Loss: 0.6227
Epoch 7 Step 851 Train Loss: 0.4945
Epoch 7 Step 901 Train Loss: 0.4352
Epoch 7 Step 951 Train Loss: 0.4599
Epoch 7 Step 1001 Train Loss: 0.6480
Epoch 7 Step 1051 Train Loss: 0.5032
Epoch 7 Step 1101 Train Loss: 0.5372
Epoch 7 Step 1151 Train Loss: 0.5293
Epoch 7 Step 1201 Train Loss: 0.4834
Epoch 7 Step 1251 Train Loss: 0.5981
Epoch 7 Step 1301 Train Loss: 0.5581
Epoch 7: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0668 Validation Top 20 DE MSE: 0.1586. 
Epoch 8 Step 1 Train Loss: 0.5328
Epoch 8 Step 51 Train Loss: 0.4753
Epoch 8 Step 101 Train Loss: 0.5162
Epoch 8 Step 151 Train Loss: 0.4579
Epoch 8 Step 201 Train Loss: 0.5801
Epoch 8 Step 251 Train Loss: 0.5680
Epoch 8 Step 301 Train Loss: 0.5452
Epoch 8 Step 351 Train Loss: 0.5080
Epoch 8 Step 401 Train Loss: 0.4498
Epoch 8 Step 451 Train Loss: 0.4948
Epoch 8 Step 501 Train Loss: 0.5364
Epoch 8 Step 551 Train Loss: 0.4713
Epoch 8 Step 601 Train Loss: 0.4987
Epoch 8 Step 651 Train Loss: 0.5219
Epoch 8 Step 701 Train Loss: 0.5623
Epoch 8 Step 751 Train Loss: 0.5270
Epoch 8 Step 801 Train Loss: 0.4760
Epoch 8 Step 851 Train Loss: 0.4859
Epoch 8 Step 901 Train Loss: 0.4748
Epoch 8 Step 951 Train Loss: 0.4690
Epoch 8 Step 1001 Train Loss: 0.5577
Epoch 8 Step 1051 Train Loss: 0.5571
Epoch 8 Step 1101 Train Loss: 0.5214
Epoch 8 Step 1151 Train Loss: 0.5450
Epoch 8 Step 1201 Train Loss: 0.5190
Epoch 8 Step 1251 Train Loss: 0.6015
Epoch 8 Step 1301 Train Loss: 0.6012
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0671 Validation Top 20 DE MSE: 0.1563. 
Epoch 9 Step 1 Train Loss: 0.5229
Epoch 9 Step 51 Train Loss: 0.5344
Epoch 9 Step 101 Train Loss: 0.5083
Epoch 9 Step 151 Train Loss: 0.5132
Epoch 9 Step 201 Train Loss: 0.4961
Epoch 9 Step 251 Train Loss: 0.5042
Epoch 9 Step 301 Train Loss: 0.5044
Epoch 9 Step 351 Train Loss: 0.5807
Epoch 9 Step 401 Train Loss: 0.5383
Epoch 9 Step 451 Train Loss: 0.5020
Epoch 9 Step 501 Train Loss: 0.5073
Epoch 9 Step 551 Train Loss: 0.5271
Epoch 9 Step 601 Train Loss: 0.5550
Epoch 9 Step 651 Train Loss: 0.4983
Epoch 9 Step 701 Train Loss: 0.4825
Epoch 9 Step 751 Train Loss: 0.5721
Epoch 9 Step 801 Train Loss: 0.5527
Epoch 9 Step 851 Train Loss: 0.4804
Epoch 9 Step 901 Train Loss: 0.5500
Epoch 9 Step 951 Train Loss: 0.5031
Epoch 9 Step 1001 Train Loss: 0.5209
Epoch 9 Step 1051 Train Loss: 0.4775
Epoch 9 Step 1101 Train Loss: 0.5533
Epoch 9 Step 1151 Train Loss: 0.4752
Epoch 9 Step 1201 Train Loss: 0.4588
Epoch 9 Step 1251 Train Loss: 0.5867
Epoch 9 Step 1301 Train Loss: 0.4941
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0658 Validation Top 20 DE MSE: 0.1538. 
Epoch 10 Step 1 Train Loss: 0.5030
Epoch 10 Step 51 Train Loss: 0.4837
Epoch 10 Step 101 Train Loss: 0.5080
Epoch 10 Step 151 Train Loss: 0.5375
Epoch 10 Step 201 Train Loss: 0.5103
Epoch 10 Step 251 Train Loss: 0.5377
Epoch 10 Step 301 Train Loss: 0.5589
Epoch 10 Step 351 Train Loss: 0.5256
Epoch 10 Step 401 Train Loss: 0.5326
Epoch 10 Step 451 Train Loss: 0.5048
Epoch 10 Step 501 Train Loss: 0.4579
Epoch 10 Step 551 Train Loss: 0.4649
Epoch 10 Step 601 Train Loss: 0.5183
Epoch 10 Step 651 Train Loss: 0.5809
Epoch 10 Step 701 Train Loss: 0.5149
Epoch 10 Step 751 Train Loss: 0.4883
Epoch 10 Step 801 Train Loss: 0.4837
Epoch 10 Step 851 Train Loss: 0.5763
Epoch 10 Step 901 Train Loss: 0.5601
Epoch 10 Step 951 Train Loss: 0.5140
Epoch 10 Step 1001 Train Loss: 0.5208
Epoch 10 Step 1051 Train Loss: 0.5325
Epoch 10 Step 1101 Train Loss: 0.4444
Epoch 10 Step 1151 Train Loss: 0.5301
Epoch 10 Step 1201 Train Loss: 0.5501
Epoch 10 Step 1251 Train Loss: 0.5037
Epoch 10 Step 1301 Train Loss: 0.4494
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0640 Validation Top 20 DE MSE: 0.1530. 
Epoch 11 Step 1 Train Loss: 0.5556
Epoch 11 Step 51 Train Loss: 0.4824
Epoch 11 Step 101 Train Loss: 0.4584
Epoch 11 Step 151 Train Loss: 0.5385
Epoch 11 Step 201 Train Loss: 0.6098
Epoch 11 Step 251 Train Loss: 0.5000
Epoch 11 Step 301 Train Loss: 0.4982
Epoch 11 Step 351 Train Loss: 0.5298
Epoch 11 Step 401 Train Loss: 0.5189
Epoch 11 Step 451 Train Loss: 0.5107
Epoch 11 Step 501 Train Loss: 0.5721
Epoch 11 Step 551 Train Loss: 0.5656
Epoch 11 Step 601 Train Loss: 0.5558
Epoch 11 Step 651 Train Loss: 0.5350
Epoch 11 Step 701 Train Loss: 0.5866
Epoch 11 Step 751 Train Loss: 0.6779
Epoch 11 Step 801 Train Loss: 0.5432
Epoch 11 Step 851 Train Loss: 0.5051
Epoch 11 Step 901 Train Loss: 0.6099
Epoch 11 Step 951 Train Loss: 0.5481
Epoch 11 Step 1001 Train Loss: 0.5414
Epoch 11 Step 1051 Train Loss: 0.5082
Epoch 11 Step 1101 Train Loss: 0.4945
Epoch 11 Step 1151 Train Loss: 0.5181
Epoch 11 Step 1201 Train Loss: 0.5216
Epoch 11 Step 1251 Train Loss: 0.4541
Epoch 11 Step 1301 Train Loss: 0.5209
Epoch 11: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0666 Validation Top 20 DE MSE: 0.1556. 
Epoch 12 Step 1 Train Loss: 0.6241
Epoch 12 Step 51 Train Loss: 0.4858
Epoch 12 Step 101 Train Loss: 0.5782
Epoch 12 Step 151 Train Loss: 0.5802
Epoch 12 Step 201 Train Loss: 0.4636
Epoch 12 Step 251 Train Loss: 0.5179
Epoch 12 Step 301 Train Loss: 0.4708
Epoch 12 Step 351 Train Loss: 0.5768
Epoch 12 Step 401 Train Loss: 0.5576
Epoch 12 Step 451 Train Loss: 0.4643
Epoch 12 Step 501 Train Loss: 0.5081
Epoch 12 Step 551 Train Loss: 0.5433
Epoch 12 Step 601 Train Loss: 0.5044
Epoch 12 Step 651 Train Loss: 0.5653
Epoch 12 Step 701 Train Loss: 0.5175
Epoch 12 Step 751 Train Loss: 0.5219
Epoch 12 Step 801 Train Loss: 0.5276
Epoch 12 Step 851 Train Loss: 0.5718
Epoch 12 Step 901 Train Loss: 0.4846
Epoch 12 Step 951 Train Loss: 0.5833
Epoch 12 Step 1001 Train Loss: 0.4932
Epoch 12 Step 1051 Train Loss: 0.5073
Epoch 12 Step 1101 Train Loss: 0.5245
Epoch 12 Step 1151 Train Loss: 0.5346
Epoch 12 Step 1201 Train Loss: 0.5592
Epoch 12 Step 1251 Train Loss: 0.5522
Epoch 12 Step 1301 Train Loss: 0.5663
Epoch 12: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0648 Validation Top 20 DE MSE: 0.1539. 
Epoch 13 Step 1 Train Loss: 0.6240
Epoch 13 Step 51 Train Loss: 0.5123
Epoch 13 Step 101 Train Loss: 0.5122
Epoch 13 Step 151 Train Loss: 0.5150
Epoch 13 Step 201 Train Loss: 0.5347
Epoch 13 Step 251 Train Loss: 0.5679
Epoch 13 Step 301 Train Loss: 0.5087
Epoch 13 Step 351 Train Loss: 0.5248
Epoch 13 Step 401 Train Loss: 0.5554
Epoch 13 Step 451 Train Loss: 0.4945
Epoch 13 Step 501 Train Loss: 0.5479
Epoch 13 Step 551 Train Loss: 0.4496
Epoch 13 Step 601 Train Loss: 0.4651
Epoch 13 Step 651 Train Loss: 0.5171
Epoch 13 Step 701 Train Loss: 0.4917
Epoch 13 Step 751 Train Loss: 0.4784
Epoch 13 Step 801 Train Loss: 0.5594
Epoch 13 Step 851 Train Loss: 0.5531
Epoch 13 Step 901 Train Loss: 0.5614
Epoch 13 Step 951 Train Loss: 0.5590
Epoch 13 Step 1001 Train Loss: 0.5323
Epoch 13 Step 1051 Train Loss: 0.5172
Epoch 13 Step 1101 Train Loss: 0.4923
Epoch 13 Step 1151 Train Loss: 0.5810
Epoch 13 Step 1201 Train Loss: 0.4845
Epoch 13 Step 1251 Train Loss: 0.5639
Epoch 13 Step 1301 Train Loss: 0.4765
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0660 Validation Top 20 DE MSE: 0.1549. 
Epoch 14 Step 1 Train Loss: 0.6093
Epoch 14 Step 51 Train Loss: 0.4690
Epoch 14 Step 101 Train Loss: 0.4867
Epoch 14 Step 151 Train Loss: 0.4962
Epoch 14 Step 201 Train Loss: 0.5607
Epoch 14 Step 251 Train Loss: 0.4604
Epoch 14 Step 301 Train Loss: 0.4802
Epoch 14 Step 351 Train Loss: 0.4709
Epoch 14 Step 401 Train Loss: 0.4579
Epoch 14 Step 451 Train Loss: 0.5840
Epoch 14 Step 501 Train Loss: 0.5881
Epoch 14 Step 551 Train Loss: 0.5676
Epoch 14 Step 601 Train Loss: 0.5752
Epoch 14 Step 651 Train Loss: 0.4857
Epoch 14 Step 701 Train Loss: 0.4982
Epoch 14 Step 751 Train Loss: 0.5770
Epoch 14 Step 801 Train Loss: 0.4971
Epoch 14 Step 851 Train Loss: 0.5447
Epoch 14 Step 901 Train Loss: 0.4595
Epoch 14 Step 951 Train Loss: 0.5303
Epoch 14 Step 1001 Train Loss: 0.4824
Epoch 14 Step 1051 Train Loss: 0.5303
Epoch 14 Step 1101 Train Loss: 0.5285
Epoch 14 Step 1151 Train Loss: 0.4938
Epoch 14 Step 1201 Train Loss: 0.5066
Epoch 14 Step 1251 Train Loss: 0.5332
Epoch 14 Step 1301 Train Loss: 0.5553
Epoch 14: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0625 Validation Top 20 DE MSE: 0.1495. 
Epoch 15 Step 1 Train Loss: 0.6052
Epoch 15 Step 51 Train Loss: 0.5494
Epoch 15 Step 101 Train Loss: 0.4980
Epoch 15 Step 151 Train Loss: 0.4778
Epoch 15 Step 201 Train Loss: 0.4761
Epoch 15 Step 251 Train Loss: 0.5599
Epoch 15 Step 301 Train Loss: 0.5069
Epoch 15 Step 351 Train Loss: 0.5400
Epoch 15 Step 401 Train Loss: 0.5582
Epoch 15 Step 451 Train Loss: 0.5064
Epoch 15 Step 501 Train Loss: 0.4157
Epoch 15 Step 551 Train Loss: 0.4872
Epoch 15 Step 601 Train Loss: 0.4779
Epoch 15 Step 651 Train Loss: 0.5607
Epoch 15 Step 701 Train Loss: 0.5250
Epoch 15 Step 751 Train Loss: 0.4869
Epoch 15 Step 801 Train Loss: 0.5772
Epoch 15 Step 851 Train Loss: 0.4699
Epoch 15 Step 901 Train Loss: 0.6071
Epoch 15 Step 951 Train Loss: 0.5043
Epoch 15 Step 1001 Train Loss: 0.5333
Epoch 15 Step 1051 Train Loss: 0.5512
Epoch 15 Step 1101 Train Loss: 0.5440
Epoch 15 Step 1151 Train Loss: 0.5268
Epoch 15 Step 1201 Train Loss: 0.4927
Epoch 15 Step 1251 Train Loss: 0.5283
Epoch 15 Step 1301 Train Loss: 0.5296
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0646 Validation Top 20 DE MSE: 0.1522. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1411
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0028239323
test_unseen_single_pearson: 0.9945709332441127
test_unseen_single_mse_de: 0.14105159
test_unseen_single_pearson_de: 0.8696232989363132
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.3339927499407086
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.27272727272727276
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8704545454545453
test_unseen_single_mse_top20_de_non_dropout: 0.14507523
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.032 MB uploadedwandb: | 0.003 MB of 0.032 MB uploadedwandb: / 0.003 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                             train_de_pearson ‚ñÇ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá
wandb:                                                    train_mse ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÜ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñÖ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá
wandb:                                                      val_mse ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñá‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.14105
wandb:                                              test_de_pearson 0.86962
wandb:               test_frac_opposite_direction_top20_non_dropout 0.27273
wandb:                          test_frac_sigma_below_1_non_dropout 0.87045
wandb:                                                     test_mse 0.00282
wandb:                                test_mse_top20_de_non_dropout 0.14508
wandb:                                                 test_pearson 0.99457
wandb:                                           test_pearson_delta 0.33399
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.27273
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.87045
wandb:                                       test_unseen_single_mse 0.00282
wandb:                                    test_unseen_single_mse_de 0.14105
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.14508
wandb:                                   test_unseen_single_pearson 0.99457
wandb:                                test_unseen_single_pearson_de 0.86962
wandb:                             test_unseen_single_pearson_delta 0.33399
wandb:                                                 train_de_mse 0.06458
wandb:                                             train_de_pearson 0.91181
wandb:                                                    train_mse 0.00174
wandb:                                                train_pearson 0.99678
wandb:                                                training_loss 0.46711
wandb:                                                   val_de_mse 0.15217
wandb:                                               val_de_pearson 0.96706
wandb:                                                      val_mse 0.00207
wandb:                                                  val_pearson 0.99612
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406681_3_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/bwwhka7l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_232738-bwwhka7l/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:22
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_000016-msk7zytw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406681_3_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/msk7zytw
wandb: WARNING Serializing object of type ndarray that is 20738176 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5660
Epoch 1 Step 51 Train Loss: 0.4754
Epoch 1 Step 101 Train Loss: 0.5128
Epoch 1 Step 151 Train Loss: 0.5599
Epoch 1 Step 201 Train Loss: 0.4804
Epoch 1 Step 251 Train Loss: 0.5152
Epoch 1 Step 301 Train Loss: 0.6273
Epoch 1 Step 351 Train Loss: 0.5170
Epoch 1 Step 401 Train Loss: 0.5097
Epoch 1 Step 451 Train Loss: 0.4891
Epoch 1 Step 501 Train Loss: 0.5560
Epoch 1 Step 551 Train Loss: 0.6673
Epoch 1 Step 601 Train Loss: 0.4830
Epoch 1 Step 651 Train Loss: 0.5039
Epoch 1 Step 701 Train Loss: 0.5741
Epoch 1 Step 751 Train Loss: 0.5856
Epoch 1 Step 801 Train Loss: 0.5002
Epoch 1 Step 851 Train Loss: 0.5954
Epoch 1 Step 901 Train Loss: 0.5723
Epoch 1 Step 951 Train Loss: 0.5408
Epoch 1 Step 1001 Train Loss: 0.5434
Epoch 1 Step 1051 Train Loss: 0.4987
Epoch 1 Step 1101 Train Loss: 0.5009
Epoch 1 Step 1151 Train Loss: 0.4715
Epoch 1 Step 1201 Train Loss: 0.4838
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.1086 Validation Top 20 DE MSE: 0.1087. 
Epoch 2 Step 1 Train Loss: 0.5465
Epoch 2 Step 51 Train Loss: 0.5274
Epoch 2 Step 101 Train Loss: 0.4400
Epoch 2 Step 151 Train Loss: 0.5237
Epoch 2 Step 201 Train Loss: 0.4579
Epoch 2 Step 251 Train Loss: 0.4933
Epoch 2 Step 301 Train Loss: 0.4535
Epoch 2 Step 351 Train Loss: 0.4563
Epoch 2 Step 401 Train Loss: 0.5061
Epoch 2 Step 451 Train Loss: 0.5991
Epoch 2 Step 501 Train Loss: 0.5293
Epoch 2 Step 551 Train Loss: 0.5060
Epoch 2 Step 601 Train Loss: 0.4834
Epoch 2 Step 651 Train Loss: 0.4821
Epoch 2 Step 701 Train Loss: 0.4714
Epoch 2 Step 751 Train Loss: 0.5794
Epoch 2 Step 801 Train Loss: 0.5088
Epoch 2 Step 851 Train Loss: 0.5527
Epoch 2 Step 901 Train Loss: 0.4926
Epoch 2 Step 951 Train Loss: 0.4936
Epoch 2 Step 1001 Train Loss: 0.5263
Epoch 2 Step 1051 Train Loss: 0.4982
Epoch 2 Step 1101 Train Loss: 0.5024
Epoch 2 Step 1151 Train Loss: 0.5365
Epoch 2 Step 1201 Train Loss: 0.5218
Epoch 2: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0825 Validation Top 20 DE MSE: 0.1062. 
Epoch 3 Step 1 Train Loss: 0.4907
Epoch 3 Step 51 Train Loss: 0.5407
Epoch 3 Step 101 Train Loss: 0.4993
Epoch 3 Step 151 Train Loss: 0.4951
Epoch 3 Step 201 Train Loss: 0.5539
Epoch 3 Step 251 Train Loss: 0.4822
Epoch 3 Step 301 Train Loss: 0.5506
Epoch 3 Step 351 Train Loss: 0.5121
Epoch 3 Step 401 Train Loss: 0.5061
Epoch 3 Step 451 Train Loss: 0.5168
Epoch 3 Step 501 Train Loss: 0.4624
Epoch 3 Step 551 Train Loss: 0.5546
Epoch 3 Step 601 Train Loss: 0.5311
Epoch 3 Step 651 Train Loss: 0.5074
Epoch 3 Step 701 Train Loss: 0.5585
Epoch 3 Step 751 Train Loss: 0.4601
Epoch 3 Step 801 Train Loss: 0.5618
Epoch 3 Step 851 Train Loss: 0.5063
Epoch 3 Step 901 Train Loss: 0.5477
Epoch 3 Step 951 Train Loss: 0.5230
Epoch 3 Step 1001 Train Loss: 0.4557
Epoch 3 Step 1051 Train Loss: 0.4697
Epoch 3 Step 1101 Train Loss: 0.5353
Epoch 3 Step 1151 Train Loss: 0.5702
Epoch 3 Step 1201 Train Loss: 0.5019
Epoch 3: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0790 Validation Top 20 DE MSE: 0.0993. 
Epoch 4 Step 1 Train Loss: 0.5536
Epoch 4 Step 51 Train Loss: 0.5257
Epoch 4 Step 101 Train Loss: 0.4797
Epoch 4 Step 151 Train Loss: 0.4947
Epoch 4 Step 201 Train Loss: 0.4840
Epoch 4 Step 251 Train Loss: 0.4614
Epoch 4 Step 301 Train Loss: 0.5506
Epoch 4 Step 351 Train Loss: 0.5011
Epoch 4 Step 401 Train Loss: 0.5386
Epoch 4 Step 451 Train Loss: 0.5482
Epoch 4 Step 501 Train Loss: 0.4873
Epoch 4 Step 551 Train Loss: 0.6142
Epoch 4 Step 601 Train Loss: 0.4958
Epoch 4 Step 651 Train Loss: 0.6538
Epoch 4 Step 701 Train Loss: 0.4951
Epoch 4 Step 751 Train Loss: 0.5413
Epoch 4 Step 801 Train Loss: 0.5405
Epoch 4 Step 851 Train Loss: 0.5386
Epoch 4 Step 901 Train Loss: 0.5106
Epoch 4 Step 951 Train Loss: 0.5171
Epoch 4 Step 1001 Train Loss: 0.5063
Epoch 4 Step 1051 Train Loss: 0.5561
Epoch 4 Step 1101 Train Loss: 0.5086
Epoch 4 Step 1151 Train Loss: 0.4811
Epoch 4 Step 1201 Train Loss: 0.5162
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0646 Validation Top 20 DE MSE: 0.0935. 
Epoch 5 Step 1 Train Loss: 0.5013
Epoch 5 Step 51 Train Loss: 0.4847
Epoch 5 Step 101 Train Loss: 0.4562
Epoch 5 Step 151 Train Loss: 0.5216
Epoch 5 Step 201 Train Loss: 0.5434
Epoch 5 Step 251 Train Loss: 0.5121
Epoch 5 Step 301 Train Loss: 0.5661
Epoch 5 Step 351 Train Loss: 0.4739
Epoch 5 Step 401 Train Loss: 0.5381
Epoch 5 Step 451 Train Loss: 0.4865
Epoch 5 Step 501 Train Loss: 0.5210
Epoch 5 Step 551 Train Loss: 0.5629
Epoch 5 Step 601 Train Loss: 0.4801
Epoch 5 Step 651 Train Loss: 0.4893
Epoch 5 Step 701 Train Loss: 0.5443
Epoch 5 Step 751 Train Loss: 0.4775
Epoch 5 Step 801 Train Loss: 0.5509
Epoch 5 Step 851 Train Loss: 0.4368
Epoch 5 Step 901 Train Loss: 0.5350
Epoch 5 Step 951 Train Loss: 0.5247
Epoch 5 Step 1001 Train Loss: 0.5075
Epoch 5 Step 1051 Train Loss: 0.5108
Epoch 5 Step 1101 Train Loss: 0.5249
Epoch 5 Step 1151 Train Loss: 0.6308
Epoch 5 Step 1201 Train Loss: 0.5729
Epoch 5: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0639 Validation Top 20 DE MSE: 0.0991. 
Epoch 6 Step 1 Train Loss: 0.4591
Epoch 6 Step 51 Train Loss: 0.5341
Epoch 6 Step 101 Train Loss: 0.5175
Epoch 6 Step 151 Train Loss: 0.5174
Epoch 6 Step 201 Train Loss: 0.5555
Epoch 6 Step 251 Train Loss: 0.5686
Epoch 6 Step 301 Train Loss: 0.5647
Epoch 6 Step 351 Train Loss: 0.5484
Epoch 6 Step 401 Train Loss: 0.5487
Epoch 6 Step 451 Train Loss: 0.5103
Epoch 6 Step 501 Train Loss: 0.5196
Epoch 6 Step 551 Train Loss: 0.4612
Epoch 6 Step 601 Train Loss: 0.4490
Epoch 6 Step 651 Train Loss: 0.4861
Epoch 6 Step 701 Train Loss: 0.5051
Epoch 6 Step 751 Train Loss: 0.5169
Epoch 6 Step 801 Train Loss: 0.5434
Epoch 6 Step 851 Train Loss: 0.5515
Epoch 6 Step 901 Train Loss: 0.5381
Epoch 6 Step 951 Train Loss: 0.5120
Epoch 6 Step 1001 Train Loss: 0.4943
Epoch 6 Step 1051 Train Loss: 0.4841
Epoch 6 Step 1101 Train Loss: 0.5401
Epoch 6 Step 1151 Train Loss: 0.4958
Epoch 6 Step 1201 Train Loss: 0.4765
Epoch 6: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0687 Validation Top 20 DE MSE: 0.0999. 
Epoch 7 Step 1 Train Loss: 0.6007
Epoch 7 Step 51 Train Loss: 0.6188
Epoch 7 Step 101 Train Loss: 0.5607
Epoch 7 Step 151 Train Loss: 0.5351
Epoch 7 Step 201 Train Loss: 0.4572
Epoch 7 Step 251 Train Loss: 0.4903
Epoch 7 Step 301 Train Loss: 0.5144
Epoch 7 Step 351 Train Loss: 0.4542
Epoch 7 Step 401 Train Loss: 0.4488
Epoch 7 Step 451 Train Loss: 0.5237
Epoch 7 Step 501 Train Loss: 0.4692
Epoch 7 Step 551 Train Loss: 0.4843
Epoch 7 Step 601 Train Loss: 0.5545
Epoch 7 Step 651 Train Loss: 0.4764
Epoch 7 Step 701 Train Loss: 0.5237
Epoch 7 Step 751 Train Loss: 0.4940
Epoch 7 Step 801 Train Loss: 0.4650
Epoch 7 Step 851 Train Loss: 0.5153
Epoch 7 Step 901 Train Loss: 0.5066
Epoch 7 Step 951 Train Loss: 0.5795
Epoch 7 Step 1001 Train Loss: 0.4878
Epoch 7 Step 1051 Train Loss: 0.5849
Epoch 7 Step 1101 Train Loss: 0.5295
Epoch 7 Step 1151 Train Loss: 0.5698
Epoch 7 Step 1201 Train Loss: 0.5702
Epoch 7: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0676 Validation Top 20 DE MSE: 0.1013. 
Epoch 8 Step 1 Train Loss: 0.5500
Epoch 8 Step 51 Train Loss: 0.5241
Epoch 8 Step 101 Train Loss: 0.5550
Epoch 8 Step 151 Train Loss: 0.4950
Epoch 8 Step 201 Train Loss: 0.5207
Epoch 8 Step 251 Train Loss: 0.5283
Epoch 8 Step 301 Train Loss: 0.4741
Epoch 8 Step 351 Train Loss: 0.5444
Epoch 8 Step 401 Train Loss: 0.5247
Epoch 8 Step 451 Train Loss: 0.5772
Epoch 8 Step 501 Train Loss: 0.5476
Epoch 8 Step 551 Train Loss: 0.4848
Epoch 8 Step 601 Train Loss: 0.4763
Epoch 8 Step 651 Train Loss: 0.5258
Epoch 8 Step 701 Train Loss: 0.4742
Epoch 8 Step 751 Train Loss: 0.4941
Epoch 8 Step 801 Train Loss: 0.5536
Epoch 8 Step 851 Train Loss: 0.5364
Epoch 8 Step 901 Train Loss: 0.5316
Epoch 8 Step 951 Train Loss: 0.5440
Epoch 8 Step 1001 Train Loss: 0.5796
Epoch 8 Step 1051 Train Loss: 0.4846
Epoch 8 Step 1101 Train Loss: 0.4445
Epoch 8 Step 1151 Train Loss: 0.4726
Epoch 8 Step 1201 Train Loss: 0.4523
Epoch 8: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0663 Validation Top 20 DE MSE: 0.0992. 
Epoch 9 Step 1 Train Loss: 0.5366
Epoch 9 Step 51 Train Loss: 0.4650
Epoch 9 Step 101 Train Loss: 0.4902
Epoch 9 Step 151 Train Loss: 0.4855
Epoch 9 Step 201 Train Loss: 0.4837
Epoch 9 Step 251 Train Loss: 0.5249
Epoch 9 Step 301 Train Loss: 0.4680
Epoch 9 Step 351 Train Loss: 0.6232
Epoch 9 Step 401 Train Loss: 0.5768
Epoch 9 Step 451 Train Loss: 0.4611
Epoch 9 Step 501 Train Loss: 0.5642
Epoch 9 Step 551 Train Loss: 0.6120
Epoch 9 Step 601 Train Loss: 0.4662
Epoch 9 Step 651 Train Loss: 0.5052
Epoch 9 Step 701 Train Loss: 0.4936
Epoch 9 Step 751 Train Loss: 0.5209
Epoch 9 Step 801 Train Loss: 0.4699
Epoch 9 Step 851 Train Loss: 0.4481
Epoch 9 Step 901 Train Loss: 0.5268
Epoch 9 Step 951 Train Loss: 0.4794
Epoch 9 Step 1001 Train Loss: 0.4800
Epoch 9 Step 1051 Train Loss: 0.5569
Epoch 9 Step 1101 Train Loss: 0.5478
Epoch 9 Step 1151 Train Loss: 0.4830
Epoch 9 Step 1201 Train Loss: 0.5197
Epoch 9: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0622 Validation Top 20 DE MSE: 0.0944. 
Epoch 10 Step 1 Train Loss: 0.4948
Epoch 10 Step 51 Train Loss: 0.4664
Epoch 10 Step 101 Train Loss: 0.5811
Epoch 10 Step 151 Train Loss: 0.5497
Epoch 10 Step 201 Train Loss: 0.5126
Epoch 10 Step 251 Train Loss: 0.5386
Epoch 10 Step 301 Train Loss: 0.4575
Epoch 10 Step 351 Train Loss: 0.4936
Epoch 10 Step 401 Train Loss: 0.5069
Epoch 10 Step 451 Train Loss: 0.5516
Epoch 10 Step 501 Train Loss: 0.5317
Epoch 10 Step 551 Train Loss: 0.5426
Epoch 10 Step 601 Train Loss: 0.5570
Epoch 10 Step 651 Train Loss: 0.4442
Epoch 10 Step 701 Train Loss: 0.6015
Epoch 10 Step 751 Train Loss: 0.5089
Epoch 10 Step 801 Train Loss: 0.4972
Epoch 10 Step 851 Train Loss: 0.5063
Epoch 10 Step 901 Train Loss: 0.5396
Epoch 10 Step 951 Train Loss: 0.5692
Epoch 10 Step 1001 Train Loss: 0.5681
Epoch 10 Step 1051 Train Loss: 0.5499
Epoch 10 Step 1101 Train Loss: 0.5338
Epoch 10 Step 1151 Train Loss: 0.5521
Epoch 10 Step 1201 Train Loss: 0.5174
Epoch 10: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0668 Validation Top 20 DE MSE: 0.1026. 
Epoch 11 Step 1 Train Loss: 0.4789
Epoch 11 Step 51 Train Loss: 0.5063
Epoch 11 Step 101 Train Loss: 0.5978
Epoch 11 Step 151 Train Loss: 0.4977
Epoch 11 Step 201 Train Loss: 0.5160
Epoch 11 Step 251 Train Loss: 0.5919
Epoch 11 Step 301 Train Loss: 0.4546
Epoch 11 Step 351 Train Loss: 0.4857
Epoch 11 Step 401 Train Loss: 0.5308
Epoch 11 Step 451 Train Loss: 0.4674
Epoch 11 Step 501 Train Loss: 0.4758
Epoch 11 Step 551 Train Loss: 0.4693
Epoch 11 Step 601 Train Loss: 0.5310
Epoch 11 Step 651 Train Loss: 0.4670
Epoch 11 Step 701 Train Loss: 0.5343
Epoch 11 Step 751 Train Loss: 0.5667
Epoch 11 Step 801 Train Loss: 0.5149
Epoch 11 Step 851 Train Loss: 0.5109
Epoch 11 Step 901 Train Loss: 0.5357
Epoch 11 Step 951 Train Loss: 0.5195
Epoch 11 Step 1001 Train Loss: 0.5077
Epoch 11 Step 1051 Train Loss: 0.4895
Epoch 11 Step 1101 Train Loss: 0.6066
Epoch 11 Step 1151 Train Loss: 0.4996
Epoch 11 Step 1201 Train Loss: 0.4781
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0624 Validation Top 20 DE MSE: 0.0970. 
Epoch 12 Step 1 Train Loss: 0.4528
Epoch 12 Step 51 Train Loss: 0.4878
Epoch 12 Step 101 Train Loss: 0.4908
Epoch 12 Step 151 Train Loss: 0.5006
Epoch 12 Step 201 Train Loss: 0.4543
Epoch 12 Step 251 Train Loss: 0.5766
Epoch 12 Step 301 Train Loss: 0.5544
Epoch 12 Step 351 Train Loss: 0.4779
Epoch 12 Step 401 Train Loss: 0.6551
Epoch 12 Step 451 Train Loss: 0.5115
Epoch 12 Step 501 Train Loss: 0.5056
Epoch 12 Step 551 Train Loss: 0.5197
Epoch 12 Step 601 Train Loss: 0.5387
Epoch 12 Step 651 Train Loss: 0.5193
Epoch 12 Step 701 Train Loss: 0.4572
Epoch 12 Step 751 Train Loss: 0.4887
Epoch 12 Step 801 Train Loss: 0.4620
Epoch 12 Step 851 Train Loss: 0.5428
Epoch 12 Step 901 Train Loss: 0.5119
Epoch 12 Step 951 Train Loss: 0.5335
Epoch 12 Step 1001 Train Loss: 0.4631
Epoch 12 Step 1051 Train Loss: 0.5715
Epoch 12 Step 1101 Train Loss: 0.5186
Epoch 12 Step 1151 Train Loss: 0.4816
Epoch 12 Step 1201 Train Loss: 0.5298
Epoch 12: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0639 Validation Top 20 DE MSE: 0.0976. 
Epoch 13 Step 1 Train Loss: 0.5751
Epoch 13 Step 51 Train Loss: 0.5217
Epoch 13 Step 101 Train Loss: 0.5153
Epoch 13 Step 151 Train Loss: 0.4879
Epoch 13 Step 201 Train Loss: 0.5838
Epoch 13 Step 251 Train Loss: 0.5577
Epoch 13 Step 301 Train Loss: 0.4605
Epoch 13 Step 351 Train Loss: 0.5558
Epoch 13 Step 401 Train Loss: 0.4678
Epoch 13 Step 451 Train Loss: 0.5242
Epoch 13 Step 501 Train Loss: 0.5048
Epoch 13 Step 551 Train Loss: 0.5022
Epoch 13 Step 601 Train Loss: 0.5215
Epoch 13 Step 651 Train Loss: 0.5113
Epoch 13 Step 701 Train Loss: 0.5212
Epoch 13 Step 751 Train Loss: 0.5224
Epoch 13 Step 801 Train Loss: 0.5374
Epoch 13 Step 851 Train Loss: 0.5899
Epoch 13 Step 901 Train Loss: 0.4774
Epoch 13 Step 951 Train Loss: 0.5714
Epoch 13 Step 1001 Train Loss: 0.4964
Epoch 13 Step 1051 Train Loss: 0.5393
Epoch 13 Step 1101 Train Loss: 0.5415
Epoch 13 Step 1151 Train Loss: 0.5064
Epoch 13 Step 1201 Train Loss: 0.5032
Epoch 13: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0642 Validation Top 20 DE MSE: 0.0985. 
Epoch 14 Step 1 Train Loss: 0.5304
Epoch 14 Step 51 Train Loss: 0.5635
Epoch 14 Step 101 Train Loss: 0.4790
Epoch 14 Step 151 Train Loss: 0.5334
Epoch 14 Step 201 Train Loss: 0.5359
Epoch 14 Step 251 Train Loss: 0.4766
Epoch 14 Step 301 Train Loss: 0.5197
Epoch 14 Step 351 Train Loss: 0.5545
Epoch 14 Step 401 Train Loss: 0.4652
Epoch 14 Step 451 Train Loss: 0.5530
Epoch 14 Step 501 Train Loss: 0.5638
Epoch 14 Step 551 Train Loss: 0.5485
Epoch 14 Step 601 Train Loss: 0.4693
Epoch 14 Step 651 Train Loss: 0.5281
Epoch 14 Step 701 Train Loss: 0.4621
Epoch 14 Step 751 Train Loss: 0.5635
Epoch 14 Step 801 Train Loss: 0.4738
Epoch 14 Step 851 Train Loss: 0.5648
Epoch 14 Step 901 Train Loss: 0.5994
Epoch 14 Step 951 Train Loss: 0.4987
Epoch 14 Step 1001 Train Loss: 0.5348
Epoch 14 Step 1051 Train Loss: 0.5278
Epoch 14 Step 1101 Train Loss: 0.5076
Epoch 14 Step 1151 Train Loss: 0.5161
Epoch 14 Step 1201 Train Loss: 0.4941
Epoch 14: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0637 Validation Top 20 DE MSE: 0.0953. 
Epoch 15 Step 1 Train Loss: 0.5480
Epoch 15 Step 51 Train Loss: 0.5693
Epoch 15 Step 101 Train Loss: 0.5196
Epoch 15 Step 151 Train Loss: 0.4657
Epoch 15 Step 201 Train Loss: 0.5345
Epoch 15 Step 251 Train Loss: 0.5551
Epoch 15 Step 301 Train Loss: 0.5194
Epoch 15 Step 351 Train Loss: 0.5576
Epoch 15 Step 401 Train Loss: 0.5503
Epoch 15 Step 451 Train Loss: 0.5603
Epoch 15 Step 501 Train Loss: 0.5701
Epoch 15 Step 551 Train Loss: 0.5495
Epoch 15 Step 601 Train Loss: 0.5143
Epoch 15 Step 651 Train Loss: 0.5115
Epoch 15 Step 701 Train Loss: 0.5215
Epoch 15 Step 751 Train Loss: 0.4363
Epoch 15 Step 801 Train Loss: 0.4878
Epoch 15 Step 851 Train Loss: 0.4914
Epoch 15 Step 901 Train Loss: 0.5347
Epoch 15 Step 951 Train Loss: 0.4756
Epoch 15 Step 1001 Train Loss: 0.4251
Epoch 15 Step 1051 Train Loss: 0.5388
Epoch 15 Step 1101 Train Loss: 0.4801
Epoch 15 Step 1151 Train Loss: 0.5056
Epoch 15 Step 1201 Train Loss: 0.7736
Epoch 15: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0650 Validation Top 20 DE MSE: 0.0961. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1546
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002808419
test_unseen_single_pearson: 0.994663607673711
test_unseen_single_mse_de: 0.15462632
test_unseen_single_pearson_de: 0.9159616630801849
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.4227154959385938
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.28409090909090906
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8795454545454543
test_unseen_single_mse_top20_de_non_dropout: 0.157994
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.031 MB uploadedwandb: | 0.003 MB of 0.031 MB uploadedwandb: / 0.009 MB of 0.031 MB uploadedwandb: - 0.025 MB of 0.031 MB uploadedwandb: \ 0.025 MB of 0.031 MB uploadedwandb: | 0.031 MB of 0.031 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñá‚ñá‚ñÜ‚ñà‚ñá
wandb:                                                      val_mse ‚ñá‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.15463
wandb:                                              test_de_pearson 0.91596
wandb:               test_frac_opposite_direction_top20_non_dropout 0.28409
wandb:                          test_frac_sigma_below_1_non_dropout 0.87955
wandb:                                                     test_mse 0.00281
wandb:                                test_mse_top20_de_non_dropout 0.15799
wandb:                                                 test_pearson 0.99466
wandb:                                           test_pearson_delta 0.42272
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.28409
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.87955
wandb:                                       test_unseen_single_mse 0.00281
wandb:                                    test_unseen_single_mse_de 0.15463
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15799
wandb:                                   test_unseen_single_pearson 0.99466
wandb:                                test_unseen_single_pearson_de 0.91596
wandb:                             test_unseen_single_pearson_delta 0.42272
wandb:                                                 train_de_mse 0.06498
wandb:                                             train_de_pearson 0.89203
wandb:                                                    train_mse 0.00185
wandb:                                                train_pearson 0.99655
wandb:                                                training_loss 0.59728
wandb:                                                   val_de_mse 0.09609
wandb:                                               val_de_pearson 0.96806
wandb:                                                      val_mse 0.00159
wandb:                                                  val_pearson 0.99697
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406681_3_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/msk7zytw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_000016-msk7zytw/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:22
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_003145-647nf25l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406681_3_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/647nf25l
wandb: WARNING Serializing object of type ndarray that is 20738176 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5422
Epoch 1 Step 51 Train Loss: 0.5296
Epoch 1 Step 101 Train Loss: 0.5392
Epoch 1 Step 151 Train Loss: 0.4982
Epoch 1 Step 201 Train Loss: 0.5515
Epoch 1 Step 251 Train Loss: 0.5048
Epoch 1 Step 301 Train Loss: 0.5561
Epoch 1 Step 351 Train Loss: 0.5678
Epoch 1 Step 401 Train Loss: 0.5564
Epoch 1 Step 451 Train Loss: 0.5082
Epoch 1 Step 501 Train Loss: 0.5451
Epoch 1 Step 551 Train Loss: 0.4785
Epoch 1 Step 601 Train Loss: 0.5402
Epoch 1 Step 651 Train Loss: 0.4962
Epoch 1 Step 701 Train Loss: 0.4767
Epoch 1 Step 751 Train Loss: 0.5881
Epoch 1 Step 801 Train Loss: 0.4616
Epoch 1 Step 851 Train Loss: 0.5802
Epoch 1 Step 901 Train Loss: 0.5702
Epoch 1 Step 951 Train Loss: 0.5730
Epoch 1 Step 1001 Train Loss: 0.4807
Epoch 1 Step 1051 Train Loss: 0.4401
Epoch 1 Step 1101 Train Loss: 0.4789
Epoch 1 Step 1151 Train Loss: 0.4927
Epoch 1 Step 1201 Train Loss: 0.5534
Epoch 1: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.1085 Validation Top 20 DE MSE: 0.1780. 
Epoch 2 Step 1 Train Loss: 0.4798
Epoch 2 Step 51 Train Loss: 0.4489
Epoch 2 Step 101 Train Loss: 0.4480
Epoch 2 Step 151 Train Loss: 0.5077
Epoch 2 Step 201 Train Loss: 0.4797
Epoch 2 Step 251 Train Loss: 0.5469
Epoch 2 Step 301 Train Loss: 0.5721
Epoch 2 Step 351 Train Loss: 0.4381
Epoch 2 Step 401 Train Loss: 0.5710
Epoch 2 Step 451 Train Loss: 0.5759
Epoch 2 Step 501 Train Loss: 0.5245
Epoch 2 Step 551 Train Loss: 0.5646
Epoch 2 Step 601 Train Loss: 0.4870
Epoch 2 Step 651 Train Loss: 0.4760
Epoch 2 Step 701 Train Loss: 0.5439
Epoch 2 Step 751 Train Loss: 0.4807
Epoch 2 Step 801 Train Loss: 0.5184
Epoch 2 Step 851 Train Loss: 0.4954
Epoch 2 Step 901 Train Loss: 0.5973
Epoch 2 Step 951 Train Loss: 0.4856
Epoch 2 Step 1001 Train Loss: 0.4713
Epoch 2 Step 1051 Train Loss: 0.4455
Epoch 2 Step 1101 Train Loss: 0.4742
Epoch 2 Step 1151 Train Loss: 0.6985
Epoch 2 Step 1201 Train Loss: 0.5379
Epoch 2: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0737 Validation Top 20 DE MSE: 0.0849. 
Epoch 3 Step 1 Train Loss: 0.4436
Epoch 3 Step 51 Train Loss: 0.4519
Epoch 3 Step 101 Train Loss: 0.5282
Epoch 3 Step 151 Train Loss: 0.4694
Epoch 3 Step 201 Train Loss: 0.5113
Epoch 3 Step 251 Train Loss: 0.4811
Epoch 3 Step 301 Train Loss: 0.5856
Epoch 3 Step 351 Train Loss: 0.5359
Epoch 3 Step 401 Train Loss: 0.6084
Epoch 3 Step 451 Train Loss: 0.4827
Epoch 3 Step 501 Train Loss: 0.5386
Epoch 3 Step 551 Train Loss: 0.4611
Epoch 3 Step 601 Train Loss: 0.4935
Epoch 3 Step 651 Train Loss: 0.5379
Epoch 3 Step 701 Train Loss: 0.5860
Epoch 3 Step 751 Train Loss: 0.4698
Epoch 3 Step 801 Train Loss: 0.5065
Epoch 3 Step 851 Train Loss: 0.5104
Epoch 3 Step 901 Train Loss: 0.5680
Epoch 3 Step 951 Train Loss: 0.4650
Epoch 3 Step 1001 Train Loss: 0.5270
Epoch 3 Step 1051 Train Loss: 0.5428
Epoch 3 Step 1101 Train Loss: 0.5324
Epoch 3 Step 1151 Train Loss: 0.4983
Epoch 3 Step 1201 Train Loss: 0.4951
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0697 Validation Top 20 DE MSE: 0.0902. 
Epoch 4 Step 1 Train Loss: 0.4948
Epoch 4 Step 51 Train Loss: 0.4781
Epoch 4 Step 101 Train Loss: 0.5237
Epoch 4 Step 151 Train Loss: 0.5888
Epoch 4 Step 201 Train Loss: 0.5108
Epoch 4 Step 251 Train Loss: 0.5136
Epoch 4 Step 301 Train Loss: 0.5040
Epoch 4 Step 351 Train Loss: 0.5090
Epoch 4 Step 401 Train Loss: 0.4643
Epoch 4 Step 451 Train Loss: 0.5037
Epoch 4 Step 501 Train Loss: 0.4514
Epoch 4 Step 551 Train Loss: 0.5650
Epoch 4 Step 601 Train Loss: 0.4512
Epoch 4 Step 651 Train Loss: 0.5523
Epoch 4 Step 701 Train Loss: 0.4143
Epoch 4 Step 751 Train Loss: 0.5380
Epoch 4 Step 801 Train Loss: 0.5020
Epoch 4 Step 851 Train Loss: 0.4585
Epoch 4 Step 901 Train Loss: 0.4815
Epoch 4 Step 951 Train Loss: 0.6184
Epoch 4 Step 1001 Train Loss: 0.4605
Epoch 4 Step 1051 Train Loss: 0.5020
Epoch 4 Step 1101 Train Loss: 0.5836
Epoch 4 Step 1151 Train Loss: 0.5258
Epoch 4 Step 1201 Train Loss: 0.4973
Epoch 4: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0682 Validation Top 20 DE MSE: 0.1071. 
Epoch 5 Step 1 Train Loss: 0.4625
Epoch 5 Step 51 Train Loss: 0.5224
Epoch 5 Step 101 Train Loss: 0.4807
Epoch 5 Step 151 Train Loss: 0.4905
Epoch 5 Step 201 Train Loss: 0.5646
Epoch 5 Step 251 Train Loss: 0.4836
Epoch 5 Step 301 Train Loss: 0.4593
Epoch 5 Step 351 Train Loss: 0.5603
Epoch 5 Step 401 Train Loss: 0.4738
Epoch 5 Step 451 Train Loss: 0.5057
Epoch 5 Step 501 Train Loss: 0.5224
Epoch 5 Step 551 Train Loss: 0.6839
Epoch 5 Step 601 Train Loss: 0.5796
Epoch 5 Step 651 Train Loss: 0.5051
Epoch 5 Step 701 Train Loss: 0.5765
Epoch 5 Step 751 Train Loss: 0.4586
Epoch 5 Step 801 Train Loss: 0.5674
Epoch 5 Step 851 Train Loss: 0.4652
Epoch 5 Step 901 Train Loss: 0.4707
Epoch 5 Step 951 Train Loss: 0.5690
Epoch 5 Step 1001 Train Loss: 0.5475
Epoch 5 Step 1051 Train Loss: 0.4649
Epoch 5 Step 1101 Train Loss: 0.4697
Epoch 5 Step 1151 Train Loss: 0.5340
Epoch 5 Step 1201 Train Loss: 0.4795
Epoch 5: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0627 Validation Top 20 DE MSE: 0.0933. 
Epoch 6 Step 1 Train Loss: 0.4844
Epoch 6 Step 51 Train Loss: 0.4522
Epoch 6 Step 101 Train Loss: 0.5393
Epoch 6 Step 151 Train Loss: 0.4831
Epoch 6 Step 201 Train Loss: 0.6077
Epoch 6 Step 251 Train Loss: 0.4995
Epoch 6 Step 301 Train Loss: 0.5277
Epoch 6 Step 351 Train Loss: 0.5715
Epoch 6 Step 401 Train Loss: 0.4763
Epoch 6 Step 451 Train Loss: 0.5114
Epoch 6 Step 501 Train Loss: 0.6783
Epoch 6 Step 551 Train Loss: 0.4740
Epoch 6 Step 601 Train Loss: 0.5119
Epoch 6 Step 651 Train Loss: 0.5069
Epoch 6 Step 701 Train Loss: 0.4367
Epoch 6 Step 751 Train Loss: 0.5430
Epoch 6 Step 801 Train Loss: 0.4863
Epoch 6 Step 851 Train Loss: 0.5034
Epoch 6 Step 901 Train Loss: 0.4398
Epoch 6 Step 951 Train Loss: 0.5767
Epoch 6 Step 1001 Train Loss: 0.5897
Epoch 6 Step 1051 Train Loss: 0.5641
Epoch 6 Step 1101 Train Loss: 0.4970
Epoch 6 Step 1151 Train Loss: 0.4580
Epoch 6 Step 1201 Train Loss: 0.4799
Epoch 6: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0629 Validation Top 20 DE MSE: 0.0899. 
Epoch 7 Step 1 Train Loss: 0.5088
Epoch 7 Step 51 Train Loss: 0.4991
Epoch 7 Step 101 Train Loss: 0.5022
Epoch 7 Step 151 Train Loss: 0.4683
Epoch 7 Step 201 Train Loss: 0.4798
Epoch 7 Step 251 Train Loss: 0.5472
Epoch 7 Step 301 Train Loss: 0.4731
Epoch 7 Step 351 Train Loss: 0.5330
Epoch 7 Step 401 Train Loss: 0.4719
Epoch 7 Step 451 Train Loss: 0.4775
Epoch 7 Step 501 Train Loss: 0.5608
Epoch 7 Step 551 Train Loss: 0.5520
Epoch 7 Step 601 Train Loss: 0.4828
Epoch 7 Step 651 Train Loss: 0.4945
Epoch 7 Step 701 Train Loss: 0.5015
Epoch 7 Step 751 Train Loss: 0.5010
Epoch 7 Step 801 Train Loss: 0.5288
Epoch 7 Step 851 Train Loss: 0.5442
Epoch 7 Step 901 Train Loss: 0.5134
Epoch 7 Step 951 Train Loss: 0.4476
Epoch 7 Step 1001 Train Loss: 0.5603
Epoch 7 Step 1051 Train Loss: 0.5083
Epoch 7 Step 1101 Train Loss: 0.4813
Epoch 7 Step 1151 Train Loss: 0.5261
Epoch 7 Step 1201 Train Loss: 0.5444
Epoch 7: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0581 Validation Top 20 DE MSE: 0.0889. 
Epoch 8 Step 1 Train Loss: 0.5016
Epoch 8 Step 51 Train Loss: 0.4902
Epoch 8 Step 101 Train Loss: 0.4984
Epoch 8 Step 151 Train Loss: 0.4615
Epoch 8 Step 201 Train Loss: 0.4522
Epoch 8 Step 251 Train Loss: 0.4963
Epoch 8 Step 301 Train Loss: 0.5360
Epoch 8 Step 351 Train Loss: 0.5623
Epoch 8 Step 401 Train Loss: 0.5145
Epoch 8 Step 451 Train Loss: 0.4905
Epoch 8 Step 501 Train Loss: 0.5852
Epoch 8 Step 551 Train Loss: 0.5353
Epoch 8 Step 601 Train Loss: 0.6380
Epoch 8 Step 651 Train Loss: 0.5535
Epoch 8 Step 701 Train Loss: 0.5314
Epoch 8 Step 751 Train Loss: 0.5139
Epoch 8 Step 801 Train Loss: 0.5888
Epoch 8 Step 851 Train Loss: 0.4472
Epoch 8 Step 901 Train Loss: 0.5146
Epoch 8 Step 951 Train Loss: 0.4872
Epoch 8 Step 1001 Train Loss: 0.5098
Epoch 8 Step 1051 Train Loss: 0.4956
Epoch 8 Step 1101 Train Loss: 0.5132
Epoch 8 Step 1151 Train Loss: 0.5124
Epoch 8 Step 1201 Train Loss: 0.5347
Epoch 8: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0668 Validation Top 20 DE MSE: 0.1012. 
Epoch 9 Step 1 Train Loss: 0.4362
Epoch 9 Step 51 Train Loss: 0.5459
Epoch 9 Step 101 Train Loss: 0.5059
Epoch 9 Step 151 Train Loss: 0.5205
Epoch 9 Step 201 Train Loss: 0.5438
Epoch 9 Step 251 Train Loss: 0.4842
Epoch 9 Step 301 Train Loss: 0.5369
Epoch 9 Step 351 Train Loss: 0.5070
Epoch 9 Step 401 Train Loss: 0.5724
Epoch 9 Step 451 Train Loss: 0.5527
Epoch 9 Step 501 Train Loss: 0.6134
Epoch 9 Step 551 Train Loss: 0.6787
Epoch 9 Step 601 Train Loss: 0.5163
Epoch 9 Step 651 Train Loss: 0.5054
Epoch 9 Step 701 Train Loss: 0.4984
Epoch 9 Step 751 Train Loss: 0.5712
Epoch 9 Step 801 Train Loss: 0.4981
Epoch 9 Step 851 Train Loss: 0.5414
Epoch 9 Step 901 Train Loss: 0.5650
Epoch 9 Step 951 Train Loss: 0.4910
Epoch 9 Step 1001 Train Loss: 0.5875
Epoch 9 Step 1051 Train Loss: 0.4998
Epoch 9 Step 1101 Train Loss: 0.4786
Epoch 9 Step 1151 Train Loss: 0.6787
Epoch 9 Step 1201 Train Loss: 0.5898
Epoch 9: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0667 Validation Top 20 DE MSE: 0.0987. 
Epoch 10 Step 1 Train Loss: 0.4742
Epoch 10 Step 51 Train Loss: 0.5424
Epoch 10 Step 101 Train Loss: 0.5179
Epoch 10 Step 151 Train Loss: 0.6314
Epoch 10 Step 201 Train Loss: 0.5895
Epoch 10 Step 251 Train Loss: 0.5772
Epoch 10 Step 301 Train Loss: 0.5673
Epoch 10 Step 351 Train Loss: 0.5932
Epoch 10 Step 401 Train Loss: 0.5531
Epoch 10 Step 451 Train Loss: 0.4739
Epoch 10 Step 501 Train Loss: 0.5252
Epoch 10 Step 551 Train Loss: 0.4402
Epoch 10 Step 601 Train Loss: 0.5199
Epoch 10 Step 651 Train Loss: 0.5402
Epoch 10 Step 701 Train Loss: 0.5285
Epoch 10 Step 751 Train Loss: 0.5699
Epoch 10 Step 801 Train Loss: 0.4636
Epoch 10 Step 851 Train Loss: 0.4988
Epoch 10 Step 901 Train Loss: 0.5094
Epoch 10 Step 951 Train Loss: 0.5415
Epoch 10 Step 1001 Train Loss: 0.5027
Epoch 10 Step 1051 Train Loss: 0.5008
Epoch 10 Step 1101 Train Loss: 0.4941
Epoch 10 Step 1151 Train Loss: 0.5508
Epoch 10 Step 1201 Train Loss: 0.4944
Epoch 10: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0621 Validation Top 20 DE MSE: 0.0924. 
Epoch 11 Step 1 Train Loss: 0.5229
Epoch 11 Step 51 Train Loss: 0.5185
Epoch 11 Step 101 Train Loss: 0.6833
Epoch 11 Step 151 Train Loss: 0.5212
Epoch 11 Step 201 Train Loss: 0.4784
Epoch 11 Step 251 Train Loss: 0.4440
Epoch 11 Step 301 Train Loss: 0.5041
Epoch 11 Step 351 Train Loss: 0.5392
Epoch 11 Step 401 Train Loss: 0.5186
Epoch 11 Step 451 Train Loss: 0.5687
Epoch 11 Step 501 Train Loss: 0.5834
Epoch 11 Step 551 Train Loss: 0.4896
Epoch 11 Step 601 Train Loss: 0.4721
Epoch 11 Step 651 Train Loss: 0.5524
Epoch 11 Step 701 Train Loss: 0.6039
Epoch 11 Step 751 Train Loss: 0.4421
Epoch 11 Step 801 Train Loss: 0.4683
Epoch 11 Step 851 Train Loss: 0.4895
Epoch 11 Step 901 Train Loss: 0.5920
Epoch 11 Step 951 Train Loss: 0.5487
Epoch 11 Step 1001 Train Loss: 0.5008
Epoch 11 Step 1051 Train Loss: 0.6233
Epoch 11 Step 1101 Train Loss: 0.4469
Epoch 11 Step 1151 Train Loss: 0.5620
Epoch 11 Step 1201 Train Loss: 0.4910
Epoch 11: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0651 Validation Top 20 DE MSE: 0.0949. 
Epoch 12 Step 1 Train Loss: 0.5179
Epoch 12 Step 51 Train Loss: 0.4583
Epoch 12 Step 101 Train Loss: 0.5490
Epoch 12 Step 151 Train Loss: 0.5055
Epoch 12 Step 201 Train Loss: 0.5101
Epoch 12 Step 251 Train Loss: 0.4642
Epoch 12 Step 301 Train Loss: 0.5222
Epoch 12 Step 351 Train Loss: 0.5265
Epoch 12 Step 401 Train Loss: 0.5349
Epoch 12 Step 451 Train Loss: 0.5743
Epoch 12 Step 501 Train Loss: 0.4786
Epoch 12 Step 551 Train Loss: 0.5665
Epoch 12 Step 601 Train Loss: 0.6352
Epoch 12 Step 651 Train Loss: 0.4555
Epoch 12 Step 701 Train Loss: 0.6120
Epoch 12 Step 751 Train Loss: 0.5486
Epoch 12 Step 801 Train Loss: 0.5804
Epoch 12 Step 851 Train Loss: 0.5085
Epoch 12 Step 901 Train Loss: 0.5523
Epoch 12 Step 951 Train Loss: 0.5666
Epoch 12 Step 1001 Train Loss: 0.5742
Epoch 12 Step 1051 Train Loss: 0.5054
Epoch 12 Step 1101 Train Loss: 0.5539
Epoch 12 Step 1151 Train Loss: 0.4940
Epoch 12 Step 1201 Train Loss: 0.4971
Epoch 12: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0645 Validation Top 20 DE MSE: 0.0982. 
Epoch 13 Step 1 Train Loss: 0.5041
Epoch 13 Step 51 Train Loss: 0.6095
Epoch 13 Step 101 Train Loss: 0.4881
Epoch 13 Step 151 Train Loss: 0.5166
Epoch 13 Step 201 Train Loss: 0.4841
Epoch 13 Step 251 Train Loss: 0.5275
Epoch 13 Step 301 Train Loss: 0.5113
Epoch 13 Step 351 Train Loss: 0.4842
Epoch 13 Step 401 Train Loss: 0.5188
Epoch 13 Step 451 Train Loss: 0.5915
Epoch 13 Step 501 Train Loss: 0.4980
Epoch 13 Step 551 Train Loss: 0.5051
Epoch 13 Step 601 Train Loss: 0.4850
Epoch 13 Step 651 Train Loss: 0.5848
Epoch 13 Step 701 Train Loss: 0.5734
Epoch 13 Step 751 Train Loss: 0.4981
Epoch 13 Step 801 Train Loss: 0.5463
Epoch 13 Step 851 Train Loss: 0.4754
Epoch 13 Step 901 Train Loss: 0.5214
Epoch 13 Step 951 Train Loss: 0.4911
Epoch 13 Step 1001 Train Loss: 0.6010
Epoch 13 Step 1051 Train Loss: 0.5675
Epoch 13 Step 1101 Train Loss: 0.6221
Epoch 13 Step 1151 Train Loss: 0.5500
Epoch 13 Step 1201 Train Loss: 0.5173
Epoch 13: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0592 Validation Top 20 DE MSE: 0.0904. 
Epoch 14 Step 1 Train Loss: 0.5692
Epoch 14 Step 51 Train Loss: 0.4972
Epoch 14 Step 101 Train Loss: 0.5463
Epoch 14 Step 151 Train Loss: 0.4829
Epoch 14 Step 201 Train Loss: 0.4966
Epoch 14 Step 251 Train Loss: 0.5507
Epoch 14 Step 301 Train Loss: 0.4684
Epoch 14 Step 351 Train Loss: 0.5828
Epoch 14 Step 401 Train Loss: 0.5852
Epoch 14 Step 451 Train Loss: 0.5213
Epoch 14 Step 501 Train Loss: 0.5202
Epoch 14 Step 551 Train Loss: 0.5478
Epoch 14 Step 601 Train Loss: 0.6036
Epoch 14 Step 651 Train Loss: 0.4962
Epoch 14 Step 701 Train Loss: 0.5241
Epoch 14 Step 751 Train Loss: 0.4891
Epoch 14 Step 801 Train Loss: 0.5628
Epoch 14 Step 851 Train Loss: 0.5568
Epoch 14 Step 901 Train Loss: 0.5404
Epoch 14 Step 951 Train Loss: 0.4679
Epoch 14 Step 1001 Train Loss: 0.5229
Epoch 14 Step 1051 Train Loss: 0.5964
Epoch 14 Step 1101 Train Loss: 0.5533
Epoch 14 Step 1151 Train Loss: 0.5329
Epoch 14 Step 1201 Train Loss: 0.5084
Epoch 14: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0641 Validation Top 20 DE MSE: 0.0949. 
Epoch 15 Step 1 Train Loss: 0.5111
Epoch 15 Step 51 Train Loss: 0.5701
Epoch 15 Step 101 Train Loss: 0.6586
Epoch 15 Step 151 Train Loss: 0.4822
Epoch 15 Step 201 Train Loss: 0.6049
Epoch 15 Step 251 Train Loss: 0.5580
Epoch 15 Step 301 Train Loss: 0.6439
Epoch 15 Step 351 Train Loss: 0.5004
Epoch 15 Step 401 Train Loss: 0.4647
Epoch 15 Step 451 Train Loss: 0.5207
Epoch 15 Step 501 Train Loss: 0.4983
Epoch 15 Step 551 Train Loss: 0.5389
Epoch 15 Step 601 Train Loss: 0.6100
Epoch 15 Step 651 Train Loss: 0.6024
Epoch 15 Step 701 Train Loss: 0.6350
Epoch 15 Step 751 Train Loss: 0.5401
Epoch 15 Step 801 Train Loss: 0.6205
Epoch 15 Step 851 Train Loss: 0.4765
Epoch 15 Step 901 Train Loss: 0.5595
Epoch 15 Step 951 Train Loss: 0.4610
Epoch 15 Step 1001 Train Loss: 0.4808
Epoch 15 Step 1051 Train Loss: 0.5410
Epoch 15 Step 1101 Train Loss: 0.5571
Epoch 15 Step 1151 Train Loss: 0.4639
Epoch 15 Step 1201 Train Loss: 0.5697
Epoch 15: Train Overall MSE: 0.0017 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0648 Validation Top 20 DE MSE: 0.0945. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1043
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0020898057
test_unseen_single_pearson: 0.9960244222627653
test_unseen_single_mse_de: 0.10433154
test_unseen_single_pearson_de: 0.9177376945155262
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.33243027047947754
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2659090909090909
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9204545454545453
test_unseen_single_mse_top20_de_non_dropout: 0.107349895
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.031 MB uploadedwandb: | 0.001 MB of 0.031 MB uploadedwandb: / 0.031 MB of 0.031 MB uploadedwandb: - 0.031 MB of 0.031 MB uploadedwandb: \ 0.031 MB of 0.031 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÅ
wandb:                                                   val_de_mse ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.10433
wandb:                                              test_de_pearson 0.91774
wandb:               test_frac_opposite_direction_top20_non_dropout 0.26591
wandb:                          test_frac_sigma_below_1_non_dropout 0.92045
wandb:                                                     test_mse 0.00209
wandb:                                test_mse_top20_de_non_dropout 0.10735
wandb:                                                 test_pearson 0.99602
wandb:                                           test_pearson_delta 0.33243
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.26591
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.92045
wandb:                                       test_unseen_single_mse 0.00209
wandb:                                    test_unseen_single_mse_de 0.10433
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.10735
wandb:                                   test_unseen_single_pearson 0.99602
wandb:                                test_unseen_single_pearson_de 0.91774
wandb:                             test_unseen_single_pearson_delta 0.33243
wandb:                                                 train_de_mse 0.06477
wandb:                                             train_de_pearson 0.91394
wandb:                                                    train_mse 0.00171
wandb:                                                train_pearson 0.99681
wandb:                                                training_loss 0.48954
wandb:                                                   val_de_mse 0.09446
wandb:                                               val_de_pearson 0.81225
wandb:                                                      val_mse 0.00312
wandb:                                                  val_pearson 0.99392
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406681_3_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/647nf25l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_003145-647nf25l/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:22
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_010243-2ngj0mms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_AdamsonWeissman2016_GSM2406681_3_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/2ngj0mms
wandb: WARNING Serializing object of type ndarray that is 20738176 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5286
Epoch 1 Step 51 Train Loss: 0.5290
Epoch 1 Step 101 Train Loss: 0.6699
Epoch 1 Step 151 Train Loss: 0.4738
Epoch 1 Step 201 Train Loss: 0.4623
Epoch 1 Step 251 Train Loss: 0.5625
Epoch 1 Step 301 Train Loss: 0.5786
Epoch 1 Step 351 Train Loss: 0.6288
Epoch 1 Step 401 Train Loss: 0.6341
Epoch 1 Step 451 Train Loss: 0.5926
Epoch 1 Step 501 Train Loss: 0.5689
Epoch 1 Step 551 Train Loss: 0.6008
Epoch 1 Step 601 Train Loss: 0.5114
Epoch 1 Step 651 Train Loss: 0.5406
Epoch 1 Step 701 Train Loss: 0.4648
Epoch 1 Step 751 Train Loss: 0.5074
Epoch 1 Step 801 Train Loss: 0.4744
Epoch 1 Step 851 Train Loss: 0.4906
Epoch 1 Step 901 Train Loss: 0.5554
Epoch 1 Step 951 Train Loss: 0.4688
Epoch 1 Step 1001 Train Loss: 0.5341
Epoch 1 Step 1051 Train Loss: 0.4691
Epoch 1 Step 1101 Train Loss: 0.5754
Epoch 1 Step 1151 Train Loss: 0.5723
Epoch 1 Step 1201 Train Loss: 0.4883
Epoch 1 Step 1251 Train Loss: 0.4626
Epoch 1 Step 1301 Train Loss: 0.5018
Epoch 1: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0958 Validation Top 20 DE MSE: 0.1804. 
Epoch 2 Step 1 Train Loss: 0.5511
Epoch 2 Step 51 Train Loss: 0.4760
Epoch 2 Step 101 Train Loss: 0.5749
Epoch 2 Step 151 Train Loss: 0.4620
Epoch 2 Step 201 Train Loss: 0.5715
Epoch 2 Step 251 Train Loss: 0.5516
Epoch 2 Step 301 Train Loss: 0.4981
Epoch 2 Step 351 Train Loss: 0.5168
Epoch 2 Step 401 Train Loss: 0.4606
Epoch 2 Step 451 Train Loss: 0.5570
Epoch 2 Step 501 Train Loss: 0.4735
Epoch 2 Step 551 Train Loss: 0.5122
Epoch 2 Step 601 Train Loss: 0.4864
Epoch 2 Step 651 Train Loss: 0.5327
Epoch 2 Step 701 Train Loss: 0.5596
Epoch 2 Step 751 Train Loss: 0.4980
Epoch 2 Step 801 Train Loss: 0.5462
Epoch 2 Step 851 Train Loss: 0.4921
Epoch 2 Step 901 Train Loss: 0.4600
Epoch 2 Step 951 Train Loss: 0.4295
Epoch 2 Step 1001 Train Loss: 0.4534
Epoch 2 Step 1051 Train Loss: 0.4312
Epoch 2 Step 1101 Train Loss: 0.5131
Epoch 2 Step 1151 Train Loss: 0.5463
Epoch 2 Step 1201 Train Loss: 0.6214
Epoch 2 Step 1251 Train Loss: 0.5535
Epoch 2 Step 1301 Train Loss: 0.5048
Epoch 2: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0733 Validation Top 20 DE MSE: 0.1709. 
Epoch 3 Step 1 Train Loss: 0.4954
Epoch 3 Step 51 Train Loss: 0.5123
Epoch 3 Step 101 Train Loss: 0.5635
Epoch 3 Step 151 Train Loss: 0.5672
Epoch 3 Step 201 Train Loss: 0.5201
Epoch 3 Step 251 Train Loss: 0.4655
Epoch 3 Step 301 Train Loss: 0.5186
Epoch 3 Step 351 Train Loss: 0.5305
Epoch 3 Step 401 Train Loss: 0.4405
Epoch 3 Step 451 Train Loss: 0.5236
Epoch 3 Step 501 Train Loss: 0.5026
Epoch 3 Step 551 Train Loss: 0.4913
Epoch 3 Step 601 Train Loss: 0.4533
Epoch 3 Step 651 Train Loss: 0.5159
Epoch 3 Step 701 Train Loss: 0.4944
Epoch 3 Step 751 Train Loss: 0.5681
Epoch 3 Step 801 Train Loss: 0.4831
Epoch 3 Step 851 Train Loss: 0.5125
Epoch 3 Step 901 Train Loss: 0.4769
Epoch 3 Step 951 Train Loss: 0.5088
Epoch 3 Step 1001 Train Loss: 0.5383
Epoch 3 Step 1051 Train Loss: 0.4701
Epoch 3 Step 1101 Train Loss: 0.5035
Epoch 3 Step 1151 Train Loss: 0.5714
Epoch 3 Step 1201 Train Loss: 0.4614
Epoch 3 Step 1251 Train Loss: 0.5108
Epoch 3 Step 1301 Train Loss: 0.5444
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0752 Validation Top 20 DE MSE: 0.1642. 
Epoch 4 Step 1 Train Loss: 0.4925
Epoch 4 Step 51 Train Loss: 0.4734
Epoch 4 Step 101 Train Loss: 0.4948
Epoch 4 Step 151 Train Loss: 0.5220
Epoch 4 Step 201 Train Loss: 0.5236
Epoch 4 Step 251 Train Loss: 0.5316
Epoch 4 Step 301 Train Loss: 0.5678
Epoch 4 Step 351 Train Loss: 0.6009
Epoch 4 Step 401 Train Loss: 0.5048
Epoch 4 Step 451 Train Loss: 0.4704
Epoch 4 Step 501 Train Loss: 0.4862
Epoch 4 Step 551 Train Loss: 0.4896
Epoch 4 Step 601 Train Loss: 0.4184
Epoch 4 Step 651 Train Loss: 0.4612
Epoch 4 Step 701 Train Loss: 0.4791
Epoch 4 Step 751 Train Loss: 0.5418
Epoch 4 Step 801 Train Loss: 0.5459
Epoch 4 Step 851 Train Loss: 0.4717
Epoch 4 Step 901 Train Loss: 0.4722
Epoch 4 Step 951 Train Loss: 0.4537
Epoch 4 Step 1001 Train Loss: 0.4711
Epoch 4 Step 1051 Train Loss: 0.5317
Epoch 4 Step 1101 Train Loss: 0.5306
Epoch 4 Step 1151 Train Loss: 0.5106
Epoch 4 Step 1201 Train Loss: 0.5518
Epoch 4 Step 1251 Train Loss: 0.4845
Epoch 4 Step 1301 Train Loss: 0.5024
Epoch 4: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0686 Validation Top 20 DE MSE: 0.1604. 
Epoch 5 Step 1 Train Loss: 0.4848
Epoch 5 Step 51 Train Loss: 0.5604
Epoch 5 Step 101 Train Loss: 0.5979
Epoch 5 Step 151 Train Loss: 0.4630
Epoch 5 Step 201 Train Loss: 0.5068
Epoch 5 Step 251 Train Loss: 0.5058
Epoch 5 Step 301 Train Loss: 0.4540
Epoch 5 Step 351 Train Loss: 0.5010
Epoch 5 Step 401 Train Loss: 0.5026
Epoch 5 Step 451 Train Loss: 0.5162
Epoch 5 Step 501 Train Loss: 0.6223
Epoch 5 Step 551 Train Loss: 0.6400
Epoch 5 Step 601 Train Loss: 0.5037
Epoch 5 Step 651 Train Loss: 0.6214
Epoch 5 Step 701 Train Loss: 0.5375
Epoch 5 Step 751 Train Loss: 0.5009
Epoch 5 Step 801 Train Loss: 0.4915
Epoch 5 Step 851 Train Loss: 0.5155
Epoch 5 Step 901 Train Loss: 0.5007
Epoch 5 Step 951 Train Loss: 0.4860
Epoch 5 Step 1001 Train Loss: 0.5745
Epoch 5 Step 1051 Train Loss: 0.5071
Epoch 5 Step 1101 Train Loss: 0.5390
Epoch 5 Step 1151 Train Loss: 0.5398
Epoch 5 Step 1201 Train Loss: 0.4989
Epoch 5 Step 1251 Train Loss: 0.4592
Epoch 5 Step 1301 Train Loss: 0.4637
Epoch 5: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0678 Validation Top 20 DE MSE: 0.1625. 
Epoch 6 Step 1 Train Loss: 0.4934
Epoch 6 Step 51 Train Loss: 0.5546
Epoch 6 Step 101 Train Loss: 0.5951
Epoch 6 Step 151 Train Loss: 0.5360
Epoch 6 Step 201 Train Loss: 0.4543
Epoch 6 Step 251 Train Loss: 0.5419
Epoch 6 Step 301 Train Loss: 0.4720
Epoch 6 Step 351 Train Loss: 0.5362
Epoch 6 Step 401 Train Loss: 0.4627
Epoch 6 Step 451 Train Loss: 0.5555
Epoch 6 Step 501 Train Loss: 0.6017
Epoch 6 Step 551 Train Loss: 0.4717
Epoch 6 Step 601 Train Loss: 0.4518
Epoch 6 Step 651 Train Loss: 0.4802
Epoch 6 Step 701 Train Loss: 0.5568
Epoch 6 Step 751 Train Loss: 0.5318
Epoch 6 Step 801 Train Loss: 0.4492
Epoch 6 Step 851 Train Loss: 0.5444
Epoch 6 Step 901 Train Loss: 0.5281
Epoch 6 Step 951 Train Loss: 0.5155
Epoch 6 Step 1001 Train Loss: 0.5073
Epoch 6 Step 1051 Train Loss: 0.4737
Epoch 6 Step 1101 Train Loss: 0.4945
Epoch 6 Step 1151 Train Loss: 0.5676
Epoch 6 Step 1201 Train Loss: 0.5252
Epoch 6 Step 1251 Train Loss: 0.4905
Epoch 6 Step 1301 Train Loss: 0.5459
Epoch 6: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0670 Validation Top 20 DE MSE: 0.1592. 
Epoch 7 Step 1 Train Loss: 0.5858
Epoch 7 Step 51 Train Loss: 0.5006
Epoch 7 Step 101 Train Loss: 0.5322
Epoch 7 Step 151 Train Loss: 0.5284
Epoch 7 Step 201 Train Loss: 0.4946
Epoch 7 Step 251 Train Loss: 0.6248
Epoch 7 Step 301 Train Loss: 0.5251
Epoch 7 Step 351 Train Loss: 0.5230
Epoch 7 Step 401 Train Loss: 0.5576
Epoch 7 Step 451 Train Loss: 0.5549
Epoch 7 Step 501 Train Loss: 0.4687
Epoch 7 Step 551 Train Loss: 0.5621
Epoch 7 Step 601 Train Loss: 0.5382
Epoch 7 Step 651 Train Loss: 0.4729
Epoch 7 Step 701 Train Loss: 0.4889
Epoch 7 Step 751 Train Loss: 0.5154
Epoch 7 Step 801 Train Loss: 0.4778
Epoch 7 Step 851 Train Loss: 0.6023
Epoch 7 Step 901 Train Loss: 0.5881
Epoch 7 Step 951 Train Loss: 0.5101
Epoch 7 Step 1001 Train Loss: 0.5673
Epoch 7 Step 1051 Train Loss: 0.5026
Epoch 7 Step 1101 Train Loss: 0.6931
Epoch 7 Step 1151 Train Loss: 0.4665
Epoch 7 Step 1201 Train Loss: 0.4678
Epoch 7 Step 1251 Train Loss: 0.4967
Epoch 7 Step 1301 Train Loss: 0.5645
Epoch 7: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0666 Validation Top 20 DE MSE: 0.1616. 
Epoch 8 Step 1 Train Loss: 0.5066
Epoch 8 Step 51 Train Loss: 0.5000
Epoch 8 Step 101 Train Loss: 0.5192
Epoch 8 Step 151 Train Loss: 0.4638
Epoch 8 Step 201 Train Loss: 0.5583
Epoch 8 Step 251 Train Loss: 0.5180
Epoch 8 Step 301 Train Loss: 0.5079
Epoch 8 Step 351 Train Loss: 0.4998
Epoch 8 Step 401 Train Loss: 0.4607
Epoch 8 Step 451 Train Loss: 0.5023
Epoch 8 Step 501 Train Loss: 0.4806
Epoch 8 Step 551 Train Loss: 0.5413
Epoch 8 Step 601 Train Loss: 0.5443
Epoch 8 Step 651 Train Loss: 0.5000
Epoch 8 Step 701 Train Loss: 0.4813
Epoch 8 Step 751 Train Loss: 0.5297
Epoch 8 Step 801 Train Loss: 0.5025
Epoch 8 Step 851 Train Loss: 0.5761
Epoch 8 Step 901 Train Loss: 0.5625
Epoch 8 Step 951 Train Loss: 0.4527
Epoch 8 Step 1001 Train Loss: 0.5317
Epoch 8 Step 1051 Train Loss: 0.4495
Epoch 8 Step 1101 Train Loss: 0.5393
Epoch 8 Step 1151 Train Loss: 0.5868
Epoch 8 Step 1201 Train Loss: 0.5472
Epoch 8 Step 1251 Train Loss: 0.5122
Epoch 8 Step 1301 Train Loss: 0.5287
Epoch 8: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0644 Validation Top 20 DE MSE: 0.1603. 
Epoch 9 Step 1 Train Loss: 0.5449
Epoch 9 Step 51 Train Loss: 0.5121
Epoch 9 Step 101 Train Loss: 0.5459
Epoch 9 Step 151 Train Loss: 0.4750
Epoch 9 Step 201 Train Loss: 0.5435
Epoch 9 Step 251 Train Loss: 0.4955
Epoch 9 Step 301 Train Loss: 0.5681
Epoch 9 Step 351 Train Loss: 0.4725
Epoch 9 Step 401 Train Loss: 0.5717
Epoch 9 Step 451 Train Loss: 0.4901
Epoch 9 Step 501 Train Loss: 0.5250
Epoch 9 Step 551 Train Loss: 0.5635
Epoch 9 Step 601 Train Loss: 0.4991
Epoch 9 Step 651 Train Loss: 0.5392
Epoch 9 Step 701 Train Loss: 0.4878
Epoch 9 Step 751 Train Loss: 0.4703
Epoch 9 Step 801 Train Loss: 0.4500
Epoch 9 Step 851 Train Loss: 0.5458
Epoch 9 Step 901 Train Loss: 0.5240
Epoch 9 Step 951 Train Loss: 0.5062
Epoch 9 Step 1001 Train Loss: 0.5480
Epoch 9 Step 1051 Train Loss: 0.5671
Epoch 9 Step 1101 Train Loss: 0.5728
Epoch 9 Step 1151 Train Loss: 0.6163
Epoch 9 Step 1201 Train Loss: 0.4958
Epoch 9 Step 1251 Train Loss: 0.5681
Epoch 9 Step 1301 Train Loss: 0.4900
Epoch 9: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0655 Validation Top 20 DE MSE: 0.1606. 
Epoch 10 Step 1 Train Loss: 0.5339
Epoch 10 Step 51 Train Loss: 0.5976
Epoch 10 Step 101 Train Loss: 0.4956
Epoch 10 Step 151 Train Loss: 0.4712
Epoch 10 Step 201 Train Loss: 0.6104
Epoch 10 Step 251 Train Loss: 0.5211
Epoch 10 Step 301 Train Loss: 0.5449
Epoch 10 Step 351 Train Loss: 0.5223
Epoch 10 Step 401 Train Loss: 0.4713
Epoch 10 Step 451 Train Loss: 0.5703
Epoch 10 Step 501 Train Loss: 0.5439
Epoch 10 Step 551 Train Loss: 0.5414
Epoch 10 Step 601 Train Loss: 0.5445
Epoch 10 Step 651 Train Loss: 0.5371
Epoch 10 Step 701 Train Loss: 0.5070
Epoch 10 Step 751 Train Loss: 0.5037
Epoch 10 Step 801 Train Loss: 0.5859
Epoch 10 Step 851 Train Loss: 0.4830
Epoch 10 Step 901 Train Loss: 0.4997
Epoch 10 Step 951 Train Loss: 0.6274
Epoch 10 Step 1001 Train Loss: 0.5677
Epoch 10 Step 1051 Train Loss: 0.5476
Epoch 10 Step 1101 Train Loss: 0.5892
Epoch 10 Step 1151 Train Loss: 0.5029
Epoch 10 Step 1201 Train Loss: 0.4630
Epoch 10 Step 1251 Train Loss: 0.5182
Epoch 10 Step 1301 Train Loss: 0.5114
Epoch 10: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0673 Validation Top 20 DE MSE: 0.1623. 
Epoch 11 Step 1 Train Loss: 0.5436
Epoch 11 Step 51 Train Loss: 0.4555
Epoch 11 Step 101 Train Loss: 0.5000
Epoch 11 Step 151 Train Loss: 0.5299
Epoch 11 Step 201 Train Loss: 0.4506
Epoch 11 Step 251 Train Loss: 0.5414
Epoch 11 Step 301 Train Loss: 0.5072
Epoch 11 Step 351 Train Loss: 0.5074
Epoch 11 Step 401 Train Loss: 0.5855
Epoch 11 Step 451 Train Loss: 0.5140
Epoch 11 Step 501 Train Loss: 0.5211
Epoch 11 Step 551 Train Loss: 0.4620
Epoch 11 Step 601 Train Loss: 0.5626
Epoch 11 Step 651 Train Loss: 0.5407
Epoch 11 Step 701 Train Loss: 0.5893
Epoch 11 Step 751 Train Loss: 0.5658
Epoch 11 Step 801 Train Loss: 0.5436
Epoch 11 Step 851 Train Loss: 0.5197
Epoch 11 Step 901 Train Loss: 0.5574
Epoch 11 Step 951 Train Loss: 0.4754
Epoch 11 Step 1001 Train Loss: 0.5007
Epoch 11 Step 1051 Train Loss: 0.5308
Epoch 11 Step 1101 Train Loss: 0.5056
Epoch 11 Step 1151 Train Loss: 0.4662
Epoch 11 Step 1201 Train Loss: 0.5054
Epoch 11 Step 1251 Train Loss: 0.5412
Epoch 11 Step 1301 Train Loss: 0.5362
Epoch 11: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0662 Validation Top 20 DE MSE: 0.1617. 
Epoch 12 Step 1 Train Loss: 0.5516
Epoch 12 Step 51 Train Loss: 0.4945
Epoch 12 Step 101 Train Loss: 0.4744
Epoch 12 Step 151 Train Loss: 0.5149
Epoch 12 Step 201 Train Loss: 0.5441
Epoch 12 Step 251 Train Loss: 0.5357
Epoch 12 Step 301 Train Loss: 0.4639
Epoch 12 Step 351 Train Loss: 0.5358
Epoch 12 Step 401 Train Loss: 0.4785
Epoch 12 Step 451 Train Loss: 0.4831
Epoch 12 Step 501 Train Loss: 0.4863
Epoch 12 Step 551 Train Loss: 0.5681
Epoch 12 Step 601 Train Loss: 0.5170
Epoch 12 Step 651 Train Loss: 0.6510
Epoch 12 Step 701 Train Loss: 0.5313
Epoch 12 Step 751 Train Loss: 0.5214
Epoch 12 Step 801 Train Loss: 0.4846
Epoch 12 Step 851 Train Loss: 0.5153
Epoch 12 Step 901 Train Loss: 0.5390
Epoch 12 Step 951 Train Loss: 0.5819
Epoch 12 Step 1001 Train Loss: 0.4950
Epoch 12 Step 1051 Train Loss: 0.6098
Epoch 12 Step 1101 Train Loss: 0.5688
Epoch 12 Step 1151 Train Loss: 0.4257
Epoch 12 Step 1201 Train Loss: 0.5539
Epoch 12 Step 1251 Train Loss: 0.5255
Epoch 12 Step 1301 Train Loss: 0.4992
Epoch 12: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0628 Validation Top 20 DE MSE: 0.1584. 
Epoch 13 Step 1 Train Loss: 0.5765
Epoch 13 Step 51 Train Loss: 0.4996
Epoch 13 Step 101 Train Loss: 0.5299
Epoch 13 Step 151 Train Loss: 0.5413
Epoch 13 Step 201 Train Loss: 0.4701
Epoch 13 Step 251 Train Loss: 0.5554
Epoch 13 Step 301 Train Loss: 0.4769
Epoch 13 Step 351 Train Loss: 0.5250
Epoch 13 Step 401 Train Loss: 0.5389
Epoch 13 Step 451 Train Loss: 0.4854
Epoch 13 Step 501 Train Loss: 0.6590
Epoch 13 Step 551 Train Loss: 0.5402
Epoch 13 Step 601 Train Loss: 0.5714
Epoch 13 Step 651 Train Loss: 0.5219
Epoch 13 Step 701 Train Loss: 0.5387
Epoch 13 Step 751 Train Loss: 0.5928
Epoch 13 Step 801 Train Loss: 0.4968
Epoch 13 Step 851 Train Loss: 0.4603
Epoch 13 Step 901 Train Loss: 0.5078
Epoch 13 Step 951 Train Loss: 0.5096
Epoch 13 Step 1001 Train Loss: 0.5408
Epoch 13 Step 1051 Train Loss: 0.4596
Epoch 13 Step 1101 Train Loss: 0.5106
Epoch 13 Step 1151 Train Loss: 0.4946
Epoch 13 Step 1201 Train Loss: 0.5327
Epoch 13 Step 1251 Train Loss: 0.4117
Epoch 13 Step 1301 Train Loss: 0.4914
Epoch 13: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0683 Validation Top 20 DE MSE: 0.1635. 
Epoch 14 Step 1 Train Loss: 0.6611
Epoch 14 Step 51 Train Loss: 0.6704
Epoch 14 Step 101 Train Loss: 0.5790
Epoch 14 Step 151 Train Loss: 0.5182
Epoch 14 Step 201 Train Loss: 0.4693
Epoch 14 Step 251 Train Loss: 0.5377
Epoch 14 Step 301 Train Loss: 0.5163
Epoch 14 Step 351 Train Loss: 0.5037
Epoch 14 Step 401 Train Loss: 0.5842
Epoch 14 Step 451 Train Loss: 0.5089
Epoch 14 Step 501 Train Loss: 0.4707
Epoch 14 Step 551 Train Loss: 0.4620
Epoch 14 Step 601 Train Loss: 0.4942
Epoch 14 Step 651 Train Loss: 0.5017
Epoch 14 Step 701 Train Loss: 0.5635
Epoch 14 Step 751 Train Loss: 0.5683
Epoch 14 Step 801 Train Loss: 0.4975
Epoch 14 Step 851 Train Loss: 0.5093
Epoch 14 Step 901 Train Loss: 0.5794
Epoch 14 Step 951 Train Loss: 0.4991
Epoch 14 Step 1001 Train Loss: 0.4865
Epoch 14 Step 1051 Train Loss: 0.5607
Epoch 14 Step 1101 Train Loss: 0.4998
Epoch 14 Step 1151 Train Loss: 0.5618
Epoch 14 Step 1201 Train Loss: 0.5781
Epoch 14 Step 1251 Train Loss: 0.5604
Epoch 14 Step 1301 Train Loss: 0.5088
Epoch 14: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0655 Validation Top 20 DE MSE: 0.1604. 
Epoch 15 Step 1 Train Loss: 0.5447
Epoch 15 Step 51 Train Loss: 0.4820
Epoch 15 Step 101 Train Loss: 0.5276
Epoch 15 Step 151 Train Loss: 0.5676
Epoch 15 Step 201 Train Loss: 0.4874
Epoch 15 Step 251 Train Loss: 0.6384
Epoch 15 Step 301 Train Loss: 0.5108
Epoch 15 Step 351 Train Loss: 0.5094
Epoch 15 Step 401 Train Loss: 0.5847
Epoch 15 Step 451 Train Loss: 0.4271
Epoch 15 Step 501 Train Loss: 0.5556
Epoch 15 Step 551 Train Loss: 0.5827
Epoch 15 Step 601 Train Loss: 0.5214
Epoch 15 Step 651 Train Loss: 0.6119
Epoch 15 Step 701 Train Loss: 0.4417
Epoch 15 Step 751 Train Loss: 0.4664
Epoch 15 Step 801 Train Loss: 0.5918
Epoch 15 Step 851 Train Loss: 0.5117
Epoch 15 Step 901 Train Loss: 0.5305
Epoch 15 Step 951 Train Loss: 0.5192
Epoch 15 Step 1001 Train Loss: 0.4470
Epoch 15 Step 1051 Train Loss: 0.5133
Epoch 15 Step 1101 Train Loss: 0.5169
Epoch 15 Step 1151 Train Loss: 0.5332
Epoch 15 Step 1201 Train Loss: 0.5178
Epoch 15 Step 1251 Train Loss: 0.4918
Epoch 15 Step 1301 Train Loss: 0.5720
Epoch 15: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0643 Validation Top 20 DE MSE: 0.1602. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1495
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.002319878
test_unseen_single_pearson: 0.9955824035801076
test_unseen_single_mse_de: 0.1495069
test_unseen_single_pearson_de: 0.9614671402351376
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.33515626230651685
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2772727272727273
test_unseen_single_frac_sigma_below_1_non_dropout: 0.918181818181818
test_unseen_single_mse_top20_de_non_dropout: 0.15087485
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.032 MB uploadedwandb: | 0.003 MB of 0.032 MB uploadedwandb: / 0.032 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: \ 0.032 MB of 0.032 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñà‚ñÖ
wandb:                                                   val_de_mse ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñÇ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.14951
wandb:                                              test_de_pearson 0.96147
wandb:               test_frac_opposite_direction_top20_non_dropout 0.27727
wandb:                          test_frac_sigma_below_1_non_dropout 0.91818
wandb:                                                     test_mse 0.00232
wandb:                                test_mse_top20_de_non_dropout 0.15087
wandb:                                                 test_pearson 0.99558
wandb:                                           test_pearson_delta 0.33516
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.27727
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.91818
wandb:                                       test_unseen_single_mse 0.00232
wandb:                                    test_unseen_single_mse_de 0.14951
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15087
wandb:                                   test_unseen_single_pearson 0.99558
wandb:                                test_unseen_single_pearson_de 0.96147
wandb:                             test_unseen_single_pearson_delta 0.33516
wandb:                                                 train_de_mse 0.06427
wandb:                                             train_de_pearson 0.87679
wandb:                                                    train_mse 0.00186
wandb:                                                train_pearson 0.99653
wandb:                                                training_loss 0.46406
wandb:                                                   val_de_mse 0.16016
wandb:                                               val_de_pearson 0.96457
wandb:                                                      val_mse 0.0028
wandb:                                                  val_pearson 0.99464
wandb: 
wandb: üöÄ View run geneformer_AdamsonWeissman2016_GSM2406681_3_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/2ngj0mms
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_010243-2ngj0mms/logs
