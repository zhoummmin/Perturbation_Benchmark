Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:8
Done!
Creating dataloaders....
Done!
Traceback (most recent call last):
  File "/home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/datlingerbock2017_sti.py", line 97, in <module>
    with open("./Geneformer_gene_embeddings.pickle", "rb") as fp:
FileNotFoundError: [Errno 2] No such file or directory: './Geneformer_gene_embeddings.pickle'
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:8
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_221839-7uh7e6ib
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2017_unstimulated_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/7uh7e6ib
wandb: WARNING Serializing object of type ndarray that is 20566144 bytes
  0%|                                                  | 0/3129 [00:00<?, ?it/s]  0%|                                          | 9/3129 [00:00<00:36, 84.71it/s]  1%|‚ñè                                        | 19/3129 [00:00<00:33, 91.67it/s]  1%|‚ñç                                       | 31/3129 [00:00<00:30, 103.09it/s]  1%|‚ñå                                       | 43/3129 [00:00<00:28, 108.37it/s]  2%|‚ñã                                       | 55/3129 [00:00<00:27, 112.06it/s]  2%|‚ñä                                       | 67/3129 [00:00<00:27, 111.04it/s]  3%|‚ñà                                       | 80/3129 [00:00<00:26, 113.40it/s]  3%|‚ñà‚ñè                                      | 92/3129 [00:00<00:27, 112.09it/s]  3%|‚ñà‚ñé                                     | 105/3129 [00:00<00:26, 115.07it/s]  4%|‚ñà‚ñç                                     | 117/3129 [00:01<00:26, 114.29it/s]  4%|‚ñà‚ñå                                     | 129/3129 [00:01<00:26, 113.79it/s]  5%|‚ñà‚ñä                                     | 141/3129 [00:01<00:26, 113.91it/s]  5%|‚ñà‚ñâ                                     | 153/3129 [00:01<00:26, 113.34it/s]  5%|‚ñà‚ñà                                     | 165/3129 [00:01<00:27, 109.26it/s]  6%|‚ñà‚ñà‚ñè                                    | 177/3129 [00:01<00:26, 112.00it/s]  6%|‚ñà‚ñà‚ñé                                    | 189/3129 [00:01<00:26, 109.63it/s]  6%|‚ñà‚ñà‚ñå                                    | 202/3129 [00:01<00:25, 112.87it/s]  7%|‚ñà‚ñà‚ñã                                    | 214/3129 [00:01<00:26, 110.25it/s]  7%|‚ñà‚ñà‚ñä                                    | 226/3129 [00:02<00:26, 111.23it/s]  8%|‚ñà‚ñà‚ñâ                                    | 238/3129 [00:02<00:25, 112.97it/s]  8%|‚ñà‚ñà‚ñà                                    | 250/3129 [00:02<00:25, 112.15it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                   | 262/3129 [00:02<00:25, 111.45it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                   | 274/3129 [00:02<00:26, 109.14it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                   | 286/3129 [00:02<00:25, 110.15it/s] 10%|‚ñà‚ñà‚ñà‚ñã                                   | 299/3129 [00:02<00:24, 113.50it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                   | 311/3129 [00:02<00:25, 112.63it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                   | 323/3129 [00:02<00:25, 112.06it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 335/3129 [00:03<00:24, 112.11it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 347/3129 [00:03<00:25, 109.51it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 360/3129 [00:03<00:24, 113.22it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 372/3129 [00:03<00:24, 112.40it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 384/3129 [00:03<00:24, 111.49it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 396/3129 [00:03<00:25, 107.71it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 408/3129 [00:03<00:24, 109.31it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 421/3129 [00:03<00:24, 112.73it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 433/3129 [00:03<00:24, 108.96it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 446/3129 [00:04<00:23, 112.56it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 458/3129 [00:04<00:24, 109.29it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 469/3129 [00:04<00:24, 109.24it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 481/3129 [00:04<00:24, 109.80it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 494/3129 [00:04<00:23, 112.67it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 507/3129 [00:04<00:22, 115.77it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 519/3129 [00:04<00:23, 112.03it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 531/3129 [00:04<00:24, 107.60it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 542/3129 [00:04<00:25, 102.11it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 553/3129 [00:05<00:25, 100.37it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 564/3129 [00:05<00:26, 95.92it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 575/3129 [00:05<00:26, 97.90it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 585/3129 [00:05<00:26, 97.51it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 595/3129 [00:05<00:26, 96.68it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 605/3129 [00:05<00:27, 93.47it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 615/3129 [00:05<00:26, 93.58it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 626/3129 [00:05<00:25, 96.44it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 636/3129 [00:05<00:25, 96.14it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 646/3129 [00:06<00:25, 96.05it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 656/3129 [00:06<00:26, 93.23it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 667/3129 [00:06<00:25, 96.76it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 677/3129 [00:06<00:26, 93.54it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 688/3129 [00:06<00:25, 95.98it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 698/3129 [00:06<00:25, 95.66it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 708/3129 [00:06<00:25, 95.44it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 718/3129 [00:06<00:25, 95.74it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 728/3129 [00:06<00:25, 95.55it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 738/3129 [00:06<00:25, 95.10it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 748/3129 [00:07<00:25, 95.17it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 758/3129 [00:07<00:25, 94.74it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 768/3129 [00:07<00:25, 94.26it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 778/3129 [00:07<00:24, 94.47it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 788/3129 [00:07<00:24, 94.63it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 798/3129 [00:07<00:24, 94.93it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 808/3129 [00:07<00:25, 92.54it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 819/3129 [00:07<00:24, 95.89it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 829/3129 [00:07<00:24, 95.41it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 839/3129 [00:08<00:23, 95.60it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 849/3129 [00:08<00:25, 89.63it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 860/3129 [00:08<00:24, 94.20it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 870/3129 [00:08<00:23, 94.30it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 881/3129 [00:08<00:23, 97.59it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 891/3129 [00:08<00:23, 96.81it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 901/3129 [00:08<00:23, 95.92it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 911/3129 [00:08<00:23, 95.20it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 921/3129 [00:08<00:23, 92.07it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 932/3129 [00:09<00:22, 95.75it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 942/3129 [00:09<00:22, 95.70it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 952/3129 [00:09<00:23, 93.21it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 963/3129 [00:09<00:22, 96.95it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 974/3129 [00:09<00:21, 98.60it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 985/3129 [00:09<00:21, 97.61it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 995/3129 [00:09<00:22, 96.23it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1008/3129 [00:09<00:20, 105.11it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1019/3129 [00:09<00:20, 105.39it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1030/3129 [00:09<00:19, 106.17it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1041/3129 [00:10<00:19, 106.78it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1052/3129 [00:10<00:19, 104.66it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1064/3129 [00:10<00:19, 105.07it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1076/3129 [00:10<00:19, 107.79it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1088/3129 [00:10<00:18, 108.72it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1099/3129 [00:10<00:19, 106.53it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1111/3129 [00:10<00:18, 107.66it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1123/3129 [00:10<00:18, 108.48it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1136/3129 [00:10<00:18, 109.17it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1147/3129 [00:11<00:18, 108.77it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1160/3129 [00:11<00:17, 111.99it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1172/3129 [00:11<00:18, 108.68it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1184/3129 [00:11<00:17, 111.52it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1196/3129 [00:11<00:17, 111.55it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1208/3129 [00:11<00:34, 55.27it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1227/3129 [00:12<00:27, 68.93it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1267/3129 [00:12<00:15, 122.08it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1284/3129 [00:12<00:15, 117.27it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1299/3129 [00:12<00:26, 69.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1311/3129 [00:13<00:39, 46.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1364/3129 [00:13<00:18, 97.14it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1385/3129 [00:13<00:17, 99.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1403/3129 [00:13<00:17, 98.57it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1419/3129 [00:14<00:16, 104.03it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1434/3129 [00:14<00:16, 104.59it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1448/3129 [00:14<00:16, 103.75it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1461/3129 [00:14<00:16, 99.24it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1473/3129 [00:14<00:19, 84.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1486/3129 [00:14<00:17, 93.02it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1497/3129 [00:14<00:17, 94.83it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1508/3129 [00:15<00:17, 90.33it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1519/3129 [00:15<00:17, 93.47it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1529/3129 [00:15<00:16, 94.60it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1540/3129 [00:15<00:16, 97.09it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1552/3129 [00:15<00:15, 99.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1563/3129 [00:15<00:15, 101.83it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1574/3129 [00:15<00:15, 102.80it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1585/3129 [00:15<00:15, 102.74it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1596/3129 [00:15<00:16, 91.57it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1608/3129 [00:16<00:31, 48.40it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1620/3129 [00:16<00:33, 44.92it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1681/3129 [00:16<00:11, 123.03it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1702/3129 [00:17<00:12, 118.36it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1720/3129 [00:17<00:12, 111.23it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1736/3129 [00:17<00:12, 108.17it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1750/3129 [00:17<00:12, 107.27it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 1763/3129 [00:17<00:13, 103.88it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1775/3129 [00:17<00:13, 100.66it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 1786/3129 [00:17<00:13, 101.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 1797/3129 [00:18<00:13, 101.77it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1808/3129 [00:18<00:13, 99.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1820/3129 [00:18<00:12, 102.41it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 1831/3129 [00:18<00:12, 101.98it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 1842/3129 [00:18<00:12, 101.88it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1853/3129 [00:18<00:13, 98.09it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 1864/3129 [00:18<00:12, 99.45it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 1875/3129 [00:18<00:12, 101.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 1886/3129 [00:18<00:12, 100.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 1897/3129 [00:19<00:12, 96.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 1907/3129 [00:19<00:12, 94.81it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 1918/3129 [00:19<00:12, 96.90it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 1928/3129 [00:19<00:12, 96.86it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 1938/3129 [00:19<00:12, 95.88it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 1948/3129 [00:19<00:12, 94.14it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 1959/3129 [00:19<00:11, 97.70it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 1969/3129 [00:19<00:11, 96.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 1980/3129 [00:19<00:11, 98.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 1991/3129 [00:20<00:11, 99.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2002/3129 [00:20<00:11, 100.95it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2013/3129 [00:20<00:10, 102.21it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2024/3129 [00:20<00:11, 97.44it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2035/3129 [00:20<00:11, 96.58it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2045/3129 [00:20<00:11, 95.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2057/3129 [00:20<00:10, 100.74it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2068/3129 [00:20<00:10, 99.68it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2078/3129 [00:20<00:10, 99.29it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2088/3129 [00:21<00:10, 96.30it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2099/3129 [00:21<00:10, 97.36it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2111/3129 [00:21<00:09, 102.47it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2122/3129 [00:21<00:09, 101.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2133/3129 [00:21<00:09, 100.13it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2145/3129 [00:21<00:09, 104.15it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2156/3129 [00:21<00:09, 99.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2167/3129 [00:21<00:09, 98.76it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2177/3129 [00:21<00:09, 97.05it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2188/3129 [00:22<00:09, 98.55it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2198/3129 [00:22<00:09, 97.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2209/3129 [00:22<00:09, 94.06it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2221/3129 [00:22<00:09, 99.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2232/3129 [00:22<00:09, 96.14it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2242/3129 [00:22<00:09, 95.22it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2252/3129 [00:22<00:09, 93.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2262/3129 [00:22<00:09, 92.74it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2273/3129 [00:22<00:08, 96.66it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2286/3129 [00:23<00:07, 105.60it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2300/3129 [00:23<00:07, 111.92it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2315/3129 [00:23<00:06, 120.24it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2329/3129 [00:23<00:06, 120.54it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2342/3129 [00:23<00:06, 123.01it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2355/3129 [00:23<00:06, 122.34it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2368/3129 [00:23<00:06, 121.17it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2381/3129 [00:23<00:06, 120.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2394/3129 [00:23<00:06, 120.22it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2408/3129 [00:24<00:05, 120.65it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2422/3129 [00:24<00:05, 123.26it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2435/3129 [00:24<00:05, 122.77it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2448/3129 [00:24<00:05, 119.51it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2460/3129 [00:24<00:05, 119.61it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2474/3129 [00:24<00:05, 120.59it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2487/3129 [00:24<00:05, 120.61it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2500/3129 [00:24<00:05, 121.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2513/3129 [00:24<00:04, 123.72it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2526/3129 [00:24<00:04, 123.63it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2539/3129 [00:25<00:04, 119.79it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2552/3129 [00:25<00:04, 118.16it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2566/3129 [00:25<00:04, 121.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2579/3129 [00:25<00:04, 122.62it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2592/3129 [00:25<00:04, 114.48it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2604/3129 [00:25<00:04, 106.82it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2615/3129 [00:25<00:04, 104.76it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2626/3129 [00:25<00:04, 105.62it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2637/3129 [00:26<00:04, 103.68it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2648/3129 [00:26<00:04, 99.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 2659/3129 [00:26<00:04, 101.77it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 2670/3129 [00:26<00:04, 97.98it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2680/3129 [00:26<00:04, 98.02it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 2690/3129 [00:26<00:04, 98.07it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2702/3129 [00:26<00:04, 101.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2713/3129 [00:26<00:04, 98.20it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2723/3129 [00:26<00:04, 98.42it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 2735/3129 [00:27<00:03, 99.31it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2746/3129 [00:27<00:03, 99.74it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2756/3129 [00:27<00:03, 99.40it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2767/3129 [00:27<00:03, 99.60it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2778/3129 [00:27<00:03, 99.36it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2788/3129 [00:27<00:03, 99.32it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2799/3129 [00:27<00:03, 99.20it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2809/3129 [00:27<00:03, 98.99it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2819/3129 [00:27<00:03, 98.97it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2829/3129 [00:27<00:03, 99.04it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2839/3129 [00:28<00:02, 98.65it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2850/3129 [00:28<00:02, 98.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2860/3129 [00:28<00:02, 98.98it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2870/3129 [00:28<00:02, 98.87it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2880/3129 [00:28<00:02, 99.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2891/3129 [00:28<00:02, 102.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2902/3129 [00:28<00:02, 98.44it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2913/3129 [00:28<00:02, 101.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2924/3129 [00:28<00:01, 103.05it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2935/3129 [00:29<00:01, 103.80it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2946/3129 [00:29<00:01, 104.82it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2959/3129 [00:29<00:01, 111.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2971/3129 [00:29<00:01, 110.89it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2983/3129 [00:29<00:01, 109.40it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2996/3129 [00:29<00:01, 111.69it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3008/3129 [00:29<00:01, 112.22it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3020/3129 [00:29<00:00, 112.24it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3033/3129 [00:29<00:00, 114.14it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3047/3129 [00:29<00:00, 115.70it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3060/3129 [00:30<00:00, 117.61it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3072/3129 [00:30<00:00, 112.20it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3129 [00:30<00:00, 112.59it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3129 [00:30<00:00, 113.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3110/3129 [00:30<00:00, 113.88it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3122/3129 [00:30<00:00, 114.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3129/3129 [00:30<00:00, 101.90it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.6514
Epoch 1 Step 51 Train Loss: 0.5705
Epoch 1: Train Overall MSE: 0.0045 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0068 Validation Top 20 DE MSE: 0.0040. 
Epoch 2 Step 1 Train Loss: 0.5994
Epoch 2 Step 51 Train Loss: 0.6661
Epoch 2: Train Overall MSE: 0.0061 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0095 Validation Top 20 DE MSE: 0.0046. 
Epoch 3 Step 1 Train Loss: 0.6191
Epoch 3 Step 51 Train Loss: 0.8180
Epoch 3: Train Overall MSE: 0.0073 Validation Overall MSE: 0.0043. 
Train Top 20 DE MSE: 0.0114 Validation Top 20 DE MSE: 0.0057. 
Epoch 4 Step 1 Train Loss: 0.6213
Epoch 4 Step 51 Train Loss: 0.5328
Epoch 4: Train Overall MSE: 0.0078 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0114 Validation Top 20 DE MSE: 0.0072. 
Epoch 5 Step 1 Train Loss: 0.7342
Epoch 5 Step 51 Train Loss: 0.6212
Epoch 5: Train Overall MSE: 0.0083 Validation Overall MSE: 0.0046. 
Train Top 20 DE MSE: 0.0124 Validation Top 20 DE MSE: 0.0072. 
Epoch 6 Step 1 Train Loss: 0.7790
Epoch 6 Step 51 Train Loss: 0.5862
Epoch 6: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0048. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.0076. 
Epoch 7 Step 1 Train Loss: 0.8397
Epoch 7 Step 51 Train Loss: 0.5269
Epoch 7: Train Overall MSE: 0.0084 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.0071. 
Epoch 8 Step 1 Train Loss: 0.5480
Epoch 8 Step 51 Train Loss: 0.4695
Epoch 8: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.0081. 
Epoch 9 Step 1 Train Loss: 0.6414
Epoch 9 Step 51 Train Loss: 0.6675
Epoch 9: Train Overall MSE: 0.0088 Validation Overall MSE: 0.0053. 
Train Top 20 DE MSE: 0.0137 Validation Top 20 DE MSE: 0.0090. 
Epoch 10 Step 1 Train Loss: 0.5673
Epoch 10 Step 51 Train Loss: 0.5798
Epoch 10: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0132 Validation Top 20 DE MSE: 0.0077. 
Epoch 11 Step 1 Train Loss: 0.6550
Epoch 11 Step 51 Train Loss: 0.5952
Epoch 11: Train Overall MSE: 0.0089 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0137 Validation Top 20 DE MSE: 0.0086. 
Epoch 12 Step 1 Train Loss: 0.6737
Epoch 12 Step 51 Train Loss: 0.5605
Epoch 12: Train Overall MSE: 0.0090 Validation Overall MSE: 0.0053. 
Train Top 20 DE MSE: 0.0139 Validation Top 20 DE MSE: 0.0089. 
Epoch 13 Step 1 Train Loss: 0.5707
Epoch 13 Step 51 Train Loss: 0.5669
Epoch 13: Train Overall MSE: 0.0086 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0084. 
Epoch 14 Step 1 Train Loss: 0.7481
Epoch 14 Step 51 Train Loss: 0.6273
Epoch 14: Train Overall MSE: 0.0086 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0084. 
Epoch 15 Step 1 Train Loss: 0.4586
Epoch 15 Step 51 Train Loss: 0.6741
Epoch 15: Train Overall MSE: 0.0092 Validation Overall MSE: 0.0051. 
Train Top 20 DE MSE: 0.0142 Validation Top 20 DE MSE: 0.0086. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0115
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0058294525
test_unseen_single_pearson: 0.9603740259087344
test_unseen_single_mse_de: 0.011504449
test_unseen_single_pearson_de: 0.5556358183131951
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.05271301272760992
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.49375
test_unseen_single_frac_sigma_below_1_non_dropout: 0.43125
test_unseen_single_mse_top20_de_non_dropout: 0.017119598
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.003 MB of 0.020 MB uploadedwandb: | 0.008 MB of 0.020 MB uploadedwandb: / 0.008 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:                                             train_de_pearson ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                    train_mse ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà
wandb:                                                train_pearson ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                training_loss ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá
wandb:                                               val_de_pearson ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:                                                      val_mse ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá
wandb:                                                  val_pearson ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.0115
wandb:                                              test_de_pearson 0.55564
wandb:               test_frac_opposite_direction_top20_non_dropout 0.49375
wandb:                          test_frac_sigma_below_1_non_dropout 0.43125
wandb:                                                     test_mse 0.00583
wandb:                                test_mse_top20_de_non_dropout 0.01712
wandb:                                                 test_pearson 0.96037
wandb:                                           test_pearson_delta 0.05271
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.49375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.43125
wandb:                                       test_unseen_single_mse 0.00583
wandb:                                    test_unseen_single_mse_de 0.0115
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01712
wandb:                                   test_unseen_single_pearson 0.96037
wandb:                                test_unseen_single_pearson_de 0.55564
wandb:                             test_unseen_single_pearson_delta 0.05271
wandb:                                                 train_de_mse 0.0142
wandb:                                             train_de_pearson 0.20616
wandb:                                                    train_mse 0.00917
wandb:                                                train_pearson 0.94008
wandb:                                                training_loss 0.68328
wandb:                                                   val_de_mse 0.00862
wandb:                                               val_de_pearson 0.36292
wandb:                                                      val_mse 0.00511
wandb:                                                  val_pearson 0.96438
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2017_unstimulated_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/7uh7e6ib
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_221839-7uh7e6ib/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:8
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222150-lyfvvu78
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2017_unstimulated_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/lyfvvu78
wandb: WARNING Serializing object of type ndarray that is 20566144 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.6972
Epoch 1 Step 51 Train Loss: 0.8899
Epoch 1: Train Overall MSE: 0.0044 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.0078 Validation Top 20 DE MSE: 0.0082. 
Epoch 2 Step 1 Train Loss: 0.7379
Epoch 2 Step 51 Train Loss: 0.7468
Epoch 2: Train Overall MSE: 0.0062 Validation Overall MSE: 0.0057. 
Train Top 20 DE MSE: 0.0106 Validation Top 20 DE MSE: 0.0084. 
Epoch 3 Step 1 Train Loss: 0.8596
Epoch 3 Step 51 Train Loss: 0.5041
Epoch 3: Train Overall MSE: 0.0089 Validation Overall MSE: 0.0081. 
Train Top 20 DE MSE: 0.0134 Validation Top 20 DE MSE: 0.0104. 
Epoch 4 Step 1 Train Loss: 0.6774
Epoch 4 Step 51 Train Loss: 0.7539
Epoch 4: Train Overall MSE: 0.0089 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.0093. 
Epoch 5 Step 1 Train Loss: 0.6959
Epoch 5 Step 51 Train Loss: 0.6326
Epoch 5: Train Overall MSE: 0.0092 Validation Overall MSE: 0.0078. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.0097. 
Epoch 6 Step 1 Train Loss: 0.6573
Epoch 6 Step 51 Train Loss: 0.5653
Epoch 6: Train Overall MSE: 0.0099 Validation Overall MSE: 0.0080. 
Train Top 20 DE MSE: 0.0147 Validation Top 20 DE MSE: 0.0097. 
Epoch 7 Step 1 Train Loss: 0.5471
Epoch 7 Step 51 Train Loss: 0.7837
Epoch 7: Train Overall MSE: 0.0095 Validation Overall MSE: 0.0080. 
Train Top 20 DE MSE: 0.0145 Validation Top 20 DE MSE: 0.0101. 
Epoch 8 Step 1 Train Loss: 0.5638
Epoch 8 Step 51 Train Loss: 0.7712
Epoch 8: Train Overall MSE: 0.0097 Validation Overall MSE: 0.0080. 
Train Top 20 DE MSE: 0.0147 Validation Top 20 DE MSE: 0.0099. 
Epoch 9 Step 1 Train Loss: 0.7042
Epoch 9 Step 51 Train Loss: 0.6628
Epoch 9: Train Overall MSE: 0.0098 Validation Overall MSE: 0.0079. 
Train Top 20 DE MSE: 0.0146 Validation Top 20 DE MSE: 0.0098. 
Epoch 10 Step 1 Train Loss: 0.4843
Epoch 10 Step 51 Train Loss: 0.7620
Epoch 10: Train Overall MSE: 0.0107 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0156 Validation Top 20 DE MSE: 0.0101. 
Epoch 11 Step 1 Train Loss: 0.6485
Epoch 11 Step 51 Train Loss: 0.7373
Epoch 11: Train Overall MSE: 0.0102 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0151 Validation Top 20 DE MSE: 0.0102. 
Epoch 12 Step 1 Train Loss: 0.6026
Epoch 12 Step 51 Train Loss: 0.6244
Epoch 12: Train Overall MSE: 0.0105 Validation Overall MSE: 0.0080. 
Train Top 20 DE MSE: 0.0149 Validation Top 20 DE MSE: 0.0097. 
Epoch 13 Step 1 Train Loss: 0.7272
Epoch 13 Step 51 Train Loss: 0.5752
Epoch 13: Train Overall MSE: 0.0096 Validation Overall MSE: 0.0078. 
Train Top 20 DE MSE: 0.0143 Validation Top 20 DE MSE: 0.0097. 
Epoch 14 Step 1 Train Loss: 0.4023
Epoch 14 Step 51 Train Loss: 0.5272
Epoch 14: Train Overall MSE: 0.0105 Validation Overall MSE: 0.0082. 
Train Top 20 DE MSE: 0.0158 Validation Top 20 DE MSE: 0.0103. 
Epoch 15 Step 1 Train Loss: 0.5742
Epoch 15 Step 51 Train Loss: 0.6082
Epoch 15: Train Overall MSE: 0.0107 Validation Overall MSE: 0.0083. 
Train Top 20 DE MSE: 0.0155 Validation Top 20 DE MSE: 0.0101. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0073
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0033534758
test_unseen_single_pearson: 0.9764353975159452
test_unseen_single_mse_de: 0.0072869714
test_unseen_single_pearson_de: 0.6894252513241114
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.014981024080510852
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.5375
test_unseen_single_frac_sigma_below_1_non_dropout: 0.525
test_unseen_single_mse_top20_de_non_dropout: 0.011277407
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.003 MB of 0.019 MB uploadedwandb: | 0.015 MB of 0.019 MB uploadedwandb: / 0.015 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                             train_de_pearson ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                    train_mse ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:                                                train_pearson ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:                                                training_loss ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñà
wandb:                                                   val_de_mse ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñá
wandb:                                               val_de_pearson ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:                                                      val_mse ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà
wandb:                                                  val_pearson ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00729
wandb:                                              test_de_pearson 0.68943
wandb:               test_frac_opposite_direction_top20_non_dropout 0.5375
wandb:                          test_frac_sigma_below_1_non_dropout 0.525
wandb:                                                     test_mse 0.00335
wandb:                                test_mse_top20_de_non_dropout 0.01128
wandb:                                                 test_pearson 0.97644
wandb:                                           test_pearson_delta 0.01498
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.5375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.525
wandb:                                       test_unseen_single_mse 0.00335
wandb:                                    test_unseen_single_mse_de 0.00729
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01128
wandb:                                   test_unseen_single_pearson 0.97644
wandb:                                test_unseen_single_pearson_de 0.68943
wandb:                             test_unseen_single_pearson_delta 0.01498
wandb:                                                 train_de_mse 0.01554
wandb:                                             train_de_pearson 0.08193
wandb:                                                    train_mse 0.0107
wandb:                                                train_pearson 0.93208
wandb:                                                training_loss 0.53604
wandb:                                                   val_de_mse 0.01012
wandb:                                               val_de_pearson 0.27508
wandb:                                                      val_mse 0.0083
wandb:                                                  val_pearson 0.94408
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2017_unstimulated_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/lyfvvu78
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222150-lyfvvu78/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:8
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222354-ca0c5j1y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2017_unstimulated_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/ca0c5j1y
wandb: WARNING Serializing object of type ndarray that is 20566144 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4601
Epoch 1 Step 51 Train Loss: 0.5768
Epoch 1: Train Overall MSE: 0.0045 Validation Overall MSE: 0.0043. 
Train Top 20 DE MSE: 0.0079 Validation Top 20 DE MSE: 0.0078. 
Epoch 2 Step 1 Train Loss: 0.6013
Epoch 2 Step 51 Train Loss: 0.5401
Epoch 2: Train Overall MSE: 0.0059 Validation Overall MSE: 0.0057. 
Train Top 20 DE MSE: 0.0094 Validation Top 20 DE MSE: 0.0103. 
Epoch 3 Step 1 Train Loss: 0.6708
Epoch 3 Step 51 Train Loss: 0.5512
Epoch 3: Train Overall MSE: 0.0075 Validation Overall MSE: 0.0057. 
Train Top 20 DE MSE: 0.0125 Validation Top 20 DE MSE: 0.0087. 
Epoch 4 Step 1 Train Loss: 0.7132
Epoch 4 Step 51 Train Loss: 0.7186
Epoch 4: Train Overall MSE: 0.0081 Validation Overall MSE: 0.0057. 
Train Top 20 DE MSE: 0.0130 Validation Top 20 DE MSE: 0.0096. 
Epoch 5 Step 1 Train Loss: 0.5391
Epoch 5 Step 51 Train Loss: 0.6160
Epoch 5: Train Overall MSE: 0.0082 Validation Overall MSE: 0.0057. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0094. 
Epoch 6 Step 1 Train Loss: 0.4621
Epoch 6 Step 51 Train Loss: 0.5399
Epoch 6: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0058. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0094. 
Epoch 7 Step 1 Train Loss: 0.6760
Epoch 7 Step 51 Train Loss: 0.6198
Epoch 7: Train Overall MSE: 0.0091 Validation Overall MSE: 0.0061. 
Train Top 20 DE MSE: 0.0148 Validation Top 20 DE MSE: 0.0101. 
Epoch 8 Step 1 Train Loss: 0.6387
Epoch 8 Step 51 Train Loss: 0.6157
Epoch 8: Train Overall MSE: 0.0088 Validation Overall MSE: 0.0060. 
Train Top 20 DE MSE: 0.0143 Validation Top 20 DE MSE: 0.0097. 
Epoch 9 Step 1 Train Loss: 0.5375
Epoch 9 Step 51 Train Loss: 0.4034
Epoch 9: Train Overall MSE: 0.0095 Validation Overall MSE: 0.0060. 
Train Top 20 DE MSE: 0.0153 Validation Top 20 DE MSE: 0.0100. 
Epoch 10 Step 1 Train Loss: 0.4945
Epoch 10 Step 51 Train Loss: 0.6328
Epoch 10: Train Overall MSE: 0.0098 Validation Overall MSE: 0.0062. 
Train Top 20 DE MSE: 0.0158 Validation Top 20 DE MSE: 0.0099. 
Epoch 11 Step 1 Train Loss: 0.5891
Epoch 11 Step 51 Train Loss: 0.6796
Epoch 11: Train Overall MSE: 0.0090 Validation Overall MSE: 0.0059. 
Train Top 20 DE MSE: 0.0147 Validation Top 20 DE MSE: 0.0095. 
Epoch 12 Step 1 Train Loss: 0.6232
Epoch 12 Step 51 Train Loss: 0.7643
Epoch 12: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0059. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.0095. 
Epoch 13 Step 1 Train Loss: 0.5766
Epoch 13 Step 51 Train Loss: 0.5809
Epoch 13: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0060. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.0097. 
Epoch 14 Step 1 Train Loss: 0.5601
Epoch 14 Step 51 Train Loss: 0.5898
Epoch 14: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0058. 
Train Top 20 DE MSE: 0.0141 Validation Top 20 DE MSE: 0.0096. 
Epoch 15 Step 1 Train Loss: 0.5498
Epoch 15 Step 51 Train Loss: 0.5581
Epoch 15: Train Overall MSE: 0.0091 Validation Overall MSE: 0.0062. 
Train Top 20 DE MSE: 0.0149 Validation Top 20 DE MSE: 0.0099. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0067
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0038195755
test_unseen_single_pearson: 0.9737107654210192
test_unseen_single_mse_de: 0.00666585
test_unseen_single_pearson_de: 0.5901510357116503
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.0310926483894441
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.48749999999999993
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6125
test_unseen_single_mse_top20_de_non_dropout: 0.010816693
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.009 MB of 0.019 MB uploadedwandb: | 0.015 MB of 0.019 MB uploadedwandb: / 0.015 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb: | 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb:                                             train_de_pearson ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                    train_mse ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb:                                                train_pearson ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:                                                training_loss ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÖ
wandb:                                                   val_de_mse ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb:                                               val_de_pearson ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                      val_mse ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                                  val_pearson ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00667
wandb:                                              test_de_pearson 0.59015
wandb:               test_frac_opposite_direction_top20_non_dropout 0.4875
wandb:                          test_frac_sigma_below_1_non_dropout 0.6125
wandb:                                                     test_mse 0.00382
wandb:                                test_mse_top20_de_non_dropout 0.01082
wandb:                                                 test_pearson 0.97371
wandb:                                           test_pearson_delta 0.03109
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.4875
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.6125
wandb:                                       test_unseen_single_mse 0.00382
wandb:                                    test_unseen_single_mse_de 0.00667
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01082
wandb:                                   test_unseen_single_pearson 0.97371
wandb:                                test_unseen_single_pearson_de 0.59015
wandb:                             test_unseen_single_pearson_delta 0.03109
wandb:                                                 train_de_mse 0.01485
wandb:                                             train_de_pearson 0.15934
wandb:                                                    train_mse 0.00914
wandb:                                                train_pearson 0.94073
wandb:                                                training_loss 0.69619
wandb:                                                   val_de_mse 0.00986
wandb:                                               val_de_pearson 0.45538
wandb:                                                      val_mse 0.00616
wandb:                                                  val_pearson 0.95726
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2017_unstimulated_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/ca0c5j1y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222354-ca0c5j1y/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:8
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222600-fovmreyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2017_unstimulated_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/fovmreyg
wandb: WARNING Serializing object of type ndarray that is 20566144 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.7844
Epoch 1 Step 51 Train Loss: 0.5239
Epoch 1: Train Overall MSE: 0.0044 Validation Overall MSE: 0.0052. 
Train Top 20 DE MSE: 0.0082 Validation Top 20 DE MSE: 0.0090. 
Epoch 2 Step 1 Train Loss: 0.4424
Epoch 2 Step 51 Train Loss: 0.5884
Epoch 2: Train Overall MSE: 0.0065 Validation Overall MSE: 0.0066. 
Train Top 20 DE MSE: 0.0103 Validation Top 20 DE MSE: 0.0132. 
Epoch 3 Step 1 Train Loss: 0.4744
Epoch 3 Step 51 Train Loss: 0.6260
Epoch 3: Train Overall MSE: 0.0073 Validation Overall MSE: 0.0071. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0136. 
Epoch 4 Step 1 Train Loss: 0.5099
Epoch 4 Step 51 Train Loss: 0.5978
Epoch 4: Train Overall MSE: 0.0085 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0127 Validation Top 20 DE MSE: 0.0136. 
Epoch 5 Step 1 Train Loss: 0.4989
Epoch 5 Step 51 Train Loss: 0.4778
Epoch 5: Train Overall MSE: 0.0084 Validation Overall MSE: 0.0070. 
Train Top 20 DE MSE: 0.0124 Validation Top 20 DE MSE: 0.0129. 
Epoch 6 Step 1 Train Loss: 0.6464
Epoch 6 Step 51 Train Loss: 0.6587
Epoch 6: Train Overall MSE: 0.0088 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0129 Validation Top 20 DE MSE: 0.0131. 
Epoch 7 Step 1 Train Loss: 0.8065
Epoch 7 Step 51 Train Loss: 0.7068
Epoch 7: Train Overall MSE: 0.0093 Validation Overall MSE: 0.0079. 
Train Top 20 DE MSE: 0.0138 Validation Top 20 DE MSE: 0.0140. 
Epoch 8 Step 1 Train Loss: 0.7304
Epoch 8 Step 51 Train Loss: 0.5484
Epoch 8: Train Overall MSE: 0.0100 Validation Overall MSE: 0.0082. 
Train Top 20 DE MSE: 0.0145 Validation Top 20 DE MSE: 0.0142. 
Epoch 9 Step 1 Train Loss: 0.5992
Epoch 9 Step 51 Train Loss: 0.7003
Epoch 9: Train Overall MSE: 0.0097 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0144 Validation Top 20 DE MSE: 0.0134. 
Epoch 10 Step 1 Train Loss: 0.5133
Epoch 10 Step 51 Train Loss: 0.8150
Epoch 10: Train Overall MSE: 0.0090 Validation Overall MSE: 0.0071. 
Train Top 20 DE MSE: 0.0135 Validation Top 20 DE MSE: 0.0131. 
Epoch 11 Step 1 Train Loss: 0.4340
Epoch 11 Step 51 Train Loss: 0.5891
Epoch 11: Train Overall MSE: 0.0095 Validation Overall MSE: 0.0073. 
Train Top 20 DE MSE: 0.0139 Validation Top 20 DE MSE: 0.0132. 
Epoch 12 Step 1 Train Loss: 0.5315
Epoch 12 Step 51 Train Loss: 0.6918
Epoch 12: Train Overall MSE: 0.0090 Validation Overall MSE: 0.0075. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0132. 
Epoch 13 Step 1 Train Loss: 0.5720
Epoch 13 Step 51 Train Loss: 0.6074
Epoch 13: Train Overall MSE: 0.0096 Validation Overall MSE: 0.0076. 
Train Top 20 DE MSE: 0.0143 Validation Top 20 DE MSE: 0.0134. 
Epoch 14 Step 1 Train Loss: 0.6303
Epoch 14 Step 51 Train Loss: 0.5482
Epoch 14: Train Overall MSE: 0.0091 Validation Overall MSE: 0.0077. 
Train Top 20 DE MSE: 0.0133 Validation Top 20 DE MSE: 0.0134. 
Epoch 15 Step 1 Train Loss: 0.5218
Epoch 15 Step 51 Train Loss: 0.5418
Epoch 15: Train Overall MSE: 0.0102 Validation Overall MSE: 0.0082. 
Train Top 20 DE MSE: 0.0151 Validation Top 20 DE MSE: 0.0142. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0085
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.004913006
test_unseen_single_pearson: 0.966791679491314
test_unseen_single_mse_de: 0.008475682
test_unseen_single_pearson_de: 0.529003138090622
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.03296449920822505
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.4375
test_unseen_single_frac_sigma_below_1_non_dropout: 0.5625
test_unseen_single_mse_top20_de_non_dropout: 0.0165611
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.019 MB uploadedwandb: | 0.010 MB of 0.019 MB uploadedwandb: / 0.013 MB of 0.019 MB uploadedwandb: - 0.013 MB of 0.019 MB uploadedwandb: \ 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà
wandb:                                             train_de_pearson ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                    train_mse ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà
wandb:                                                train_pearson ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                training_loss ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                               val_de_pearson ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                      val_mse ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:                                                  val_pearson ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00848
wandb:                                              test_de_pearson 0.529
wandb:               test_frac_opposite_direction_top20_non_dropout 0.4375
wandb:                          test_frac_sigma_below_1_non_dropout 0.5625
wandb:                                                     test_mse 0.00491
wandb:                                test_mse_top20_de_non_dropout 0.01656
wandb:                                                 test_pearson 0.96679
wandb:                                           test_pearson_delta 0.03296
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.4375
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.5625
wandb:                                       test_unseen_single_mse 0.00491
wandb:                                    test_unseen_single_mse_de 0.00848
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01656
wandb:                                   test_unseen_single_pearson 0.96679
wandb:                                test_unseen_single_pearson_de 0.529
wandb:                             test_unseen_single_pearson_delta 0.03296
wandb:                                                 train_de_mse 0.01512
wandb:                                             train_de_pearson 0.17974
wandb:                                                    train_mse 0.01022
wandb:                                                train_pearson 0.93677
wandb:                                                training_loss 0.58812
wandb:                                                   val_de_mse 0.01423
wandb:                                               val_de_pearson 0.47284
wandb:                                                      val_mse 0.0082
wandb:                                                  val_pearson 0.94216
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2017_unstimulated_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/fovmreyg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222600-fovmreyg/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:8
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_222808-v8f8de6e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2017_unstimulated_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/v8f8de6e
wandb: WARNING Serializing object of type ndarray that is 20566144 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.5326
Epoch 1 Step 51 Train Loss: 0.7737
Epoch 1: Train Overall MSE: 0.0049 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0090 Validation Top 20 DE MSE: 0.0042. 
Epoch 2 Step 1 Train Loss: 0.7481
Epoch 2 Step 51 Train Loss: 0.6251
Epoch 2: Train Overall MSE: 0.0067 Validation Overall MSE: 0.0045. 
Train Top 20 DE MSE: 0.0112 Validation Top 20 DE MSE: 0.0060. 
Epoch 3 Step 1 Train Loss: 0.7180
Epoch 3 Step 51 Train Loss: 0.6132
Epoch 3: Train Overall MSE: 0.0080 Validation Overall MSE: 0.0054. 
Train Top 20 DE MSE: 0.0143 Validation Top 20 DE MSE: 0.0077. 
Epoch 4 Step 1 Train Loss: 0.5374
Epoch 4 Step 51 Train Loss: 0.7061
Epoch 4: Train Overall MSE: 0.0089 Validation Overall MSE: 0.0065. 
Train Top 20 DE MSE: 0.0160 Validation Top 20 DE MSE: 0.0089. 
Epoch 5 Step 1 Train Loss: 0.5461
Epoch 5 Step 51 Train Loss: 0.7146
Epoch 5: Train Overall MSE: 0.0096 Validation Overall MSE: 0.0064. 
Train Top 20 DE MSE: 0.0174 Validation Top 20 DE MSE: 0.0090. 
Epoch 6 Step 1 Train Loss: 0.7179
Epoch 6 Step 51 Train Loss: 0.6375
Epoch 6: Train Overall MSE: 0.0101 Validation Overall MSE: 0.0063. 
Train Top 20 DE MSE: 0.0182 Validation Top 20 DE MSE: 0.0089. 
Epoch 7 Step 1 Train Loss: 0.5894
Epoch 7 Step 51 Train Loss: 0.5691
Epoch 7: Train Overall MSE: 0.0102 Validation Overall MSE: 0.0066. 
Train Top 20 DE MSE: 0.0184 Validation Top 20 DE MSE: 0.0092. 
Epoch 8 Step 1 Train Loss: 0.4487
Epoch 8 Step 51 Train Loss: 0.5918
Epoch 8: Train Overall MSE: 0.0100 Validation Overall MSE: 0.0063. 
Train Top 20 DE MSE: 0.0180 Validation Top 20 DE MSE: 0.0088. 
Epoch 9 Step 1 Train Loss: 0.6554
Epoch 9 Step 51 Train Loss: 0.5890
Epoch 9: Train Overall MSE: 0.0094 Validation Overall MSE: 0.0054. 
Train Top 20 DE MSE: 0.0170 Validation Top 20 DE MSE: 0.0079. 
Epoch 10 Step 1 Train Loss: 0.5810
Epoch 10 Step 51 Train Loss: 0.7964
Epoch 10: Train Overall MSE: 0.0105 Validation Overall MSE: 0.0063. 
Train Top 20 DE MSE: 0.0187 Validation Top 20 DE MSE: 0.0086. 
Epoch 11 Step 1 Train Loss: 0.7350
Epoch 11 Step 51 Train Loss: 0.5452
Epoch 11: Train Overall MSE: 0.0102 Validation Overall MSE: 0.0061. 
Train Top 20 DE MSE: 0.0182 Validation Top 20 DE MSE: 0.0086. 
Epoch 12 Step 1 Train Loss: 0.5693
Epoch 12 Step 51 Train Loss: 0.4738
Epoch 12: Train Overall MSE: 0.0099 Validation Overall MSE: 0.0062. 
Train Top 20 DE MSE: 0.0180 Validation Top 20 DE MSE: 0.0088. 
Epoch 13 Step 1 Train Loss: 0.5373
Epoch 13 Step 51 Train Loss: 0.4706
Epoch 13: Train Overall MSE: 0.0099 Validation Overall MSE: 0.0060. 
Train Top 20 DE MSE: 0.0177 Validation Top 20 DE MSE: 0.0086. 
Epoch 14 Step 1 Train Loss: 0.6717
Epoch 14 Step 51 Train Loss: 0.6836
Epoch 14: Train Overall MSE: 0.0098 Validation Overall MSE: 0.0062. 
Train Top 20 DE MSE: 0.0178 Validation Top 20 DE MSE: 0.0086. 
Epoch 15 Step 1 Train Loss: 0.6200
Epoch 15 Step 51 Train Loss: 0.6456
Epoch 15: Train Overall MSE: 0.0108 Validation Overall MSE: 0.0066. 
Train Top 20 DE MSE: 0.0191 Validation Top 20 DE MSE: 0.0089. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0087
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.004827627
test_unseen_single_pearson: 0.9664634944534631
test_unseen_single_mse_de: 0.00874362
test_unseen_single_pearson_de: 0.5428660884433809
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: -0.009802393798672808
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.46875
test_unseen_single_frac_sigma_below_1_non_dropout: 0.51875
test_unseen_single_mse_top20_de_non_dropout: 0.013899636
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.003 MB of 0.019 MB uploadedwandb: | 0.009 MB of 0.019 MB uploadedwandb: / 0.009 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                             train_de_pearson ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                    train_mse ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                                train_pearson ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                training_loss ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÇ
wandb:                                                   val_de_mse ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:                                               val_de_pearson ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                      val_mse ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                                  val_pearson ‚ñà‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00874
wandb:                                              test_de_pearson 0.54287
wandb:               test_frac_opposite_direction_top20_non_dropout 0.46875
wandb:                          test_frac_sigma_below_1_non_dropout 0.51875
wandb:                                                     test_mse 0.00483
wandb:                                test_mse_top20_de_non_dropout 0.0139
wandb:                                                 test_pearson 0.96646
wandb:                                           test_pearson_delta -0.0098
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.46875
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.51875
wandb:                                       test_unseen_single_mse 0.00483
wandb:                                    test_unseen_single_mse_de 0.00874
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.0139
wandb:                                   test_unseen_single_pearson 0.96646
wandb:                                test_unseen_single_pearson_de 0.54287
wandb:                             test_unseen_single_pearson_delta -0.0098
wandb:                                                 train_de_mse 0.01913
wandb:                                             train_de_pearson 0.22033
wandb:                                                    train_mse 0.01079
wandb:                                                train_pearson 0.93208
wandb:                                                training_loss 0.58046
wandb:                                                   val_de_mse 0.0089
wandb:                                               val_de_pearson 0.19211
wandb:                                                      val_mse 0.00655
wandb:                                                  val_pearson 0.95518
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2017_unstimulated_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/v8f8de6e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_222808-v8f8de6e/logs
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
[]
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_223050-v2wz1t9z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2021_stimulated_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/v2wz1t9z
wandb: WARNING Serializing object of type ndarray that is 20521088 bytes
  0%|                                                  | 0/4113 [00:00<?, ?it/s]  0%|                                          | 6/4113 [00:00<01:12, 56.97it/s]  0%|‚ñè                                        | 13/4113 [00:00<01:03, 64.17it/s]  0%|‚ñè                                        | 20/4113 [00:00<01:01, 66.03it/s]  1%|‚ñé                                        | 27/4113 [00:00<01:29, 45.89it/s]  1%|‚ñç                                        | 44/4113 [00:00<00:51, 79.65it/s]  1%|‚ñå                                        | 54/4113 [00:00<00:51, 79.24it/s]  2%|‚ñã                                        | 63/4113 [00:00<00:51, 79.09it/s]  2%|‚ñã                                        | 72/4113 [00:00<00:51, 79.02it/s]  2%|‚ñä                                        | 81/4113 [00:01<01:03, 63.94it/s]  2%|‚ñâ                                        | 89/4113 [00:01<01:00, 66.96it/s]  2%|‚ñâ                                        | 97/4113 [00:01<01:10, 57.36it/s]  3%|‚ñà                                       | 111/4113 [00:01<00:53, 75.07it/s]  3%|‚ñà‚ñè                                      | 120/4113 [00:01<01:04, 61.60it/s]  3%|‚ñà‚ñé                                      | 135/4113 [00:01<00:50, 79.55it/s]  4%|‚ñà‚ñç                                      | 145/4113 [00:02<00:54, 72.33it/s]  4%|‚ñà‚ñç                                      | 154/4113 [00:02<00:56, 69.60it/s]  4%|‚ñà‚ñã                                      | 170/4113 [00:02<00:45, 85.89it/s]  4%|‚ñà‚ñä                                      | 181/4113 [00:02<00:43, 91.32it/s]  5%|‚ñà‚ñä                                      | 191/4113 [00:02<00:44, 87.73it/s]  5%|‚ñà‚ñâ                                      | 201/4113 [00:02<00:46, 83.50it/s]  5%|‚ñà‚ñà                                      | 210/4113 [00:02<00:47, 82.34it/s]  5%|‚ñà‚ñà‚ñè                                     | 220/4113 [00:02<00:45, 84.76it/s]  6%|‚ñà‚ñà‚ñè                                     | 231/4113 [00:03<00:44, 87.87it/s]  6%|‚ñà‚ñà‚ñé                                     | 241/4113 [00:03<00:42, 90.70it/s]  6%|‚ñà‚ñà‚ñç                                     | 251/4113 [00:03<00:42, 91.03it/s]  6%|‚ñà‚ñà‚ñå                                     | 261/4113 [00:03<00:45, 84.60it/s]  7%|‚ñà‚ñà‚ñã                                     | 270/4113 [00:03<00:50, 76.31it/s]  7%|‚ñà‚ñà‚ñã                                     | 278/4113 [00:03<00:53, 71.23it/s]  7%|‚ñà‚ñà‚ñä                                     | 286/4113 [00:03<00:55, 68.46it/s]  7%|‚ñà‚ñà‚ñä                                     | 293/4113 [00:03<00:56, 67.81it/s]  7%|‚ñà‚ñà‚ñâ                                     | 300/4113 [00:04<00:57, 66.08it/s]  7%|‚ñà‚ñà‚ñâ                                     | 307/4113 [00:04<00:59, 64.28it/s]  8%|‚ñà‚ñà‚ñà                                     | 315/4113 [00:04<00:56, 67.30it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 325/4113 [00:04<00:49, 75.98it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 335/4113 [00:04<00:46, 81.78it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 346/4113 [00:04<00:43, 87.22it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 356/4113 [00:04<00:41, 90.68it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 366/4113 [00:04<00:40, 93.22it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 376/4113 [00:04<00:39, 94.62it/s]  9%|‚ñà‚ñà‚ñà‚ñä                                    | 386/4113 [00:04<00:39, 95.21it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 396/4113 [00:05<00:39, 93.25it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 407/4113 [00:05<00:38, 96.70it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 417/4113 [00:05<00:38, 96.82it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 428/4113 [00:05<00:38, 95.76it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 438/4113 [00:05<00:38, 96.34it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 448/4113 [00:05<00:38, 96.45it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 458/4113 [00:05<00:37, 96.24it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 468/4113 [00:05<00:37, 97.13it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 478/4113 [00:05<00:37, 96.40it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 488/4113 [00:06<00:37, 96.63it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 498/4113 [00:06<00:37, 96.96it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 508/4113 [00:06<00:37, 96.72it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 518/4113 [00:06<00:36, 97.31it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 528/4113 [00:06<00:37, 96.61it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 538/4113 [00:06<00:36, 97.03it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 548/4113 [00:06<00:36, 97.00it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 558/4113 [00:06<00:36, 96.61it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 568/4113 [00:06<00:36, 96.78it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 578/4113 [00:06<00:37, 93.28it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 590/4113 [00:07<00:36, 97.15it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 600/4113 [00:07<00:37, 93.21it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 610/4113 [00:07<00:37, 94.39it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 621/4113 [00:07<00:35, 98.10it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 631/4113 [00:07<00:35, 96.83it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 641/4113 [00:07<00:35, 97.09it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 652/4113 [00:07<00:35, 96.39it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 662/4113 [00:07<00:35, 96.77it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 672/4113 [00:07<00:35, 96.83it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 682/4113 [00:08<00:37, 92.48it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 693/4113 [00:08<00:35, 95.43it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 703/4113 [00:08<00:35, 96.03it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 713/4113 [00:08<00:35, 95.59it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 723/4113 [00:08<00:35, 96.07it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 733/4113 [00:08<00:35, 95.12it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 743/4113 [00:08<00:35, 94.53it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 753/4113 [00:08<00:35, 94.93it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 763/4113 [00:08<00:35, 94.05it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 773/4113 [00:09<00:35, 94.99it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 783/4113 [00:09<00:36, 90.37it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 793/4113 [00:09<00:36, 91.28it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 804/4113 [00:09<00:34, 95.77it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 814/4113 [00:09<00:35, 91.85it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 824/4113 [00:09<00:36, 89.81it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 834/4113 [00:09<00:36, 90.08it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 844/4113 [00:09<00:36, 90.58it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 854/4113 [00:09<00:37, 87.47it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 863/4113 [00:10<00:43, 75.51it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 874/4113 [00:10<00:39, 82.52it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 883/4113 [00:10<00:40, 79.38it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 892/4113 [00:10<00:40, 79.55it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 901/4113 [00:10<00:41, 78.01it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 909/4113 [00:10<00:41, 77.13it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 917/4113 [00:10<00:41, 76.91it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                               | 925/4113 [00:10<00:41, 76.75it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 933/4113 [00:10<00:41, 76.95it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 941/4113 [00:11<00:41, 76.25it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 949/4113 [00:11<00:41, 76.06it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 957/4113 [00:11<00:41, 75.93it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 965/4113 [00:11<00:41, 75.30it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 973/4113 [00:11<00:41, 74.97it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 981/4113 [00:11<00:41, 74.64it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 989/4113 [00:11<00:41, 74.81it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 997/4113 [00:11<00:41, 75.18it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1005/4113 [00:11<00:41, 75.37it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 1013/4113 [00:12<00:41, 75.58it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 1021/4113 [00:12<00:44, 69.02it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1029/4113 [00:12<00:45, 67.41it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 1036/4113 [00:12<00:46, 65.51it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1043/4113 [00:12<00:47, 65.02it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 1051/4113 [00:12<00:44, 68.84it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 1061/4113 [00:12<00:39, 77.01it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 1071/4113 [00:12<00:36, 82.40it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1081/4113 [00:12<00:34, 87.13it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 1090/4113 [00:13<00:34, 87.94it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 1099/4113 [00:13<00:34, 88.11it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1110/4113 [00:13<00:32, 93.05it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 1120/4113 [00:13<00:34, 86.91it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 1130/4113 [00:13<00:33, 89.19it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 1141/4113 [00:13<00:32, 90.35it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                            | 1151/4113 [00:13<00:32, 90.47it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1161/4113 [00:13<00:32, 91.67it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 1171/4113 [00:13<00:31, 92.86it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                           | 1181/4113 [00:14<00:31, 93.21it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                           | 1192/4113 [00:14<00:31, 93.85it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1202/4113 [00:14<00:31, 93.27it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 1212/4113 [00:14<00:30, 93.89it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1222/4113 [00:14<00:32, 87.80it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1232/4113 [00:14<00:33, 87.28it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1242/4113 [00:14<00:31, 89.87it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1252/4113 [00:14<00:31, 91.05it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1262/4113 [00:14<00:30, 92.09it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1272/4113 [00:15<00:31, 89.43it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1283/4113 [00:15<00:30, 93.60it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1293/4113 [00:15<00:29, 94.19it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1303/4113 [00:15<00:30, 93.56it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1313/4113 [00:15<00:32, 86.55it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1322/4113 [00:15<00:33, 82.59it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1331/4113 [00:15<00:34, 80.29it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1340/4113 [00:15<00:37, 74.47it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1348/4113 [00:16<00:39, 69.45it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1356/4113 [00:16<00:42, 65.57it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1365/4113 [00:16<00:38, 71.00it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1374/4113 [00:16<00:36, 75.06it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1383/4113 [00:16<00:35, 77.12it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1394/4113 [00:16<00:32, 84.06it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1404/4113 [00:16<00:30, 87.56it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1413/4113 [00:16<00:31, 85.55it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1424/4113 [00:16<00:30, 88.93it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1435/4113 [00:17<00:28, 93.05it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1445/4113 [00:17<00:28, 93.13it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1455/4113 [00:17<00:28, 91.85it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1466/4113 [00:17<00:28, 92.35it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1478/4113 [00:17<00:27, 96.09it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1488/4113 [00:17<00:27, 97.15it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1498/4113 [00:17<00:28, 92.09it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1510/4113 [00:17<00:27, 93.03it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1521/4113 [00:17<00:26, 96.56it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1531/4113 [00:18<00:26, 96.76it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1542/4113 [00:18<00:26, 96.20it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1552/4113 [00:18<00:26, 97.03it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1562/4113 [00:18<00:29, 86.78it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1571/4113 [00:18<00:30, 82.11it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1580/4113 [00:18<00:32, 78.03it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1588/4113 [00:18<00:34, 72.90it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1596/4113 [00:18<00:36, 69.80it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1604/4113 [00:19<00:37, 66.96it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1611/4113 [00:19<00:38, 64.61it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1618/4113 [00:19<00:38, 64.38it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1625/4113 [00:19<00:38, 64.69it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1632/4113 [00:19<00:39, 62.82it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1639/4113 [00:19<00:39, 63.24it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1646/4113 [00:19<00:38, 63.96it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1653/4113 [00:19<00:37, 65.30it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1660/4113 [00:19<00:37, 65.57it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1667/4113 [00:20<00:37, 66.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1674/4113 [00:20<00:36, 66.51it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1682/4113 [00:20<00:35, 68.00it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1694/4113 [00:20<00:29, 80.94it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1703/4113 [00:20<00:29, 81.75it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1714/4113 [00:20<00:27, 87.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1723/4113 [00:20<00:27, 85.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1734/4113 [00:20<00:26, 90.33it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1744/4113 [00:20<00:25, 91.32it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1754/4113 [00:20<00:26, 88.92it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1764/4113 [00:21<00:26, 89.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1775/4113 [00:21<00:25, 92.77it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1785/4113 [00:21<00:25, 93.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1795/4113 [00:21<00:25, 89.70it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1805/4113 [00:21<00:25, 90.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1815/4113 [00:21<00:25, 90.42it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1825/4113 [00:21<00:25, 90.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1836/4113 [00:21<00:24, 92.06it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1846/4113 [00:21<00:24, 92.10it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1856/4113 [00:22<00:24, 92.52it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1866/4113 [00:22<00:24, 93.18it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1876/4113 [00:22<00:26, 84.40it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1885/4113 [00:22<00:27, 79.93it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1894/4113 [00:22<00:28, 77.43it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1903/4113 [00:22<00:28, 78.60it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1911/4113 [00:22<00:29, 75.93it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1919/4113 [00:22<00:31, 70.57it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1927/4113 [00:23<00:29, 72.93it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1935/4113 [00:23<00:31, 70.13it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1943/4113 [00:23<00:31, 68.00it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1950/4113 [00:23<00:33, 65.24it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1957/4113 [00:23<00:34, 62.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1965/4113 [00:23<00:32, 66.88it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1972/4113 [00:23<00:32, 66.47it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1979/4113 [00:23<00:32, 66.55it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1987/4113 [00:23<00:31, 68.52it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1994/4113 [00:24<00:31, 68.32it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 2001/4113 [00:24<00:30, 68.32it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 2009/4113 [00:24<00:30, 69.12it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2017/4113 [00:24<00:30, 69.75it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 2024/4113 [00:24<00:29, 69.75it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2031/4113 [00:24<00:30, 68.88it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 2038/4113 [00:24<00:30, 68.31it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 2048/4113 [00:24<00:27, 74.38it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2058/4113 [00:24<00:25, 80.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 2069/4113 [00:25<00:24, 83.92it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 2078/4113 [00:25<00:24, 84.31it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 2089/4113 [00:25<00:23, 86.82it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2099/4113 [00:25<00:22, 88.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 2109/4113 [00:25<00:22, 90.76it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 2119/4113 [00:25<00:22, 87.19it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 2128/4113 [00:25<00:22, 87.46it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2138/4113 [00:25<00:21, 90.19it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 2148/4113 [00:25<00:22, 88.05it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 2158/4113 [00:26<00:21, 90.27it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 2168/4113 [00:26<00:22, 87.20it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2178/4113 [00:26<00:21, 88.02it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 2188/4113 [00:26<00:21, 90.05it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 2198/4113 [00:26<00:21, 90.28it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 2208/4113 [00:26<00:21, 88.73it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2217/4113 [00:26<00:22, 85.58it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 2226/4113 [00:26<00:21, 86.75it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 2235/4113 [00:26<00:22, 84.31it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2244/4113 [00:27<00:21, 85.81it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 2253/4113 [00:27<00:21, 86.06it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 2263/4113 [00:27<00:20, 89.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 2273/4113 [00:27<00:21, 87.54it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2282/4113 [00:27<00:20, 87.29it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 2292/4113 [00:27<00:20, 87.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 2302/4113 [00:27<00:20, 87.15it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 2311/4113 [00:27<00:20, 87.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2321/4113 [00:27<00:20, 89.00it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2331/4113 [00:28<00:20, 87.36it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 2342/4113 [00:28<00:19, 88.83it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 2351/4113 [00:28<00:19, 88.82it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2360/4113 [00:28<00:25, 68.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 2372/4113 [00:28<00:21, 80.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 2381/4113 [00:28<00:21, 80.19it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 2391/4113 [00:28<00:20, 85.28it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2400/4113 [00:28<00:19, 85.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2410/4113 [00:28<00:19, 87.84it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 2420/4113 [00:29<00:19, 88.99it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2430/4113 [00:29<00:18, 89.79it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2440/4113 [00:29<00:19, 86.90it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2452/4113 [00:29<00:18, 91.19it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2462/4113 [00:29<00:18, 91.44it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2472/4113 [00:29<00:17, 92.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2482/4113 [00:29<00:17, 91.80it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2492/4113 [00:29<00:17, 92.53it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2502/4113 [00:29<00:17, 92.63it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2513/4113 [00:30<00:16, 95.40it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2523/4113 [00:30<00:17, 89.04it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2534/4113 [00:30<00:16, 93.85it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2544/4113 [00:30<00:16, 94.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2554/4113 [00:30<00:16, 93.78it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2564/4113 [00:30<00:17, 89.06it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2573/4113 [00:30<00:18, 82.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2582/4113 [00:30<00:19, 77.75it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2590/4113 [00:31<00:19, 78.23it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2598/4113 [00:31<00:20, 74.81it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2606/4113 [00:31<00:21, 70.91it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2614/4113 [00:31<00:20, 71.87it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2622/4113 [00:31<00:20, 71.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2630/4113 [00:31<00:20, 73.41it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2639/4113 [00:31<00:19, 76.81it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2648/4113 [00:31<00:18, 78.97it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2658/4113 [00:31<00:17, 83.43it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2667/4113 [00:32<00:17, 84.92it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2676/4113 [00:32<00:17, 83.98it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2686/4113 [00:32<00:17, 82.54it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2698/4113 [00:32<00:15, 88.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2708/4113 [00:32<00:15, 91.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2718/4113 [00:32<00:15, 91.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2728/4113 [00:32<00:15, 89.37it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 2737/4113 [00:32<00:15, 89.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2747/4113 [00:32<00:15, 90.99it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2757/4113 [00:33<00:14, 92.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2767/4113 [00:33<00:15, 87.19it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2776/4113 [00:33<00:15, 85.42it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2786/4113 [00:33<00:14, 89.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2796/4113 [00:33<00:15, 87.34it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2805/4113 [00:33<00:14, 88.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2815/4113 [00:33<00:14, 89.26it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2824/4113 [00:33<00:14, 87.51it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2835/4113 [00:33<00:14, 89.05it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2845/4113 [00:34<00:13, 91.15it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2856/4113 [00:34<00:13, 91.43it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2867/4113 [00:34<00:13, 92.61it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2877/4113 [00:34<00:13, 92.48it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2887/4113 [00:34<00:13, 92.70it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2897/4113 [00:34<00:13, 92.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2907/4113 [00:34<00:14, 85.12it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2916/4113 [00:34<00:14, 80.06it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2925/4113 [00:34<00:15, 76.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2933/4113 [00:35<00:16, 72.61it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2941/4113 [00:35<00:16, 70.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2949/4113 [00:35<00:16, 69.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2960/4113 [00:35<00:14, 77.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2969/4113 [00:35<00:14, 80.22it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2980/4113 [00:35<00:13, 86.20it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2990/4113 [00:35<00:12, 88.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2999/4113 [00:35<00:12, 87.92it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 3008/4113 [00:35<00:12, 87.18it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3019/4113 [00:36<00:11, 92.54it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 3029/4113 [00:36<00:11, 93.21it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 3040/4113 [00:36<00:11, 93.23it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 3050/4113 [00:36<00:11, 90.87it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 3061/4113 [00:36<00:11, 93.97it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 3071/4113 [00:36<00:11, 93.71it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 3082/4113 [00:36<00:10, 96.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 3092/4113 [00:36<00:10, 96.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 3102/4113 [00:36<00:10, 92.74it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3112/4113 [00:37<00:10, 93.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 3122/4113 [00:37<00:10, 93.94it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 3133/4113 [00:37<00:10, 93.70it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 3143/4113 [00:37<00:10, 94.38it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 3153/4113 [00:37<00:10, 92.40it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3164/4113 [00:37<00:09, 95.62it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 3175/4113 [00:37<00:09, 98.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 3185/4113 [00:37<00:09, 96.05it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 3195/4113 [00:37<00:09, 95.67it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3205/4113 [00:38<00:09, 92.87it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 3215/4113 [00:38<00:09, 93.38it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3225/4113 [00:38<00:09, 93.34it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 3236/4113 [00:38<00:09, 93.27it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3246/4113 [00:38<00:09, 93.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 3256/4113 [00:38<00:09, 94.06it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 3267/4113 [00:38<00:08, 94.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 3277/4113 [00:38<00:08, 94.53it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 3287/4113 [00:38<00:08, 94.29it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3297/4113 [00:39<00:08, 94.30it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 3307/4113 [00:39<00:08, 93.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 3317/4113 [00:39<00:08, 93.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3327/4113 [00:39<00:08, 94.36it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 3338/4113 [00:39<00:08, 93.89it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3349/4113 [00:39<00:08, 93.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 3359/4113 [00:39<00:08, 93.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 3369/4113 [00:40<00:15, 49.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 3383/4113 [00:40<00:14, 50.84it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 3407/4113 [00:40<00:13, 54.23it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 3427/4113 [00:41<00:11, 61.83it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 3440/4113 [00:41<00:10, 64.42it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 3465/4113 [00:41<00:10, 63.32it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 3473/4113 [00:41<00:10, 63.96it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3482/4113 [00:41<00:10, 61.69it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 3489/4113 [00:42<00:10, 58.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 3504/4113 [00:42<00:11, 55.20it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3520/4113 [00:42<00:08, 70.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 3531/4113 [00:42<00:10, 55.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 3547/4113 [00:42<00:08, 66.66it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3590/4113 [00:43<00:04, 128.30it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3609/4113 [00:43<00:07, 65.63it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3648/4113 [00:43<00:04, 102.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3669/4113 [00:44<00:04, 96.25it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3686/4113 [00:44<00:04, 95.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3701/4113 [00:44<00:04, 94.21it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3714/4113 [00:44<00:04, 96.05it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3727/4113 [00:44<00:04, 92.74it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3738/4113 [00:44<00:04, 93.23it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 3749/4113 [00:44<00:03, 94.39it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3760/4113 [00:45<00:03, 89.71it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3772/4113 [00:45<00:03, 94.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3782/4113 [00:45<00:03, 93.59it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3792/4113 [00:45<00:03, 93.81it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3802/4113 [00:45<00:03, 89.99it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3813/4113 [00:45<00:03, 91.48it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3823/4113 [00:45<00:03, 91.24it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3833/4113 [00:45<00:03, 91.83it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3843/4113 [00:45<00:02, 92.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3853/4113 [00:46<00:02, 94.31it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3863/4113 [00:46<00:02, 92.26it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3873/4113 [00:46<00:02, 92.34it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3884/4113 [00:46<00:02, 92.71it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3895/4113 [00:46<00:02, 92.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3906/4113 [00:46<00:02, 95.21it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3916/4113 [00:46<00:02, 92.42it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3926/4113 [00:46<00:02, 91.91it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3936/4113 [00:46<00:01, 92.63it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3947/4113 [00:47<00:01, 94.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3957/4113 [00:47<00:01, 91.69it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3967/4113 [00:47<00:01, 88.85it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3976/4113 [00:47<00:01, 80.85it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3985/4113 [00:47<00:01, 74.90it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3993/4113 [00:47<00:01, 74.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4001/4113 [00:47<00:01, 70.63it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4010/4113 [00:47<00:01, 74.91it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4019/4113 [00:48<00:01, 77.01it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4028/4113 [00:48<00:01, 80.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4038/4113 [00:48<00:00, 85.22it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4048/4113 [00:48<00:00, 86.81it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 4059/4113 [00:48<00:00, 88.12it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 4068/4113 [00:48<00:00, 79.45it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 4081/4113 [00:48<00:00, 92.47it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 4091/4113 [00:48<00:00, 90.26it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 4101/4113 [00:48<00:00, 88.88it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 4111/4113 [00:49<00:00, 90.98it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4113/4113 [00:49<00:00, 83.79it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.3228
Epoch 1 Step 51 Train Loss: 0.3399
Epoch 1 Step 101 Train Loss: 0.3443
Epoch 1 Step 151 Train Loss: 0.3531
Epoch 1 Step 201 Train Loss: 0.3244
Epoch 1 Step 251 Train Loss: 0.3359
Epoch 1 Step 301 Train Loss: 0.3242
Epoch 1: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0080. 
Epoch 2 Step 1 Train Loss: 0.3448
Epoch 2 Step 51 Train Loss: 0.3323
Epoch 2 Step 101 Train Loss: 0.3566
Epoch 2 Step 151 Train Loss: 0.3395
Epoch 2 Step 201 Train Loss: 0.3541
Epoch 2 Step 251 Train Loss: 0.3478
Epoch 2 Step 301 Train Loss: 0.3538
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0081. 
Epoch 3 Step 1 Train Loss: 0.3587
Epoch 3 Step 51 Train Loss: 0.3521
Epoch 3 Step 101 Train Loss: 0.3438
Epoch 3 Step 151 Train Loss: 0.3462
Epoch 3 Step 201 Train Loss: 0.3501
Epoch 3 Step 251 Train Loss: 0.3772
Epoch 3 Step 301 Train Loss: 0.3447
Epoch 3: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0095. 
Epoch 4 Step 1 Train Loss: 0.3393
Epoch 4 Step 51 Train Loss: 0.3445
Epoch 4 Step 101 Train Loss: 0.3236
Epoch 4 Step 151 Train Loss: 0.3346
Epoch 4 Step 201 Train Loss: 0.3496
Epoch 4 Step 251 Train Loss: 0.3488
Epoch 4 Step 301 Train Loss: 0.3485
Epoch 4: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0026. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0117. 
Epoch 5 Step 1 Train Loss: 0.3613
Epoch 5 Step 51 Train Loss: 0.3543
Epoch 5 Step 101 Train Loss: 0.3266
Epoch 5 Step 151 Train Loss: 0.3994
Epoch 5 Step 201 Train Loss: 0.3503
Epoch 5 Step 251 Train Loss: 0.3936
Epoch 5 Step 301 Train Loss: 0.3440
Epoch 5: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0078. 
Epoch 6 Step 1 Train Loss: 0.3202
Epoch 6 Step 51 Train Loss: 0.3726
Epoch 6 Step 101 Train Loss: 0.3442
Epoch 6 Step 151 Train Loss: 0.3502
Epoch 6 Step 201 Train Loss: 0.3297
Epoch 6 Step 251 Train Loss: 0.3631
Epoch 6 Step 301 Train Loss: 0.3580
Epoch 6: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0121. 
Epoch 7 Step 1 Train Loss: 0.3403
Epoch 7 Step 51 Train Loss: 0.3529
Epoch 7 Step 101 Train Loss: 0.3603
Epoch 7 Step 151 Train Loss: 0.3515
Epoch 7 Step 201 Train Loss: 0.3728
Epoch 7 Step 251 Train Loss: 0.3508
Epoch 7 Step 301 Train Loss: 0.3625
Epoch 7: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0050 Validation Top 20 DE MSE: 0.0123. 
Epoch 8 Step 1 Train Loss: 0.3554
Epoch 8 Step 51 Train Loss: 0.3114
Epoch 8 Step 101 Train Loss: 0.3569
Epoch 8 Step 151 Train Loss: 0.3613
Epoch 8 Step 201 Train Loss: 0.3557
Epoch 8 Step 251 Train Loss: 0.3692
Epoch 8 Step 301 Train Loss: 0.3449
Epoch 8: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0088. 
Epoch 9 Step 1 Train Loss: 0.3385
Epoch 9 Step 51 Train Loss: 0.3828
Epoch 9 Step 101 Train Loss: 0.3296
Epoch 9 Step 151 Train Loss: 0.3389
Epoch 9 Step 201 Train Loss: 0.3268
Epoch 9 Step 251 Train Loss: 0.3238
Epoch 9 Step 301 Train Loss: 0.3489
Epoch 9: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0103. 
Epoch 10 Step 1 Train Loss: 0.3375
Epoch 10 Step 51 Train Loss: 0.3414
Epoch 10 Step 101 Train Loss: 0.3534
Epoch 10 Step 151 Train Loss: 0.3432
Epoch 10 Step 201 Train Loss: 0.3330
Epoch 10 Step 251 Train Loss: 0.3617
Epoch 10 Step 301 Train Loss: 0.3415
Epoch 10: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0065 Validation Top 20 DE MSE: 0.0137. 
Epoch 11 Step 1 Train Loss: 0.3366
Epoch 11 Step 51 Train Loss: 0.3458
Epoch 11 Step 101 Train Loss: 0.3508
Epoch 11 Step 151 Train Loss: 0.3498
Epoch 11 Step 201 Train Loss: 0.3306
Epoch 11 Step 251 Train Loss: 0.3422
Epoch 11 Step 301 Train Loss: 0.3388
Epoch 11: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0064 Validation Top 20 DE MSE: 0.0116. 
Epoch 12 Step 1 Train Loss: 0.3722
Epoch 12 Step 51 Train Loss: 0.3463
Epoch 12 Step 101 Train Loss: 0.3647
Epoch 12 Step 151 Train Loss: 0.3488
Epoch 12 Step 201 Train Loss: 0.3907
Epoch 12 Step 251 Train Loss: 0.3540
Epoch 12 Step 301 Train Loss: 0.3436
Epoch 12: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0025. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0121. 
Epoch 13 Step 1 Train Loss: 0.3085
Epoch 13 Step 51 Train Loss: 0.3487
Epoch 13 Step 101 Train Loss: 0.3516
Epoch 13 Step 151 Train Loss: 0.3426
Epoch 13 Step 201 Train Loss: 0.3503
Epoch 13 Step 251 Train Loss: 0.3773
Epoch 13 Step 301 Train Loss: 0.3597
Epoch 13: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0113. 
Epoch 14 Step 1 Train Loss: 0.3620
Epoch 14 Step 51 Train Loss: 0.3383
Epoch 14 Step 101 Train Loss: 0.3685
Epoch 14 Step 151 Train Loss: 0.3484
Epoch 14 Step 201 Train Loss: 0.3367
Epoch 14 Step 251 Train Loss: 0.3336
Epoch 14 Step 301 Train Loss: 0.3585
Epoch 14: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0065 Validation Top 20 DE MSE: 0.0123. 
Epoch 15 Step 1 Train Loss: 0.3795
Epoch 15 Step 51 Train Loss: 0.3570
Epoch 15 Step 101 Train Loss: 0.3301
Epoch 15 Step 151 Train Loss: 0.3620
Epoch 15 Step 201 Train Loss: 0.3435
Epoch 15 Step 251 Train Loss: 0.3560
Epoch 15 Step 301 Train Loss: 0.3625
Epoch 15: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0023. 
Train Top 20 DE MSE: 0.0063 Validation Top 20 DE MSE: 0.0112. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0109
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0025100424
test_unseen_single_pearson: 0.7645607062598598
test_unseen_single_mse_de: 0.010864412
test_unseen_single_pearson_de: 0.7715459769288431
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.1911654061443232
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.26
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8200000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.010670589
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.018 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.005 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñá
wandb:                                             train_de_pearson ‚ñà‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                    train_mse ‚ñÖ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ
wandb:                                                train_pearson ‚ñÇ‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñÜ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:                                                training_loss ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ
wandb:                                                   val_de_mse ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ
wandb:                                               val_de_pearson ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ
wandb:                                                      val_mse ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ
wandb:                                                  val_pearson ‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01086
wandb:                                              test_de_pearson 0.77155
wandb:               test_frac_opposite_direction_top20_non_dropout 0.26
wandb:                          test_frac_sigma_below_1_non_dropout 0.82
wandb:                                                     test_mse 0.00251
wandb:                                test_mse_top20_de_non_dropout 0.01067
wandb:                                                 test_pearson 0.76456
wandb:                                           test_pearson_delta 0.19117
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.26
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.82
wandb:                                       test_unseen_single_mse 0.00251
wandb:                                    test_unseen_single_mse_de 0.01086
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01067
wandb:                                   test_unseen_single_pearson 0.76456
wandb:                                test_unseen_single_pearson_de 0.77155
wandb:                             test_unseen_single_pearson_delta 0.19117
wandb:                                                 train_de_mse 0.0063
wandb:                                             train_de_pearson 0.53274
wandb:                                                    train_mse 0.00278
wandb:                                                train_pearson 0.76614
wandb:                                                training_loss 0.356
wandb:                                                   val_de_mse 0.01125
wandb:                                               val_de_pearson 0.95819
wandb:                                                      val_mse 0.00233
wandb:                                                  val_pearson 0.78589
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2021_stimulated_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/v2wz1t9z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_223050-v2wz1t9z/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_224101-gvrsmu8b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2021_stimulated_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/gvrsmu8b
wandb: WARNING Serializing object of type ndarray that is 20521088 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3116
Epoch 1 Step 51 Train Loss: 0.3219
Epoch 1 Step 101 Train Loss: 0.3395
Epoch 1 Step 151 Train Loss: 0.3366
Epoch 1 Step 201 Train Loss: 0.3434
Epoch 1 Step 251 Train Loss: 0.3196
Epoch 1 Step 301 Train Loss: 0.3702
Epoch 1 Step 351 Train Loss: 0.3345
Epoch 1: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0030. 
Epoch 2 Step 1 Train Loss: 0.3562
Epoch 2 Step 51 Train Loss: 0.3261
Epoch 2 Step 101 Train Loss: 0.3368
Epoch 2 Step 151 Train Loss: 0.3164
Epoch 2 Step 201 Train Loss: 0.3212
Epoch 2 Step 251 Train Loss: 0.3334
Epoch 2 Step 301 Train Loss: 0.3273
Epoch 2 Step 351 Train Loss: 0.3308
Epoch 2: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0042 Validation Top 20 DE MSE: 0.0033. 
Epoch 3 Step 1 Train Loss: 0.3340
Epoch 3 Step 51 Train Loss: 0.3602
Epoch 3 Step 101 Train Loss: 0.3798
Epoch 3 Step 151 Train Loss: 0.3432
Epoch 3 Step 201 Train Loss: 0.3369
Epoch 3 Step 251 Train Loss: 0.3486
Epoch 3 Step 301 Train Loss: 0.3415
Epoch 3 Step 351 Train Loss: 0.3147
Epoch 3: Train Overall MSE: 0.0028 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0052. 
Epoch 4 Step 1 Train Loss: 0.3124
Epoch 4 Step 51 Train Loss: 0.3428
Epoch 4 Step 101 Train Loss: 0.3474
Epoch 4 Step 151 Train Loss: 0.3634
Epoch 4 Step 201 Train Loss: 0.3577
Epoch 4 Step 251 Train Loss: 0.3496
Epoch 4 Step 301 Train Loss: 0.3446
Epoch 4 Step 351 Train Loss: 0.3443
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0029. 
Epoch 5 Step 1 Train Loss: 0.3433
Epoch 5 Step 51 Train Loss: 0.3505
Epoch 5 Step 101 Train Loss: 0.3338
Epoch 5 Step 151 Train Loss: 0.3623
Epoch 5 Step 201 Train Loss: 0.3257
Epoch 5 Step 251 Train Loss: 0.3427
Epoch 5 Step 301 Train Loss: 0.3466
Epoch 5 Step 351 Train Loss: 0.3498
Epoch 5: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0029. 
Epoch 6 Step 1 Train Loss: 0.3443
Epoch 6 Step 51 Train Loss: 0.3465
Epoch 6 Step 101 Train Loss: 0.3461
Epoch 6 Step 151 Train Loss: 0.3292
Epoch 6 Step 201 Train Loss: 0.3365
Epoch 6 Step 251 Train Loss: 0.3594
Epoch 6 Step 301 Train Loss: 0.3611
Epoch 6 Step 351 Train Loss: 0.3433
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0026. 
Epoch 7 Step 1 Train Loss: 0.3660
Epoch 7 Step 51 Train Loss: 0.3432
Epoch 7 Step 101 Train Loss: 0.3515
Epoch 7 Step 151 Train Loss: 0.3382
Epoch 7 Step 201 Train Loss: 0.3689
Epoch 7 Step 251 Train Loss: 0.3440
Epoch 7 Step 301 Train Loss: 0.3451
Epoch 7 Step 351 Train Loss: 0.3445
Epoch 7: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0017. 
Epoch 8 Step 1 Train Loss: 0.3604
Epoch 8 Step 51 Train Loss: 0.3439
Epoch 8 Step 101 Train Loss: 0.3233
Epoch 8 Step 151 Train Loss: 0.3573
Epoch 8 Step 201 Train Loss: 0.3492
Epoch 8 Step 251 Train Loss: 0.3215
Epoch 8 Step 301 Train Loss: 0.3438
Epoch 8 Step 351 Train Loss: 0.3505
Epoch 8: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0050 Validation Top 20 DE MSE: 0.0020. 
Epoch 9 Step 1 Train Loss: 0.3351
Epoch 9 Step 51 Train Loss: 0.3167
Epoch 9 Step 101 Train Loss: 0.3400
Epoch 9 Step 151 Train Loss: 0.3231
Epoch 9 Step 201 Train Loss: 0.3392
Epoch 9 Step 251 Train Loss: 0.3467
Epoch 9 Step 301 Train Loss: 0.3225
Epoch 9 Step 351 Train Loss: 0.3104
Epoch 9: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0012. 
Train Top 20 DE MSE: 0.0047 Validation Top 20 DE MSE: 0.0020. 
Epoch 10 Step 1 Train Loss: 0.3476
Epoch 10 Step 51 Train Loss: 0.3302
Epoch 10 Step 101 Train Loss: 0.3339
Epoch 10 Step 151 Train Loss: 0.3651
Epoch 10 Step 201 Train Loss: 0.3242
Epoch 10 Step 251 Train Loss: 0.3213
Epoch 10 Step 301 Train Loss: 0.3395
Epoch 10 Step 351 Train Loss: 0.3465
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0051 Validation Top 20 DE MSE: 0.0024. 
Epoch 11 Step 1 Train Loss: 0.3519
Epoch 11 Step 51 Train Loss: 0.3325
Epoch 11 Step 101 Train Loss: 0.3193
Epoch 11 Step 151 Train Loss: 0.3411
Epoch 11 Step 201 Train Loss: 0.3550
Epoch 11 Step 251 Train Loss: 0.3238
Epoch 11 Step 301 Train Loss: 0.3545
Epoch 11 Step 351 Train Loss: 0.3553
Epoch 11: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0020. 
Epoch 12 Step 1 Train Loss: 0.3353
Epoch 12 Step 51 Train Loss: 0.3257
Epoch 12 Step 101 Train Loss: 0.3359
Epoch 12 Step 151 Train Loss: 0.3472
Epoch 12 Step 201 Train Loss: 0.3403
Epoch 12 Step 251 Train Loss: 0.3599
Epoch 12 Step 301 Train Loss: 0.3321
Epoch 12 Step 351 Train Loss: 0.3190
Epoch 12: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0049 Validation Top 20 DE MSE: 0.0020. 
Epoch 13 Step 1 Train Loss: 0.3376
Epoch 13 Step 51 Train Loss: 0.3551
Epoch 13 Step 101 Train Loss: 0.3494
Epoch 13 Step 151 Train Loss: 0.3238
Epoch 13 Step 201 Train Loss: 0.3100
Epoch 13 Step 251 Train Loss: 0.3371
Epoch 13 Step 301 Train Loss: 0.3740
Epoch 13 Step 351 Train Loss: 0.3219
Epoch 13: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0019. 
Epoch 14 Step 1 Train Loss: 0.3257
Epoch 14 Step 51 Train Loss: 0.3507
Epoch 14 Step 101 Train Loss: 0.3339
Epoch 14 Step 151 Train Loss: 0.3695
Epoch 14 Step 201 Train Loss: 0.3314
Epoch 14 Step 251 Train Loss: 0.3454
Epoch 14 Step 301 Train Loss: 0.3365
Epoch 14 Step 351 Train Loss: 0.3642
Epoch 14: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0019. 
Epoch 15 Step 1 Train Loss: 0.3596
Epoch 15 Step 51 Train Loss: 0.3604
Epoch 15 Step 101 Train Loss: 0.3259
Epoch 15 Step 151 Train Loss: 0.3529
Epoch 15 Step 201 Train Loss: 0.3364
Epoch 15 Step 251 Train Loss: 0.3409
Epoch 15 Step 301 Train Loss: 0.3346
Epoch 15 Step 351 Train Loss: 0.3383
Epoch 15: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0009. 
Train Top 20 DE MSE: 0.0045 Validation Top 20 DE MSE: 0.0017. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0110
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0014209418
test_unseen_single_pearson: 0.8396717375872
test_unseen_single_mse_de: 0.010991121
test_unseen_single_pearson_de: 0.734739991714901
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.23060759220991586
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.19
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8300000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.011170966
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.011 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñÑ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ
wandb:                                                    train_mse ‚ñà‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñÖ
wandb:                                                   val_de_mse ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                               val_de_pearson ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà
wandb:                                                      val_mse ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.01099
wandb:                                              test_de_pearson 0.73474
wandb:               test_frac_opposite_direction_top20_non_dropout 0.19
wandb:                          test_frac_sigma_below_1_non_dropout 0.83
wandb:                                                     test_mse 0.00142
wandb:                                test_mse_top20_de_non_dropout 0.01117
wandb:                                                 test_pearson 0.83967
wandb:                                           test_pearson_delta 0.23061
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.19
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.83
wandb:                                       test_unseen_single_mse 0.00142
wandb:                                    test_unseen_single_mse_de 0.01099
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.01117
wandb:                                   test_unseen_single_pearson 0.83967
wandb:                                test_unseen_single_pearson_de 0.73474
wandb:                             test_unseen_single_pearson_delta 0.23061
wandb:                                                 train_de_mse 0.0045
wandb:                                             train_de_pearson 0.736
wandb:                                                    train_mse 0.00194
wandb:                                                train_pearson 0.81534
wandb:                                                training_loss 0.33154
wandb:                                                   val_de_mse 0.00168
wandb:                                               val_de_pearson 0.35143
wandb:                                                      val_mse 0.00091
wandb:                                                  val_pearson 0.88771
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2021_stimulated_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/gvrsmu8b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_224101-gvrsmu8b/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_224940-f7wy6lgz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2021_stimulated_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/f7wy6lgz
wandb: WARNING Serializing object of type ndarray that is 20521088 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3024
Epoch 1 Step 51 Train Loss: 0.3468
Epoch 1 Step 101 Train Loss: 0.3300
Epoch 1 Step 151 Train Loss: 0.3612
Epoch 1 Step 201 Train Loss: 0.3377
Epoch 1 Step 251 Train Loss: 0.3541
Epoch 1 Step 301 Train Loss: 0.3336
Epoch 1 Step 351 Train Loss: 0.3564
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0070 Validation Top 20 DE MSE: 0.0085. 
Epoch 2 Step 1 Train Loss: 0.3364
Epoch 2 Step 51 Train Loss: 0.3375
Epoch 2 Step 101 Train Loss: 0.3668
Epoch 2 Step 151 Train Loss: 0.3547
Epoch 2 Step 201 Train Loss: 0.3432
Epoch 2 Step 251 Train Loss: 0.3550
Epoch 2 Step 301 Train Loss: 0.3477
Epoch 2 Step 351 Train Loss: 0.3454
Epoch 2: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0053 Validation Top 20 DE MSE: 0.0076. 
Epoch 3 Step 1 Train Loss: 0.3329
Epoch 3 Step 51 Train Loss: 0.3307
Epoch 3 Step 101 Train Loss: 0.3439
Epoch 3 Step 151 Train Loss: 0.3441
Epoch 3 Step 201 Train Loss: 0.3603
Epoch 3 Step 251 Train Loss: 0.3356
Epoch 3 Step 301 Train Loss: 0.3905
Epoch 3 Step 351 Train Loss: 0.3593
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0010. 
Train Top 20 DE MSE: 0.0047 Validation Top 20 DE MSE: 0.0080. 
Epoch 4 Step 1 Train Loss: 0.3634
Epoch 4 Step 51 Train Loss: 0.3469
Epoch 4 Step 101 Train Loss: 0.3423
Epoch 4 Step 151 Train Loss: 0.3680
Epoch 4 Step 201 Train Loss: 0.3604
Epoch 4 Step 251 Train Loss: 0.3612
Epoch 4 Step 301 Train Loss: 0.3577
Epoch 4 Step 351 Train Loss: 0.3443
Epoch 4: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0058. 
Epoch 5 Step 1 Train Loss: 0.3496
Epoch 5 Step 51 Train Loss: 0.3395
Epoch 5 Step 101 Train Loss: 0.3537
Epoch 5 Step 151 Train Loss: 0.3495
Epoch 5 Step 201 Train Loss: 0.3706
Epoch 5 Step 251 Train Loss: 0.3272
Epoch 5 Step 301 Train Loss: 0.3343
Epoch 5 Step 351 Train Loss: 0.3817
Epoch 5: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0064 Validation Top 20 DE MSE: 0.0071. 
Epoch 6 Step 1 Train Loss: 0.3368
Epoch 6 Step 51 Train Loss: 0.3574
Epoch 6 Step 101 Train Loss: 0.3577
Epoch 6 Step 151 Train Loss: 0.3714
Epoch 6 Step 201 Train Loss: 0.3657
Epoch 6 Step 251 Train Loss: 0.3415
Epoch 6 Step 301 Train Loss: 0.3350
Epoch 6 Step 351 Train Loss: 0.3997
Epoch 6: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0068 Validation Top 20 DE MSE: 0.0085. 
Epoch 7 Step 1 Train Loss: 0.3461
Epoch 7 Step 51 Train Loss: 0.3565
Epoch 7 Step 101 Train Loss: 0.3485
Epoch 7 Step 151 Train Loss: 0.3364
Epoch 7 Step 201 Train Loss: 0.3466
Epoch 7 Step 251 Train Loss: 0.3571
Epoch 7 Step 301 Train Loss: 0.3719
Epoch 7 Step 351 Train Loss: 0.3613
Epoch 7: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.0082. 
Epoch 8 Step 1 Train Loss: 0.3331
Epoch 8 Step 51 Train Loss: 0.3674
Epoch 8 Step 101 Train Loss: 0.3647
Epoch 8 Step 151 Train Loss: 0.3452
Epoch 8 Step 201 Train Loss: 0.3678
Epoch 8 Step 251 Train Loss: 0.3440
Epoch 8 Step 301 Train Loss: 0.3682
Epoch 8 Step 351 Train Loss: 0.3382
Epoch 8: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0087. 
Epoch 9 Step 1 Train Loss: 0.3355
Epoch 9 Step 51 Train Loss: 0.3338
Epoch 9 Step 101 Train Loss: 0.3523
Epoch 9 Step 151 Train Loss: 0.3496
Epoch 9 Step 201 Train Loss: 0.3243
Epoch 9 Step 251 Train Loss: 0.3300
Epoch 9 Step 301 Train Loss: 0.3476
Epoch 9 Step 351 Train Loss: 0.3660
Epoch 9: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0077. 
Epoch 10 Step 1 Train Loss: 0.3500
Epoch 10 Step 51 Train Loss: 0.3542
Epoch 10 Step 101 Train Loss: 0.3475
Epoch 10 Step 151 Train Loss: 0.3593
Epoch 10 Step 201 Train Loss: 0.3343
Epoch 10 Step 251 Train Loss: 0.3532
Epoch 10 Step 301 Train Loss: 0.3498
Epoch 10 Step 351 Train Loss: 0.3371
Epoch 10: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0069 Validation Top 20 DE MSE: 0.0073. 
Epoch 11 Step 1 Train Loss: 0.3474
Epoch 11 Step 51 Train Loss: 0.3367
Epoch 11 Step 101 Train Loss: 0.3596
Epoch 11 Step 151 Train Loss: 0.3555
Epoch 11 Step 201 Train Loss: 0.3405
Epoch 11 Step 251 Train Loss: 0.2967
Epoch 11 Step 301 Train Loss: 0.3222
Epoch 11 Step 351 Train Loss: 0.3513
Epoch 11: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0077. 
Epoch 12 Step 1 Train Loss: 0.3615
Epoch 12 Step 51 Train Loss: 0.3199
Epoch 12 Step 101 Train Loss: 0.3457
Epoch 12 Step 151 Train Loss: 0.3486
Epoch 12 Step 201 Train Loss: 0.3316
Epoch 12 Step 251 Train Loss: 0.3386
Epoch 12 Step 301 Train Loss: 0.3485
Epoch 12 Step 351 Train Loss: 0.3754
Epoch 12: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0068 Validation Top 20 DE MSE: 0.0092. 
Epoch 13 Step 1 Train Loss: 0.3639
Epoch 13 Step 51 Train Loss: 0.3410
Epoch 13 Step 101 Train Loss: 0.3424
Epoch 13 Step 151 Train Loss: 0.3519
Epoch 13 Step 201 Train Loss: 0.3640
Epoch 13 Step 251 Train Loss: 0.3239
Epoch 13 Step 301 Train Loss: 0.3548
Epoch 13 Step 351 Train Loss: 0.3829
Epoch 13: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0074 Validation Top 20 DE MSE: 0.0071. 
Epoch 14 Step 1 Train Loss: 0.3633
Epoch 14 Step 51 Train Loss: 0.3552
Epoch 14 Step 101 Train Loss: 0.3598
Epoch 14 Step 151 Train Loss: 0.3559
Epoch 14 Step 201 Train Loss: 0.3555
Epoch 14 Step 251 Train Loss: 0.3438
Epoch 14 Step 301 Train Loss: 0.3404
Epoch 14 Step 351 Train Loss: 0.3556
Epoch 14: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0069 Validation Top 20 DE MSE: 0.0080. 
Epoch 15 Step 1 Train Loss: 0.3487
Epoch 15 Step 51 Train Loss: 0.3566
Epoch 15 Step 101 Train Loss: 0.3485
Epoch 15 Step 151 Train Loss: 0.3333
Epoch 15 Step 201 Train Loss: 0.3247
Epoch 15 Step 251 Train Loss: 0.3598
Epoch 15 Step 301 Train Loss: 0.3489
Epoch 15 Step 351 Train Loss: 0.3640
Epoch 15: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0067 Validation Top 20 DE MSE: 0.0078. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0059
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0023291218
test_unseen_single_pearson: 0.7756433389442181
test_unseen_single_mse_de: 0.0059358724
test_unseen_single_pearson_de: 0.9115092209053239
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.26883915348384746
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.21999999999999997
test_unseen_single_frac_sigma_below_1_non_dropout: 0.72
test_unseen_single_mse_top20_de_non_dropout: 0.005965918
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.015 MB of 0.022 MB uploadedwandb: - 0.015 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ
wandb:                                                training_loss ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÑ
wandb:                                                   val_de_mse ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñÖ
wandb:                                               val_de_pearson ‚ñÜ‚ñÑ‚ñÜ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ
wandb:                                                  val_pearson ‚ñÅ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00594
wandb:                                              test_de_pearson 0.91151
wandb:               test_frac_opposite_direction_top20_non_dropout 0.22
wandb:                          test_frac_sigma_below_1_non_dropout 0.72
wandb:                                                     test_mse 0.00233
wandb:                                test_mse_top20_de_non_dropout 0.00597
wandb:                                                 test_pearson 0.77564
wandb:                                           test_pearson_delta 0.26884
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.22
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.72
wandb:                                       test_unseen_single_mse 0.00233
wandb:                                    test_unseen_single_mse_de 0.00594
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00597
wandb:                                   test_unseen_single_pearson 0.77564
wandb:                                test_unseen_single_pearson_de 0.91151
wandb:                             test_unseen_single_pearson_delta 0.26884
wandb:                                                 train_de_mse 0.00667
wandb:                                             train_de_pearson 0.57408
wandb:                                                    train_mse 0.00251
wandb:                                                train_pearson 0.78042
wandb:                                                training_loss 0.36399
wandb:                                                   val_de_mse 0.00776
wandb:                                               val_de_pearson 0.62712
wandb:                                                      val_mse 0.00165
wandb:                                                  val_pearson 0.82305
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2021_stimulated_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/f7wy6lgz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_224940-f7wy6lgz/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_225742-95od5jf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2021_stimulated_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/95od5jf8
wandb: WARNING Serializing object of type ndarray that is 20521088 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.2899
Epoch 1 Step 51 Train Loss: 0.3507
Epoch 1 Step 101 Train Loss: 0.3241
Epoch 1 Step 151 Train Loss: 0.3366
Epoch 1 Step 201 Train Loss: 0.3050
Epoch 1 Step 251 Train Loss: 0.3257
Epoch 1 Step 301 Train Loss: 0.3806
Epoch 1 Step 351 Train Loss: 0.3496
Epoch 1: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0110 Validation Top 20 DE MSE: 0.0149. 
Epoch 2 Step 1 Train Loss: 0.3472
Epoch 2 Step 51 Train Loss: 0.3443
Epoch 2 Step 101 Train Loss: 0.3341
Epoch 2 Step 151 Train Loss: 0.3610
Epoch 2 Step 201 Train Loss: 0.3529
Epoch 2 Step 251 Train Loss: 0.3274
Epoch 2 Step 301 Train Loss: 0.3485
Epoch 2 Step 351 Train Loss: 0.3347
Epoch 2: Train Overall MSE: 0.0038 Validation Overall MSE: 0.0047. 
Train Top 20 DE MSE: 0.0075 Validation Top 20 DE MSE: 0.0171. 
Epoch 3 Step 1 Train Loss: 0.3615
Epoch 3 Step 51 Train Loss: 0.3378
Epoch 3 Step 101 Train Loss: 0.3592
Epoch 3 Step 151 Train Loss: 0.3284
Epoch 3 Step 201 Train Loss: 0.3193
Epoch 3 Step 251 Train Loss: 0.3376
Epoch 3 Step 301 Train Loss: 0.3574
Epoch 3 Step 351 Train Loss: 0.3308
Epoch 3: Train Overall MSE: 0.0011 Validation Overall MSE: 0.0014. 
Train Top 20 DE MSE: 0.0048 Validation Top 20 DE MSE: 0.0113. 
Epoch 4 Step 1 Train Loss: 0.3297
Epoch 4 Step 51 Train Loss: 0.3486
Epoch 4 Step 101 Train Loss: 0.3377
Epoch 4 Step 151 Train Loss: 0.3446
Epoch 4 Step 201 Train Loss: 0.3559
Epoch 4 Step 251 Train Loss: 0.3331
Epoch 4 Step 301 Train Loss: 0.3501
Epoch 4 Step 351 Train Loss: 0.3488
Epoch 4: Train Overall MSE: 0.0018 Validation Overall MSE: 0.0013. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0123. 
Epoch 5 Step 1 Train Loss: 0.3478
Epoch 5 Step 51 Train Loss: 0.3136
Epoch 5 Step 101 Train Loss: 0.3284
Epoch 5 Step 151 Train Loss: 0.3514
Epoch 5 Step 201 Train Loss: 0.3379
Epoch 5 Step 251 Train Loss: 0.3443
Epoch 5 Step 301 Train Loss: 0.3417
Epoch 5 Step 351 Train Loss: 0.3413
Epoch 5: Train Overall MSE: 0.0016 Validation Overall MSE: 0.0011. 
Train Top 20 DE MSE: 0.0052 Validation Top 20 DE MSE: 0.0095. 
Epoch 6 Step 1 Train Loss: 0.3490
Epoch 6 Step 51 Train Loss: 0.3383
Epoch 6 Step 101 Train Loss: 0.3563
Epoch 6 Step 151 Train Loss: 0.3132
Epoch 6 Step 201 Train Loss: 0.3615
Epoch 6 Step 251 Train Loss: 0.3260
Epoch 6 Step 301 Train Loss: 0.3751
Epoch 6 Step 351 Train Loss: 0.3467
Epoch 6: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0015. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0103. 
Epoch 7 Step 1 Train Loss: 0.3342
Epoch 7 Step 51 Train Loss: 0.3424
Epoch 7 Step 101 Train Loss: 0.3346
Epoch 7 Step 151 Train Loss: 0.3456
Epoch 7 Step 201 Train Loss: 0.3388
Epoch 7 Step 251 Train Loss: 0.3343
Epoch 7 Step 301 Train Loss: 0.3364
Epoch 7 Step 351 Train Loss: 0.3559
Epoch 7: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0112. 
Epoch 8 Step 1 Train Loss: 0.3407
Epoch 8 Step 51 Train Loss: 0.3399
Epoch 8 Step 101 Train Loss: 0.3461
Epoch 8 Step 151 Train Loss: 0.3490
Epoch 8 Step 201 Train Loss: 0.3412
Epoch 8 Step 251 Train Loss: 0.3258
Epoch 8 Step 301 Train Loss: 0.3494
Epoch 8 Step 351 Train Loss: 0.3686
Epoch 8: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0057 Validation Top 20 DE MSE: 0.0110. 
Epoch 9 Step 1 Train Loss: 0.3602
Epoch 9 Step 51 Train Loss: 0.3157
Epoch 9 Step 101 Train Loss: 0.3299
Epoch 9 Step 151 Train Loss: 0.3518
Epoch 9 Step 201 Train Loss: 0.3549
Epoch 9 Step 251 Train Loss: 0.3437
Epoch 9 Step 301 Train Loss: 0.3417
Epoch 9 Step 351 Train Loss: 0.3580
Epoch 9: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0062 Validation Top 20 DE MSE: 0.0090. 
Epoch 10 Step 1 Train Loss: 0.3540
Epoch 10 Step 51 Train Loss: 0.3428
Epoch 10 Step 101 Train Loss: 0.3193
Epoch 10 Step 151 Train Loss: 0.3612
Epoch 10 Step 201 Train Loss: 0.3639
Epoch 10 Step 251 Train Loss: 0.3379
Epoch 10 Step 301 Train Loss: 0.3556
Epoch 10 Step 351 Train Loss: 0.3619
Epoch 10: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0029. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0140. 
Epoch 11 Step 1 Train Loss: 0.3362
Epoch 11 Step 51 Train Loss: 0.3499
Epoch 11 Step 101 Train Loss: 0.3777
Epoch 11 Step 151 Train Loss: 0.3483
Epoch 11 Step 201 Train Loss: 0.3349
Epoch 11 Step 251 Train Loss: 0.3244
Epoch 11 Step 301 Train Loss: 0.3448
Epoch 11 Step 351 Train Loss: 0.3370
Epoch 11: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0089. 
Epoch 12 Step 1 Train Loss: 0.3568
Epoch 12 Step 51 Train Loss: 0.3571
Epoch 12 Step 101 Train Loss: 0.3256
Epoch 12 Step 151 Train Loss: 0.3327
Epoch 12 Step 201 Train Loss: 0.3419
Epoch 12 Step 251 Train Loss: 0.3408
Epoch 12 Step 301 Train Loss: 0.3470
Epoch 12 Step 351 Train Loss: 0.3659
Epoch 12: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0086. 
Epoch 13 Step 1 Train Loss: 0.3450
Epoch 13 Step 51 Train Loss: 0.3385
Epoch 13 Step 101 Train Loss: 0.3338
Epoch 13 Step 151 Train Loss: 0.3217
Epoch 13 Step 201 Train Loss: 0.3609
Epoch 13 Step 251 Train Loss: 0.3653
Epoch 13 Step 301 Train Loss: 0.3503
Epoch 13 Step 351 Train Loss: 0.3251
Epoch 13: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0059 Validation Top 20 DE MSE: 0.0095. 
Epoch 14 Step 1 Train Loss: 0.3607
Epoch 14 Step 51 Train Loss: 0.3561
Epoch 14 Step 101 Train Loss: 0.3482
Epoch 14 Step 151 Train Loss: 0.3282
Epoch 14 Step 201 Train Loss: 0.3524
Epoch 14 Step 251 Train Loss: 0.3348
Epoch 14 Step 301 Train Loss: 0.3056
Epoch 14 Step 351 Train Loss: 0.3335
Epoch 14: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0103. 
Epoch 15 Step 1 Train Loss: 0.3323
Epoch 15 Step 51 Train Loss: 0.3330
Epoch 15 Step 101 Train Loss: 0.3510
Epoch 15 Step 151 Train Loss: 0.3278
Epoch 15 Step 201 Train Loss: 0.3393
Epoch 15 Step 251 Train Loss: 0.3390
Epoch 15 Step 301 Train Loss: 0.3374
Epoch 15 Step 351 Train Loss: 0.3521
Epoch 15: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0060 Validation Top 20 DE MSE: 0.0107. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0054
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0011887049
test_unseen_single_pearson: 0.8684843910844625
test_unseen_single_mse_de: 0.005361487
test_unseen_single_pearson_de: 0.7147014745199798
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.15047854266490482
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.33
test_unseen_single_frac_sigma_below_1_non_dropout: 0.6799999999999999
test_unseen_single_mse_top20_de_non_dropout: 0.0055634314
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.001 MB of 0.022 MB uploadedwandb: / 0.005 MB of 0.022 MB uploadedwandb: - 0.015 MB of 0.022 MB uploadedwandb: \ 0.015 MB of 0.022 MB uploadedwandb: | 0.015 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                             train_de_pearson ‚ñÖ‚ñÅ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ
wandb:                                                    train_mse ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb:                                                train_pearson ‚ñÇ‚ñÅ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:                                                training_loss ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñá
wandb:                                                   val_de_mse ‚ñÜ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:                                               val_de_pearson ‚ñÑ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:                                                  val_pearson ‚ñÉ‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00536
wandb:                                              test_de_pearson 0.7147
wandb:               test_frac_opposite_direction_top20_non_dropout 0.33
wandb:                          test_frac_sigma_below_1_non_dropout 0.68
wandb:                                                     test_mse 0.00119
wandb:                                test_mse_top20_de_non_dropout 0.00556
wandb:                                                 test_pearson 0.86848
wandb:                                           test_pearson_delta 0.15048
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.33
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.68
wandb:                                       test_unseen_single_mse 0.00119
wandb:                                    test_unseen_single_mse_de 0.00536
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00556
wandb:                                   test_unseen_single_pearson 0.86848
wandb:                                test_unseen_single_pearson_de 0.7147
wandb:                             test_unseen_single_pearson_delta 0.15048
wandb:                                                 train_de_mse 0.00599
wandb:                                             train_de_pearson 0.57032
wandb:                                                    train_mse 0.00246
wandb:                                                train_pearson 0.78287
wandb:                                                training_loss 0.35211
wandb:                                                   val_de_mse 0.01069
wandb:                                               val_de_pearson 0.99409
wandb:                                                      val_mse 0.00202
wandb:                                                  val_pearson 0.806
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2021_stimulated_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/95od5jf8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_225742-95od5jf8/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:0
combo_seen1:0
combo_seen2:0
unseen_single:5
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_230550-vv4x4ot0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_DatlingerBock2021_stimulated_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/vv4x4ot0
wandb: WARNING Serializing object of type ndarray that is 20521088 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.3093
Epoch 1 Step 51 Train Loss: 0.3225
Epoch 1 Step 101 Train Loss: 0.3493
Epoch 1 Step 151 Train Loss: 0.3336
Epoch 1 Step 201 Train Loss: 0.3345
Epoch 1 Step 251 Train Loss: 0.3565
Epoch 1 Step 301 Train Loss: 0.3593
Epoch 1 Step 351 Train Loss: 0.3335
Epoch 1: Train Overall MSE: 0.0029 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.0048. 
Epoch 2 Step 1 Train Loss: 0.3541
Epoch 2 Step 51 Train Loss: 0.3781
Epoch 2 Step 101 Train Loss: 0.3292
Epoch 2 Step 151 Train Loss: 0.3619
Epoch 2 Step 201 Train Loss: 0.3452
Epoch 2 Step 251 Train Loss: 0.3455
Epoch 2 Step 301 Train Loss: 0.3445
Epoch 2 Step 351 Train Loss: 0.3670
Epoch 2: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0024. 
Train Top 20 DE MSE: 0.0058 Validation Top 20 DE MSE: 0.0036. 
Epoch 3 Step 1 Train Loss: 0.3436
Epoch 3 Step 51 Train Loss: 0.3399
Epoch 3 Step 101 Train Loss: 0.3704
Epoch 3 Step 151 Train Loss: 0.3487
Epoch 3 Step 201 Train Loss: 0.3497
Epoch 3 Step 251 Train Loss: 0.3432
Epoch 3 Step 301 Train Loss: 0.3431
Epoch 3 Step 351 Train Loss: 0.3737
Epoch 3: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0050 Validation Top 20 DE MSE: 0.0031. 
Epoch 4 Step 1 Train Loss: 0.3345
Epoch 4 Step 51 Train Loss: 0.3563
Epoch 4 Step 101 Train Loss: 0.3504
Epoch 4 Step 151 Train Loss: 0.3375
Epoch 4 Step 201 Train Loss: 0.3521
Epoch 4 Step 251 Train Loss: 0.3353
Epoch 4 Step 301 Train Loss: 0.3695
Epoch 4 Step 351 Train Loss: 0.3508
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0016. 
Train Top 20 DE MSE: 0.0054 Validation Top 20 DE MSE: 0.0035. 
Epoch 5 Step 1 Train Loss: 0.3607
Epoch 5 Step 51 Train Loss: 0.3601
Epoch 5 Step 101 Train Loss: 0.3601
Epoch 5 Step 151 Train Loss: 0.3383
Epoch 5 Step 201 Train Loss: 0.3630
Epoch 5 Step 251 Train Loss: 0.3580
Epoch 5 Step 301 Train Loss: 0.3583
Epoch 5 Step 351 Train Loss: 0.3351
Epoch 5: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0072 Validation Top 20 DE MSE: 0.0036. 
Epoch 6 Step 1 Train Loss: 0.3312
Epoch 6 Step 51 Train Loss: 0.3547
Epoch 6 Step 101 Train Loss: 0.3447
Epoch 6 Step 151 Train Loss: 0.3708
Epoch 6 Step 201 Train Loss: 0.3543
Epoch 6 Step 251 Train Loss: 0.3362
Epoch 6 Step 301 Train Loss: 0.3318
Epoch 6 Step 351 Train Loss: 0.3276
Epoch 6: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0071 Validation Top 20 DE MSE: 0.0041. 
Epoch 7 Step 1 Train Loss: 0.3552
Epoch 7 Step 51 Train Loss: 0.3477
Epoch 7 Step 101 Train Loss: 0.3392
Epoch 7 Step 151 Train Loss: 0.3478
Epoch 7 Step 201 Train Loss: 0.3621
Epoch 7 Step 251 Train Loss: 0.3495
Epoch 7 Step 301 Train Loss: 0.3336
Epoch 7 Step 351 Train Loss: 0.3301
Epoch 7: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0059 Validation Top 20 DE MSE: 0.0040. 
Epoch 8 Step 1 Train Loss: 0.3469
Epoch 8 Step 51 Train Loss: 0.3359
Epoch 8 Step 101 Train Loss: 0.3513
Epoch 8 Step 151 Train Loss: 0.3367
Epoch 8 Step 201 Train Loss: 0.3748
Epoch 8 Step 251 Train Loss: 0.3501
Epoch 8 Step 301 Train Loss: 0.3198
Epoch 8 Step 351 Train Loss: 0.3613
Epoch 8: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0021. 
Train Top 20 DE MSE: 0.0068 Validation Top 20 DE MSE: 0.0042. 
Epoch 9 Step 1 Train Loss: 0.3566
Epoch 9 Step 51 Train Loss: 0.3332
Epoch 9 Step 101 Train Loss: 0.3735
Epoch 9 Step 151 Train Loss: 0.3653
Epoch 9 Step 201 Train Loss: 0.3423
Epoch 9 Step 251 Train Loss: 0.3411
Epoch 9 Step 301 Train Loss: 0.3452
Epoch 9 Step 351 Train Loss: 0.3502
Epoch 9: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0039. 
Epoch 10 Step 1 Train Loss: 0.3494
Epoch 10 Step 51 Train Loss: 0.3181
Epoch 10 Step 101 Train Loss: 0.3468
Epoch 10 Step 151 Train Loss: 0.3597
Epoch 10 Step 201 Train Loss: 0.3271
Epoch 10 Step 251 Train Loss: 0.3371
Epoch 10 Step 301 Train Loss: 0.3311
Epoch 10 Step 351 Train Loss: 0.3306
Epoch 10: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0017. 
Train Top 20 DE MSE: 0.0065 Validation Top 20 DE MSE: 0.0038. 
Epoch 11 Step 1 Train Loss: 0.3252
Epoch 11 Step 51 Train Loss: 0.3522
Epoch 11 Step 101 Train Loss: 0.3514
Epoch 11 Step 151 Train Loss: 0.3500
Epoch 11 Step 201 Train Loss: 0.3490
Epoch 11 Step 251 Train Loss: 0.3320
Epoch 11 Step 301 Train Loss: 0.3667
Epoch 11 Step 351 Train Loss: 0.3571
Epoch 11: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0066 Validation Top 20 DE MSE: 0.0039. 
Epoch 12 Step 1 Train Loss: 0.3470
Epoch 12 Step 51 Train Loss: 0.3335
Epoch 12 Step 101 Train Loss: 0.3423
Epoch 12 Step 151 Train Loss: 0.3642
Epoch 12 Step 201 Train Loss: 0.3495
Epoch 12 Step 251 Train Loss: 0.3307
Epoch 12 Step 301 Train Loss: 0.3252
Epoch 12 Step 351 Train Loss: 0.3212
Epoch 12: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0039. 
Epoch 13 Step 1 Train Loss: 0.3536
Epoch 13 Step 51 Train Loss: 0.3765
Epoch 13 Step 101 Train Loss: 0.3398
Epoch 13 Step 151 Train Loss: 0.3426
Epoch 13 Step 201 Train Loss: 0.3498
Epoch 13 Step 251 Train Loss: 0.3310
Epoch 13 Step 301 Train Loss: 0.3522
Epoch 13 Step 351 Train Loss: 0.3421
Epoch 13: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0018. 
Train Top 20 DE MSE: 0.0063 Validation Top 20 DE MSE: 0.0040. 
Epoch 14 Step 1 Train Loss: 0.3332
Epoch 14 Step 51 Train Loss: 0.3301
Epoch 14 Step 101 Train Loss: 0.3694
Epoch 14 Step 151 Train Loss: 0.3592
Epoch 14 Step 201 Train Loss: 0.3363
Epoch 14 Step 251 Train Loss: 0.3456
Epoch 14 Step 301 Train Loss: 0.3469
Epoch 14 Step 351 Train Loss: 0.3488
Epoch 14: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0020. 
Train Top 20 DE MSE: 0.0070 Validation Top 20 DE MSE: 0.0042. 
Epoch 15 Step 1 Train Loss: 0.3575
Epoch 15 Step 51 Train Loss: 0.3542
Epoch 15 Step 101 Train Loss: 0.3412
Epoch 15 Step 151 Train Loss: 0.3477
Epoch 15 Step 201 Train Loss: 0.3445
Epoch 15 Step 251 Train Loss: 0.3655
Epoch 15 Step 301 Train Loss: 0.3335
Epoch 15 Step 351 Train Loss: 0.3473
Epoch 15: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0019. 
Train Top 20 DE MSE: 0.0061 Validation Top 20 DE MSE: 0.0040. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.0061
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: nan
test_combo_seen0_pearson: nan
test_combo_seen0_mse_de: nan
test_combo_seen0_pearson_de: nan
test_combo_seen1_mse: nan
test_combo_seen1_pearson: nan
test_combo_seen1_mse_de: nan
test_combo_seen1_pearson_de: nan
test_combo_seen2_mse: nan
test_combo_seen2_pearson: nan
test_combo_seen2_mse_de: nan
test_combo_seen2_pearson_de: nan
test_unseen_single_mse: 0.0016706738
test_unseen_single_pearson: 0.8207584181377516
test_unseen_single_mse_de: 0.0060919193
test_unseen_single_pearson_de: 0.8496180781454484
test_combo_seen0_pearson_delta: nan
test_combo_seen0_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen0_frac_sigma_below_1_non_dropout: nan
test_combo_seen0_mse_top20_de_non_dropout: nan
test_combo_seen1_pearson_delta: nan
test_combo_seen1_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen1_frac_sigma_below_1_non_dropout: nan
test_combo_seen1_mse_top20_de_non_dropout: nan
test_combo_seen2_pearson_delta: nan
test_combo_seen2_frac_opposite_direction_top20_non_dropout: nan
test_combo_seen2_frac_sigma_below_1_non_dropout: nan
test_combo_seen2_mse_top20_de_non_dropout: nan
test_unseen_single_pearson_delta: 0.233181539250258
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.27
test_unseen_single_frac_sigma_below_1_non_dropout: 0.73
test_unseen_single_mse_top20_de_non_dropout: 0.0061945077
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.022 MB uploadedwandb: | 0.005 MB of 0.022 MB uploadedwandb: / 0.022 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.022 MB uploadedwandb: | 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñà‚ñá‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÖ
wandb:                                             train_de_pearson ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÑ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá
wandb:                                                training_loss ‚ñÅ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ
wandb:                                                   val_de_mse ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:                                               val_de_pearson ‚ñÅ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ
wandb:                                                      val_mse ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÑ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen0_mse nan
wandb:                                      test_combo_seen0_mse_de nan
wandb:                    test_combo_seen0_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen0_pearson nan
wandb:                                  test_combo_seen0_pearson_de nan
wandb:                               test_combo_seen0_pearson_delta nan
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen1_mse nan
wandb:                                      test_combo_seen1_mse_de nan
wandb:                    test_combo_seen1_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen1_pearson nan
wandb:                                  test_combo_seen1_pearson_de nan
wandb:                               test_combo_seen1_pearson_delta nan
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout nan
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout nan
wandb:                                         test_combo_seen2_mse nan
wandb:                                      test_combo_seen2_mse_de nan
wandb:                    test_combo_seen2_mse_top20_de_non_dropout nan
wandb:                                     test_combo_seen2_pearson nan
wandb:                                  test_combo_seen2_pearson_de nan
wandb:                               test_combo_seen2_pearson_delta nan
wandb:                                                  test_de_mse 0.00609
wandb:                                              test_de_pearson 0.84962
wandb:               test_frac_opposite_direction_top20_non_dropout 0.27
wandb:                          test_frac_sigma_below_1_non_dropout 0.73
wandb:                                                     test_mse 0.00167
wandb:                                test_mse_top20_de_non_dropout 0.00619
wandb:                                                 test_pearson 0.82076
wandb:                                           test_pearson_delta 0.23318
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.27
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.73
wandb:                                       test_unseen_single_mse 0.00167
wandb:                                    test_unseen_single_mse_de 0.00609
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.00619
wandb:                                   test_unseen_single_pearson 0.82076
wandb:                                test_unseen_single_pearson_de 0.84962
wandb:                             test_unseen_single_pearson_delta 0.23318
wandb:                                                 train_de_mse 0.00614
wandb:                                             train_de_pearson 0.56555
wandb:                                                    train_mse 0.00229
wandb:                                                train_pearson 0.79616
wandb:                                                training_loss 0.38402
wandb:                                                   val_de_mse 0.00405
wandb:                                               val_de_pearson 0.92151
wandb:                                                      val_mse 0.00188
wandb:                                                  val_pearson 0.80727
wandb: 
wandb: üöÄ View run geneformer_DatlingerBock2021_stimulated_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/vv4x4ot0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_230550-vv4x4ot0/logs
/home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/datlingerbock.sh: line 21: y: command not found
