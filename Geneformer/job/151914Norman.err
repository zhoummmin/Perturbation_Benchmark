Loading compilers/gcc/12.2.0
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload compilers/gcc" first.
cmake-3.27.0 loaded successful
Seed set to 42
Found local copy...
These perturbations are not in the GO graph and their perturbation can thus not be predicted
['LYL1+IER5L' 'IER5L+ctrl' 'KIAA1804+ctrl']
Local copy of pyg dataset is detected. Loading...
Done!
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:9
combo_seen1:52
combo_seen2:17
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: Currently logged in as: zhoumin1130. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_221812-hos1hq7n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_NormanWeissman2019_split1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/hos1hq7n
wandb: WARNING Serializing object of type ndarray that is 20631680 bytes
  0%|                                                  | 0/3397 [00:00<?, ?it/s]  0%|                                          | 1/3397 [00:00<07:15,  7.80it/s]  0%|                                          | 6/3397 [00:00<01:53, 29.92it/s]  0%|                                         | 10/3397 [00:00<02:04, 27.27it/s]  1%|‚ñé                                        | 21/3397 [00:00<01:12, 46.50it/s]  1%|‚ñé                                        | 26/3397 [00:00<01:11, 47.09it/s]  1%|‚ñå                                        | 45/3397 [00:00<00:38, 85.97it/s]  2%|‚ñã                                        | 55/3397 [00:00<00:37, 88.03it/s]  2%|‚ñä                                        | 65/3397 [00:00<00:38, 85.96it/s]  2%|‚ñâ                                        | 74/3397 [00:01<00:39, 84.82it/s]  2%|‚ñà                                        | 83/3397 [00:01<00:39, 84.52it/s]  3%|‚ñà                                        | 92/3397 [00:01<00:39, 83.79it/s]  3%|‚ñà‚ñè                                      | 101/3397 [00:01<00:39, 83.06it/s]  3%|‚ñà‚ñé                                      | 110/3397 [00:01<00:39, 82.87it/s]  4%|‚ñà‚ñç                                      | 119/3397 [00:01<00:39, 83.00it/s]  4%|‚ñà‚ñå                                      | 128/3397 [00:01<00:39, 83.12it/s]  4%|‚ñà‚ñå                                      | 137/3397 [00:01<00:39, 83.30it/s]  4%|‚ñà‚ñã                                      | 146/3397 [00:01<00:39, 82.64it/s]  5%|‚ñà‚ñä                                      | 155/3397 [00:02<00:38, 83.25it/s]  5%|‚ñà‚ñâ                                      | 164/3397 [00:02<00:40, 80.41it/s]  5%|‚ñà‚ñà                                      | 173/3397 [00:02<00:43, 74.17it/s]  5%|‚ñà‚ñà‚ñè                                     | 186/3397 [00:02<00:37, 86.69it/s]  6%|‚ñà‚ñà‚ñé                                     | 195/3397 [00:02<00:38, 83.51it/s]  6%|‚ñà‚ñà‚ñç                                     | 205/3397 [00:02<00:36, 86.39it/s]  6%|‚ñà‚ñà‚ñå                                     | 214/3397 [00:02<00:38, 82.96it/s]  7%|‚ñà‚ñà‚ñã                                     | 224/3397 [00:02<00:38, 83.03it/s]  7%|‚ñà‚ñà‚ñä                                     | 234/3397 [00:03<00:37, 85.24it/s]  7%|‚ñà‚ñà‚ñä                                     | 243/3397 [00:03<00:37, 84.45it/s]  7%|‚ñà‚ñà‚ñâ                                     | 252/3397 [00:03<00:37, 83.69it/s]  8%|‚ñà‚ñà‚ñà                                     | 261/3397 [00:03<00:37, 83.71it/s]  8%|‚ñà‚ñà‚ñà‚ñè                                    | 270/3397 [00:03<00:38, 80.30it/s]  8%|‚ñà‚ñà‚ñà‚ñé                                    | 280/3397 [00:03<00:38, 80.93it/s]  9%|‚ñà‚ñà‚ñà‚ñç                                    | 289/3397 [00:03<00:38, 81.62it/s]  9%|‚ñà‚ñà‚ñà‚ñå                                    | 299/3397 [00:03<00:36, 84.65it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 308/3397 [00:03<00:37, 81.81it/s]  9%|‚ñà‚ñà‚ñà‚ñã                                    | 317/3397 [00:04<00:36, 83.99it/s] 10%|‚ñà‚ñà‚ñà‚ñä                                    | 326/3397 [00:04<00:36, 83.45it/s] 10%|‚ñà‚ñà‚ñà‚ñâ                                    | 335/3397 [00:04<00:36, 83.43it/s] 10%|‚ñà‚ñà‚ñà‚ñà                                    | 344/3397 [00:04<00:36, 83.65it/s] 10%|‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 353/3397 [00:04<00:36, 83.41it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 362/3397 [00:04<00:36, 83.19it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñé                                   | 371/3397 [00:04<00:36, 83.34it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 380/3397 [00:04<00:35, 83.85it/s] 11%|‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 389/3397 [00:04<00:35, 83.83it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 398/3397 [00:04<00:35, 83.33it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 407/3397 [00:05<00:35, 83.43it/s] 12%|‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 416/3397 [00:05<00:37, 80.44it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 425/3397 [00:05<00:35, 83.00it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 434/3397 [00:05<00:35, 82.70it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 443/3397 [00:05<00:35, 82.69it/s] 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                  | 452/3397 [00:05<00:35, 82.68it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                  | 461/3397 [00:05<00:35, 82.95it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 470/3397 [00:05<00:36, 79.62it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                  | 480/3397 [00:06<00:36, 80.90it/s] 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                  | 490/3397 [00:06<00:34, 83.67it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 499/3397 [00:06<00:35, 81.88it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                  | 508/3397 [00:06<00:35, 82.38it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 517/3397 [00:06<00:34, 82.84it/s] 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                 | 526/3397 [00:06<00:34, 82.99it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 535/3397 [00:06<00:34, 82.91it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 544/3397 [00:06<00:34, 82.51it/s] 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 553/3397 [00:06<00:35, 79.84it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                 | 562/3397 [00:06<00:34, 81.51it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 571/3397 [00:07<00:34, 81.58it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                 | 580/3397 [00:07<00:34, 81.77it/s] 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                 | 589/3397 [00:07<00:35, 78.92it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 599/3397 [00:07<00:34, 82.16it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 608/3397 [00:07<00:34, 81.30it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 617/3397 [00:07<00:33, 82.19it/s] 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 626/3397 [00:07<00:33, 82.38it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 635/3397 [00:07<00:33, 82.35it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                | 644/3397 [00:08<00:34, 79.49it/s] 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                | 654/3397 [00:08<00:32, 83.43it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 663/3397 [00:08<00:34, 80.39it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                | 673/3397 [00:08<00:32, 83.10it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 682/3397 [00:08<00:32, 82.90it/s] 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 691/3397 [00:08<00:32, 83.36it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                               | 700/3397 [00:08<00:32, 82.82it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 709/3397 [00:08<00:33, 80.22it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 719/3397 [00:08<00:32, 81.23it/s] 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                               | 728/3397 [00:09<00:32, 81.68it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                               | 738/3397 [00:09<00:32, 82.51it/s] 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 747/3397 [00:09<00:44, 59.75it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               | 765/3397 [00:09<00:32, 81.75it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 775/3397 [00:09<00:31, 84.34it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 785/3397 [00:09<00:30, 84.26it/s] 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                              | 794/3397 [00:09<00:30, 84.09it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                              | 803/3397 [00:09<00:31, 83.02it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 812/3397 [00:10<00:31, 82.60it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                              | 821/3397 [00:10<00:31, 82.38it/s] 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                              | 830/3397 [00:10<00:31, 82.53it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 839/3397 [00:10<00:31, 80.12it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 848/3397 [00:10<00:32, 77.67it/s] 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 856/3397 [00:10<00:36, 69.20it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                             | 869/3397 [00:10<00:30, 82.66it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 878/3397 [00:10<00:37, 68.00it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                             | 886/3397 [00:11<00:36, 68.37it/s] 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 894/3397 [00:11<00:36, 68.59it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 902/3397 [00:11<00:45, 55.32it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                             | 917/3397 [00:11<00:33, 75.14it/s] 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 927/3397 [00:11<00:30, 80.20it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 936/3397 [00:11<00:30, 81.28it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 945/3397 [00:11<00:29, 81.79it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                            | 954/3397 [00:11<00:30, 81.08it/s] 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 963/3397 [00:12<00:31, 78.43it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                            | 973/3397 [00:12<00:30, 80.24it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 983/3397 [00:12<00:29, 80.80it/s] 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                            | 992/3397 [00:12<00:37, 64.15it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                           | 1008/3397 [00:12<00:28, 82.54it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 1018/3397 [00:12<00:31, 74.64it/s] 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 1030/3397 [00:12<00:27, 84.79it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 1040/3397 [00:13<00:32, 72.14it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                           | 1054/3397 [00:13<00:27, 84.98it/s] 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1064/3397 [00:13<00:28, 81.79it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 1074/3397 [00:13<00:28, 81.93it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 1084/3397 [00:13<00:27, 84.38it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 1093/3397 [00:13<00:27, 83.56it/s] 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 1102/3397 [00:13<00:28, 81.71it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 1112/3397 [00:13<00:28, 81.25it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1122/3397 [00:14<00:27, 83.68it/s] 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 1131/3397 [00:14<00:26, 84.21it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                          | 1140/3397 [00:14<00:27, 83.34it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 1149/3397 [00:14<00:27, 80.62it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 1158/3397 [00:14<00:27, 81.14it/s] 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 1167/3397 [00:14<00:27, 80.34it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1177/3397 [00:14<00:26, 83.73it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1186/3397 [00:14<00:27, 80.45it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 1196/3397 [00:14<00:27, 80.76it/s] 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 1205/3397 [00:15<00:26, 81.50it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 1215/3397 [00:15<00:25, 84.55it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 1224/3397 [00:15<00:25, 84.64it/s] 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                        | 1233/3397 [00:15<00:25, 84.02it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1242/3397 [00:15<00:25, 83.13it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 1251/3397 [00:15<00:25, 82.97it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 1260/3397 [00:15<00:26, 80.35it/s] 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 1269/3397 [00:15<00:27, 78.03it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 1277/3397 [00:15<00:28, 75.70it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1285/3397 [00:16<00:29, 71.34it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 1294/3397 [00:16<00:28, 74.25it/s] 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 1302/3397 [00:16<00:28, 73.59it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 1310/3397 [00:16<00:29, 70.62it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1318/3397 [00:16<00:28, 72.01it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 1326/3397 [00:16<00:31, 64.90it/s] 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 1337/3397 [00:16<00:27, 75.31it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 1346/3397 [00:16<00:26, 77.10it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1354/3397 [00:17<00:26, 76.77it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1362/3397 [00:17<00:26, 76.51it/s] 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                       | 1370/3397 [00:17<00:26, 75.68it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 1378/3397 [00:17<00:27, 72.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 1386/3397 [00:17<00:27, 72.76it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1394/3397 [00:17<00:27, 72.77it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 1402/3397 [00:17<00:27, 73.23it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 1411/3397 [00:17<00:26, 73.96it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 1420/3397 [00:17<00:25, 76.84it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1428/3397 [00:18<00:25, 77.09it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 1436/3397 [00:18<00:26, 74.21it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 1445/3397 [00:18<00:25, 76.81it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 1453/3397 [00:18<00:25, 75.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 1461/3397 [00:18<00:26, 72.69it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1470/3397 [00:18<00:25, 75.74it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 1478/3397 [00:18<00:25, 75.73it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1486/3397 [00:18<00:25, 75.35it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1494/3397 [00:18<00:26, 72.29it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 1502/3397 [00:19<00:25, 73.07it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 1511/3397 [00:19<00:25, 73.74it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 1520/3397 [00:19<00:24, 75.94it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 1528/3397 [00:19<00:25, 73.23it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1536/3397 [00:19<00:25, 72.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                     | 1545/3397 [00:19<00:24, 75.45it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 1553/3397 [00:19<00:25, 72.12it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 1561/3397 [00:19<00:25, 73.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1570/3397 [00:19<00:23, 76.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 1578/3397 [00:20<00:23, 76.76it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1586/3397 [00:20<00:23, 76.40it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 1594/3397 [00:20<00:23, 76.54it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1602/3397 [00:20<00:24, 73.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                    | 1610/3397 [00:20<00:24, 74.25it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 1619/3397 [00:20<00:24, 71.30it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 1628/3397 [00:20<00:23, 75.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1636/3397 [00:20<00:23, 74.74it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 1644/3397 [00:20<00:24, 71.76it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 1653/3397 [00:21<00:23, 74.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 1661/3397 [00:21<00:23, 73.28it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 1670/3397 [00:21<00:22, 77.68it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1678/3397 [00:21<00:22, 77.82it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 1686/3397 [00:21<00:22, 77.28it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 1694/3397 [00:21<00:21, 77.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 1702/3397 [00:21<00:21, 77.53it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1710/3397 [00:21<00:21, 77.51it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 1718/3397 [00:21<00:21, 77.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 1726/3397 [00:22<00:22, 74.71it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 1735/3397 [00:22<00:21, 77.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1743/3397 [00:22<00:22, 74.38it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 1751/3397 [00:22<00:21, 75.49it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 1759/3397 [00:22<00:24, 68.21it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 1767/3397 [00:22<00:22, 71.07it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1775/3397 [00:22<00:22, 71.16it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 1784/3397 [00:22<00:21, 75.76it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 1793/3397 [00:22<00:21, 74.71it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 1802/3397 [00:23<00:20, 76.52it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1810/3397 [00:23<00:22, 69.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 1818/3397 [00:23<00:22, 69.54it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 1828/3397 [00:23<00:20, 75.32it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1836/3397 [00:23<00:21, 73.35it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1844/3397 [00:23<00:22, 68.67it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1853/3397 [00:23<00:21, 72.05it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 1861/3397 [00:23<00:20, 73.74it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 1871/3397 [00:23<00:19, 79.82it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1880/3397 [00:24<00:21, 69.30it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 1889/3397 [00:24<00:20, 72.78it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 1899/3397 [00:24<00:19, 77.05it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1908/3397 [00:24<00:19, 77.85it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 1916/3397 [00:24<00:18, 78.00it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 1925/3397 [00:24<00:18, 81.27it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 1935/3397 [00:24<00:17, 83.49it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 1944/3397 [00:24<00:17, 84.61it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 1954/3397 [00:25<00:16, 86.65it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 1963/3397 [00:25<00:16, 86.71it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 1972/3397 [00:25<00:16, 86.80it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 1981/3397 [00:25<00:19, 72.75it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 1997/3397 [00:25<00:14, 93.70it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 2007/3397 [00:25<00:15, 89.17it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 2017/3397 [00:25<00:15, 89.59it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 2027/3397 [00:25<00:14, 91.72it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 2037/3397 [00:25<00:16, 84.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2047/3397 [00:26<00:15, 86.81it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 2057/3397 [00:26<00:15, 88.86it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 2067/3397 [00:26<00:15, 86.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 2076/3397 [00:26<00:17, 76.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 2084/3397 [00:26<00:18, 72.10it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2094/3397 [00:26<00:18, 71.03it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 2107/3397 [00:26<00:15, 84.29it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 2116/3397 [00:26<00:15, 84.36it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 2125/3397 [00:27<00:14, 85.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2135/3397 [00:27<00:14, 88.17it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 2144/3397 [00:27<00:14, 88.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 2153/3397 [00:27<00:14, 83.96it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 2162/3397 [00:27<00:14, 85.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 2172/3397 [00:27<00:13, 88.84it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 2181/3397 [00:27<00:13, 87.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2190/3397 [00:27<00:14, 85.15it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2200/3397 [00:27<00:13, 87.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2209/3397 [00:28<00:13, 86.44it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 2218/3397 [00:28<00:13, 86.44it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 2227/3397 [00:28<00:14, 82.46it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 2236/3397 [00:28<00:14, 80.14it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2245/3397 [00:28<00:15, 75.23it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 2253/3397 [00:28<00:16, 68.17it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2265/3397 [00:28<00:14, 80.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 2274/3397 [00:28<00:15, 73.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 2282/3397 [00:29<00:15, 73.95it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 2292/3397 [00:29<00:14, 77.27it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 2301/3397 [00:29<00:13, 79.51it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2310/3397 [00:29<00:13, 81.60it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2320/3397 [00:29<00:12, 85.60it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 2329/3397 [00:29<00:12, 84.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 2338/3397 [00:29<00:12, 83.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 2348/3397 [00:29<00:12, 83.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 2357/3397 [00:29<00:12, 81.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 2369/3397 [00:30<00:11, 89.84it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 2379/3397 [00:30<00:11, 87.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 2388/3397 [00:30<00:11, 87.83it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2397/3397 [00:30<00:11, 87.47it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 2406/3397 [00:30<00:11, 85.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 2415/3397 [00:30<00:11, 85.84it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 2424/3397 [00:30<00:11, 86.66it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 2433/3397 [00:30<00:11, 87.58it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 2443/3397 [00:30<00:10, 88.69it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 2453/3397 [00:30<00:10, 90.95it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 2463/3397 [00:31<00:10, 87.74it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 2473/3397 [00:31<00:10, 91.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 2483/3397 [00:31<00:11, 78.18it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 2497/3397 [00:31<00:09, 92.08it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 2507/3397 [00:31<00:11, 79.82it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 2519/3397 [00:31<00:09, 88.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 2529/3397 [00:31<00:10, 85.33it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 2538/3397 [00:31<00:10, 84.89it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2548/3397 [00:32<00:09, 87.21it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 2558/3397 [00:32<00:09, 87.74it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 2567/3397 [00:32<00:11, 73.97it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 2579/3397 [00:32<00:09, 83.98it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2588/3397 [00:32<00:09, 84.92it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 2597/3397 [00:32<00:09, 86.09it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 2606/3397 [00:32<00:11, 70.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 2615/3397 [00:32<00:10, 74.52it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2625/3397 [00:33<00:09, 80.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 2634/3397 [00:33<00:09, 79.88it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 2644/3397 [00:33<00:09, 82.25it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 2653/3397 [00:33<00:09, 77.50it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 2661/3397 [00:33<00:11, 66.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 2674/3397 [00:33<00:09, 79.58it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 2683/3397 [00:33<00:09, 78.62it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2692/3397 [00:33<00:09, 77.68it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 2700/3397 [00:34<00:09, 74.25it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 2709/3397 [00:34<00:08, 77.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 2717/3397 [00:34<00:09, 73.32it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 2727/3397 [00:34<00:08, 76.72it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 2737/3397 [00:34<00:08, 78.91it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 2747/3397 [00:34<00:07, 81.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2756/3397 [00:34<00:08, 79.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 2765/3397 [00:34<00:08, 71.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 2773/3397 [00:35<00:08, 73.16it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 2781/3397 [00:35<00:08, 71.18it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 2790/3397 [00:35<00:08, 72.84it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2800/3397 [00:35<00:07, 78.89it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 2808/3397 [00:35<00:07, 76.45it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 2818/3397 [00:35<00:07, 81.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 2827/3397 [00:35<00:06, 82.07it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 2836/3397 [00:35<00:06, 83.62it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 2845/3397 [00:35<00:07, 77.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 2853/3397 [00:36<00:07, 72.77it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 2866/3397 [00:36<00:06, 84.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2875/3397 [00:36<00:06, 78.71it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 2884/3397 [00:36<00:06, 76.48it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 2892/3397 [00:36<00:06, 72.62it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 2900/3397 [00:36<00:06, 74.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2908/3397 [00:36<00:06, 72.92it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 2916/3397 [00:36<00:06, 72.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 2924/3397 [00:37<00:06, 71.76it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 2932/3397 [00:37<00:06, 69.26it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 2939/3397 [00:37<00:06, 68.30it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 2949/3397 [00:37<00:05, 74.91it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2957/3397 [00:37<00:06, 71.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 2966/3397 [00:37<00:05, 74.17it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2974/3397 [00:37<00:05, 73.21it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2982/3397 [00:37<00:05, 73.00it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2990/3397 [00:37<00:05, 72.89it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 2998/3397 [00:38<00:06, 63.42it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3006/3397 [00:38<00:06, 65.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3014/3397 [00:38<00:05, 67.73it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 3022/3397 [00:38<00:05, 69.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3030/3397 [00:38<00:05, 70.53it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3038/3397 [00:38<00:05, 71.34it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 3046/3397 [00:38<00:04, 71.46it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3054/3397 [00:38<00:04, 70.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3063/3397 [00:38<00:04, 73.60it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3071/3397 [00:39<00:04, 71.81it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 3079/3397 [00:39<00:04, 70.04it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 3087/3397 [00:39<00:05, 55.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 3105/3397 [00:39<00:03, 82.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3115/3397 [00:39<00:03, 82.10it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 3124/3397 [00:39<00:03, 81.97it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3133/3397 [00:39<00:03, 68.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3141/3397 [00:40<00:03, 70.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 3151/3397 [00:40<00:03, 75.76it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3160/3397 [00:40<00:03, 78.21it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3169/3397 [00:40<00:02, 78.95it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 3178/3397 [00:40<00:02, 77.43it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3186/3397 [00:40<00:02, 73.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 3198/3397 [00:40<00:02, 83.33it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 3207/3397 [00:40<00:02, 83.93it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3216/3397 [00:40<00:02, 82.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3225/3397 [00:41<00:02, 75.96it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 3233/3397 [00:41<00:02, 75.54it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 3241/3397 [00:41<00:02, 74.97it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 3249/3397 [00:41<00:02, 73.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 3259/3397 [00:41<00:01, 78.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3268/3397 [00:41<00:01, 79.10it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3277/3397 [00:41<00:01, 63.13it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3292/3397 [00:41<00:01, 82.88it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3302/3397 [00:42<00:01, 82.11it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3311/3397 [00:42<00:01, 60.82it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3332/3397 [00:42<00:00, 91.42it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3344/3397 [00:42<00:00, 87.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3355/3397 [00:42<00:00, 83.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3365/3397 [00:42<00:00, 79.20it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3374/3397 [00:43<00:00, 77.12it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3383/3397 [00:43<00:00, 75.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3391/3397 [00:43<00:00, 73.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3397/3397 [00:43<00:00, 78.34it/s]
Start Training...
Epoch 1 Step 1 Train Loss: 0.5148
Epoch 1 Step 51 Train Loss: 0.4735
Epoch 1 Step 101 Train Loss: 0.5093
Epoch 1 Step 151 Train Loss: 0.4732
Epoch 1 Step 201 Train Loss: 0.4064
Epoch 1 Step 251 Train Loss: 0.4401
Epoch 1 Step 301 Train Loss: 0.4645
Epoch 1 Step 351 Train Loss: 0.4312
Epoch 1 Step 401 Train Loss: 0.4504
Epoch 1 Step 451 Train Loss: 0.4465
Epoch 1 Step 501 Train Loss: 0.4354
Epoch 1 Step 551 Train Loss: 0.4255
Epoch 1 Step 601 Train Loss: 0.4060
Epoch 1 Step 651 Train Loss: 0.4449
Epoch 1 Step 701 Train Loss: 0.4415
Epoch 1 Step 751 Train Loss: 0.4274
Epoch 1 Step 801 Train Loss: 0.4633
Epoch 1 Step 851 Train Loss: 0.4178
Epoch 1 Step 901 Train Loss: 0.4717
Epoch 1 Step 951 Train Loss: 0.4178
Epoch 1 Step 1001 Train Loss: 0.4699
Epoch 1 Step 1051 Train Loss: 0.5559
Epoch 1 Step 1101 Train Loss: 0.4250
Epoch 1 Step 1151 Train Loss: 0.4326
Epoch 1 Step 1201 Train Loss: 0.4491
Epoch 1 Step 1251 Train Loss: 0.4867
Epoch 1 Step 1301 Train Loss: 0.4632
Epoch 1 Step 1351 Train Loss: 0.4313
Epoch 1 Step 1401 Train Loss: 0.3667
Epoch 1 Step 1451 Train Loss: 0.4175
Epoch 1 Step 1501 Train Loss: 0.4524
Epoch 1 Step 1551 Train Loss: 0.4364
Epoch 1 Step 1601 Train Loss: 0.3918
Epoch 1 Step 1651 Train Loss: 0.3720
Epoch 1 Step 1701 Train Loss: 0.5178
Epoch 1 Step 1751 Train Loss: 0.4320
Epoch 1 Step 1801 Train Loss: 0.4479
Epoch 1 Step 1851 Train Loss: 0.4613
Epoch 1: Train Overall MSE: 0.0033 Validation Overall MSE: 0.0045. 
Train Top 20 DE MSE: 0.1514 Validation Top 20 DE MSE: 0.1703. 
Epoch 2 Step 1 Train Loss: 0.5117
Epoch 2 Step 51 Train Loss: 0.4154
Epoch 2 Step 101 Train Loss: 0.3670
Epoch 2 Step 151 Train Loss: 0.4360
Epoch 2 Step 201 Train Loss: 0.4598
Epoch 2 Step 251 Train Loss: 0.4896
Epoch 2 Step 301 Train Loss: 0.4455
Epoch 2 Step 351 Train Loss: 0.4390
Epoch 2 Step 401 Train Loss: 0.4756
Epoch 2 Step 451 Train Loss: 0.4065
Epoch 2 Step 501 Train Loss: 0.4411
Epoch 2 Step 551 Train Loss: 0.4325
Epoch 2 Step 601 Train Loss: 0.4290
Epoch 2 Step 651 Train Loss: 0.4097
Epoch 2 Step 701 Train Loss: 0.4742
Epoch 2 Step 751 Train Loss: 0.4805
Epoch 2 Step 801 Train Loss: 0.4488
Epoch 2 Step 851 Train Loss: 0.4298
Epoch 2 Step 901 Train Loss: 0.4342
Epoch 2 Step 951 Train Loss: 0.5069
Epoch 2 Step 1001 Train Loss: 0.4791
Epoch 2 Step 1051 Train Loss: 0.4693
Epoch 2 Step 1101 Train Loss: 0.5519
Epoch 2 Step 1151 Train Loss: 0.4358
Epoch 2 Step 1201 Train Loss: 0.4280
Epoch 2 Step 1251 Train Loss: 0.4025
Epoch 2 Step 1301 Train Loss: 0.4223
Epoch 2 Step 1351 Train Loss: 0.4696
Epoch 2 Step 1401 Train Loss: 0.4170
Epoch 2 Step 1451 Train Loss: 0.4041
Epoch 2 Step 1501 Train Loss: 0.4214
Epoch 2 Step 1551 Train Loss: 0.4036
Epoch 2 Step 1601 Train Loss: 0.4134
Epoch 2 Step 1651 Train Loss: 0.4308
Epoch 2 Step 1701 Train Loss: 0.4290
Epoch 2 Step 1751 Train Loss: 0.4319
Epoch 2 Step 1801 Train Loss: 0.4286
Epoch 2 Step 1851 Train Loss: 0.4928
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0037. 
Train Top 20 DE MSE: 0.1339 Validation Top 20 DE MSE: 0.1771. 
Epoch 3 Step 1 Train Loss: 0.4394
Epoch 3 Step 51 Train Loss: 0.4304
Epoch 3 Step 101 Train Loss: 0.4478
Epoch 3 Step 151 Train Loss: 0.5152
Epoch 3 Step 201 Train Loss: 0.4624
Epoch 3 Step 251 Train Loss: 0.4414
Epoch 3 Step 301 Train Loss: 0.4319
Epoch 3 Step 351 Train Loss: 0.4251
Epoch 3 Step 401 Train Loss: 0.5389
Epoch 3 Step 451 Train Loss: 0.4436
Epoch 3 Step 501 Train Loss: 0.5148
Epoch 3 Step 551 Train Loss: 0.4813
Epoch 3 Step 601 Train Loss: 0.4836
Epoch 3 Step 651 Train Loss: 0.3877
Epoch 3 Step 701 Train Loss: 0.4524
Epoch 3 Step 751 Train Loss: 0.4366
Epoch 3 Step 801 Train Loss: 0.4529
Epoch 3 Step 851 Train Loss: 0.4140
Epoch 3 Step 901 Train Loss: 0.4738
Epoch 3 Step 951 Train Loss: 0.4485
Epoch 3 Step 1001 Train Loss: 0.4306
Epoch 3 Step 1051 Train Loss: 0.4815
Epoch 3 Step 1101 Train Loss: 0.4520
Epoch 3 Step 1151 Train Loss: 0.4223
Epoch 3 Step 1201 Train Loss: 0.4392
Epoch 3 Step 1251 Train Loss: 0.5022
Epoch 3 Step 1301 Train Loss: 0.4702
Epoch 3 Step 1351 Train Loss: 0.4403
Epoch 3 Step 1401 Train Loss: 0.4424
Epoch 3 Step 1451 Train Loss: 0.4543
Epoch 3 Step 1501 Train Loss: 0.4893
Epoch 3 Step 1551 Train Loss: 0.4015
Epoch 3 Step 1601 Train Loss: 0.4587
Epoch 3 Step 1651 Train Loss: 0.4759
Epoch 3 Step 1701 Train Loss: 0.4407
Epoch 3 Step 1751 Train Loss: 0.4999
Epoch 3 Step 1801 Train Loss: 0.4739
Epoch 3 Step 1851 Train Loss: 0.4769
Epoch 3: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0901 Validation Top 20 DE MSE: 0.1253. 
Epoch 4 Step 1 Train Loss: 0.4947
Epoch 4 Step 51 Train Loss: 0.4859
Epoch 4 Step 101 Train Loss: 0.4177
Epoch 4 Step 151 Train Loss: 0.4635
Epoch 4 Step 201 Train Loss: 0.4890
Epoch 4 Step 251 Train Loss: 0.4444
Epoch 4 Step 301 Train Loss: 0.4737
Epoch 4 Step 351 Train Loss: 0.4552
Epoch 4 Step 401 Train Loss: 0.4137
Epoch 4 Step 451 Train Loss: 0.4652
Epoch 4 Step 501 Train Loss: 0.4511
Epoch 4 Step 551 Train Loss: 0.4318
Epoch 4 Step 601 Train Loss: 0.4602
Epoch 4 Step 651 Train Loss: 0.4260
Epoch 4 Step 701 Train Loss: 0.4582
Epoch 4 Step 751 Train Loss: 0.4942
Epoch 4 Step 801 Train Loss: 0.4395
Epoch 4 Step 851 Train Loss: 0.4438
Epoch 4 Step 901 Train Loss: 0.3945
Epoch 4 Step 951 Train Loss: 0.4990
Epoch 4 Step 1001 Train Loss: 0.5379
Epoch 4 Step 1051 Train Loss: 0.4133
Epoch 4 Step 1101 Train Loss: 0.4346
Epoch 4 Step 1151 Train Loss: 0.4936
Epoch 4 Step 1201 Train Loss: 0.4569
Epoch 4 Step 1251 Train Loss: 0.4263
Epoch 4 Step 1301 Train Loss: 0.4989
Epoch 4 Step 1351 Train Loss: 0.4392
Epoch 4 Step 1401 Train Loss: 0.4787
Epoch 4 Step 1451 Train Loss: 0.4542
Epoch 4 Step 1501 Train Loss: 0.4363
Epoch 4 Step 1551 Train Loss: 0.4285
Epoch 4 Step 1601 Train Loss: 0.4229
Epoch 4 Step 1651 Train Loss: 0.5420
Epoch 4 Step 1701 Train Loss: 0.4271
Epoch 4 Step 1751 Train Loss: 0.4683
Epoch 4 Step 1801 Train Loss: 0.4996
Epoch 4 Step 1851 Train Loss: 0.4300
Epoch 4: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0877 Validation Top 20 DE MSE: 0.1282. 
Epoch 5 Step 1 Train Loss: 0.4390
Epoch 5 Step 51 Train Loss: 0.4558
Epoch 5 Step 101 Train Loss: 0.4754
Epoch 5 Step 151 Train Loss: 0.4502
Epoch 5 Step 201 Train Loss: 0.4272
Epoch 5 Step 251 Train Loss: 0.4083
Epoch 5 Step 301 Train Loss: 0.4415
Epoch 5 Step 351 Train Loss: 0.5201
Epoch 5 Step 401 Train Loss: 0.4841
Epoch 5 Step 451 Train Loss: 0.4652
Epoch 5 Step 501 Train Loss: 0.4887
Epoch 5 Step 551 Train Loss: 0.4679
Epoch 5 Step 601 Train Loss: 0.4814
Epoch 5 Step 651 Train Loss: 0.5113
Epoch 5 Step 701 Train Loss: 0.5273
Epoch 5 Step 751 Train Loss: 0.4916
Epoch 5 Step 801 Train Loss: 0.4761
Epoch 5 Step 851 Train Loss: 0.4919
Epoch 5 Step 901 Train Loss: 0.5147
Epoch 5 Step 951 Train Loss: 0.5002
Epoch 5 Step 1001 Train Loss: 0.4986
Epoch 5 Step 1051 Train Loss: 0.4613
Epoch 5 Step 1101 Train Loss: 0.4637
Epoch 5 Step 1151 Train Loss: 0.4358
Epoch 5 Step 1201 Train Loss: 0.4657
Epoch 5 Step 1251 Train Loss: 0.4562
Epoch 5 Step 1301 Train Loss: 0.4604
Epoch 5 Step 1351 Train Loss: 0.4334
Epoch 5 Step 1401 Train Loss: 0.4493
Epoch 5 Step 1451 Train Loss: 0.5333
Epoch 5 Step 1501 Train Loss: 0.4412
Epoch 5 Step 1551 Train Loss: 0.4343
Epoch 5 Step 1601 Train Loss: 0.4730
Epoch 5 Step 1651 Train Loss: 0.4760
Epoch 5 Step 1701 Train Loss: 0.4530
Epoch 5 Step 1751 Train Loss: 0.5004
Epoch 5 Step 1801 Train Loss: 0.4100
Epoch 5 Step 1851 Train Loss: 0.4136
Epoch 5: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0805 Validation Top 20 DE MSE: 0.1177. 
Epoch 6 Step 1 Train Loss: 0.4699
Epoch 6 Step 51 Train Loss: 0.4370
Epoch 6 Step 101 Train Loss: 0.4641
Epoch 6 Step 151 Train Loss: 0.6052
Epoch 6 Step 201 Train Loss: 0.5038
Epoch 6 Step 251 Train Loss: 0.4269
Epoch 6 Step 301 Train Loss: 0.4358
Epoch 6 Step 351 Train Loss: 0.4770
Epoch 6 Step 401 Train Loss: 0.4985
Epoch 6 Step 451 Train Loss: 0.4466
Epoch 6 Step 501 Train Loss: 0.4359
Epoch 6 Step 551 Train Loss: 0.4645
Epoch 6 Step 601 Train Loss: 0.4599
Epoch 6 Step 651 Train Loss: 0.4574
Epoch 6 Step 701 Train Loss: 0.4856
Epoch 6 Step 751 Train Loss: 0.4907
Epoch 6 Step 801 Train Loss: 0.4716
Epoch 6 Step 851 Train Loss: 0.4457
Epoch 6 Step 901 Train Loss: 0.4436
Epoch 6 Step 951 Train Loss: 0.4511
Epoch 6 Step 1001 Train Loss: 0.4589
Epoch 6 Step 1051 Train Loss: 0.4752
Epoch 6 Step 1101 Train Loss: 0.4668
Epoch 6 Step 1151 Train Loss: 0.4306
Epoch 6 Step 1201 Train Loss: 0.4380
Epoch 6 Step 1251 Train Loss: 0.4864
Epoch 6 Step 1301 Train Loss: 0.4616
Epoch 6 Step 1351 Train Loss: 0.4603
Epoch 6 Step 1401 Train Loss: 0.4601
Epoch 6 Step 1451 Train Loss: 0.4278
Epoch 6 Step 1501 Train Loss: 0.4770
Epoch 6 Step 1551 Train Loss: 0.4322
Epoch 6 Step 1601 Train Loss: 0.4563
Epoch 6 Step 1651 Train Loss: 0.4759
Epoch 6 Step 1701 Train Loss: 0.4894
Epoch 6 Step 1751 Train Loss: 0.4721
Epoch 6 Step 1801 Train Loss: 0.4536
Epoch 6 Step 1851 Train Loss: 0.4730
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0798 Validation Top 20 DE MSE: 0.1227. 
Epoch 7 Step 1 Train Loss: 0.5112
Epoch 7 Step 51 Train Loss: 0.5960
Epoch 7 Step 101 Train Loss: 0.4439
Epoch 7 Step 151 Train Loss: 0.4566
Epoch 7 Step 201 Train Loss: 0.4748
Epoch 7 Step 251 Train Loss: 0.4648
Epoch 7 Step 301 Train Loss: 0.4265
Epoch 7 Step 351 Train Loss: 0.4994
Epoch 7 Step 401 Train Loss: 0.4685
Epoch 7 Step 451 Train Loss: 0.4952
Epoch 7 Step 501 Train Loss: 0.4698
Epoch 7 Step 551 Train Loss: 0.4475
Epoch 7 Step 601 Train Loss: 0.4260
Epoch 7 Step 651 Train Loss: 0.4951
Epoch 7 Step 701 Train Loss: 0.4837
Epoch 7 Step 751 Train Loss: 0.4359
Epoch 7 Step 801 Train Loss: 0.5283
Epoch 7 Step 851 Train Loss: 0.4672
Epoch 7 Step 901 Train Loss: 0.4192
Epoch 7 Step 951 Train Loss: 0.4797
Epoch 7 Step 1001 Train Loss: 0.4857
Epoch 7 Step 1051 Train Loss: 0.4185
Epoch 7 Step 1101 Train Loss: 0.4403
Epoch 7 Step 1151 Train Loss: 0.4180
Epoch 7 Step 1201 Train Loss: 0.4340
Epoch 7 Step 1251 Train Loss: 0.4549
Epoch 7 Step 1301 Train Loss: 0.4515
Epoch 7 Step 1351 Train Loss: 0.4647
Epoch 7 Step 1401 Train Loss: 0.4689
Epoch 7 Step 1451 Train Loss: 0.4545
Epoch 7 Step 1501 Train Loss: 0.4593
Epoch 7 Step 1551 Train Loss: 0.4612
Epoch 7 Step 1601 Train Loss: 0.4341
Epoch 7 Step 1651 Train Loss: 0.4443
Epoch 7 Step 1701 Train Loss: 0.5161
Epoch 7 Step 1751 Train Loss: 0.4980
Epoch 7 Step 1801 Train Loss: 0.4948
Epoch 7 Step 1851 Train Loss: 0.5090
Epoch 7: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0776 Validation Top 20 DE MSE: 0.1180. 
Epoch 8 Step 1 Train Loss: 0.4871
Epoch 8 Step 51 Train Loss: 0.4949
Epoch 8 Step 101 Train Loss: 0.5143
Epoch 8 Step 151 Train Loss: 0.4990
Epoch 8 Step 201 Train Loss: 0.4621
Epoch 8 Step 251 Train Loss: 0.4722
Epoch 8 Step 301 Train Loss: 0.4586
Epoch 8 Step 351 Train Loss: 0.4664
Epoch 8 Step 401 Train Loss: 0.4183
Epoch 8 Step 451 Train Loss: 0.4669
Epoch 8 Step 501 Train Loss: 0.4903
Epoch 8 Step 551 Train Loss: 0.4838
Epoch 8 Step 601 Train Loss: 0.4538
Epoch 8 Step 651 Train Loss: 0.4655
Epoch 8 Step 701 Train Loss: 0.4455
Epoch 8 Step 751 Train Loss: 0.6038
Epoch 8 Step 801 Train Loss: 0.4440
Epoch 8 Step 851 Train Loss: 0.4350
Epoch 8 Step 901 Train Loss: 0.5373
Epoch 8 Step 951 Train Loss: 0.4978
Epoch 8 Step 1001 Train Loss: 0.4319
Epoch 8 Step 1051 Train Loss: 0.5024
Epoch 8 Step 1101 Train Loss: 0.4320
Epoch 8 Step 1151 Train Loss: 0.4618
Epoch 8 Step 1201 Train Loss: 0.4494
Epoch 8 Step 1251 Train Loss: 0.4445
Epoch 8 Step 1301 Train Loss: 0.4638
Epoch 8 Step 1351 Train Loss: 0.5063
Epoch 8 Step 1401 Train Loss: 0.5063
Epoch 8 Step 1451 Train Loss: 0.4576
Epoch 8 Step 1501 Train Loss: 0.5168
Epoch 8 Step 1551 Train Loss: 0.4626
Epoch 8 Step 1601 Train Loss: 0.5140
Epoch 8 Step 1651 Train Loss: 0.4345
Epoch 8 Step 1701 Train Loss: 0.4913
Epoch 8 Step 1751 Train Loss: 0.4335
Epoch 8 Step 1801 Train Loss: 0.4614
Epoch 8 Step 1851 Train Loss: 0.4605
Epoch 8: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0786 Validation Top 20 DE MSE: 0.1221. 
Epoch 9 Step 1 Train Loss: 0.4190
Epoch 9 Step 51 Train Loss: 0.4162
Epoch 9 Step 101 Train Loss: 0.4011
Epoch 9 Step 151 Train Loss: 0.4971
Epoch 9 Step 201 Train Loss: 0.5316
Epoch 9 Step 251 Train Loss: 0.4821
Epoch 9 Step 301 Train Loss: 0.4564
Epoch 9 Step 351 Train Loss: 0.4535
Epoch 9 Step 401 Train Loss: 0.4297
Epoch 9 Step 451 Train Loss: 0.4919
Epoch 9 Step 501 Train Loss: 0.4951
Epoch 9 Step 551 Train Loss: 0.5217
Epoch 9 Step 601 Train Loss: 0.4428
Epoch 9 Step 651 Train Loss: 0.5577
Epoch 9 Step 701 Train Loss: 0.4663
Epoch 9 Step 751 Train Loss: 0.4661
Epoch 9 Step 801 Train Loss: 0.4174
Epoch 9 Step 851 Train Loss: 0.4428
Epoch 9 Step 901 Train Loss: 0.4247
Epoch 9 Step 951 Train Loss: 0.5250
Epoch 9 Step 1001 Train Loss: 0.4580
Epoch 9 Step 1051 Train Loss: 0.4598
Epoch 9 Step 1101 Train Loss: 0.5086
Epoch 9 Step 1151 Train Loss: 0.4284
Epoch 9 Step 1201 Train Loss: 0.4938
Epoch 9 Step 1251 Train Loss: 0.4703
Epoch 9 Step 1301 Train Loss: 0.4945
Epoch 9 Step 1351 Train Loss: 0.4769
Epoch 9 Step 1401 Train Loss: 0.4291
Epoch 9 Step 1451 Train Loss: 0.4152
Epoch 9 Step 1501 Train Loss: 0.4741
Epoch 9 Step 1551 Train Loss: 0.4760
Epoch 9 Step 1601 Train Loss: 0.4476
Epoch 9 Step 1651 Train Loss: 0.4613
Epoch 9 Step 1701 Train Loss: 0.5023
Epoch 9 Step 1751 Train Loss: 0.4653
Epoch 9 Step 1801 Train Loss: 0.4651
Epoch 9 Step 1851 Train Loss: 0.5083
Epoch 9: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0814 Validation Top 20 DE MSE: 0.1234. 
Epoch 10 Step 1 Train Loss: 0.5117
Epoch 10 Step 51 Train Loss: 0.4627
Epoch 10 Step 101 Train Loss: 0.5359
Epoch 10 Step 151 Train Loss: 0.4541
Epoch 10 Step 201 Train Loss: 0.4247
Epoch 10 Step 251 Train Loss: 0.4685
Epoch 10 Step 301 Train Loss: 0.4686
Epoch 10 Step 351 Train Loss: 0.4889
Epoch 10 Step 401 Train Loss: 0.4624
Epoch 10 Step 451 Train Loss: 0.4598
Epoch 10 Step 501 Train Loss: 0.4781
Epoch 10 Step 551 Train Loss: 0.4576
Epoch 10 Step 601 Train Loss: 0.4835
Epoch 10 Step 651 Train Loss: 0.4476
Epoch 10 Step 701 Train Loss: 0.4334
Epoch 10 Step 751 Train Loss: 0.4679
Epoch 10 Step 801 Train Loss: 0.4367
Epoch 10 Step 851 Train Loss: 0.4404
Epoch 10 Step 901 Train Loss: 0.4365
Epoch 10 Step 951 Train Loss: 0.4659
Epoch 10 Step 1001 Train Loss: 0.4790
Epoch 10 Step 1051 Train Loss: 0.5081
Epoch 10 Step 1101 Train Loss: 0.4616
Epoch 10 Step 1151 Train Loss: 0.4560
Epoch 10 Step 1201 Train Loss: 0.5344
Epoch 10 Step 1251 Train Loss: 0.4685
Epoch 10 Step 1301 Train Loss: 0.5315
Epoch 10 Step 1351 Train Loss: 0.4828
Epoch 10 Step 1401 Train Loss: 0.4763
Epoch 10 Step 1451 Train Loss: 0.4832
Epoch 10 Step 1501 Train Loss: 0.4249
Epoch 10 Step 1551 Train Loss: 0.4655
Epoch 10 Step 1601 Train Loss: 0.4604
Epoch 10 Step 1651 Train Loss: 0.4547
Epoch 10 Step 1701 Train Loss: 0.4543
Epoch 10 Step 1751 Train Loss: 0.5065
Epoch 10 Step 1801 Train Loss: 0.4683
Epoch 10 Step 1851 Train Loss: 0.5044
Epoch 10: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0860 Validation Top 20 DE MSE: 0.1281. 
Epoch 11 Step 1 Train Loss: 0.4694
Epoch 11 Step 51 Train Loss: 0.5021
Epoch 11 Step 101 Train Loss: 0.4597
Epoch 11 Step 151 Train Loss: 0.4618
Epoch 11 Step 201 Train Loss: 0.4908
Epoch 11 Step 251 Train Loss: 0.4885
Epoch 11 Step 301 Train Loss: 0.4596
Epoch 11 Step 351 Train Loss: 0.4939
Epoch 11 Step 401 Train Loss: 0.4739
Epoch 11 Step 451 Train Loss: 0.5007
Epoch 11 Step 501 Train Loss: 0.4623
Epoch 11 Step 551 Train Loss: 0.4320
Epoch 11 Step 601 Train Loss: 0.4960
Epoch 11 Step 651 Train Loss: 0.4627
Epoch 11 Step 701 Train Loss: 0.4536
Epoch 11 Step 751 Train Loss: 0.4522
Epoch 11 Step 801 Train Loss: 0.4679
Epoch 11 Step 851 Train Loss: 0.4909
Epoch 11 Step 901 Train Loss: 0.4791
Epoch 11 Step 951 Train Loss: 0.4382
Epoch 11 Step 1001 Train Loss: 0.5399
Epoch 11 Step 1051 Train Loss: 0.4877
Epoch 11 Step 1101 Train Loss: 0.4584
Epoch 11 Step 1151 Train Loss: 0.4736
Epoch 11 Step 1201 Train Loss: 0.4456
Epoch 11 Step 1251 Train Loss: 0.4315
Epoch 11 Step 1301 Train Loss: 0.4407
Epoch 11 Step 1351 Train Loss: 0.5286
Epoch 11 Step 1401 Train Loss: 0.4766
Epoch 11 Step 1451 Train Loss: 0.4500
Epoch 11 Step 1501 Train Loss: 0.5345
Epoch 11 Step 1551 Train Loss: 0.4806
Epoch 11 Step 1601 Train Loss: 0.4604
Epoch 11 Step 1651 Train Loss: 0.4864
Epoch 11 Step 1701 Train Loss: 0.4570
Epoch 11 Step 1751 Train Loss: 0.4750
Epoch 11 Step 1801 Train Loss: 0.5300
Epoch 11 Step 1851 Train Loss: 0.4236
Epoch 11: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0799 Validation Top 20 DE MSE: 0.1222. 
Epoch 12 Step 1 Train Loss: 0.4695
Epoch 12 Step 51 Train Loss: 0.4974
Epoch 12 Step 101 Train Loss: 0.5266
Epoch 12 Step 151 Train Loss: 0.4972
Epoch 12 Step 201 Train Loss: 0.4082
Epoch 12 Step 251 Train Loss: 0.4492
Epoch 12 Step 301 Train Loss: 0.4721
Epoch 12 Step 351 Train Loss: 0.4811
Epoch 12 Step 401 Train Loss: 0.4493
Epoch 12 Step 451 Train Loss: 0.4400
Epoch 12 Step 501 Train Loss: 0.4415
Epoch 12 Step 551 Train Loss: 0.5179
Epoch 12 Step 601 Train Loss: 0.4468
Epoch 12 Step 651 Train Loss: 0.4437
Epoch 12 Step 701 Train Loss: 0.5045
Epoch 12 Step 751 Train Loss: 0.4182
Epoch 12 Step 801 Train Loss: 0.4587
Epoch 12 Step 851 Train Loss: 0.4414
Epoch 12 Step 901 Train Loss: 0.4855
Epoch 12 Step 951 Train Loss: 0.4681
Epoch 12 Step 1001 Train Loss: 0.4610
Epoch 12 Step 1051 Train Loss: 0.5412
Epoch 12 Step 1101 Train Loss: 0.5054
Epoch 12 Step 1151 Train Loss: 0.4892
Epoch 12 Step 1201 Train Loss: 0.4763
Epoch 12 Step 1251 Train Loss: 0.4372
Epoch 12 Step 1301 Train Loss: 0.4632
Epoch 12 Step 1351 Train Loss: 0.4774
Epoch 12 Step 1401 Train Loss: 0.4616
Epoch 12 Step 1451 Train Loss: 0.4090
Epoch 12 Step 1501 Train Loss: 0.4807
Epoch 12 Step 1551 Train Loss: 0.4598
Epoch 12 Step 1601 Train Loss: 0.4594
Epoch 12 Step 1651 Train Loss: 0.4788
Epoch 12 Step 1701 Train Loss: 0.4516
Epoch 12 Step 1751 Train Loss: 0.4511
Epoch 12 Step 1801 Train Loss: 0.4750
Epoch 12 Step 1851 Train Loss: 0.4646
Epoch 12: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0807 Validation Top 20 DE MSE: 0.1225. 
Epoch 13 Step 1 Train Loss: 0.4650
Epoch 13 Step 51 Train Loss: 0.4892
Epoch 13 Step 101 Train Loss: 0.4682
Epoch 13 Step 151 Train Loss: 0.4504
Epoch 13 Step 201 Train Loss: 0.4737
Epoch 13 Step 251 Train Loss: 0.4859
Epoch 13 Step 301 Train Loss: 0.5020
Epoch 13 Step 351 Train Loss: 0.4458
Epoch 13 Step 401 Train Loss: 0.4553
Epoch 13 Step 451 Train Loss: 0.4245
Epoch 13 Step 501 Train Loss: 0.4308
Epoch 13 Step 551 Train Loss: 0.5216
Epoch 13 Step 601 Train Loss: 0.4835
Epoch 13 Step 651 Train Loss: 0.4527
Epoch 13 Step 701 Train Loss: 0.4343
Epoch 13 Step 751 Train Loss: 0.5529
Epoch 13 Step 801 Train Loss: 0.4891
Epoch 13 Step 851 Train Loss: 0.5265
Epoch 13 Step 901 Train Loss: 0.4432
Epoch 13 Step 951 Train Loss: 0.4588
Epoch 13 Step 1001 Train Loss: 0.4947
Epoch 13 Step 1051 Train Loss: 0.4508
Epoch 13 Step 1101 Train Loss: 0.4612
Epoch 13 Step 1151 Train Loss: 0.4406
Epoch 13 Step 1201 Train Loss: 0.4428
Epoch 13 Step 1251 Train Loss: 0.4355
Epoch 13 Step 1301 Train Loss: 0.4954
Epoch 13 Step 1351 Train Loss: 0.4779
Epoch 13 Step 1401 Train Loss: 0.5284
Epoch 13 Step 1451 Train Loss: 0.4808
Epoch 13 Step 1501 Train Loss: 0.4630
Epoch 13 Step 1551 Train Loss: 0.4852
Epoch 13 Step 1601 Train Loss: 0.4643
Epoch 13 Step 1651 Train Loss: 0.5316
Epoch 13 Step 1701 Train Loss: 0.5774
Epoch 13 Step 1751 Train Loss: 0.4633
Epoch 13 Step 1801 Train Loss: 0.5174
Epoch 13 Step 1851 Train Loss: 0.4881
Epoch 13: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0827 Validation Top 20 DE MSE: 0.1247. 
Epoch 14 Step 1 Train Loss: 0.4775
Epoch 14 Step 51 Train Loss: 0.5315
Epoch 14 Step 101 Train Loss: 0.4826
Epoch 14 Step 151 Train Loss: 0.4311
Epoch 14 Step 201 Train Loss: 0.5262
Epoch 14 Step 251 Train Loss: 0.4633
Epoch 14 Step 301 Train Loss: 0.5171
Epoch 14 Step 351 Train Loss: 0.4768
Epoch 14 Step 401 Train Loss: 0.5033
Epoch 14 Step 451 Train Loss: 0.4591
Epoch 14 Step 501 Train Loss: 0.4422
Epoch 14 Step 551 Train Loss: 0.4489
Epoch 14 Step 601 Train Loss: 0.4568
Epoch 14 Step 651 Train Loss: 0.4467
Epoch 14 Step 701 Train Loss: 0.5347
Epoch 14 Step 751 Train Loss: 0.4619
Epoch 14 Step 801 Train Loss: 0.4896
Epoch 14 Step 851 Train Loss: 0.4949
Epoch 14 Step 901 Train Loss: 0.4212
Epoch 14 Step 951 Train Loss: 0.4610
Epoch 14 Step 1001 Train Loss: 0.4587
Epoch 14 Step 1051 Train Loss: 0.4675
Epoch 14 Step 1101 Train Loss: 0.4763
Epoch 14 Step 1151 Train Loss: 0.4584
Epoch 14 Step 1201 Train Loss: 0.4539
Epoch 14 Step 1251 Train Loss: 0.4893
Epoch 14 Step 1301 Train Loss: 0.4428
Epoch 14 Step 1351 Train Loss: 0.4855
Epoch 14 Step 1401 Train Loss: 0.4484
Epoch 14 Step 1451 Train Loss: 0.5142
Epoch 14 Step 1501 Train Loss: 0.5356
Epoch 14 Step 1551 Train Loss: 0.4949
Epoch 14 Step 1601 Train Loss: 0.4424
Epoch 14 Step 1651 Train Loss: 0.4517
Epoch 14 Step 1701 Train Loss: 0.4421
Epoch 14 Step 1751 Train Loss: 0.5085
Epoch 14 Step 1801 Train Loss: 0.4795
Epoch 14 Step 1851 Train Loss: 0.4271
Epoch 14: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0789 Validation Top 20 DE MSE: 0.1211. 
Epoch 15 Step 1 Train Loss: 0.4791
Epoch 15 Step 51 Train Loss: 0.4407
Epoch 15 Step 101 Train Loss: 0.4627
Epoch 15 Step 151 Train Loss: 0.4431
Epoch 15 Step 201 Train Loss: 0.4672
Epoch 15 Step 251 Train Loss: 0.4602
Epoch 15 Step 301 Train Loss: 0.4331
Epoch 15 Step 351 Train Loss: 0.4167
Epoch 15 Step 401 Train Loss: 0.4548
Epoch 15 Step 451 Train Loss: 0.4579
Epoch 15 Step 501 Train Loss: 0.4846
Epoch 15 Step 551 Train Loss: 0.4798
Epoch 15 Step 601 Train Loss: 0.4747
Epoch 15 Step 651 Train Loss: 0.5116
Epoch 15 Step 701 Train Loss: 0.4762
Epoch 15 Step 751 Train Loss: 0.4030
Epoch 15 Step 801 Train Loss: 0.5272
Epoch 15 Step 851 Train Loss: 0.4968
Epoch 15 Step 901 Train Loss: 0.4669
Epoch 15 Step 951 Train Loss: 0.4623
Epoch 15 Step 1001 Train Loss: 0.4680
Epoch 15 Step 1051 Train Loss: 0.5024
Epoch 15 Step 1101 Train Loss: 0.4415
Epoch 15 Step 1151 Train Loss: 0.4600
Epoch 15 Step 1201 Train Loss: 0.4464
Epoch 15 Step 1251 Train Loss: 0.4341
Epoch 15 Step 1301 Train Loss: 0.4782
Epoch 15 Step 1351 Train Loss: 0.4680
Epoch 15 Step 1401 Train Loss: 0.4897
Epoch 15 Step 1451 Train Loss: 0.4267
Epoch 15 Step 1501 Train Loss: 0.4657
Epoch 15 Step 1551 Train Loss: 0.4931
Epoch 15 Step 1601 Train Loss: 0.4800
Epoch 15 Step 1651 Train Loss: 0.4891
Epoch 15 Step 1701 Train Loss: 0.4924
Epoch 15 Step 1751 Train Loss: 0.5305
Epoch 15 Step 1801 Train Loss: 0.4903
Epoch 15 Step 1851 Train Loss: 0.4291
Epoch 15: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0805 Validation Top 20 DE MSE: 0.1224. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1259
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.0040438725
test_combo_seen0_pearson: 0.9907694350118788
test_combo_seen0_mse_de: 0.10141131
test_combo_seen0_pearson_de: 0.6456405624570642
test_combo_seen1_mse: 0.004763228
test_combo_seen1_pearson: 0.9893693476337381
test_combo_seen1_mse_de: 0.13062118
test_combo_seen1_pearson_de: 0.7734484928459786
test_combo_seen2_mse: 0.0036216646
test_combo_seen2_pearson: 0.992146671496395
test_combo_seen2_mse_de: 0.10608461
test_combo_seen2_pearson_de: 0.955352658504735
test_unseen_single_mse: 0.0024377897
test_unseen_single_pearson: 0.994387184446637
test_unseen_single_mse_de: 0.1373963
test_unseen_single_pearson_de: 0.9119504000333615
test_combo_seen0_pearson_delta: 0.6199085876117522
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.10555555555555557
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.9166666666666666
test_combo_seen0_mse_top20_de_non_dropout: 0.14046647
test_combo_seen1_pearson_delta: 0.597248226544245
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.12019230769230768
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8807692307692309
test_combo_seen1_mse_top20_de_non_dropout: 0.16629666
test_combo_seen2_pearson_delta: 0.6266800350854522
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.09705882352941177
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.9411764705882352
test_combo_seen2_mse_top20_de_non_dropout: 0.11177315
test_unseen_single_pearson_delta: 0.4208471559827482
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.24814814814814815
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9037037037037037
test_unseen_single_mse_top20_de_non_dropout: 0.15573166
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.039 MB uploadedwandb: | 0.001 MB of 0.039 MB uploadedwandb: / 0.037 MB of 0.039 MB uploadedwandb: - 0.037 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÉ
wandb:                                                   val_de_mse ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                                  val_pearson ‚ñÅ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.10556
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.91667
wandb:                                         test_combo_seen0_mse 0.00404
wandb:                                      test_combo_seen0_mse_de 0.10141
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.14047
wandb:                                     test_combo_seen0_pearson 0.99077
wandb:                                  test_combo_seen0_pearson_de 0.64564
wandb:                               test_combo_seen0_pearson_delta 0.61991
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.12019
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.88077
wandb:                                         test_combo_seen1_mse 0.00476
wandb:                                      test_combo_seen1_mse_de 0.13062
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.1663
wandb:                                     test_combo_seen1_pearson 0.98937
wandb:                                  test_combo_seen1_pearson_de 0.77345
wandb:                               test_combo_seen1_pearson_delta 0.59725
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.09706
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.94118
wandb:                                         test_combo_seen2_mse 0.00362
wandb:                                      test_combo_seen2_mse_de 0.10608
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.11177
wandb:                                     test_combo_seen2_pearson 0.99215
wandb:                                  test_combo_seen2_pearson_de 0.95535
wandb:                               test_combo_seen2_pearson_delta 0.62668
wandb:                                                  test_de_mse 0.12589
wandb:                                              test_de_pearson 0.82756
wandb:               test_frac_opposite_direction_top20_non_dropout 0.1481
wandb:                          test_frac_sigma_below_1_non_dropout 0.89952
wandb:                                                     test_mse 0.00392
wandb:                                test_mse_top20_de_non_dropout 0.15254
wandb:                                                 test_pearson 0.99123
wandb:                                           test_pearson_delta 0.5586
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.24815
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.9037
wandb:                                       test_unseen_single_mse 0.00244
wandb:                                    test_unseen_single_mse_de 0.1374
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.15573
wandb:                                   test_unseen_single_pearson 0.99439
wandb:                                test_unseen_single_pearson_de 0.91195
wandb:                             test_unseen_single_pearson_delta 0.42085
wandb:                                                 train_de_mse 0.08048
wandb:                                             train_de_pearson 0.92109
wandb:                                                    train_mse 0.00194
wandb:                                                train_pearson 0.99568
wandb:                                                training_loss 0.47081
wandb:                                                   val_de_mse 0.12241
wandb:                                               val_de_pearson 0.82872
wandb:                                                      val_mse 0.00326
wandb:                                                  val_pearson 0.99263
wandb: 
wandb: üöÄ View run geneformer_NormanWeissman2019_split1 at: https://wandb.ai/zhoumin1130/New_gears/runs/hos1hq7n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_221812-hos1hq7n/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:7
combo_seen1:51
combo_seen2:18
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240921_232804-jyar7byl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_NormanWeissman2019_split2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/jyar7byl
wandb: WARNING Serializing object of type ndarray that is 20631680 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4456
Epoch 1 Step 51 Train Loss: 0.4295
Epoch 1 Step 101 Train Loss: 0.4882
Epoch 1 Step 151 Train Loss: 0.4450
Epoch 1 Step 201 Train Loss: 0.4710
Epoch 1 Step 251 Train Loss: 0.4153
Epoch 1 Step 301 Train Loss: 0.4172
Epoch 1 Step 351 Train Loss: 0.4398
Epoch 1 Step 401 Train Loss: 0.3994
Epoch 1 Step 451 Train Loss: 0.5307
Epoch 1 Step 501 Train Loss: 0.4562
Epoch 1 Step 551 Train Loss: 0.4172
Epoch 1 Step 601 Train Loss: 0.4929
Epoch 1 Step 651 Train Loss: 0.4404
Epoch 1 Step 701 Train Loss: 0.4454
Epoch 1 Step 751 Train Loss: 0.4769
Epoch 1 Step 801 Train Loss: 0.4758
Epoch 1 Step 851 Train Loss: 0.4505
Epoch 1 Step 901 Train Loss: 0.4724
Epoch 1 Step 951 Train Loss: 0.4257
Epoch 1 Step 1001 Train Loss: 0.4902
Epoch 1 Step 1051 Train Loss: 0.4735
Epoch 1 Step 1101 Train Loss: 0.4531
Epoch 1 Step 1151 Train Loss: 0.4211
Epoch 1 Step 1201 Train Loss: 0.4559
Epoch 1 Step 1251 Train Loss: 0.4923
Epoch 1 Step 1301 Train Loss: 0.4246
Epoch 1 Step 1351 Train Loss: 0.4350
Epoch 1 Step 1401 Train Loss: 0.3818
Epoch 1 Step 1451 Train Loss: 0.4635
Epoch 1 Step 1501 Train Loss: 0.4256
Epoch 1 Step 1551 Train Loss: 0.4321
Epoch 1 Step 1601 Train Loss: 0.4479
Epoch 1 Step 1651 Train Loss: 0.4307
Epoch 1 Step 1701 Train Loss: 0.4439
Epoch 1 Step 1751 Train Loss: 0.4801
Epoch 1 Step 1801 Train Loss: 0.4090
Epoch 1 Step 1851 Train Loss: 0.4124
Epoch 1: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.1387 Validation Top 20 DE MSE: 0.1753. 
Epoch 2 Step 1 Train Loss: 0.5251
Epoch 2 Step 51 Train Loss: 0.4665
Epoch 2 Step 101 Train Loss: 0.4985
Epoch 2 Step 151 Train Loss: 0.4711
Epoch 2 Step 201 Train Loss: 0.4417
Epoch 2 Step 251 Train Loss: 0.4455
Epoch 2 Step 301 Train Loss: 0.4725
Epoch 2 Step 351 Train Loss: 0.4499
Epoch 2 Step 401 Train Loss: 0.4160
Epoch 2 Step 451 Train Loss: 0.3989
Epoch 2 Step 501 Train Loss: 0.5134
Epoch 2 Step 551 Train Loss: 0.4468
Epoch 2 Step 601 Train Loss: 0.4717
Epoch 2 Step 651 Train Loss: 0.4237
Epoch 2 Step 701 Train Loss: 0.4915
Epoch 2 Step 751 Train Loss: 0.4558
Epoch 2 Step 801 Train Loss: 0.4405
Epoch 2 Step 851 Train Loss: 0.5318
Epoch 2 Step 901 Train Loss: 0.4302
Epoch 2 Step 951 Train Loss: 0.4447
Epoch 2 Step 1001 Train Loss: 0.4232
Epoch 2 Step 1051 Train Loss: 0.5175
Epoch 2 Step 1101 Train Loss: 0.4290
Epoch 2 Step 1151 Train Loss: 0.4766
Epoch 2 Step 1201 Train Loss: 0.4671
Epoch 2 Step 1251 Train Loss: 0.4648
Epoch 2 Step 1301 Train Loss: 0.4120
Epoch 2 Step 1351 Train Loss: 0.4206
Epoch 2 Step 1401 Train Loss: 0.4154
Epoch 2 Step 1451 Train Loss: 0.4569
Epoch 2 Step 1501 Train Loss: 0.4053
Epoch 2 Step 1551 Train Loss: 0.4985
Epoch 2 Step 1601 Train Loss: 0.5202
Epoch 2 Step 1651 Train Loss: 0.4338
Epoch 2 Step 1701 Train Loss: 0.4286
Epoch 2 Step 1751 Train Loss: 0.4032
Epoch 2 Step 1801 Train Loss: 0.4497
Epoch 2 Step 1851 Train Loss: 0.4622
Epoch 2: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.1097 Validation Top 20 DE MSE: 0.1683. 
Epoch 3 Step 1 Train Loss: 0.4331
Epoch 3 Step 51 Train Loss: 0.4767
Epoch 3 Step 101 Train Loss: 0.5237
Epoch 3 Step 151 Train Loss: 0.4656
Epoch 3 Step 201 Train Loss: 0.4466
Epoch 3 Step 251 Train Loss: 0.4606
Epoch 3 Step 301 Train Loss: 0.4345
Epoch 3 Step 351 Train Loss: 0.4393
Epoch 3 Step 401 Train Loss: 0.4847
Epoch 3 Step 451 Train Loss: 0.4410
Epoch 3 Step 501 Train Loss: 0.4874
Epoch 3 Step 551 Train Loss: 0.4760
Epoch 3 Step 601 Train Loss: 0.4759
Epoch 3 Step 651 Train Loss: 0.4524
Epoch 3 Step 701 Train Loss: 0.4703
Epoch 3 Step 751 Train Loss: 0.4478
Epoch 3 Step 801 Train Loss: 0.4379
Epoch 3 Step 851 Train Loss: 0.5134
Epoch 3 Step 901 Train Loss: 0.4052
Epoch 3 Step 951 Train Loss: 0.4165
Epoch 3 Step 1001 Train Loss: 0.4751
Epoch 3 Step 1051 Train Loss: 0.4620
Epoch 3 Step 1101 Train Loss: 0.4216
Epoch 3 Step 1151 Train Loss: 0.4211
Epoch 3 Step 1201 Train Loss: 0.4025
Epoch 3 Step 1251 Train Loss: 0.5094
Epoch 3 Step 1301 Train Loss: 0.4649
Epoch 3 Step 1351 Train Loss: 0.4533
Epoch 3 Step 1401 Train Loss: 0.4585
Epoch 3 Step 1451 Train Loss: 0.4556
Epoch 3 Step 1501 Train Loss: 0.4541
Epoch 3 Step 1551 Train Loss: 0.4085
Epoch 3 Step 1601 Train Loss: 0.4491
Epoch 3 Step 1651 Train Loss: 0.4051
Epoch 3 Step 1701 Train Loss: 0.4240
Epoch 3 Step 1751 Train Loss: 0.4244
Epoch 3 Step 1801 Train Loss: 0.4253
Epoch 3 Step 1851 Train Loss: 0.3981
Epoch 3: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.1093 Validation Top 20 DE MSE: 0.1599. 
Epoch 4 Step 1 Train Loss: 0.4241
Epoch 4 Step 51 Train Loss: 0.4671
Epoch 4 Step 101 Train Loss: 0.4341
Epoch 4 Step 151 Train Loss: 0.4554
Epoch 4 Step 201 Train Loss: 0.4587
Epoch 4 Step 251 Train Loss: 0.5159
Epoch 4 Step 301 Train Loss: 0.4439
Epoch 4 Step 351 Train Loss: 0.5163
Epoch 4 Step 401 Train Loss: 0.4227
Epoch 4 Step 451 Train Loss: 0.4740
Epoch 4 Step 501 Train Loss: 0.5608
Epoch 4 Step 551 Train Loss: 0.4281
Epoch 4 Step 601 Train Loss: 0.6055
Epoch 4 Step 651 Train Loss: 0.4855
Epoch 4 Step 701 Train Loss: 0.4477
Epoch 4 Step 751 Train Loss: 0.4425
Epoch 4 Step 801 Train Loss: 0.4493
Epoch 4 Step 851 Train Loss: 0.4900
Epoch 4 Step 901 Train Loss: 0.4888
Epoch 4 Step 951 Train Loss: 0.4502
Epoch 4 Step 1001 Train Loss: 0.5189
Epoch 4 Step 1051 Train Loss: 0.4871
Epoch 4 Step 1101 Train Loss: 0.4421
Epoch 4 Step 1151 Train Loss: 0.5495
Epoch 4 Step 1201 Train Loss: 0.4778
Epoch 4 Step 1251 Train Loss: 0.5125
Epoch 4 Step 1301 Train Loss: 0.4443
Epoch 4 Step 1351 Train Loss: 0.4744
Epoch 4 Step 1401 Train Loss: 0.4229
Epoch 4 Step 1451 Train Loss: 0.4435
Epoch 4 Step 1501 Train Loss: 0.4441
Epoch 4 Step 1551 Train Loss: 0.4249
Epoch 4 Step 1601 Train Loss: 0.5067
Epoch 4 Step 1651 Train Loss: 0.4688
Epoch 4 Step 1701 Train Loss: 0.4640
Epoch 4 Step 1751 Train Loss: 0.4546
Epoch 4 Step 1801 Train Loss: 0.4571
Epoch 4 Step 1851 Train Loss: 0.4911
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.1015 Validation Top 20 DE MSE: 0.1514. 
Epoch 5 Step 1 Train Loss: 0.4412
Epoch 5 Step 51 Train Loss: 0.4255
Epoch 5 Step 101 Train Loss: 0.4441
Epoch 5 Step 151 Train Loss: 0.4295
Epoch 5 Step 201 Train Loss: 0.5080
Epoch 5 Step 251 Train Loss: 0.4641
Epoch 5 Step 301 Train Loss: 0.4461
Epoch 5 Step 351 Train Loss: 0.4988
Epoch 5 Step 401 Train Loss: 0.4593
Epoch 5 Step 451 Train Loss: 0.4993
Epoch 5 Step 501 Train Loss: 0.4968
Epoch 5 Step 551 Train Loss: 0.4398
Epoch 5 Step 601 Train Loss: 0.4570
Epoch 5 Step 651 Train Loss: 0.4431
Epoch 5 Step 701 Train Loss: 0.4524
Epoch 5 Step 751 Train Loss: 0.4035
Epoch 5 Step 801 Train Loss: 0.4520
Epoch 5 Step 851 Train Loss: 0.4232
Epoch 5 Step 901 Train Loss: 0.4757
Epoch 5 Step 951 Train Loss: 0.4946
Epoch 5 Step 1001 Train Loss: 0.4601
Epoch 5 Step 1051 Train Loss: 0.5280
Epoch 5 Step 1101 Train Loss: 0.4312
Epoch 5 Step 1151 Train Loss: 0.4378
Epoch 5 Step 1201 Train Loss: 0.4995
Epoch 5 Step 1251 Train Loss: 0.4370
Epoch 5 Step 1301 Train Loss: 0.4460
Epoch 5 Step 1351 Train Loss: 0.4858
Epoch 5 Step 1401 Train Loss: 0.4846
Epoch 5 Step 1451 Train Loss: 0.5322
Epoch 5 Step 1501 Train Loss: 0.4440
Epoch 5 Step 1551 Train Loss: 0.4411
Epoch 5 Step 1601 Train Loss: 0.4171
Epoch 5 Step 1651 Train Loss: 0.4442
Epoch 5 Step 1701 Train Loss: 0.4519
Epoch 5 Step 1751 Train Loss: 0.4757
Epoch 5 Step 1801 Train Loss: 0.4722
Epoch 5 Step 1851 Train Loss: 0.4473
Epoch 5: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0030. 
Train Top 20 DE MSE: 0.0925 Validation Top 20 DE MSE: 0.1578. 
Epoch 6 Step 1 Train Loss: 0.4494
Epoch 6 Step 51 Train Loss: 0.4739
Epoch 6 Step 101 Train Loss: 0.4219
Epoch 6 Step 151 Train Loss: 0.4978
Epoch 6 Step 201 Train Loss: 0.4946
Epoch 6 Step 251 Train Loss: 0.4559
Epoch 6 Step 301 Train Loss: 0.5117
Epoch 6 Step 351 Train Loss: 0.5099
Epoch 6 Step 401 Train Loss: 0.4556
Epoch 6 Step 451 Train Loss: 0.4352
Epoch 6 Step 501 Train Loss: 0.4578
Epoch 6 Step 551 Train Loss: 0.4888
Epoch 6 Step 601 Train Loss: 0.4360
Epoch 6 Step 651 Train Loss: 0.4714
Epoch 6 Step 701 Train Loss: 0.4522
Epoch 6 Step 751 Train Loss: 0.4572
Epoch 6 Step 801 Train Loss: 0.4550
Epoch 6 Step 851 Train Loss: 0.4371
Epoch 6 Step 901 Train Loss: 0.5362
Epoch 6 Step 951 Train Loss: 0.4808
Epoch 6 Step 1001 Train Loss: 0.4668
Epoch 6 Step 1051 Train Loss: 0.4623
Epoch 6 Step 1101 Train Loss: 0.4652
Epoch 6 Step 1151 Train Loss: 0.4951
Epoch 6 Step 1201 Train Loss: 0.4220
Epoch 6 Step 1251 Train Loss: 0.4522
Epoch 6 Step 1301 Train Loss: 0.5181
Epoch 6 Step 1351 Train Loss: 0.4261
Epoch 6 Step 1401 Train Loss: 0.5135
Epoch 6 Step 1451 Train Loss: 0.5060
Epoch 6 Step 1501 Train Loss: 0.4626
Epoch 6 Step 1551 Train Loss: 0.4526
Epoch 6 Step 1601 Train Loss: 0.4481
Epoch 6 Step 1651 Train Loss: 0.4460
Epoch 6 Step 1701 Train Loss: 0.5176
Epoch 6 Step 1751 Train Loss: 0.4618
Epoch 6 Step 1801 Train Loss: 0.4409
Epoch 6 Step 1851 Train Loss: 0.4828
Epoch 6: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0872 Validation Top 20 DE MSE: 0.1429. 
Epoch 7 Step 1 Train Loss: 0.4657
Epoch 7 Step 51 Train Loss: 0.4959
Epoch 7 Step 101 Train Loss: 0.4498
Epoch 7 Step 151 Train Loss: 0.5182
Epoch 7 Step 201 Train Loss: 0.4440
Epoch 7 Step 251 Train Loss: 0.4432
Epoch 7 Step 301 Train Loss: 0.4906
Epoch 7 Step 351 Train Loss: 0.4744
Epoch 7 Step 401 Train Loss: 0.4934
Epoch 7 Step 451 Train Loss: 0.4557
Epoch 7 Step 501 Train Loss: 0.4933
Epoch 7 Step 551 Train Loss: 0.4689
Epoch 7 Step 601 Train Loss: 0.4519
Epoch 7 Step 651 Train Loss: 0.4840
Epoch 7 Step 701 Train Loss: 0.4662
Epoch 7 Step 751 Train Loss: 0.4928
Epoch 7 Step 801 Train Loss: 0.4554
Epoch 7 Step 851 Train Loss: 0.4768
Epoch 7 Step 901 Train Loss: 0.4704
Epoch 7 Step 951 Train Loss: 0.5199
Epoch 7 Step 1001 Train Loss: 0.4934
Epoch 7 Step 1051 Train Loss: 0.4228
Epoch 7 Step 1101 Train Loss: 0.4901
Epoch 7 Step 1151 Train Loss: 0.4671
Epoch 7 Step 1201 Train Loss: 0.5023
Epoch 7 Step 1251 Train Loss: 0.5183
Epoch 7 Step 1301 Train Loss: 0.4916
Epoch 7 Step 1351 Train Loss: 0.4653
Epoch 7 Step 1401 Train Loss: 0.4989
Epoch 7 Step 1451 Train Loss: 0.4514
Epoch 7 Step 1501 Train Loss: 0.4099
Epoch 7 Step 1551 Train Loss: 0.4607
Epoch 7 Step 1601 Train Loss: 0.5223
Epoch 7 Step 1651 Train Loss: 0.4467
Epoch 7 Step 1701 Train Loss: 0.4725
Epoch 7 Step 1751 Train Loss: 0.4591
Epoch 7 Step 1801 Train Loss: 0.4814
Epoch 7 Step 1851 Train Loss: 0.5297
Epoch 7: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0812 Validation Top 20 DE MSE: 0.1445. 
Epoch 8 Step 1 Train Loss: 0.4618
Epoch 8 Step 51 Train Loss: 0.4481
Epoch 8 Step 101 Train Loss: 0.4573
Epoch 8 Step 151 Train Loss: 0.4645
Epoch 8 Step 201 Train Loss: 0.4773
Epoch 8 Step 251 Train Loss: 0.5246
Epoch 8 Step 301 Train Loss: 0.5212
Epoch 8 Step 351 Train Loss: 0.4714
Epoch 8 Step 401 Train Loss: 0.5158
Epoch 8 Step 451 Train Loss: 0.4634
Epoch 8 Step 501 Train Loss: 0.5033
Epoch 8 Step 551 Train Loss: 0.5075
Epoch 8 Step 601 Train Loss: 0.5459
Epoch 8 Step 651 Train Loss: 0.4803
Epoch 8 Step 701 Train Loss: 0.4619
Epoch 8 Step 751 Train Loss: 0.4287
Epoch 8 Step 801 Train Loss: 0.4856
Epoch 8 Step 851 Train Loss: 0.4859
Epoch 8 Step 901 Train Loss: 0.4751
Epoch 8 Step 951 Train Loss: 0.4864
Epoch 8 Step 1001 Train Loss: 0.4995
Epoch 8 Step 1051 Train Loss: 0.5096
Epoch 8 Step 1101 Train Loss: 0.4836
Epoch 8 Step 1151 Train Loss: 0.4674
Epoch 8 Step 1201 Train Loss: 0.4231
Epoch 8 Step 1251 Train Loss: 0.4480
Epoch 8 Step 1301 Train Loss: 0.4759
Epoch 8 Step 1351 Train Loss: 0.4734
Epoch 8 Step 1401 Train Loss: 0.4891
Epoch 8 Step 1451 Train Loss: 0.4678
Epoch 8 Step 1501 Train Loss: 0.4930
Epoch 8 Step 1551 Train Loss: 0.4454
Epoch 8 Step 1601 Train Loss: 0.4781
Epoch 8 Step 1651 Train Loss: 0.5471
Epoch 8 Step 1701 Train Loss: 0.4573
Epoch 8 Step 1751 Train Loss: 0.4642
Epoch 8 Step 1801 Train Loss: 0.4750
Epoch 8 Step 1851 Train Loss: 0.4582
Epoch 8: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0833 Validation Top 20 DE MSE: 0.1443. 
Epoch 9 Step 1 Train Loss: 0.5143
Epoch 9 Step 51 Train Loss: 0.4998
Epoch 9 Step 101 Train Loss: 0.4626
Epoch 9 Step 151 Train Loss: 0.4561
Epoch 9 Step 201 Train Loss: 0.4538
Epoch 9 Step 251 Train Loss: 0.4918
Epoch 9 Step 301 Train Loss: 0.6045
Epoch 9 Step 351 Train Loss: 0.4589
Epoch 9 Step 401 Train Loss: 0.4368
Epoch 9 Step 451 Train Loss: 0.5075
Epoch 9 Step 501 Train Loss: 0.4640
Epoch 9 Step 551 Train Loss: 0.5161
Epoch 9 Step 601 Train Loss: 0.4643
Epoch 9 Step 651 Train Loss: 0.4852
Epoch 9 Step 701 Train Loss: 0.4836
Epoch 9 Step 751 Train Loss: 0.4654
Epoch 9 Step 801 Train Loss: 0.5503
Epoch 9 Step 851 Train Loss: 0.4912
Epoch 9 Step 901 Train Loss: 0.4862
Epoch 9 Step 951 Train Loss: 0.4960
Epoch 9 Step 1001 Train Loss: 0.5159
Epoch 9 Step 1051 Train Loss: 0.4524
Epoch 9 Step 1101 Train Loss: 0.4674
Epoch 9 Step 1151 Train Loss: 0.4705
Epoch 9 Step 1201 Train Loss: 0.4657
Epoch 9 Step 1251 Train Loss: 0.4601
Epoch 9 Step 1301 Train Loss: 0.5276
Epoch 9 Step 1351 Train Loss: 0.4912
Epoch 9 Step 1401 Train Loss: 0.4863
Epoch 9 Step 1451 Train Loss: 0.4911
Epoch 9 Step 1501 Train Loss: 0.4997
Epoch 9 Step 1551 Train Loss: 0.4874
Epoch 9 Step 1601 Train Loss: 0.4754
Epoch 9 Step 1651 Train Loss: 0.4905
Epoch 9 Step 1701 Train Loss: 0.4774
Epoch 9 Step 1751 Train Loss: 0.5253
Epoch 9 Step 1801 Train Loss: 0.4922
Epoch 9 Step 1851 Train Loss: 0.4473
Epoch 9: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0776 Validation Top 20 DE MSE: 0.1406. 
Epoch 10 Step 1 Train Loss: 0.4544
Epoch 10 Step 51 Train Loss: 0.4414
Epoch 10 Step 101 Train Loss: 0.4466
Epoch 10 Step 151 Train Loss: 0.4745
Epoch 10 Step 201 Train Loss: 0.4614
Epoch 10 Step 251 Train Loss: 0.4727
Epoch 10 Step 301 Train Loss: 0.4526
Epoch 10 Step 351 Train Loss: 0.4783
Epoch 10 Step 401 Train Loss: 0.4329
Epoch 10 Step 451 Train Loss: 0.4881
Epoch 10 Step 501 Train Loss: 0.4720
Epoch 10 Step 551 Train Loss: 0.5020
Epoch 10 Step 601 Train Loss: 0.4811
Epoch 10 Step 651 Train Loss: 0.4904
Epoch 10 Step 701 Train Loss: 0.4989
Epoch 10 Step 751 Train Loss: 0.4744
Epoch 10 Step 801 Train Loss: 0.4722
Epoch 10 Step 851 Train Loss: 0.5149
Epoch 10 Step 901 Train Loss: 0.4881
Epoch 10 Step 951 Train Loss: 0.4623
Epoch 10 Step 1001 Train Loss: 0.5054
Epoch 10 Step 1051 Train Loss: 0.5057
Epoch 10 Step 1101 Train Loss: 0.5492
Epoch 10 Step 1151 Train Loss: 0.5080
Epoch 10 Step 1201 Train Loss: 0.4633
Epoch 10 Step 1251 Train Loss: 0.4622
Epoch 10 Step 1301 Train Loss: 0.4970
Epoch 10 Step 1351 Train Loss: 0.4718
Epoch 10 Step 1401 Train Loss: 0.4573
Epoch 10 Step 1451 Train Loss: 0.5334
Epoch 10 Step 1501 Train Loss: 0.5263
Epoch 10 Step 1551 Train Loss: 0.4701
Epoch 10 Step 1601 Train Loss: 0.4869
Epoch 10 Step 1651 Train Loss: 0.4931
Epoch 10 Step 1701 Train Loss: 0.4925
Epoch 10 Step 1751 Train Loss: 0.4287
Epoch 10 Step 1801 Train Loss: 0.3981
Epoch 10 Step 1851 Train Loss: 0.4679
Epoch 10: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0843 Validation Top 20 DE MSE: 0.1467. 
Epoch 11 Step 1 Train Loss: 0.4634
Epoch 11 Step 51 Train Loss: 0.5211
Epoch 11 Step 101 Train Loss: 0.4559
Epoch 11 Step 151 Train Loss: 0.5060
Epoch 11 Step 201 Train Loss: 0.4408
Epoch 11 Step 251 Train Loss: 0.4931
Epoch 11 Step 301 Train Loss: 0.5049
Epoch 11 Step 351 Train Loss: 0.4989
Epoch 11 Step 401 Train Loss: 0.4869
Epoch 11 Step 451 Train Loss: 0.4556
Epoch 11 Step 501 Train Loss: 0.4992
Epoch 11 Step 551 Train Loss: 0.4824
Epoch 11 Step 601 Train Loss: 0.4548
Epoch 11 Step 651 Train Loss: 0.5139
Epoch 11 Step 701 Train Loss: 0.4579
Epoch 11 Step 751 Train Loss: 0.4194
Epoch 11 Step 801 Train Loss: 0.4437
Epoch 11 Step 851 Train Loss: 0.4943
Epoch 11 Step 901 Train Loss: 0.4943
Epoch 11 Step 951 Train Loss: 0.5158
Epoch 11 Step 1001 Train Loss: 0.4234
Epoch 11 Step 1051 Train Loss: 0.4939
Epoch 11 Step 1101 Train Loss: 0.5057
Epoch 11 Step 1151 Train Loss: 0.4781
Epoch 11 Step 1201 Train Loss: 0.4825
Epoch 11 Step 1251 Train Loss: 0.4568
Epoch 11 Step 1301 Train Loss: 0.4390
Epoch 11 Step 1351 Train Loss: 0.4929
Epoch 11 Step 1401 Train Loss: 0.5247
Epoch 11 Step 1451 Train Loss: 0.4420
Epoch 11 Step 1501 Train Loss: 0.5701
Epoch 11 Step 1551 Train Loss: 0.4584
Epoch 11 Step 1601 Train Loss: 0.4824
Epoch 11 Step 1651 Train Loss: 0.5289
Epoch 11 Step 1701 Train Loss: 0.4265
Epoch 11 Step 1751 Train Loss: 0.4867
Epoch 11 Step 1801 Train Loss: 0.4710
Epoch 11 Step 1851 Train Loss: 0.4993
Epoch 11: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0774 Validation Top 20 DE MSE: 0.1403. 
Epoch 12 Step 1 Train Loss: 0.4635
Epoch 12 Step 51 Train Loss: 0.4961
Epoch 12 Step 101 Train Loss: 0.4306
Epoch 12 Step 151 Train Loss: 0.4717
Epoch 12 Step 201 Train Loss: 0.5029
Epoch 12 Step 251 Train Loss: 0.4729
Epoch 12 Step 301 Train Loss: 0.4739
Epoch 12 Step 351 Train Loss: 0.5070
Epoch 12 Step 401 Train Loss: 0.4748
Epoch 12 Step 451 Train Loss: 0.4359
Epoch 12 Step 501 Train Loss: 0.4747
Epoch 12 Step 551 Train Loss: 0.4710
Epoch 12 Step 601 Train Loss: 0.4624
Epoch 12 Step 651 Train Loss: 0.4959
Epoch 12 Step 701 Train Loss: 0.5298
Epoch 12 Step 751 Train Loss: 0.4637
Epoch 12 Step 801 Train Loss: 0.4997
Epoch 12 Step 851 Train Loss: 0.4506
Epoch 12 Step 901 Train Loss: 0.4740
Epoch 12 Step 951 Train Loss: 0.4863
Epoch 12 Step 1001 Train Loss: 0.4961
Epoch 12 Step 1051 Train Loss: 0.4826
Epoch 12 Step 1101 Train Loss: 0.4863
Epoch 12 Step 1151 Train Loss: 0.4536
Epoch 12 Step 1201 Train Loss: 0.5094
Epoch 12 Step 1251 Train Loss: 0.5615
Epoch 12 Step 1301 Train Loss: 0.4881
Epoch 12 Step 1351 Train Loss: 0.4653
Epoch 12 Step 1401 Train Loss: 0.5213
Epoch 12 Step 1451 Train Loss: 0.4988
Epoch 12 Step 1501 Train Loss: 0.4812
Epoch 12 Step 1551 Train Loss: 0.4614
Epoch 12 Step 1601 Train Loss: 0.5368
Epoch 12 Step 1651 Train Loss: 0.4649
Epoch 12 Step 1701 Train Loss: 0.4794
Epoch 12 Step 1751 Train Loss: 0.4665
Epoch 12 Step 1801 Train Loss: 0.4630
Epoch 12 Step 1851 Train Loss: 0.4901
Epoch 12: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0823 Validation Top 20 DE MSE: 0.1429. 
Epoch 13 Step 1 Train Loss: 0.5058
Epoch 13 Step 51 Train Loss: 0.4779
Epoch 13 Step 101 Train Loss: 0.5359
Epoch 13 Step 151 Train Loss: 0.4246
Epoch 13 Step 201 Train Loss: 0.4659
Epoch 13 Step 251 Train Loss: 0.4429
Epoch 13 Step 301 Train Loss: 0.4419
Epoch 13 Step 351 Train Loss: 0.5265
Epoch 13 Step 401 Train Loss: 0.4615
Epoch 13 Step 451 Train Loss: 0.5527
Epoch 13 Step 501 Train Loss: 0.4539
Epoch 13 Step 551 Train Loss: 0.4668
Epoch 13 Step 601 Train Loss: 0.4850
Epoch 13 Step 651 Train Loss: 0.5010
Epoch 13 Step 701 Train Loss: 0.4664
Epoch 13 Step 751 Train Loss: 0.4726
Epoch 13 Step 801 Train Loss: 0.4673
Epoch 13 Step 851 Train Loss: 0.5497
Epoch 13 Step 901 Train Loss: 0.4758
Epoch 13 Step 951 Train Loss: 0.4764
Epoch 13 Step 1001 Train Loss: 0.4970
Epoch 13 Step 1051 Train Loss: 0.4844
Epoch 13 Step 1101 Train Loss: 0.4583
Epoch 13 Step 1151 Train Loss: 0.5142
Epoch 13 Step 1201 Train Loss: 0.5009
Epoch 13 Step 1251 Train Loss: 0.4751
Epoch 13 Step 1301 Train Loss: 0.4705
Epoch 13 Step 1351 Train Loss: 0.4833
Epoch 13 Step 1401 Train Loss: 0.5142
Epoch 13 Step 1451 Train Loss: 0.4472
Epoch 13 Step 1501 Train Loss: 0.5022
Epoch 13 Step 1551 Train Loss: 0.4685
Epoch 13 Step 1601 Train Loss: 0.4884
Epoch 13 Step 1651 Train Loss: 0.4574
Epoch 13 Step 1701 Train Loss: 0.4807
Epoch 13 Step 1751 Train Loss: 0.4653
Epoch 13 Step 1801 Train Loss: 0.4761
Epoch 13 Step 1851 Train Loss: 0.5273
Epoch 13: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0822 Validation Top 20 DE MSE: 0.1439. 
Epoch 14 Step 1 Train Loss: 0.4416
Epoch 14 Step 51 Train Loss: 0.4516
Epoch 14 Step 101 Train Loss: 0.4598
Epoch 14 Step 151 Train Loss: 0.5077
Epoch 14 Step 201 Train Loss: 0.4969
Epoch 14 Step 251 Train Loss: 0.4646
Epoch 14 Step 301 Train Loss: 0.4698
Epoch 14 Step 351 Train Loss: 0.4740
Epoch 14 Step 401 Train Loss: 0.4509
Epoch 14 Step 451 Train Loss: 0.4716
Epoch 14 Step 501 Train Loss: 0.5176
Epoch 14 Step 551 Train Loss: 0.4809
Epoch 14 Step 601 Train Loss: 0.4327
Epoch 14 Step 651 Train Loss: 0.4833
Epoch 14 Step 701 Train Loss: 0.5034
Epoch 14 Step 751 Train Loss: 0.5355
Epoch 14 Step 801 Train Loss: 0.4912
Epoch 14 Step 851 Train Loss: 0.4723
Epoch 14 Step 901 Train Loss: 0.4929
Epoch 14 Step 951 Train Loss: 0.4570
Epoch 14 Step 1001 Train Loss: 0.4844
Epoch 14 Step 1051 Train Loss: 0.4848
Epoch 14 Step 1101 Train Loss: 0.5160
Epoch 14 Step 1151 Train Loss: 0.5145
Epoch 14 Step 1201 Train Loss: 0.4831
Epoch 14 Step 1251 Train Loss: 0.5474
Epoch 14 Step 1301 Train Loss: 0.4443
Epoch 14 Step 1351 Train Loss: 0.4474
Epoch 14 Step 1401 Train Loss: 0.4963
Epoch 14 Step 1451 Train Loss: 0.5090
Epoch 14 Step 1501 Train Loss: 0.4768
Epoch 14 Step 1551 Train Loss: 0.4581
Epoch 14 Step 1601 Train Loss: 0.5165
Epoch 14 Step 1651 Train Loss: 0.4280
Epoch 14 Step 1701 Train Loss: 0.4895
Epoch 14 Step 1751 Train Loss: 0.5263
Epoch 14 Step 1801 Train Loss: 0.5348
Epoch 14 Step 1851 Train Loss: 0.4387
Epoch 14: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0778 Validation Top 20 DE MSE: 0.1409. 
Epoch 15 Step 1 Train Loss: 0.5204
Epoch 15 Step 51 Train Loss: 0.4843
Epoch 15 Step 101 Train Loss: 0.4340
Epoch 15 Step 151 Train Loss: 0.4793
Epoch 15 Step 201 Train Loss: 0.4662
Epoch 15 Step 251 Train Loss: 0.4557
Epoch 15 Step 301 Train Loss: 0.4651
Epoch 15 Step 351 Train Loss: 0.4763
Epoch 15 Step 401 Train Loss: 0.4468
Epoch 15 Step 451 Train Loss: 0.4830
Epoch 15 Step 501 Train Loss: 0.4816
Epoch 15 Step 551 Train Loss: 0.5210
Epoch 15 Step 601 Train Loss: 0.5094
Epoch 15 Step 651 Train Loss: 0.5208
Epoch 15 Step 701 Train Loss: 0.5758
Epoch 15 Step 751 Train Loss: 0.4466
Epoch 15 Step 801 Train Loss: 0.4848
Epoch 15 Step 851 Train Loss: 0.4752
Epoch 15 Step 901 Train Loss: 0.5142
Epoch 15 Step 951 Train Loss: 0.5307
Epoch 15 Step 1001 Train Loss: 0.5062
Epoch 15 Step 1051 Train Loss: 0.4466
Epoch 15 Step 1101 Train Loss: 0.5180
Epoch 15 Step 1151 Train Loss: 0.4591
Epoch 15 Step 1201 Train Loss: 0.4396
Epoch 15 Step 1251 Train Loss: 0.4783
Epoch 15 Step 1301 Train Loss: 0.4858
Epoch 15 Step 1351 Train Loss: 0.5264
Epoch 15 Step 1401 Train Loss: 0.5047
Epoch 15 Step 1451 Train Loss: 0.5274
Epoch 15 Step 1501 Train Loss: 0.4663
Epoch 15 Step 1551 Train Loss: 0.4826
Epoch 15 Step 1601 Train Loss: 0.4952
Epoch 15 Step 1651 Train Loss: 0.5253
Epoch 15 Step 1701 Train Loss: 0.5115
Epoch 15 Step 1751 Train Loss: 0.4548
Epoch 15 Step 1801 Train Loss: 0.4689
Epoch 15 Step 1851 Train Loss: 0.4693
Epoch 15: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0857 Validation Top 20 DE MSE: 0.1462. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1361
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.004771271
test_combo_seen0_pearson: 0.9890698352809267
test_combo_seen0_mse_de: 0.13137236
test_combo_seen0_pearson_de: 0.8167727773864841
test_combo_seen1_mse: 0.0039206925
test_combo_seen1_pearson: 0.9910646071887091
test_combo_seen1_mse_de: 0.14736222
test_combo_seen1_pearson_de: 0.7958783736191195
test_combo_seen2_mse: 0.0034480677
test_combo_seen2_pearson: 0.9922412289771366
test_combo_seen2_mse_de: 0.083755895
test_combo_seen2_pearson_de: 0.8052004125706524
test_unseen_single_mse: 0.0026148972
test_unseen_single_pearson: 0.9939779683242848
test_unseen_single_mse_de: 0.15092379
test_unseen_single_pearson_de: 0.9072905822037192
test_combo_seen0_pearson_delta: 0.6121883549980609
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.1142857142857143
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.9071428571428571
test_combo_seen0_mse_top20_de_non_dropout: 0.196426
test_combo_seen1_pearson_delta: 0.6048038777352264
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.12156862745098042
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8852941176470588
test_combo_seen1_mse_top20_de_non_dropout: 0.1792681
test_combo_seen2_pearson_delta: 0.6325035504106236
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.11388888888888887
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.9055555555555556
test_combo_seen2_mse_top20_de_non_dropout: 0.10694437
test_unseen_single_pearson_delta: 0.4164556417356117
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.2222222222222222
test_unseen_single_frac_sigma_below_1_non_dropout: 0.898148148148148
test_unseen_single_mse_top20_de_non_dropout: 0.17757946
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.039 MB uploadedwandb: | 0.004 MB of 0.039 MB uploadedwandb: / 0.004 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                             train_de_pearson ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:                                                    train_mse ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñá‚ñÜ
wandb:                                                   val_de_mse ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                                               val_de_pearson ‚ñÇ‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ
wandb:                                                  val_pearson ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.11429
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.90714
wandb:                                         test_combo_seen0_mse 0.00477
wandb:                                      test_combo_seen0_mse_de 0.13137
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.19643
wandb:                                     test_combo_seen0_pearson 0.98907
wandb:                                  test_combo_seen0_pearson_de 0.81677
wandb:                               test_combo_seen0_pearson_delta 0.61219
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.12157
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.88529
wandb:                                         test_combo_seen1_mse 0.00392
wandb:                                      test_combo_seen1_mse_de 0.14736
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.17927
wandb:                                     test_combo_seen1_pearson 0.99106
wandb:                                  test_combo_seen1_pearson_de 0.79588
wandb:                               test_combo_seen1_pearson_delta 0.6048
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.11389
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.90556
wandb:                                         test_combo_seen2_mse 0.00345
wandb:                                      test_combo_seen2_mse_de 0.08376
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.10694
wandb:                                     test_combo_seen2_pearson 0.99224
wandb:                                  test_combo_seen2_pearson_de 0.8052
wandb:                               test_combo_seen2_pearson_delta 0.6325
wandb:                                                  test_de_mse 0.13609
wandb:                                              test_de_pearson 0.82813
wandb:               test_frac_opposite_direction_top20_non_dropout 0.14612
wandb:                          test_frac_sigma_below_1_non_dropout 0.89369
wandb:                                                     test_mse 0.00355
wandb:                                test_mse_top20_de_non_dropout 0.16735
wandb:                                                 test_pearson 0.9919
wandb:                                           test_pearson_delta 0.56077
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.22222
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.89815
wandb:                                       test_unseen_single_mse 0.00261
wandb:                                    test_unseen_single_mse_de 0.15092
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.17758
wandb:                                   test_unseen_single_pearson 0.99398
wandb:                                test_unseen_single_pearson_de 0.90729
wandb:                             test_unseen_single_pearson_delta 0.41646
wandb:                                                 train_de_mse 0.08566
wandb:                                             train_de_pearson 0.92053
wandb:                                                    train_mse 0.00202
wandb:                                                train_pearson 0.99541
wandb:                                                training_loss 0.48022
wandb:                                                   val_de_mse 0.14622
wandb:                                               val_de_pearson 0.79236
wandb:                                                      val_mse 0.00314
wandb:                                                  val_pearson 0.99286
wandb: 
wandb: üöÄ View run geneformer_NormanWeissman2019_split2 at: https://wandb.ai/zhoumin1130/New_gears/runs/jyar7byl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240921_232804-jyar7byl/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:2
combo_seen1:39
combo_seen2:23
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_003538-zn7bzoud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_NormanWeissman2019_split3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/zn7bzoud
wandb: WARNING Serializing object of type ndarray that is 20631680 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4719
Epoch 1 Step 51 Train Loss: 0.4556
Epoch 1 Step 101 Train Loss: 0.4634
Epoch 1 Step 151 Train Loss: 0.4655
Epoch 1 Step 201 Train Loss: 0.4344
Epoch 1 Step 251 Train Loss: 0.4732
Epoch 1 Step 301 Train Loss: 0.4269
Epoch 1 Step 351 Train Loss: 0.4483
Epoch 1 Step 401 Train Loss: 0.4228
Epoch 1 Step 451 Train Loss: 0.4539
Epoch 1 Step 501 Train Loss: 0.5031
Epoch 1 Step 551 Train Loss: 0.4910
Epoch 1 Step 601 Train Loss: 0.4264
Epoch 1 Step 651 Train Loss: 0.4323
Epoch 1 Step 701 Train Loss: 0.4635
Epoch 1 Step 751 Train Loss: 0.4816
Epoch 1 Step 801 Train Loss: 0.4658
Epoch 1 Step 851 Train Loss: 0.4534
Epoch 1 Step 901 Train Loss: 0.3983
Epoch 1 Step 951 Train Loss: 0.4212
Epoch 1 Step 1001 Train Loss: 0.4213
Epoch 1 Step 1051 Train Loss: 0.4536
Epoch 1 Step 1101 Train Loss: 0.4483
Epoch 1 Step 1151 Train Loss: 0.4535
Epoch 1 Step 1201 Train Loss: 0.5116
Epoch 1 Step 1251 Train Loss: 0.4628
Epoch 1 Step 1301 Train Loss: 0.4457
Epoch 1 Step 1351 Train Loss: 0.4340
Epoch 1 Step 1401 Train Loss: 0.4919
Epoch 1 Step 1451 Train Loss: 0.4251
Epoch 1 Step 1501 Train Loss: 0.4457
Epoch 1 Step 1551 Train Loss: 0.4596
Epoch 1 Step 1601 Train Loss: 0.3917
Epoch 1 Step 1651 Train Loss: 0.4533
Epoch 1 Step 1701 Train Loss: 0.4768
Epoch 1 Step 1751 Train Loss: 0.4175
Epoch 1 Step 1801 Train Loss: 0.4976
Epoch 1 Step 1851 Train Loss: 0.4633
Epoch 1: Train Overall MSE: 0.0053 Validation Overall MSE: 0.0058. 
Train Top 20 DE MSE: 0.1997 Validation Top 20 DE MSE: 0.1896. 
Epoch 2 Step 1 Train Loss: 0.4642
Epoch 2 Step 51 Train Loss: 0.4740
Epoch 2 Step 101 Train Loss: 0.4449
Epoch 2 Step 151 Train Loss: 0.4271
Epoch 2 Step 201 Train Loss: 0.4125
Epoch 2 Step 251 Train Loss: 0.4882
Epoch 2 Step 301 Train Loss: 0.4347
Epoch 2 Step 351 Train Loss: 0.4865
Epoch 2 Step 401 Train Loss: 0.4082
Epoch 2 Step 451 Train Loss: 0.4881
Epoch 2 Step 501 Train Loss: 0.4344
Epoch 2 Step 551 Train Loss: 0.4895
Epoch 2 Step 601 Train Loss: 0.5152
Epoch 2 Step 651 Train Loss: 0.4598
Epoch 2 Step 701 Train Loss: 0.4687
Epoch 2 Step 751 Train Loss: 0.5265
Epoch 2 Step 801 Train Loss: 0.4342
Epoch 2 Step 851 Train Loss: 0.4315
Epoch 2 Step 901 Train Loss: 0.4714
Epoch 2 Step 951 Train Loss: 0.4898
Epoch 2 Step 1001 Train Loss: 0.3902
Epoch 2 Step 1051 Train Loss: 0.4696
Epoch 2 Step 1101 Train Loss: 0.4416
Epoch 2 Step 1151 Train Loss: 0.4803
Epoch 2 Step 1201 Train Loss: 0.4159
Epoch 2 Step 1251 Train Loss: 0.4785
Epoch 2 Step 1301 Train Loss: 0.4446
Epoch 2 Step 1351 Train Loss: 0.4980
Epoch 2 Step 1401 Train Loss: 0.4709
Epoch 2 Step 1451 Train Loss: 0.4854
Epoch 2 Step 1501 Train Loss: 0.4303
Epoch 2 Step 1551 Train Loss: 0.4692
Epoch 2 Step 1601 Train Loss: 0.4677
Epoch 2 Step 1651 Train Loss: 0.4405
Epoch 2 Step 1701 Train Loss: 0.4670
Epoch 2 Step 1751 Train Loss: 0.4328
Epoch 2 Step 1801 Train Loss: 0.5017
Epoch 2 Step 1851 Train Loss: 0.4537
Epoch 2: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0988 Validation Top 20 DE MSE: 0.1871. 
Epoch 3 Step 1 Train Loss: 0.4961
Epoch 3 Step 51 Train Loss: 0.5052
Epoch 3 Step 101 Train Loss: 0.4780
Epoch 3 Step 151 Train Loss: 0.4530
Epoch 3 Step 201 Train Loss: 0.4986
Epoch 3 Step 251 Train Loss: 0.4602
Epoch 3 Step 301 Train Loss: 0.4465
Epoch 3 Step 351 Train Loss: 0.5252
Epoch 3 Step 401 Train Loss: 0.5126
Epoch 3 Step 451 Train Loss: 0.5267
Epoch 3 Step 501 Train Loss: 0.4409
Epoch 3 Step 551 Train Loss: 0.4343
Epoch 3 Step 601 Train Loss: 0.4895
Epoch 3 Step 651 Train Loss: 0.5513
Epoch 3 Step 701 Train Loss: 0.5233
Epoch 3 Step 751 Train Loss: 0.5202
Epoch 3 Step 801 Train Loss: 0.4802
Epoch 3 Step 851 Train Loss: 0.4371
Epoch 3 Step 901 Train Loss: 0.4728
Epoch 3 Step 951 Train Loss: 0.5016
Epoch 3 Step 1001 Train Loss: 0.5294
Epoch 3 Step 1051 Train Loss: 0.4549
Epoch 3 Step 1101 Train Loss: 0.4548
Epoch 3 Step 1151 Train Loss: 0.5317
Epoch 3 Step 1201 Train Loss: 0.4999
Epoch 3 Step 1251 Train Loss: 0.5013
Epoch 3 Step 1301 Train Loss: 0.5126
Epoch 3 Step 1351 Train Loss: 0.5090
Epoch 3 Step 1401 Train Loss: 0.5510
Epoch 3 Step 1451 Train Loss: 0.4564
Epoch 3 Step 1501 Train Loss: 0.4477
Epoch 3 Step 1551 Train Loss: 0.4964
Epoch 3 Step 1601 Train Loss: 0.4647
Epoch 3 Step 1651 Train Loss: 0.4872
Epoch 3 Step 1701 Train Loss: 0.4746
Epoch 3 Step 1751 Train Loss: 0.4164
Epoch 3 Step 1801 Train Loss: 0.4427
Epoch 3 Step 1851 Train Loss: 0.4341
Epoch 3: Train Overall MSE: 0.0037 Validation Overall MSE: 0.0050. 
Train Top 20 DE MSE: 0.1171 Validation Top 20 DE MSE: 0.2235. 
Epoch 4 Step 1 Train Loss: 0.4874
Epoch 4 Step 51 Train Loss: 0.4612
Epoch 4 Step 101 Train Loss: 0.4825
Epoch 4 Step 151 Train Loss: 0.4525
Epoch 4 Step 201 Train Loss: 0.4835
Epoch 4 Step 251 Train Loss: 0.4359
Epoch 4 Step 301 Train Loss: 0.4993
Epoch 4 Step 351 Train Loss: 0.4968
Epoch 4 Step 401 Train Loss: 0.5114
Epoch 4 Step 451 Train Loss: 0.4960
Epoch 4 Step 501 Train Loss: 0.5019
Epoch 4 Step 551 Train Loss: 0.5511
Epoch 4 Step 601 Train Loss: 0.4533
Epoch 4 Step 651 Train Loss: 0.4418
Epoch 4 Step 701 Train Loss: 0.4846
Epoch 4 Step 751 Train Loss: 0.4841
Epoch 4 Step 801 Train Loss: 0.4962
Epoch 4 Step 851 Train Loss: 0.5517
Epoch 4 Step 901 Train Loss: 0.4601
Epoch 4 Step 951 Train Loss: 0.4510
Epoch 4 Step 1001 Train Loss: 0.4978
Epoch 4 Step 1051 Train Loss: 0.5224
Epoch 4 Step 1101 Train Loss: 0.4758
Epoch 4 Step 1151 Train Loss: 0.4957
Epoch 4 Step 1201 Train Loss: 0.4628
Epoch 4 Step 1251 Train Loss: 0.5003
Epoch 4 Step 1301 Train Loss: 0.5213
Epoch 4 Step 1351 Train Loss: 0.4423
Epoch 4 Step 1401 Train Loss: 0.5093
Epoch 4 Step 1451 Train Loss: 0.4508
Epoch 4 Step 1501 Train Loss: 0.4745
Epoch 4 Step 1551 Train Loss: 0.5016
Epoch 4 Step 1601 Train Loss: 0.4231
Epoch 4 Step 1651 Train Loss: 0.4641
Epoch 4 Step 1701 Train Loss: 0.5231
Epoch 4 Step 1751 Train Loss: 0.4854
Epoch 4 Step 1801 Train Loss: 0.4584
Epoch 4 Step 1851 Train Loss: 0.4760
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0883 Validation Top 20 DE MSE: 0.1746. 
Epoch 5 Step 1 Train Loss: 0.4848
Epoch 5 Step 51 Train Loss: 0.4629
Epoch 5 Step 101 Train Loss: 0.4182
Epoch 5 Step 151 Train Loss: 0.5120
Epoch 5 Step 201 Train Loss: 0.4775
Epoch 5 Step 251 Train Loss: 0.4337
Epoch 5 Step 301 Train Loss: 0.4571
Epoch 5 Step 351 Train Loss: 0.4748
Epoch 5 Step 401 Train Loss: 0.5681
Epoch 5 Step 451 Train Loss: 0.4807
Epoch 5 Step 501 Train Loss: 0.4849
Epoch 5 Step 551 Train Loss: 0.4905
Epoch 5 Step 601 Train Loss: 0.4873
Epoch 5 Step 651 Train Loss: 0.4408
Epoch 5 Step 701 Train Loss: 0.4707
Epoch 5 Step 751 Train Loss: 0.4597
Epoch 5 Step 801 Train Loss: 0.4570
Epoch 5 Step 851 Train Loss: 0.4387
Epoch 5 Step 901 Train Loss: 0.5497
Epoch 5 Step 951 Train Loss: 0.4725
Epoch 5 Step 1001 Train Loss: 0.5375
Epoch 5 Step 1051 Train Loss: 0.4865
Epoch 5 Step 1101 Train Loss: 0.4624
Epoch 5 Step 1151 Train Loss: 0.5343
Epoch 5 Step 1201 Train Loss: 0.4775
Epoch 5 Step 1251 Train Loss: 0.4793
Epoch 5 Step 1301 Train Loss: 0.4626
Epoch 5 Step 1351 Train Loss: 0.5005
Epoch 5 Step 1401 Train Loss: 0.5436
Epoch 5 Step 1451 Train Loss: 0.4654
Epoch 5 Step 1501 Train Loss: 0.4844
Epoch 5 Step 1551 Train Loss: 0.4401
Epoch 5 Step 1601 Train Loss: 0.4315
Epoch 5 Step 1651 Train Loss: 0.4874
Epoch 5 Step 1701 Train Loss: 0.4806
Epoch 5 Step 1751 Train Loss: 0.4704
Epoch 5 Step 1801 Train Loss: 0.4718
Epoch 5 Step 1851 Train Loss: 0.4916
Epoch 5: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0831 Validation Top 20 DE MSE: 0.1630. 
Epoch 6 Step 1 Train Loss: 0.4824
Epoch 6 Step 51 Train Loss: 0.5328
Epoch 6 Step 101 Train Loss: 0.4497
Epoch 6 Step 151 Train Loss: 0.4471
Epoch 6 Step 201 Train Loss: 0.5010
Epoch 6 Step 251 Train Loss: 0.5275
Epoch 6 Step 301 Train Loss: 0.4538
Epoch 6 Step 351 Train Loss: 0.4810
Epoch 6 Step 401 Train Loss: 0.4571
Epoch 6 Step 451 Train Loss: 0.5181
Epoch 6 Step 501 Train Loss: 0.4598
Epoch 6 Step 551 Train Loss: 0.5200
Epoch 6 Step 601 Train Loss: 0.5104
Epoch 6 Step 651 Train Loss: 0.5138
Epoch 6 Step 701 Train Loss: 0.4916
Epoch 6 Step 751 Train Loss: 0.4868
Epoch 6 Step 801 Train Loss: 0.4819
Epoch 6 Step 851 Train Loss: 0.4476
Epoch 6 Step 901 Train Loss: 0.5012
Epoch 6 Step 951 Train Loss: 0.4801
Epoch 6 Step 1001 Train Loss: 0.4531
Epoch 6 Step 1051 Train Loss: 0.4495
Epoch 6 Step 1101 Train Loss: 0.4821
Epoch 6 Step 1151 Train Loss: 0.5307
Epoch 6 Step 1201 Train Loss: 0.5116
Epoch 6 Step 1251 Train Loss: 0.4889
Epoch 6 Step 1301 Train Loss: 0.5002
Epoch 6 Step 1351 Train Loss: 0.5599
Epoch 6 Step 1401 Train Loss: 0.4833
Epoch 6 Step 1451 Train Loss: 0.5150
Epoch 6 Step 1501 Train Loss: 0.4606
Epoch 6 Step 1551 Train Loss: 0.4848
Epoch 6 Step 1601 Train Loss: 0.5155
Epoch 6 Step 1651 Train Loss: 0.5004
Epoch 6 Step 1701 Train Loss: 0.4628
Epoch 6 Step 1751 Train Loss: 0.4549
Epoch 6 Step 1801 Train Loss: 0.4857
Epoch 6 Step 1851 Train Loss: 0.4504
Epoch 6: Train Overall MSE: 0.0023 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0787 Validation Top 20 DE MSE: 0.1663. 
Epoch 7 Step 1 Train Loss: 0.4683
Epoch 7 Step 51 Train Loss: 0.5065
Epoch 7 Step 101 Train Loss: 0.4529
Epoch 7 Step 151 Train Loss: 0.4654
Epoch 7 Step 201 Train Loss: 0.4655
Epoch 7 Step 251 Train Loss: 0.5007
Epoch 7 Step 301 Train Loss: 0.5192
Epoch 7 Step 351 Train Loss: 0.4917
Epoch 7 Step 401 Train Loss: 0.4437
Epoch 7 Step 451 Train Loss: 0.5308
Epoch 7 Step 501 Train Loss: 0.4947
Epoch 7 Step 551 Train Loss: 0.4724
Epoch 7 Step 601 Train Loss: 0.4628
Epoch 7 Step 651 Train Loss: 0.4542
Epoch 7 Step 701 Train Loss: 0.4650
Epoch 7 Step 751 Train Loss: 0.4592
Epoch 7 Step 801 Train Loss: 0.4834
Epoch 7 Step 851 Train Loss: 0.5322
Epoch 7 Step 901 Train Loss: 0.4872
Epoch 7 Step 951 Train Loss: 0.4290
Epoch 7 Step 1001 Train Loss: 0.5049
Epoch 7 Step 1051 Train Loss: 0.5000
Epoch 7 Step 1101 Train Loss: 0.4509
Epoch 7 Step 1151 Train Loss: 0.4877
Epoch 7 Step 1201 Train Loss: 0.5057
Epoch 7 Step 1251 Train Loss: 0.5642
Epoch 7 Step 1301 Train Loss: 0.5122
Epoch 7 Step 1351 Train Loss: 0.5773
Epoch 7 Step 1401 Train Loss: 0.5028
Epoch 7 Step 1451 Train Loss: 0.5008
Epoch 7 Step 1501 Train Loss: 0.4438
Epoch 7 Step 1551 Train Loss: 0.5468
Epoch 7 Step 1601 Train Loss: 0.4681
Epoch 7 Step 1651 Train Loss: 0.5738
Epoch 7 Step 1701 Train Loss: 0.4637
Epoch 7 Step 1751 Train Loss: 0.4889
Epoch 7 Step 1801 Train Loss: 0.4977
Epoch 7 Step 1851 Train Loss: 0.4552
Epoch 7: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0824 Validation Top 20 DE MSE: 0.1657. 
Epoch 8 Step 1 Train Loss: 0.5382
Epoch 8 Step 51 Train Loss: 0.4942
Epoch 8 Step 101 Train Loss: 0.5190
Epoch 8 Step 151 Train Loss: 0.4912
Epoch 8 Step 201 Train Loss: 0.4219
Epoch 8 Step 251 Train Loss: 0.4891
Epoch 8 Step 301 Train Loss: 0.5735
Epoch 8 Step 351 Train Loss: 0.5651
Epoch 8 Step 401 Train Loss: 0.5072
Epoch 8 Step 451 Train Loss: 0.5003
Epoch 8 Step 501 Train Loss: 0.4720
Epoch 8 Step 551 Train Loss: 0.5269
Epoch 8 Step 601 Train Loss: 0.4680
Epoch 8 Step 651 Train Loss: 0.4829
Epoch 8 Step 701 Train Loss: 0.4931
Epoch 8 Step 751 Train Loss: 0.4898
Epoch 8 Step 801 Train Loss: 0.5033
Epoch 8 Step 851 Train Loss: 0.4925
Epoch 8 Step 901 Train Loss: 0.5666
Epoch 8 Step 951 Train Loss: 0.5108
Epoch 8 Step 1001 Train Loss: 0.4734
Epoch 8 Step 1051 Train Loss: 0.4620
Epoch 8 Step 1101 Train Loss: 0.5231
Epoch 8 Step 1151 Train Loss: 0.5438
Epoch 8 Step 1201 Train Loss: 0.4640
Epoch 8 Step 1251 Train Loss: 0.4358
Epoch 8 Step 1301 Train Loss: 0.4875
Epoch 8 Step 1351 Train Loss: 0.4917
Epoch 8 Step 1401 Train Loss: 0.4578
Epoch 8 Step 1451 Train Loss: 0.5436
Epoch 8 Step 1501 Train Loss: 0.4440
Epoch 8 Step 1551 Train Loss: 0.4560
Epoch 8 Step 1601 Train Loss: 0.4747
Epoch 8 Step 1651 Train Loss: 0.5200
Epoch 8 Step 1701 Train Loss: 0.5118
Epoch 8 Step 1751 Train Loss: 0.4756
Epoch 8 Step 1801 Train Loss: 0.5112
Epoch 8 Step 1851 Train Loss: 0.5504
Epoch 8: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0811 Validation Top 20 DE MSE: 0.1627. 
Epoch 9 Step 1 Train Loss: 0.5027
Epoch 9 Step 51 Train Loss: 0.4827
Epoch 9 Step 101 Train Loss: 0.5006
Epoch 9 Step 151 Train Loss: 0.5200
Epoch 9 Step 201 Train Loss: 0.5069
Epoch 9 Step 251 Train Loss: 0.4866
Epoch 9 Step 301 Train Loss: 0.5486
Epoch 9 Step 351 Train Loss: 0.4826
Epoch 9 Step 401 Train Loss: 0.5775
Epoch 9 Step 451 Train Loss: 0.4596
Epoch 9 Step 501 Train Loss: 0.5618
Epoch 9 Step 551 Train Loss: 0.4833
Epoch 9 Step 601 Train Loss: 0.5140
Epoch 9 Step 651 Train Loss: 0.4741
Epoch 9 Step 701 Train Loss: 0.4431
Epoch 9 Step 751 Train Loss: 0.4910
Epoch 9 Step 801 Train Loss: 0.4614
Epoch 9 Step 851 Train Loss: 0.4707
Epoch 9 Step 901 Train Loss: 0.4927
Epoch 9 Step 951 Train Loss: 0.4747
Epoch 9 Step 1001 Train Loss: 0.4516
Epoch 9 Step 1051 Train Loss: 0.4406
Epoch 9 Step 1101 Train Loss: 0.5162
Epoch 9 Step 1151 Train Loss: 0.5111
Epoch 9 Step 1201 Train Loss: 0.5462
Epoch 9 Step 1251 Train Loss: 0.5086
Epoch 9 Step 1301 Train Loss: 0.5064
Epoch 9 Step 1351 Train Loss: 0.4754
Epoch 9 Step 1401 Train Loss: 0.5003
Epoch 9 Step 1451 Train Loss: 0.5625
Epoch 9 Step 1501 Train Loss: 0.4671
Epoch 9 Step 1551 Train Loss: 0.5210
Epoch 9 Step 1601 Train Loss: 0.5329
Epoch 9 Step 1651 Train Loss: 0.5735
Epoch 9 Step 1701 Train Loss: 0.4791
Epoch 9 Step 1751 Train Loss: 0.4605
Epoch 9 Step 1801 Train Loss: 0.4956
Epoch 9 Step 1851 Train Loss: 0.4604
Epoch 9: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0788 Validation Top 20 DE MSE: 0.1610. 
Epoch 10 Step 1 Train Loss: 0.5010
Epoch 10 Step 51 Train Loss: 0.4673
Epoch 10 Step 101 Train Loss: 0.5009
Epoch 10 Step 151 Train Loss: 0.4816
Epoch 10 Step 201 Train Loss: 0.4757
Epoch 10 Step 251 Train Loss: 0.4699
Epoch 10 Step 301 Train Loss: 0.5357
Epoch 10 Step 351 Train Loss: 0.4745
Epoch 10 Step 401 Train Loss: 0.5513
Epoch 10 Step 451 Train Loss: 0.4626
Epoch 10 Step 501 Train Loss: 0.4509
Epoch 10 Step 551 Train Loss: 0.4892
Epoch 10 Step 601 Train Loss: 0.5058
Epoch 10 Step 651 Train Loss: 0.4619
Epoch 10 Step 701 Train Loss: 0.4501
Epoch 10 Step 751 Train Loss: 0.5034
Epoch 10 Step 801 Train Loss: 0.4425
Epoch 10 Step 851 Train Loss: 0.4672
Epoch 10 Step 901 Train Loss: 0.4842
Epoch 10 Step 951 Train Loss: 0.5004
Epoch 10 Step 1001 Train Loss: 0.4628
Epoch 10 Step 1051 Train Loss: 0.5224
Epoch 10 Step 1101 Train Loss: 0.5139
Epoch 10 Step 1151 Train Loss: 0.4925
Epoch 10 Step 1201 Train Loss: 0.4633
Epoch 10 Step 1251 Train Loss: 0.4681
Epoch 10 Step 1301 Train Loss: 0.5082
Epoch 10 Step 1351 Train Loss: 0.5007
Epoch 10 Step 1401 Train Loss: 0.4834
Epoch 10 Step 1451 Train Loss: 0.5014
Epoch 10 Step 1501 Train Loss: 0.4771
Epoch 10 Step 1551 Train Loss: 0.6147
Epoch 10 Step 1601 Train Loss: 0.5339
Epoch 10 Step 1651 Train Loss: 0.4889
Epoch 10 Step 1701 Train Loss: 0.5288
Epoch 10 Step 1751 Train Loss: 0.4859
Epoch 10 Step 1801 Train Loss: 0.4553
Epoch 10 Step 1851 Train Loss: 0.5086
Epoch 10: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0795 Validation Top 20 DE MSE: 0.1623. 
Epoch 11 Step 1 Train Loss: 0.4520
Epoch 11 Step 51 Train Loss: 0.5500
Epoch 11 Step 101 Train Loss: 0.4697
Epoch 11 Step 151 Train Loss: 0.5154
Epoch 11 Step 201 Train Loss: 0.5033
Epoch 11 Step 251 Train Loss: 0.4842
Epoch 11 Step 301 Train Loss: 0.5641
Epoch 11 Step 351 Train Loss: 0.4879
Epoch 11 Step 401 Train Loss: 0.5049
Epoch 11 Step 451 Train Loss: 0.5245
Epoch 11 Step 501 Train Loss: 0.4437
Epoch 11 Step 551 Train Loss: 0.4784
Epoch 11 Step 601 Train Loss: 0.4741
Epoch 11 Step 651 Train Loss: 0.4673
Epoch 11 Step 701 Train Loss: 0.4663
Epoch 11 Step 751 Train Loss: 0.4941
Epoch 11 Step 801 Train Loss: 0.5026
Epoch 11 Step 851 Train Loss: 0.4803
Epoch 11 Step 901 Train Loss: 0.4926
Epoch 11 Step 951 Train Loss: 0.5195
Epoch 11 Step 1001 Train Loss: 0.4961
Epoch 11 Step 1051 Train Loss: 0.4914
Epoch 11 Step 1101 Train Loss: 0.5290
Epoch 11 Step 1151 Train Loss: 0.4782
Epoch 11 Step 1201 Train Loss: 0.5393
Epoch 11 Step 1251 Train Loss: 0.4426
Epoch 11 Step 1301 Train Loss: 0.5282
Epoch 11 Step 1351 Train Loss: 0.4997
Epoch 11 Step 1401 Train Loss: 0.4625
Epoch 11 Step 1451 Train Loss: 0.5633
Epoch 11 Step 1501 Train Loss: 0.5485
Epoch 11 Step 1551 Train Loss: 0.5140
Epoch 11 Step 1601 Train Loss: 0.4326
Epoch 11 Step 1651 Train Loss: 0.4795
Epoch 11 Step 1701 Train Loss: 0.4677
Epoch 11 Step 1751 Train Loss: 0.5158
Epoch 11 Step 1801 Train Loss: 0.4711
Epoch 11 Step 1851 Train Loss: 0.5595
Epoch 11: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0786 Validation Top 20 DE MSE: 0.1595. 
Epoch 12 Step 1 Train Loss: 0.4821
Epoch 12 Step 51 Train Loss: 0.4538
Epoch 12 Step 101 Train Loss: 0.4506
Epoch 12 Step 151 Train Loss: 0.4626
Epoch 12 Step 201 Train Loss: 0.4908
Epoch 12 Step 251 Train Loss: 0.4755
Epoch 12 Step 301 Train Loss: 0.5283
Epoch 12 Step 351 Train Loss: 0.5371
Epoch 12 Step 401 Train Loss: 0.4807
Epoch 12 Step 451 Train Loss: 0.5052
Epoch 12 Step 501 Train Loss: 0.4494
Epoch 12 Step 551 Train Loss: 0.4561
Epoch 12 Step 601 Train Loss: 0.5734
Epoch 12 Step 651 Train Loss: 0.5663
Epoch 12 Step 701 Train Loss: 0.5419
Epoch 12 Step 751 Train Loss: 0.4938
Epoch 12 Step 801 Train Loss: 0.5139
Epoch 12 Step 851 Train Loss: 0.5385
Epoch 12 Step 901 Train Loss: 0.5010
Epoch 12 Step 951 Train Loss: 0.4634
Epoch 12 Step 1001 Train Loss: 0.5058
Epoch 12 Step 1051 Train Loss: 0.4834
Epoch 12 Step 1101 Train Loss: 0.5124
Epoch 12 Step 1151 Train Loss: 0.4863
Epoch 12 Step 1201 Train Loss: 0.4941
Epoch 12 Step 1251 Train Loss: 0.4813
Epoch 12 Step 1301 Train Loss: 0.4920
Epoch 12 Step 1351 Train Loss: 0.4729
Epoch 12 Step 1401 Train Loss: 0.5077
Epoch 12 Step 1451 Train Loss: 0.4988
Epoch 12 Step 1501 Train Loss: 0.4969
Epoch 12 Step 1551 Train Loss: 0.5038
Epoch 12 Step 1601 Train Loss: 0.5070
Epoch 12 Step 1651 Train Loss: 0.4893
Epoch 12 Step 1701 Train Loss: 0.4735
Epoch 12 Step 1751 Train Loss: 0.4714
Epoch 12 Step 1801 Train Loss: 0.4799
Epoch 12 Step 1851 Train Loss: 0.5108
Epoch 12: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0031. 
Train Top 20 DE MSE: 0.0831 Validation Top 20 DE MSE: 0.1647. 
Epoch 13 Step 1 Train Loss: 0.4927
Epoch 13 Step 51 Train Loss: 0.4658
Epoch 13 Step 101 Train Loss: 0.4573
Epoch 13 Step 151 Train Loss: 0.5122
Epoch 13 Step 201 Train Loss: 0.5127
Epoch 13 Step 251 Train Loss: 0.5193
Epoch 13 Step 301 Train Loss: 0.4520
Epoch 13 Step 351 Train Loss: 0.5410
Epoch 13 Step 401 Train Loss: 0.4575
Epoch 13 Step 451 Train Loss: 0.4587
Epoch 13 Step 501 Train Loss: 0.4991
Epoch 13 Step 551 Train Loss: 0.5224
Epoch 13 Step 601 Train Loss: 0.4996
Epoch 13 Step 651 Train Loss: 0.4814
Epoch 13 Step 701 Train Loss: 0.4759
Epoch 13 Step 751 Train Loss: 0.5122
Epoch 13 Step 801 Train Loss: 0.4833
Epoch 13 Step 851 Train Loss: 0.4468
Epoch 13 Step 901 Train Loss: 0.4562
Epoch 13 Step 951 Train Loss: 0.5084
Epoch 13 Step 1001 Train Loss: 0.4878
Epoch 13 Step 1051 Train Loss: 0.4648
Epoch 13 Step 1101 Train Loss: 0.5169
Epoch 13 Step 1151 Train Loss: 0.4970
Epoch 13 Step 1201 Train Loss: 0.4904
Epoch 13 Step 1251 Train Loss: 0.4503
Epoch 13 Step 1301 Train Loss: 0.4556
Epoch 13 Step 1351 Train Loss: 0.4529
Epoch 13 Step 1401 Train Loss: 0.4801
Epoch 13 Step 1451 Train Loss: 0.4757
Epoch 13 Step 1501 Train Loss: 0.4737
Epoch 13 Step 1551 Train Loss: 0.4722
Epoch 13 Step 1601 Train Loss: 0.5015
Epoch 13 Step 1651 Train Loss: 0.5094
Epoch 13 Step 1701 Train Loss: 0.4894
Epoch 13 Step 1751 Train Loss: 0.4877
Epoch 13 Step 1801 Train Loss: 0.4866
Epoch 13 Step 1851 Train Loss: 0.4484
Epoch 13: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0768 Validation Top 20 DE MSE: 0.1587. 
Epoch 14 Step 1 Train Loss: 0.5550
Epoch 14 Step 51 Train Loss: 0.5064
Epoch 14 Step 101 Train Loss: 0.4875
Epoch 14 Step 151 Train Loss: 0.4962
Epoch 14 Step 201 Train Loss: 0.5112
Epoch 14 Step 251 Train Loss: 0.5162
Epoch 14 Step 301 Train Loss: 0.4956
Epoch 14 Step 351 Train Loss: 0.4828
Epoch 14 Step 401 Train Loss: 0.4979
Epoch 14 Step 451 Train Loss: 0.4810
Epoch 14 Step 501 Train Loss: 0.4641
Epoch 14 Step 551 Train Loss: 0.5411
Epoch 14 Step 601 Train Loss: 0.5237
Epoch 14 Step 651 Train Loss: 0.4713
Epoch 14 Step 701 Train Loss: 0.4914
Epoch 14 Step 751 Train Loss: 0.4514
Epoch 14 Step 801 Train Loss: 0.4853
Epoch 14 Step 851 Train Loss: 0.4751
Epoch 14 Step 901 Train Loss: 0.5275
Epoch 14 Step 951 Train Loss: 0.4647
Epoch 14 Step 1001 Train Loss: 0.5055
Epoch 14 Step 1051 Train Loss: 0.5315
Epoch 14 Step 1101 Train Loss: 0.5021
Epoch 14 Step 1151 Train Loss: 0.4656
Epoch 14 Step 1201 Train Loss: 0.5114
Epoch 14 Step 1251 Train Loss: 0.4938
Epoch 14 Step 1301 Train Loss: 0.5148
Epoch 14 Step 1351 Train Loss: 0.5332
Epoch 14 Step 1401 Train Loss: 0.5021
Epoch 14 Step 1451 Train Loss: 0.4792
Epoch 14 Step 1501 Train Loss: 0.5184
Epoch 14 Step 1551 Train Loss: 0.4781
Epoch 14 Step 1601 Train Loss: 0.5107
Epoch 14 Step 1651 Train Loss: 0.4799
Epoch 14 Step 1701 Train Loss: 0.4995
Epoch 14 Step 1751 Train Loss: 0.4598
Epoch 14 Step 1801 Train Loss: 0.4779
Epoch 14 Step 1851 Train Loss: 0.4991
Epoch 14: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0816 Validation Top 20 DE MSE: 0.1641. 
Epoch 15 Step 1 Train Loss: 0.4613
Epoch 15 Step 51 Train Loss: 0.4445
Epoch 15 Step 101 Train Loss: 0.5003
Epoch 15 Step 151 Train Loss: 0.4956
Epoch 15 Step 201 Train Loss: 0.4710
Epoch 15 Step 251 Train Loss: 0.5206
Epoch 15 Step 301 Train Loss: 0.5509
Epoch 15 Step 351 Train Loss: 0.4747
Epoch 15 Step 401 Train Loss: 0.5241
Epoch 15 Step 451 Train Loss: 0.4643
Epoch 15 Step 501 Train Loss: 0.4428
Epoch 15 Step 551 Train Loss: 0.4937
Epoch 15 Step 601 Train Loss: 0.4601
Epoch 15 Step 651 Train Loss: 0.4733
Epoch 15 Step 701 Train Loss: 0.5791
Epoch 15 Step 751 Train Loss: 0.4696
Epoch 15 Step 801 Train Loss: 0.5316
Epoch 15 Step 851 Train Loss: 0.4960
Epoch 15 Step 901 Train Loss: 0.4589
Epoch 15 Step 951 Train Loss: 0.5071
Epoch 15 Step 1001 Train Loss: 0.5003
Epoch 15 Step 1051 Train Loss: 0.4797
Epoch 15 Step 1101 Train Loss: 0.4926
Epoch 15 Step 1151 Train Loss: 0.5083
Epoch 15 Step 1201 Train Loss: 0.4466
Epoch 15 Step 1251 Train Loss: 0.5063
Epoch 15 Step 1301 Train Loss: 0.4963
Epoch 15 Step 1351 Train Loss: 0.4617
Epoch 15 Step 1401 Train Loss: 0.4339
Epoch 15 Step 1451 Train Loss: 0.4799
Epoch 15 Step 1501 Train Loss: 0.4739
Epoch 15 Step 1551 Train Loss: 0.4598
Epoch 15 Step 1601 Train Loss: 0.4340
Epoch 15 Step 1651 Train Loss: 0.4790
Epoch 15 Step 1701 Train Loss: 0.5546
Epoch 15 Step 1751 Train Loss: 0.4933
Epoch 15 Step 1801 Train Loss: 0.4783
Epoch 15 Step 1851 Train Loss: 0.5108
Epoch 15: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0032. 
Train Top 20 DE MSE: 0.0789 Validation Top 20 DE MSE: 0.1611. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1454
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.005056897
test_combo_seen0_pearson: 0.9882871741403236
test_combo_seen0_mse_de: 0.4691045
test_combo_seen0_pearson_de: 0.9215962781958
test_combo_seen1_mse: 0.003980663
test_combo_seen1_pearson: 0.9911242581977521
test_combo_seen1_mse_de: 0.1460705
test_combo_seen1_pearson_de: 0.8264463266740671
test_combo_seen2_mse: 0.0039942637
test_combo_seen2_pearson: 0.9910049218761705
test_combo_seen2_mse_de: 0.14255418
test_combo_seen2_pearson_de: 0.8750184207627685
test_unseen_single_mse: 0.0022569152
test_unseen_single_pearson: 0.9948756571181198
test_unseen_single_mse_de: 0.12288199
test_unseen_single_pearson_de: 0.9480142061512081
test_combo_seen0_pearson_delta: 0.654711406820965
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.025
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.675
test_combo_seen0_mse_top20_de_non_dropout: 0.4770227
test_combo_seen1_pearson_delta: 0.5636324905875014
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.14102564102564105
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8974358974358972
test_combo_seen1_mse_top20_de_non_dropout: 0.17628738
test_combo_seen2_pearson_delta: 0.6108472198925872
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.12391304347826088
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.8804347826086953
test_combo_seen2_mse_top20_de_non_dropout: 0.16247258
test_unseen_single_pearson_delta: 0.4437024416334172
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.24629629629629632
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9351851851851852
test_unseen_single_mse_top20_de_non_dropout: 0.13589959
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.035 MB uploadedwandb: | 0.001 MB of 0.039 MB uploadedwandb: / 0.001 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÅ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ
wandb:                                                   val_de_mse ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                                               val_de_pearson ‚ñÉ‚ñÖ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.025
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.675
wandb:                                         test_combo_seen0_mse 0.00506
wandb:                                      test_combo_seen0_mse_de 0.4691
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.47702
wandb:                                     test_combo_seen0_pearson 0.98829
wandb:                                  test_combo_seen0_pearson_de 0.9216
wandb:                               test_combo_seen0_pearson_delta 0.65471
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.14103
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.89744
wandb:                                         test_combo_seen1_mse 0.00398
wandb:                                      test_combo_seen1_mse_de 0.14607
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.17629
wandb:                                     test_combo_seen1_pearson 0.99112
wandb:                                  test_combo_seen1_pearson_de 0.82645
wandb:                               test_combo_seen1_pearson_delta 0.56363
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.12391
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.88043
wandb:                                         test_combo_seen2_mse 0.00399
wandb:                                      test_combo_seen2_mse_de 0.14255
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.16247
wandb:                                     test_combo_seen2_pearson 0.991
wandb:                                  test_combo_seen2_pearson_de 0.87502
wandb:                               test_combo_seen2_pearson_delta 0.61085
wandb:                                                  test_de_mse 0.1454
wandb:                                              test_de_pearson 0.87688
wandb:               test_frac_opposite_direction_top20_non_dropout 0.16538
wandb:                          test_frac_sigma_below_1_non_dropout 0.89945
wandb:                                                     test_mse 0.0035
wandb:                                test_mse_top20_de_non_dropout 0.16742
wandb:                                                 test_pearson 0.99214
wandb:                                           test_pearson_delta 0.54198
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.2463
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.93519
wandb:                                       test_unseen_single_mse 0.00226
wandb:                                    test_unseen_single_mse_de 0.12288
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.1359
wandb:                                   test_unseen_single_pearson 0.99488
wandb:                                test_unseen_single_pearson_de 0.94801
wandb:                             test_unseen_single_pearson_delta 0.4437
wandb:                                                 train_de_mse 0.07892
wandb:                                             train_de_pearson 0.86546
wandb:                                                    train_mse 0.00213
wandb:                                                train_pearson 0.99524
wandb:                                                training_loss 0.49521
wandb:                                                   val_de_mse 0.16115
wandb:                                               val_de_pearson 0.82872
wandb:                                                      val_mse 0.00323
wandb:                                                  val_pearson 0.99278
wandb: 
wandb: üöÄ View run geneformer_NormanWeissman2019_split3 at: https://wandb.ai/zhoumin1130/New_gears/runs/zn7bzoud
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_003538-zn7bzoud/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:5
combo_seen1:44
combo_seen2:21
unseen_single:25
Done!
Creating dataloaders....
Done!
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_014620-y2od24se
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_NormanWeissman2019_split4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/y2od24se
wandb: WARNING Serializing object of type ndarray that is 20631680 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4891
Epoch 1 Step 51 Train Loss: 0.4760
Epoch 1 Step 101 Train Loss: 0.5241
Epoch 1 Step 151 Train Loss: 0.5147
Epoch 1 Step 201 Train Loss: 0.4727
Epoch 1 Step 251 Train Loss: 0.5089
Epoch 1 Step 301 Train Loss: 0.4647
Epoch 1 Step 351 Train Loss: 0.4688
Epoch 1 Step 401 Train Loss: 0.4438
Epoch 1 Step 451 Train Loss: 0.4979
Epoch 1 Step 501 Train Loss: 0.5294
Epoch 1 Step 551 Train Loss: 0.4283
Epoch 1 Step 601 Train Loss: 0.4458
Epoch 1 Step 651 Train Loss: 0.4458
Epoch 1 Step 701 Train Loss: 0.4811
Epoch 1 Step 751 Train Loss: 0.4583
Epoch 1 Step 801 Train Loss: 0.4768
Epoch 1 Step 851 Train Loss: 0.4396
Epoch 1 Step 901 Train Loss: 0.4774
Epoch 1 Step 951 Train Loss: 0.4591
Epoch 1 Step 1001 Train Loss: 0.4693
Epoch 1 Step 1051 Train Loss: 0.4327
Epoch 1 Step 1101 Train Loss: 0.4372
Epoch 1 Step 1151 Train Loss: 0.5269
Epoch 1 Step 1201 Train Loss: 0.4276
Epoch 1 Step 1251 Train Loss: 0.4390
Epoch 1 Step 1301 Train Loss: 0.5178
Epoch 1 Step 1351 Train Loss: 0.4058
Epoch 1 Step 1401 Train Loss: 0.4233
Epoch 1 Step 1451 Train Loss: 0.4403
Epoch 1 Step 1501 Train Loss: 0.4800
Epoch 1 Step 1551 Train Loss: 0.4362
Epoch 1 Step 1601 Train Loss: 0.5400
Epoch 1 Step 1651 Train Loss: 0.3940
Epoch 1 Step 1701 Train Loss: 0.4543
Epoch 1 Step 1751 Train Loss: 0.4094
Epoch 1 Step 1801 Train Loss: 0.4458
Epoch 1: Train Overall MSE: 0.0049 Validation Overall MSE: 0.0068. 
Train Top 20 DE MSE: 0.1317 Validation Top 20 DE MSE: 0.2148. 
Epoch 2 Step 1 Train Loss: 0.4054
Epoch 2 Step 51 Train Loss: 0.4693
Epoch 2 Step 101 Train Loss: 0.4650
Epoch 2 Step 151 Train Loss: 0.4466
Epoch 2 Step 201 Train Loss: 0.4707
Epoch 2 Step 251 Train Loss: 0.4780
Epoch 2 Step 301 Train Loss: 0.4462
Epoch 2 Step 351 Train Loss: 0.4738
Epoch 2 Step 401 Train Loss: 0.4080
Epoch 2 Step 451 Train Loss: 0.5278
Epoch 2 Step 501 Train Loss: 0.4910
Epoch 2 Step 551 Train Loss: 0.4523
Epoch 2 Step 601 Train Loss: 0.4973
Epoch 2 Step 651 Train Loss: 0.5250
Epoch 2 Step 701 Train Loss: 0.4447
Epoch 2 Step 751 Train Loss: 0.4470
Epoch 2 Step 801 Train Loss: 0.4713
Epoch 2 Step 851 Train Loss: 0.4474
Epoch 2 Step 901 Train Loss: 0.4720
Epoch 2 Step 951 Train Loss: 0.4041
Epoch 2 Step 1001 Train Loss: 0.4591
Epoch 2 Step 1051 Train Loss: 0.4589
Epoch 2 Step 1101 Train Loss: 0.5075
Epoch 2 Step 1151 Train Loss: 0.4362
Epoch 2 Step 1201 Train Loss: 0.4857
Epoch 2 Step 1251 Train Loss: 0.5404
Epoch 2 Step 1301 Train Loss: 0.5514
Epoch 2 Step 1351 Train Loss: 0.5655
Epoch 2 Step 1401 Train Loss: 0.4623
Epoch 2 Step 1451 Train Loss: 0.4448
Epoch 2 Step 1501 Train Loss: 0.4826
Epoch 2 Step 1551 Train Loss: 0.4844
Epoch 2 Step 1601 Train Loss: 0.4058
Epoch 2 Step 1651 Train Loss: 0.5274
Epoch 2 Step 1701 Train Loss: 0.4951
Epoch 2 Step 1751 Train Loss: 0.4789
Epoch 2 Step 1801 Train Loss: 0.5106
Epoch 2: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0040. 
Train Top 20 DE MSE: 0.0918 Validation Top 20 DE MSE: 0.1578. 
Epoch 3 Step 1 Train Loss: 0.4735
Epoch 3 Step 51 Train Loss: 0.4452
Epoch 3 Step 101 Train Loss: 0.4527
Epoch 3 Step 151 Train Loss: 0.4782
Epoch 3 Step 201 Train Loss: 0.4894
Epoch 3 Step 251 Train Loss: 0.4406
Epoch 3 Step 301 Train Loss: 0.5192
Epoch 3 Step 351 Train Loss: 0.4546
Epoch 3 Step 401 Train Loss: 0.5689
Epoch 3 Step 451 Train Loss: 0.4826
Epoch 3 Step 501 Train Loss: 0.4710
Epoch 3 Step 551 Train Loss: 0.5216
Epoch 3 Step 601 Train Loss: 0.4441
Epoch 3 Step 651 Train Loss: 0.4860
Epoch 3 Step 701 Train Loss: 0.4834
Epoch 3 Step 751 Train Loss: 0.4772
Epoch 3 Step 801 Train Loss: 0.4595
Epoch 3 Step 851 Train Loss: 0.4593
Epoch 3 Step 901 Train Loss: 0.4532
Epoch 3 Step 951 Train Loss: 0.4542
Epoch 3 Step 1001 Train Loss: 0.4369
Epoch 3 Step 1051 Train Loss: 0.4259
Epoch 3 Step 1101 Train Loss: 0.4062
Epoch 3 Step 1151 Train Loss: 0.4803
Epoch 3 Step 1201 Train Loss: 0.4401
Epoch 3 Step 1251 Train Loss: 0.4398
Epoch 3 Step 1301 Train Loss: 0.5041
Epoch 3 Step 1351 Train Loss: 0.5196
Epoch 3 Step 1401 Train Loss: 0.5560
Epoch 3 Step 1451 Train Loss: 0.4653
Epoch 3 Step 1501 Train Loss: 0.4860
Epoch 3 Step 1551 Train Loss: 0.4859
Epoch 3 Step 1601 Train Loss: 0.4848
Epoch 3 Step 1651 Train Loss: 0.5286
Epoch 3 Step 1701 Train Loss: 0.4805
Epoch 3 Step 1751 Train Loss: 0.4798
Epoch 3 Step 1801 Train Loss: 0.4752
Epoch 3: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0947 Validation Top 20 DE MSE: 0.1664. 
Epoch 4 Step 1 Train Loss: 0.4334
Epoch 4 Step 51 Train Loss: 0.4652
Epoch 4 Step 101 Train Loss: 0.4465
Epoch 4 Step 151 Train Loss: 0.4965
Epoch 4 Step 201 Train Loss: 0.4832
Epoch 4 Step 251 Train Loss: 0.4874
Epoch 4 Step 301 Train Loss: 0.4307
Epoch 4 Step 351 Train Loss: 0.4911
Epoch 4 Step 401 Train Loss: 0.4883
Epoch 4 Step 451 Train Loss: 0.4921
Epoch 4 Step 501 Train Loss: 0.5445
Epoch 4 Step 551 Train Loss: 0.4796
Epoch 4 Step 601 Train Loss: 0.4840
Epoch 4 Step 651 Train Loss: 0.4885
Epoch 4 Step 701 Train Loss: 0.4797
Epoch 4 Step 751 Train Loss: 0.4408
Epoch 4 Step 801 Train Loss: 0.4706
Epoch 4 Step 851 Train Loss: 0.4935
Epoch 4 Step 901 Train Loss: 0.4633
Epoch 4 Step 951 Train Loss: 0.5312
Epoch 4 Step 1001 Train Loss: 0.5492
Epoch 4 Step 1051 Train Loss: 0.4716
Epoch 4 Step 1101 Train Loss: 0.5365
Epoch 4 Step 1151 Train Loss: 0.4583
Epoch 4 Step 1201 Train Loss: 0.5548
Epoch 4 Step 1251 Train Loss: 0.4724
Epoch 4 Step 1301 Train Loss: 0.4510
Epoch 4 Step 1351 Train Loss: 0.4724
Epoch 4 Step 1401 Train Loss: 0.5266
Epoch 4 Step 1451 Train Loss: 0.5003
Epoch 4 Step 1501 Train Loss: 0.5055
Epoch 4 Step 1551 Train Loss: 0.4931
Epoch 4 Step 1601 Train Loss: 0.5225
Epoch 4 Step 1651 Train Loss: 0.4919
Epoch 4 Step 1701 Train Loss: 0.4820
Epoch 4 Step 1751 Train Loss: 0.4409
Epoch 4 Step 1801 Train Loss: 0.4477
Epoch 4: Train Overall MSE: 0.0027 Validation Overall MSE: 0.0038. 
Train Top 20 DE MSE: 0.0878 Validation Top 20 DE MSE: 0.1516. 
Epoch 5 Step 1 Train Loss: 0.4515
Epoch 5 Step 51 Train Loss: 0.4935
Epoch 5 Step 101 Train Loss: 0.4622
Epoch 5 Step 151 Train Loss: 0.5634
Epoch 5 Step 201 Train Loss: 0.5531
Epoch 5 Step 251 Train Loss: 0.5624
Epoch 5 Step 301 Train Loss: 0.5132
Epoch 5 Step 351 Train Loss: 0.4615
Epoch 5 Step 401 Train Loss: 0.5278
Epoch 5 Step 451 Train Loss: 0.5038
Epoch 5 Step 501 Train Loss: 0.4817
Epoch 5 Step 551 Train Loss: 0.5069
Epoch 5 Step 601 Train Loss: 0.5021
Epoch 5 Step 651 Train Loss: 0.5017
Epoch 5 Step 701 Train Loss: 0.4730
Epoch 5 Step 751 Train Loss: 0.4924
Epoch 5 Step 801 Train Loss: 0.4752
Epoch 5 Step 851 Train Loss: 0.4937
Epoch 5 Step 901 Train Loss: 0.4845
Epoch 5 Step 951 Train Loss: 0.5053
Epoch 5 Step 1001 Train Loss: 0.4500
Epoch 5 Step 1051 Train Loss: 0.4830
Epoch 5 Step 1101 Train Loss: 0.4653
Epoch 5 Step 1151 Train Loss: 0.4618
Epoch 5 Step 1201 Train Loss: 0.4548
Epoch 5 Step 1251 Train Loss: 0.5096
Epoch 5 Step 1301 Train Loss: 0.5673
Epoch 5 Step 1351 Train Loss: 0.4806
Epoch 5 Step 1401 Train Loss: 0.4618
Epoch 5 Step 1451 Train Loss: 0.4865
Epoch 5 Step 1501 Train Loss: 0.4770
Epoch 5 Step 1551 Train Loss: 0.4785
Epoch 5 Step 1601 Train Loss: 0.4733
Epoch 5 Step 1651 Train Loss: 0.5252
Epoch 5 Step 1701 Train Loss: 0.4972
Epoch 5 Step 1751 Train Loss: 0.5156
Epoch 5 Step 1801 Train Loss: 0.4995
Epoch 5: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0033. 
Train Top 20 DE MSE: 0.0923 Validation Top 20 DE MSE: 0.1609. 
Epoch 6 Step 1 Train Loss: 0.5302
Epoch 6 Step 51 Train Loss: 0.4967
Epoch 6 Step 101 Train Loss: 0.5093
Epoch 6 Step 151 Train Loss: 0.5896
Epoch 6 Step 201 Train Loss: 0.5037
Epoch 6 Step 251 Train Loss: 0.4549
Epoch 6 Step 301 Train Loss: 0.4980
Epoch 6 Step 351 Train Loss: 0.4872
Epoch 6 Step 401 Train Loss: 0.4310
Epoch 6 Step 451 Train Loss: 0.4788
Epoch 6 Step 501 Train Loss: 0.5222
Epoch 6 Step 551 Train Loss: 0.4928
Epoch 6 Step 601 Train Loss: 0.4519
Epoch 6 Step 651 Train Loss: 0.5094
Epoch 6 Step 701 Train Loss: 0.4840
Epoch 6 Step 751 Train Loss: 0.4872
Epoch 6 Step 801 Train Loss: 0.4899
Epoch 6 Step 851 Train Loss: 0.4695
Epoch 6 Step 901 Train Loss: 0.5309
Epoch 6 Step 951 Train Loss: 0.5138
Epoch 6 Step 1001 Train Loss: 0.4663
Epoch 6 Step 1051 Train Loss: 0.4827
Epoch 6 Step 1101 Train Loss: 0.4749
Epoch 6 Step 1151 Train Loss: 0.4904
Epoch 6 Step 1201 Train Loss: 0.4840
Epoch 6 Step 1251 Train Loss: 0.5952
Epoch 6 Step 1301 Train Loss: 0.5514
Epoch 6 Step 1351 Train Loss: 0.4784
Epoch 6 Step 1401 Train Loss: 0.4836
Epoch 6 Step 1451 Train Loss: 0.4967
Epoch 6 Step 1501 Train Loss: 0.4854
Epoch 6 Step 1551 Train Loss: 0.5036
Epoch 6 Step 1601 Train Loss: 0.5205
Epoch 6 Step 1651 Train Loss: 0.5370
Epoch 6 Step 1701 Train Loss: 0.4778
Epoch 6 Step 1751 Train Loss: 0.5788
Epoch 6 Step 1801 Train Loss: 0.4718
Epoch 6: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0915 Validation Top 20 DE MSE: 0.1627. 
Epoch 7 Step 1 Train Loss: 0.5254
Epoch 7 Step 51 Train Loss: 0.4974
Epoch 7 Step 101 Train Loss: 0.4541
Epoch 7 Step 151 Train Loss: 0.6049
Epoch 7 Step 201 Train Loss: 0.4986
Epoch 7 Step 251 Train Loss: 0.4633
Epoch 7 Step 301 Train Loss: 0.5107
Epoch 7 Step 351 Train Loss: 0.4889
Epoch 7 Step 401 Train Loss: 0.5180
Epoch 7 Step 451 Train Loss: 0.4762
Epoch 7 Step 501 Train Loss: 0.4860
Epoch 7 Step 551 Train Loss: 0.4995
Epoch 7 Step 601 Train Loss: 0.4692
Epoch 7 Step 651 Train Loss: 0.5454
Epoch 7 Step 701 Train Loss: 0.5002
Epoch 7 Step 751 Train Loss: 0.5548
Epoch 7 Step 801 Train Loss: 0.5063
Epoch 7 Step 851 Train Loss: 0.5373
Epoch 7 Step 901 Train Loss: 0.5032
Epoch 7 Step 951 Train Loss: 0.4540
Epoch 7 Step 1001 Train Loss: 0.5194
Epoch 7 Step 1051 Train Loss: 0.4983
Epoch 7 Step 1101 Train Loss: 0.5441
Epoch 7 Step 1151 Train Loss: 0.5097
Epoch 7 Step 1201 Train Loss: 0.4427
Epoch 7 Step 1251 Train Loss: 0.4852
Epoch 7 Step 1301 Train Loss: 0.4836
Epoch 7 Step 1351 Train Loss: 0.5344
Epoch 7 Step 1401 Train Loss: 0.5126
Epoch 7 Step 1451 Train Loss: 0.4600
Epoch 7 Step 1501 Train Loss: 0.4854
Epoch 7 Step 1551 Train Loss: 0.5112
Epoch 7 Step 1601 Train Loss: 0.5031
Epoch 7 Step 1651 Train Loss: 0.5505
Epoch 7 Step 1701 Train Loss: 0.4887
Epoch 7 Step 1751 Train Loss: 0.4722
Epoch 7 Step 1801 Train Loss: 0.5482
Epoch 7: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0894 Validation Top 20 DE MSE: 0.1575. 
Epoch 8 Step 1 Train Loss: 0.5299
Epoch 8 Step 51 Train Loss: 0.4778
Epoch 8 Step 101 Train Loss: 0.5303
Epoch 8 Step 151 Train Loss: 0.4804
Epoch 8 Step 201 Train Loss: 0.4670
Epoch 8 Step 251 Train Loss: 0.4810
Epoch 8 Step 301 Train Loss: 0.4978
Epoch 8 Step 351 Train Loss: 0.4767
Epoch 8 Step 401 Train Loss: 0.4553
Epoch 8 Step 451 Train Loss: 0.5032
Epoch 8 Step 501 Train Loss: 0.5070
Epoch 8 Step 551 Train Loss: 0.5222
Epoch 8 Step 601 Train Loss: 0.4791
Epoch 8 Step 651 Train Loss: 0.4548
Epoch 8 Step 701 Train Loss: 0.4769
Epoch 8 Step 751 Train Loss: 0.4836
Epoch 8 Step 801 Train Loss: 0.5291
Epoch 8 Step 851 Train Loss: 0.4788
Epoch 8 Step 901 Train Loss: 0.5608
Epoch 8 Step 951 Train Loss: 0.4865
Epoch 8 Step 1001 Train Loss: 0.5321
Epoch 8 Step 1051 Train Loss: 0.5321
Epoch 8 Step 1101 Train Loss: 0.4870
Epoch 8 Step 1151 Train Loss: 0.5276
Epoch 8 Step 1201 Train Loss: 0.5050
Epoch 8 Step 1251 Train Loss: 0.5296
Epoch 8 Step 1301 Train Loss: 0.5032
Epoch 8 Step 1351 Train Loss: 0.5468
Epoch 8 Step 1401 Train Loss: 0.5023
Epoch 8 Step 1451 Train Loss: 0.5016
Epoch 8 Step 1501 Train Loss: 0.4705
Epoch 8 Step 1551 Train Loss: 0.4846
Epoch 8 Step 1601 Train Loss: 0.5119
Epoch 8 Step 1651 Train Loss: 0.4835
Epoch 8 Step 1701 Train Loss: 0.4937
Epoch 8 Step 1751 Train Loss: 0.5527
Epoch 8 Step 1801 Train Loss: 0.5414
Epoch 8: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0894 Validation Top 20 DE MSE: 0.1591. 
Epoch 9 Step 1 Train Loss: 0.5383
Epoch 9 Step 51 Train Loss: 0.4825
Epoch 9 Step 101 Train Loss: 0.5248
Epoch 9 Step 151 Train Loss: 0.5969
Epoch 9 Step 201 Train Loss: 0.4733
Epoch 9 Step 251 Train Loss: 0.5475
Epoch 9 Step 301 Train Loss: 0.4795
Epoch 9 Step 351 Train Loss: 0.4577
Epoch 9 Step 401 Train Loss: 0.4910
Epoch 9 Step 451 Train Loss: 0.5279
Epoch 9 Step 501 Train Loss: 0.5250
Epoch 9 Step 551 Train Loss: 0.4877
Epoch 9 Step 601 Train Loss: 0.5083
Epoch 9 Step 651 Train Loss: 0.5451
Epoch 9 Step 701 Train Loss: 0.4917
Epoch 9 Step 751 Train Loss: 0.4807
Epoch 9 Step 801 Train Loss: 0.5389
Epoch 9 Step 851 Train Loss: 0.5208
Epoch 9 Step 901 Train Loss: 0.4651
Epoch 9 Step 951 Train Loss: 0.5415
Epoch 9 Step 1001 Train Loss: 0.5593
Epoch 9 Step 1051 Train Loss: 0.4560
Epoch 9 Step 1101 Train Loss: 0.5237
Epoch 9 Step 1151 Train Loss: 0.5262
Epoch 9 Step 1201 Train Loss: 0.4738
Epoch 9 Step 1251 Train Loss: 0.5024
Epoch 9 Step 1301 Train Loss: 0.4856
Epoch 9 Step 1351 Train Loss: 0.5031
Epoch 9 Step 1401 Train Loss: 0.5325
Epoch 9 Step 1451 Train Loss: 0.5162
Epoch 9 Step 1501 Train Loss: 0.5117
Epoch 9 Step 1551 Train Loss: 0.5213
Epoch 9 Step 1601 Train Loss: 0.4971
Epoch 9 Step 1651 Train Loss: 0.5143
Epoch 9 Step 1701 Train Loss: 0.5427
Epoch 9 Step 1751 Train Loss: 0.5161
Epoch 9 Step 1801 Train Loss: 0.5057
Epoch 9: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0871 Validation Top 20 DE MSE: 0.1585. 
Epoch 10 Step 1 Train Loss: 0.5050
Epoch 10 Step 51 Train Loss: 0.5469
Epoch 10 Step 101 Train Loss: 0.5240
Epoch 10 Step 151 Train Loss: 0.4860
Epoch 10 Step 201 Train Loss: 0.5896
Epoch 10 Step 251 Train Loss: 0.5110
Epoch 10 Step 301 Train Loss: 0.5033
Epoch 10 Step 351 Train Loss: 0.5039
Epoch 10 Step 401 Train Loss: 0.5195
Epoch 10 Step 451 Train Loss: 0.5340
Epoch 10 Step 501 Train Loss: 0.5525
Epoch 10 Step 551 Train Loss: 0.5538
Epoch 10 Step 601 Train Loss: 0.5485
Epoch 10 Step 651 Train Loss: 0.6103
Epoch 10 Step 701 Train Loss: 0.5752
Epoch 10 Step 751 Train Loss: 0.4894
Epoch 10 Step 801 Train Loss: 0.5277
Epoch 10 Step 851 Train Loss: 0.4765
Epoch 10 Step 901 Train Loss: 0.5001
Epoch 10 Step 951 Train Loss: 0.5241
Epoch 10 Step 1001 Train Loss: 0.5043
Epoch 10 Step 1051 Train Loss: 0.5116
Epoch 10 Step 1101 Train Loss: 0.5352
Epoch 10 Step 1151 Train Loss: 0.4556
Epoch 10 Step 1201 Train Loss: 0.4941
Epoch 10 Step 1251 Train Loss: 0.5200
Epoch 10 Step 1301 Train Loss: 0.5793
Epoch 10 Step 1351 Train Loss: 0.5223
Epoch 10 Step 1401 Train Loss: 0.5057
Epoch 10 Step 1451 Train Loss: 0.4715
Epoch 10 Step 1501 Train Loss: 0.5281
Epoch 10 Step 1551 Train Loss: 0.5317
Epoch 10 Step 1601 Train Loss: 0.5608
Epoch 10 Step 1651 Train Loss: 0.5360
Epoch 10 Step 1701 Train Loss: 0.4974
Epoch 10 Step 1751 Train Loss: 0.5151
Epoch 10 Step 1801 Train Loss: 0.5389
Epoch 10: Train Overall MSE: 0.0026 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0880 Validation Top 20 DE MSE: 0.1580. 
Epoch 11 Step 1 Train Loss: 0.4731
Epoch 11 Step 51 Train Loss: 0.5157
Epoch 11 Step 101 Train Loss: 0.5099
Epoch 11 Step 151 Train Loss: 0.5025
Epoch 11 Step 201 Train Loss: 0.4992
Epoch 11 Step 251 Train Loss: 0.4647
Epoch 11 Step 301 Train Loss: 0.5017
Epoch 11 Step 351 Train Loss: 0.4671
Epoch 11 Step 401 Train Loss: 0.5140
Epoch 11 Step 451 Train Loss: 0.5157
Epoch 11 Step 501 Train Loss: 0.4386
Epoch 11 Step 551 Train Loss: 0.5071
Epoch 11 Step 601 Train Loss: 0.5216
Epoch 11 Step 651 Train Loss: 0.4952
Epoch 11 Step 701 Train Loss: 0.4974
Epoch 11 Step 751 Train Loss: 0.4763
Epoch 11 Step 801 Train Loss: 0.4478
Epoch 11 Step 851 Train Loss: 0.4905
Epoch 11 Step 901 Train Loss: 0.6160
Epoch 11 Step 951 Train Loss: 0.5602
Epoch 11 Step 1001 Train Loss: 0.4672
Epoch 11 Step 1051 Train Loss: 0.5162
Epoch 11 Step 1101 Train Loss: 0.5475
Epoch 11 Step 1151 Train Loss: 0.4992
Epoch 11 Step 1201 Train Loss: 0.4568
Epoch 11 Step 1251 Train Loss: 0.5430
Epoch 11 Step 1301 Train Loss: 0.5007
Epoch 11 Step 1351 Train Loss: 0.4995
Epoch 11 Step 1401 Train Loss: 0.4927
Epoch 11 Step 1451 Train Loss: 0.4991
Epoch 11 Step 1501 Train Loss: 0.4580
Epoch 11 Step 1551 Train Loss: 0.4990
Epoch 11 Step 1601 Train Loss: 0.4749
Epoch 11 Step 1651 Train Loss: 0.4956
Epoch 11 Step 1701 Train Loss: 0.5411
Epoch 11 Step 1751 Train Loss: 0.5132
Epoch 11 Step 1801 Train Loss: 0.5293
Epoch 11: Train Overall MSE: 0.0024 Validation Overall MSE: 0.0034. 
Train Top 20 DE MSE: 0.0918 Validation Top 20 DE MSE: 0.1620. 
Epoch 12 Step 1 Train Loss: 0.4311
Epoch 12 Step 51 Train Loss: 0.5556
Epoch 12 Step 101 Train Loss: 0.4971
Epoch 12 Step 151 Train Loss: 0.4772
Epoch 12 Step 201 Train Loss: 0.5483
Epoch 12 Step 251 Train Loss: 0.5171
Epoch 12 Step 301 Train Loss: 0.5122
Epoch 12 Step 351 Train Loss: 0.4937
Epoch 12 Step 401 Train Loss: 0.5107
Epoch 12 Step 451 Train Loss: 0.5312
Epoch 12 Step 501 Train Loss: 0.5091
Epoch 12 Step 551 Train Loss: 0.5304
Epoch 12 Step 601 Train Loss: 0.4728
Epoch 12 Step 651 Train Loss: 0.4470
Epoch 12 Step 701 Train Loss: 0.4879
Epoch 12 Step 751 Train Loss: 0.4568
Epoch 12 Step 801 Train Loss: 0.5270
Epoch 12 Step 851 Train Loss: 0.4899
Epoch 12 Step 901 Train Loss: 0.5107
Epoch 12 Step 951 Train Loss: 0.5058
Epoch 12 Step 1001 Train Loss: 0.4528
Epoch 12 Step 1051 Train Loss: 0.4617
Epoch 12 Step 1101 Train Loss: 0.4554
Epoch 12 Step 1151 Train Loss: 0.5383
Epoch 12 Step 1201 Train Loss: 0.5014
Epoch 12 Step 1251 Train Loss: 0.5113
Epoch 12 Step 1301 Train Loss: 0.5281
Epoch 12 Step 1351 Train Loss: 0.4818
Epoch 12 Step 1401 Train Loss: 0.4791
Epoch 12 Step 1451 Train Loss: 0.4889
Epoch 12 Step 1501 Train Loss: 0.5172
Epoch 12 Step 1551 Train Loss: 0.5661
Epoch 12 Step 1601 Train Loss: 0.5171
Epoch 12 Step 1651 Train Loss: 0.5458
Epoch 12 Step 1701 Train Loss: 0.5220
Epoch 12 Step 1751 Train Loss: 0.4882
Epoch 12 Step 1801 Train Loss: 0.5996
Epoch 12: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0920 Validation Top 20 DE MSE: 0.1617. 
Epoch 13 Step 1 Train Loss: 0.5156
Epoch 13 Step 51 Train Loss: 0.4507
Epoch 13 Step 101 Train Loss: 0.4955
Epoch 13 Step 151 Train Loss: 0.4801
Epoch 13 Step 201 Train Loss: 0.4878
Epoch 13 Step 251 Train Loss: 0.4477
Epoch 13 Step 301 Train Loss: 0.5868
Epoch 13 Step 351 Train Loss: 0.4881
Epoch 13 Step 401 Train Loss: 0.4952
Epoch 13 Step 451 Train Loss: 0.4730
Epoch 13 Step 501 Train Loss: 0.5799
Epoch 13 Step 551 Train Loss: 0.5295
Epoch 13 Step 601 Train Loss: 0.5196
Epoch 13 Step 651 Train Loss: 0.5001
Epoch 13 Step 701 Train Loss: 0.4775
Epoch 13 Step 751 Train Loss: 0.5078
Epoch 13 Step 801 Train Loss: 0.5232
Epoch 13 Step 851 Train Loss: 0.5000
Epoch 13 Step 901 Train Loss: 0.4836
Epoch 13 Step 951 Train Loss: 0.4793
Epoch 13 Step 1001 Train Loss: 0.5501
Epoch 13 Step 1051 Train Loss: 0.5539
Epoch 13 Step 1101 Train Loss: 0.5044
Epoch 13 Step 1151 Train Loss: 0.5005
Epoch 13 Step 1201 Train Loss: 0.4732
Epoch 13 Step 1251 Train Loss: 0.5077
Epoch 13 Step 1301 Train Loss: 0.5096
Epoch 13 Step 1351 Train Loss: 0.4959
Epoch 13 Step 1401 Train Loss: 0.5273
Epoch 13 Step 1451 Train Loss: 0.5253
Epoch 13 Step 1501 Train Loss: 0.4728
Epoch 13 Step 1551 Train Loss: 0.4719
Epoch 13 Step 1601 Train Loss: 0.5216
Epoch 13 Step 1651 Train Loss: 0.5153
Epoch 13 Step 1701 Train Loss: 0.5560
Epoch 13 Step 1751 Train Loss: 0.5053
Epoch 13 Step 1801 Train Loss: 0.4877
Epoch 13: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0892 Validation Top 20 DE MSE: 0.1608. 
Epoch 14 Step 1 Train Loss: 0.5407
Epoch 14 Step 51 Train Loss: 0.5450
Epoch 14 Step 101 Train Loss: 0.5273
Epoch 14 Step 151 Train Loss: 0.5159
Epoch 14 Step 201 Train Loss: 0.5259
Epoch 14 Step 251 Train Loss: 0.4838
Epoch 14 Step 301 Train Loss: 0.5072
Epoch 14 Step 351 Train Loss: 0.4849
Epoch 14 Step 401 Train Loss: 0.5285
Epoch 14 Step 451 Train Loss: 0.5713
Epoch 14 Step 501 Train Loss: 0.5482
Epoch 14 Step 551 Train Loss: 0.5273
Epoch 14 Step 601 Train Loss: 0.4814
Epoch 14 Step 651 Train Loss: 0.5743
Epoch 14 Step 701 Train Loss: 0.5800
Epoch 14 Step 751 Train Loss: 0.4911
Epoch 14 Step 801 Train Loss: 0.4670
Epoch 14 Step 851 Train Loss: 0.4653
Epoch 14 Step 901 Train Loss: 0.5351
Epoch 14 Step 951 Train Loss: 0.4731
Epoch 14 Step 1001 Train Loss: 0.5440
Epoch 14 Step 1051 Train Loss: 0.4840
Epoch 14 Step 1101 Train Loss: 0.4832
Epoch 14 Step 1151 Train Loss: 0.5085
Epoch 14 Step 1201 Train Loss: 0.4841
Epoch 14 Step 1251 Train Loss: 0.4851
Epoch 14 Step 1301 Train Loss: 0.4564
Epoch 14 Step 1351 Train Loss: 0.4923
Epoch 14 Step 1401 Train Loss: 0.5126
Epoch 14 Step 1451 Train Loss: 0.5031
Epoch 14 Step 1501 Train Loss: 0.4889
Epoch 14 Step 1551 Train Loss: 0.4935
Epoch 14 Step 1601 Train Loss: 0.5109
Epoch 14 Step 1651 Train Loss: 0.4568
Epoch 14 Step 1701 Train Loss: 0.5123
Epoch 14 Step 1751 Train Loss: 0.4872
Epoch 14 Step 1801 Train Loss: 0.4740
Epoch 14: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0036. 
Train Top 20 DE MSE: 0.0897 Validation Top 20 DE MSE: 0.1608. 
Epoch 15 Step 1 Train Loss: 0.5121
Epoch 15 Step 51 Train Loss: 0.5552
Epoch 15 Step 101 Train Loss: 0.5692
Epoch 15 Step 151 Train Loss: 0.4788
Epoch 15 Step 201 Train Loss: 0.4833
Epoch 15 Step 251 Train Loss: 0.4993
Epoch 15 Step 301 Train Loss: 0.4837
Epoch 15 Step 351 Train Loss: 0.4444
Epoch 15 Step 401 Train Loss: 0.4843
Epoch 15 Step 451 Train Loss: 0.4611
Epoch 15 Step 501 Train Loss: 0.4607
Epoch 15 Step 551 Train Loss: 0.5610
Epoch 15 Step 601 Train Loss: 0.4948
Epoch 15 Step 651 Train Loss: 0.5483
Epoch 15 Step 701 Train Loss: 0.5057
Epoch 15 Step 751 Train Loss: 0.5264
Epoch 15 Step 801 Train Loss: 0.5865
Epoch 15 Step 851 Train Loss: 0.5006
Epoch 15 Step 901 Train Loss: 0.4702
Epoch 15 Step 951 Train Loss: 0.4480
Epoch 15 Step 1001 Train Loss: 0.4948
Epoch 15 Step 1051 Train Loss: 0.4845
Epoch 15 Step 1101 Train Loss: 0.5612
Epoch 15 Step 1151 Train Loss: 0.4684
Epoch 15 Step 1201 Train Loss: 0.4958
Epoch 15 Step 1251 Train Loss: 0.5038
Epoch 15 Step 1301 Train Loss: 0.4775
Epoch 15 Step 1351 Train Loss: 0.5345
Epoch 15 Step 1401 Train Loss: 0.5071
Epoch 15 Step 1451 Train Loss: 0.4701
Epoch 15 Step 1501 Train Loss: 0.5064
Epoch 15 Step 1551 Train Loss: 0.4437
Epoch 15 Step 1601 Train Loss: 0.5689
Epoch 15 Step 1651 Train Loss: 0.4798
Epoch 15 Step 1701 Train Loss: 0.4824
Epoch 15 Step 1751 Train Loss: 0.4890
Epoch 15 Step 1801 Train Loss: 0.5167
Epoch 15: Train Overall MSE: 0.0025 Validation Overall MSE: 0.0035. 
Train Top 20 DE MSE: 0.0893 Validation Top 20 DE MSE: 0.1610. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1507
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.005391398
test_combo_seen0_pearson: 0.9882025159430639
test_combo_seen0_mse_de: 0.20952031
test_combo_seen0_pearson_de: 0.758381295581129
test_combo_seen1_mse: 0.0044572502
test_combo_seen1_pearson: 0.9901923402489317
test_combo_seen1_mse_de: 0.15274216
test_combo_seen1_pearson_de: 0.8450677636406062
test_combo_seen2_mse: 0.0047544762
test_combo_seen2_pearson: 0.9893383671123926
test_combo_seen2_mse_de: 0.114464656
test_combo_seen2_pearson_de: 0.7810657133133095
test_unseen_single_mse: 0.0024497767
test_unseen_single_pearson: 0.9944819810851817
test_unseen_single_mse_de: 0.16594385
test_unseen_single_pearson_de: 0.9535332866699443
test_combo_seen0_pearson_delta: 0.4736326946282089
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.15
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.8099999999999999
test_combo_seen0_mse_top20_de_non_dropout: 0.23997231
test_combo_seen1_pearson_delta: 0.5528505159966323
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.12159090909090908
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.8806818181818183
test_combo_seen1_mse_top20_de_non_dropout: 0.17194901
test_combo_seen2_pearson_delta: 0.5428486504196736
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.1857142857142857
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.8738095238095239
test_combo_seen2_mse_top20_de_non_dropout: 0.15061341
test_unseen_single_pearson_delta: 0.4070480424509064
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.25
test_unseen_single_frac_sigma_below_1_non_dropout: 0.8980000000000001
test_unseen_single_mse_top20_de_non_dropout: 0.17799412
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.001 MB uploadedwandb: | 0.001 MB of 0.001 MB uploadedwandb: / 0.001 MB of 0.034 MB uploadedwandb: - 0.004 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb: | 0.038 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ
wandb:                                                   val_de_mse ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                               val_de_pearson ‚ñÅ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                                                      val_mse ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                                  val_pearson ‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.15
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.81
wandb:                                         test_combo_seen0_mse 0.00539
wandb:                                      test_combo_seen0_mse_de 0.20952
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.23997
wandb:                                     test_combo_seen0_pearson 0.9882
wandb:                                  test_combo_seen0_pearson_de 0.75838
wandb:                               test_combo_seen0_pearson_delta 0.47363
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.12159
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.88068
wandb:                                         test_combo_seen1_mse 0.00446
wandb:                                      test_combo_seen1_mse_de 0.15274
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.17195
wandb:                                     test_combo_seen1_pearson 0.99019
wandb:                                  test_combo_seen1_pearson_de 0.84507
wandb:                               test_combo_seen1_pearson_delta 0.55285
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.18571
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.87381
wandb:                                         test_combo_seen2_mse 0.00475
wandb:                                      test_combo_seen2_mse_de 0.11446
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.15061
wandb:                                     test_combo_seen2_pearson 0.98934
wandb:                                  test_combo_seen2_pearson_de 0.78107
wandb:                               test_combo_seen2_pearson_delta 0.54285
wandb:                                                  test_de_mse 0.15074
wandb:                                              test_de_pearson 0.8549
wandb:               test_frac_opposite_direction_top20_non_dropout 0.17105
wandb:                          test_frac_sigma_below_1_non_dropout 0.88
wandb:                                                     test_mse 0.00404
wandb:                                test_mse_top20_de_non_dropout 0.1724
wandb:                                                 test_pearson 0.99103
wandb:                                           test_pearson_delta 0.5081
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.25
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.898
wandb:                                       test_unseen_single_mse 0.00245
wandb:                                    test_unseen_single_mse_de 0.16594
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.17799
wandb:                                   test_unseen_single_pearson 0.99448
wandb:                                test_unseen_single_pearson_de 0.95353
wandb:                             test_unseen_single_pearson_delta 0.40705
wandb:                                                 train_de_mse 0.08931
wandb:                                             train_de_pearson 0.89255
wandb:                                                    train_mse 0.00246
wandb:                                                train_pearson 0.99453
wandb:                                                training_loss 0.57754
wandb:                                                   val_de_mse 0.161
wandb:                                               val_de_pearson 0.76673
wandb:                                                      val_mse 0.00348
wandb:                                                  val_pearson 0.99211
wandb: 
wandb: üöÄ View run geneformer_NormanWeissman2019_split4 at: https://wandb.ai/zhoumin1130/New_gears/runs/y2od24se
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_014620-y2od24se/logs
Local copy of split is detected. Loading...
Simulation split test composition:
combo_seen0:10
combo_seen1:58
combo_seen2:16
unseen_single:27
Done!
Creating dataloaders....
Done!
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/share/huadjyin/home/zhoumin3/zhoumin/Gears_change/geneformer/wandb/run-20240922_025400-5grim295
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run geneformer_NormanWeissman2019_split5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhoumin1130/New_gears
wandb: üöÄ View run at https://wandb.ai/zhoumin1130/New_gears/runs/5grim295
wandb: WARNING Serializing object of type ndarray that is 20631680 bytes
Start Training...
Epoch 1 Step 1 Train Loss: 0.4230
Epoch 1 Step 51 Train Loss: 0.4865
Epoch 1 Step 101 Train Loss: 0.4390
Epoch 1 Step 151 Train Loss: 0.4763
Epoch 1 Step 201 Train Loss: 0.4827
Epoch 1 Step 251 Train Loss: 0.4313
Epoch 1 Step 301 Train Loss: 0.4642
Epoch 1 Step 351 Train Loss: 0.4362
Epoch 1 Step 401 Train Loss: 0.4811
Epoch 1 Step 451 Train Loss: 0.4500
Epoch 1 Step 501 Train Loss: 0.5000
Epoch 1 Step 551 Train Loss: 0.4169
Epoch 1 Step 601 Train Loss: 0.5124
Epoch 1 Step 651 Train Loss: 0.4965
Epoch 1 Step 701 Train Loss: 0.4232
Epoch 1 Step 751 Train Loss: 0.5131
Epoch 1 Step 801 Train Loss: 0.4859
Epoch 1 Step 851 Train Loss: 0.3960
Epoch 1 Step 901 Train Loss: 0.4080
Epoch 1 Step 951 Train Loss: 0.4757
Epoch 1 Step 1001 Train Loss: 0.4388
Epoch 1 Step 1051 Train Loss: 0.4034
Epoch 1 Step 1101 Train Loss: 0.4307
Epoch 1 Step 1151 Train Loss: 0.4936
Epoch 1 Step 1201 Train Loss: 0.4075
Epoch 1 Step 1251 Train Loss: 0.4925
Epoch 1 Step 1301 Train Loss: 0.4437
Epoch 1 Step 1351 Train Loss: 0.4195
Epoch 1 Step 1401 Train Loss: 0.5050
Epoch 1 Step 1451 Train Loss: 0.4270
Epoch 1 Step 1501 Train Loss: 0.4827
Epoch 1 Step 1551 Train Loss: 0.4602
Epoch 1 Step 1601 Train Loss: 0.4283
Epoch 1 Step 1651 Train Loss: 0.4845
Epoch 1 Step 1701 Train Loss: 0.4422
Epoch 1 Step 1751 Train Loss: 0.4960
Epoch 1 Step 1801 Train Loss: 0.4550
Epoch 1: Train Overall MSE: 0.0030 Validation Overall MSE: 0.0039. 
Train Top 20 DE MSE: 0.1166 Validation Top 20 DE MSE: 0.1667. 
Epoch 2 Step 1 Train Loss: 0.4676
Epoch 2 Step 51 Train Loss: 0.4174
Epoch 2 Step 101 Train Loss: 0.4033
Epoch 2 Step 151 Train Loss: 0.5339
Epoch 2 Step 201 Train Loss: 0.4314
Epoch 2 Step 251 Train Loss: 0.4342
Epoch 2 Step 301 Train Loss: 0.4412
Epoch 2 Step 351 Train Loss: 0.4554
Epoch 2 Step 401 Train Loss: 0.4606
Epoch 2 Step 451 Train Loss: 0.4313
Epoch 2 Step 501 Train Loss: 0.5324
Epoch 2 Step 551 Train Loss: 0.4319
Epoch 2 Step 601 Train Loss: 0.4701
Epoch 2 Step 651 Train Loss: 0.4899
Epoch 2 Step 701 Train Loss: 0.4627
Epoch 2 Step 751 Train Loss: 0.3805
Epoch 2 Step 801 Train Loss: 0.5247
Epoch 2 Step 851 Train Loss: 0.4724
Epoch 2 Step 901 Train Loss: 0.4301
Epoch 2 Step 951 Train Loss: 0.4447
Epoch 2 Step 1001 Train Loss: 0.4585
Epoch 2 Step 1051 Train Loss: 0.4390
Epoch 2 Step 1101 Train Loss: 0.4081
Epoch 2 Step 1151 Train Loss: 0.4765
Epoch 2 Step 1201 Train Loss: 0.5231
Epoch 2 Step 1251 Train Loss: 0.4366
Epoch 2 Step 1301 Train Loss: 0.3953
Epoch 2 Step 1351 Train Loss: 0.5207
Epoch 2 Step 1401 Train Loss: 0.4446
Epoch 2 Step 1451 Train Loss: 0.4431
Epoch 2 Step 1501 Train Loss: 0.4067
Epoch 2 Step 1551 Train Loss: 0.4303
Epoch 2 Step 1601 Train Loss: 0.4348
Epoch 2 Step 1651 Train Loss: 0.4402
Epoch 2 Step 1701 Train Loss: 0.5427
Epoch 2 Step 1751 Train Loss: 0.4193
Epoch 2 Step 1801 Train Loss: 0.5195
Epoch 2: Train Overall MSE: 0.0031 Validation Overall MSE: 0.0042. 
Train Top 20 DE MSE: 0.1093 Validation Top 20 DE MSE: 0.1938. 
Epoch 3 Step 1 Train Loss: 0.4405
Epoch 3 Step 51 Train Loss: 0.5036
Epoch 3 Step 101 Train Loss: 0.4923
Epoch 3 Step 151 Train Loss: 0.4993
Epoch 3 Step 201 Train Loss: 0.4481
Epoch 3 Step 251 Train Loss: 0.4560
Epoch 3 Step 301 Train Loss: 0.4171
Epoch 3 Step 351 Train Loss: 0.4642
Epoch 3 Step 401 Train Loss: 0.4732
Epoch 3 Step 451 Train Loss: 0.5145
Epoch 3 Step 501 Train Loss: 0.4204
Epoch 3 Step 551 Train Loss: 0.5238
Epoch 3 Step 601 Train Loss: 0.4804
Epoch 3 Step 651 Train Loss: 0.4794
Epoch 3 Step 701 Train Loss: 0.4721
Epoch 3 Step 751 Train Loss: 0.4072
Epoch 3 Step 801 Train Loss: 0.4334
Epoch 3 Step 851 Train Loss: 0.4596
Epoch 3 Step 901 Train Loss: 0.5163
Epoch 3 Step 951 Train Loss: 0.4635
Epoch 3 Step 1001 Train Loss: 0.4646
Epoch 3 Step 1051 Train Loss: 0.4882
Epoch 3 Step 1101 Train Loss: 0.4966
Epoch 3 Step 1151 Train Loss: 0.4379
Epoch 3 Step 1201 Train Loss: 0.4690
Epoch 3 Step 1251 Train Loss: 0.4426
Epoch 3 Step 1301 Train Loss: 0.4230
Epoch 3 Step 1351 Train Loss: 0.4289
Epoch 3 Step 1401 Train Loss: 0.4491
Epoch 3 Step 1451 Train Loss: 0.5047
Epoch 3 Step 1501 Train Loss: 0.4208
Epoch 3 Step 1551 Train Loss: 0.4188
Epoch 3 Step 1601 Train Loss: 0.4477
Epoch 3 Step 1651 Train Loss: 0.4591
Epoch 3 Step 1701 Train Loss: 0.4900
Epoch 3 Step 1751 Train Loss: 0.4345
Epoch 3 Step 1801 Train Loss: 0.4753
Epoch 3: Train Overall MSE: 0.0021 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0834 Validation Top 20 DE MSE: 0.1428. 
Epoch 4 Step 1 Train Loss: 0.4736
Epoch 4 Step 51 Train Loss: 0.4234
Epoch 4 Step 101 Train Loss: 0.4700
Epoch 4 Step 151 Train Loss: 0.4248
Epoch 4 Step 201 Train Loss: 0.4996
Epoch 4 Step 251 Train Loss: 0.4751
Epoch 4 Step 301 Train Loss: 0.4602
Epoch 4 Step 351 Train Loss: 0.5040
Epoch 4 Step 401 Train Loss: 0.5018
Epoch 4 Step 451 Train Loss: 0.4191
Epoch 4 Step 501 Train Loss: 0.4575
Epoch 4 Step 551 Train Loss: 0.4905
Epoch 4 Step 601 Train Loss: 0.4283
Epoch 4 Step 651 Train Loss: 0.4590
Epoch 4 Step 701 Train Loss: 0.4256
Epoch 4 Step 751 Train Loss: 0.4791
Epoch 4 Step 801 Train Loss: 0.4430
Epoch 4 Step 851 Train Loss: 0.4894
Epoch 4 Step 901 Train Loss: 0.4424
Epoch 4 Step 951 Train Loss: 0.4791
Epoch 4 Step 1001 Train Loss: 0.4293
Epoch 4 Step 1051 Train Loss: 0.4787
Epoch 4 Step 1101 Train Loss: 0.4322
Epoch 4 Step 1151 Train Loss: 0.4417
Epoch 4 Step 1201 Train Loss: 0.4389
Epoch 4 Step 1251 Train Loss: 0.4612
Epoch 4 Step 1301 Train Loss: 0.4288
Epoch 4 Step 1351 Train Loss: 0.4558
Epoch 4 Step 1401 Train Loss: 0.4516
Epoch 4 Step 1451 Train Loss: 0.4645
Epoch 4 Step 1501 Train Loss: 0.4645
Epoch 4 Step 1551 Train Loss: 0.4618
Epoch 4 Step 1601 Train Loss: 0.4490
Epoch 4 Step 1651 Train Loss: 0.4119
Epoch 4 Step 1701 Train Loss: 0.4465
Epoch 4 Step 1751 Train Loss: 0.4987
Epoch 4 Step 1801 Train Loss: 0.4696
Epoch 4: Train Overall MSE: 0.0022 Validation Overall MSE: 0.0028. 
Train Top 20 DE MSE: 0.0721 Validation Top 20 DE MSE: 0.1334. 
Epoch 5 Step 1 Train Loss: 0.4946
Epoch 5 Step 51 Train Loss: 0.4398
Epoch 5 Step 101 Train Loss: 0.4775
Epoch 5 Step 151 Train Loss: 0.5144
Epoch 5 Step 201 Train Loss: 0.4766
Epoch 5 Step 251 Train Loss: 0.4972
Epoch 5 Step 301 Train Loss: 0.4758
Epoch 5 Step 351 Train Loss: 0.4733
Epoch 5 Step 401 Train Loss: 0.4574
Epoch 5 Step 451 Train Loss: 0.4602
Epoch 5 Step 501 Train Loss: 0.5204
Epoch 5 Step 551 Train Loss: 0.4827
Epoch 5 Step 601 Train Loss: 0.4809
Epoch 5 Step 651 Train Loss: 0.4851
Epoch 5 Step 701 Train Loss: 0.4555
Epoch 5 Step 751 Train Loss: 0.4240
Epoch 5 Step 801 Train Loss: 0.4989
Epoch 5 Step 851 Train Loss: 0.4082
Epoch 5 Step 901 Train Loss: 0.4056
Epoch 5 Step 951 Train Loss: 0.4771
Epoch 5 Step 1001 Train Loss: 0.4774
Epoch 5 Step 1051 Train Loss: 0.4393
Epoch 5 Step 1101 Train Loss: 0.4720
Epoch 5 Step 1151 Train Loss: 0.4612
Epoch 5 Step 1201 Train Loss: 0.4822
Epoch 5 Step 1251 Train Loss: 0.4808
Epoch 5 Step 1301 Train Loss: 0.4545
Epoch 5 Step 1351 Train Loss: 0.4507
Epoch 5 Step 1401 Train Loss: 0.4083
Epoch 5 Step 1451 Train Loss: 0.4322
Epoch 5 Step 1501 Train Loss: 0.4931
Epoch 5 Step 1551 Train Loss: 0.4648
Epoch 5 Step 1601 Train Loss: 0.5016
Epoch 5 Step 1651 Train Loss: 0.4444
Epoch 5 Step 1701 Train Loss: 0.4638
Epoch 5 Step 1751 Train Loss: 0.4880
Epoch 5 Step 1801 Train Loss: 0.4136
Epoch 5: Train Overall MSE: 0.0020 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0727 Validation Top 20 DE MSE: 0.1364. 
Epoch 6 Step 1 Train Loss: 0.4892
Epoch 6 Step 51 Train Loss: 0.4765
Epoch 6 Step 101 Train Loss: 0.4690
Epoch 6 Step 151 Train Loss: 0.4925
Epoch 6 Step 201 Train Loss: 0.4756
Epoch 6 Step 251 Train Loss: 0.5275
Epoch 6 Step 301 Train Loss: 0.3922
Epoch 6 Step 351 Train Loss: 0.4845
Epoch 6 Step 401 Train Loss: 0.4661
Epoch 6 Step 451 Train Loss: 0.4477
Epoch 6 Step 501 Train Loss: 0.4598
Epoch 6 Step 551 Train Loss: 0.5014
Epoch 6 Step 601 Train Loss: 0.5245
Epoch 6 Step 651 Train Loss: 0.5087
Epoch 6 Step 701 Train Loss: 0.5053
Epoch 6 Step 751 Train Loss: 0.5009
Epoch 6 Step 801 Train Loss: 0.5075
Epoch 6 Step 851 Train Loss: 0.4826
Epoch 6 Step 901 Train Loss: 0.4427
Epoch 6 Step 951 Train Loss: 0.4706
Epoch 6 Step 1001 Train Loss: 0.4401
Epoch 6 Step 1051 Train Loss: 0.4808
Epoch 6 Step 1101 Train Loss: 0.4548
Epoch 6 Step 1151 Train Loss: 0.4958
Epoch 6 Step 1201 Train Loss: 0.4425
Epoch 6 Step 1251 Train Loss: 0.4678
Epoch 6 Step 1301 Train Loss: 0.4669
Epoch 6 Step 1351 Train Loss: 0.4562
Epoch 6 Step 1401 Train Loss: 0.4549
Epoch 6 Step 1451 Train Loss: 0.4702
Epoch 6 Step 1501 Train Loss: 0.4582
Epoch 6 Step 1551 Train Loss: 0.4821
Epoch 6 Step 1601 Train Loss: 0.4951
Epoch 6 Step 1651 Train Loss: 0.4900
Epoch 6 Step 1701 Train Loss: 0.4795
Epoch 6 Step 1751 Train Loss: 0.4838
Epoch 6 Step 1801 Train Loss: 0.4788
Epoch 6: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0761 Validation Top 20 DE MSE: 0.1468. 
Epoch 7 Step 1 Train Loss: 0.4512
Epoch 7 Step 51 Train Loss: 0.4914
Epoch 7 Step 101 Train Loss: 0.4329
Epoch 7 Step 151 Train Loss: 0.4822
Epoch 7 Step 201 Train Loss: 0.4727
Epoch 7 Step 251 Train Loss: 0.4739
Epoch 7 Step 301 Train Loss: 0.4821
Epoch 7 Step 351 Train Loss: 0.4947
Epoch 7 Step 401 Train Loss: 0.4957
Epoch 7 Step 451 Train Loss: 0.4753
Epoch 7 Step 501 Train Loss: 0.4795
Epoch 7 Step 551 Train Loss: 0.5431
Epoch 7 Step 601 Train Loss: 0.4352
Epoch 7 Step 651 Train Loss: 0.4434
Epoch 7 Step 701 Train Loss: 0.5184
Epoch 7 Step 751 Train Loss: 0.4705
Epoch 7 Step 801 Train Loss: 0.4635
Epoch 7 Step 851 Train Loss: 0.4395
Epoch 7 Step 901 Train Loss: 0.4587
Epoch 7 Step 951 Train Loss: 0.4674
Epoch 7 Step 1001 Train Loss: 0.5299
Epoch 7 Step 1051 Train Loss: 0.4971
Epoch 7 Step 1101 Train Loss: 0.5004
Epoch 7 Step 1151 Train Loss: 0.5812
Epoch 7 Step 1201 Train Loss: 0.5385
Epoch 7 Step 1251 Train Loss: 0.4659
Epoch 7 Step 1301 Train Loss: 0.4733
Epoch 7 Step 1351 Train Loss: 0.4474
Epoch 7 Step 1401 Train Loss: 0.5034
Epoch 7 Step 1451 Train Loss: 0.4143
Epoch 7 Step 1501 Train Loss: 0.5109
Epoch 7 Step 1551 Train Loss: 0.4639
Epoch 7 Step 1601 Train Loss: 0.4975
Epoch 7 Step 1651 Train Loss: 0.4694
Epoch 7 Step 1701 Train Loss: 0.4999
Epoch 7 Step 1751 Train Loss: 0.4822
Epoch 7 Step 1801 Train Loss: 0.4713
Epoch 7: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0731 Validation Top 20 DE MSE: 0.1400. 
Epoch 8 Step 1 Train Loss: 0.5105
Epoch 8 Step 51 Train Loss: 0.4697
Epoch 8 Step 101 Train Loss: 0.4641
Epoch 8 Step 151 Train Loss: 0.4781
Epoch 8 Step 201 Train Loss: 0.5158
Epoch 8 Step 251 Train Loss: 0.5497
Epoch 8 Step 301 Train Loss: 0.4880
Epoch 8 Step 351 Train Loss: 0.4774
Epoch 8 Step 401 Train Loss: 0.4646
Epoch 8 Step 451 Train Loss: 0.4846
Epoch 8 Step 501 Train Loss: 0.4322
Epoch 8 Step 551 Train Loss: 0.4875
Epoch 8 Step 601 Train Loss: 0.5237
Epoch 8 Step 651 Train Loss: 0.4764
Epoch 8 Step 701 Train Loss: 0.4275
Epoch 8 Step 751 Train Loss: 0.4554
Epoch 8 Step 801 Train Loss: 0.5248
Epoch 8 Step 851 Train Loss: 0.4633
Epoch 8 Step 901 Train Loss: 0.5248
Epoch 8 Step 951 Train Loss: 0.4852
Epoch 8 Step 1001 Train Loss: 0.5004
Epoch 8 Step 1051 Train Loss: 0.4788
Epoch 8 Step 1101 Train Loss: 0.4716
Epoch 8 Step 1151 Train Loss: 0.4568
Epoch 8 Step 1201 Train Loss: 0.4325
Epoch 8 Step 1251 Train Loss: 0.4642
Epoch 8 Step 1301 Train Loss: 0.5447
Epoch 8 Step 1351 Train Loss: 0.5116
Epoch 8 Step 1401 Train Loss: 0.4804
Epoch 8 Step 1451 Train Loss: 0.4546
Epoch 8 Step 1501 Train Loss: 0.5032
Epoch 8 Step 1551 Train Loss: 0.5934
Epoch 8 Step 1601 Train Loss: 0.4649
Epoch 8 Step 1651 Train Loss: 0.4638
Epoch 8 Step 1701 Train Loss: 0.4551
Epoch 8 Step 1751 Train Loss: 0.4568
Epoch 8 Step 1801 Train Loss: 0.4744
Epoch 8: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0774 Validation Top 20 DE MSE: 0.1420. 
Epoch 9 Step 1 Train Loss: 0.4440
Epoch 9 Step 51 Train Loss: 0.4764
Epoch 9 Step 101 Train Loss: 0.4459
Epoch 9 Step 151 Train Loss: 0.4701
Epoch 9 Step 201 Train Loss: 0.4813
Epoch 9 Step 251 Train Loss: 0.4564
Epoch 9 Step 301 Train Loss: 0.4784
Epoch 9 Step 351 Train Loss: 0.5038
Epoch 9 Step 401 Train Loss: 0.4659
Epoch 9 Step 451 Train Loss: 0.4932
Epoch 9 Step 501 Train Loss: 0.4758
Epoch 9 Step 551 Train Loss: 0.4699
Epoch 9 Step 601 Train Loss: 0.4346
Epoch 9 Step 651 Train Loss: 0.4640
Epoch 9 Step 701 Train Loss: 0.4672
Epoch 9 Step 751 Train Loss: 0.4901
Epoch 9 Step 801 Train Loss: 0.5007
Epoch 9 Step 851 Train Loss: 0.5629
Epoch 9 Step 901 Train Loss: 0.4937
Epoch 9 Step 951 Train Loss: 0.4638
Epoch 9 Step 1001 Train Loss: 0.5091
Epoch 9 Step 1051 Train Loss: 0.4555
Epoch 9 Step 1101 Train Loss: 0.4859
Epoch 9 Step 1151 Train Loss: 0.4534
Epoch 9 Step 1201 Train Loss: 0.4622
Epoch 9 Step 1251 Train Loss: 0.4755
Epoch 9 Step 1301 Train Loss: 0.4778
Epoch 9 Step 1351 Train Loss: 0.4852
Epoch 9 Step 1401 Train Loss: 0.4562
Epoch 9 Step 1451 Train Loss: 0.4948
Epoch 9 Step 1501 Train Loss: 0.4711
Epoch 9 Step 1551 Train Loss: 0.4596
Epoch 9 Step 1601 Train Loss: 0.5463
Epoch 9 Step 1651 Train Loss: 0.4811
Epoch 9 Step 1701 Train Loss: 0.4533
Epoch 9 Step 1751 Train Loss: 0.4367
Epoch 9 Step 1801 Train Loss: 0.4844
Epoch 9: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0766 Validation Top 20 DE MSE: 0.1430. 
Epoch 10 Step 1 Train Loss: 0.4838
Epoch 10 Step 51 Train Loss: 0.4905
Epoch 10 Step 101 Train Loss: 0.4849
Epoch 10 Step 151 Train Loss: 0.4690
Epoch 10 Step 201 Train Loss: 0.4689
Epoch 10 Step 251 Train Loss: 0.4585
Epoch 10 Step 301 Train Loss: 0.4296
Epoch 10 Step 351 Train Loss: 0.5412
Epoch 10 Step 401 Train Loss: 0.4617
Epoch 10 Step 451 Train Loss: 0.4408
Epoch 10 Step 501 Train Loss: 0.4344
Epoch 10 Step 551 Train Loss: 0.4428
Epoch 10 Step 601 Train Loss: 0.5059
Epoch 10 Step 651 Train Loss: 0.4539
Epoch 10 Step 701 Train Loss: 0.4619
Epoch 10 Step 751 Train Loss: 0.4949
Epoch 10 Step 801 Train Loss: 0.5147
Epoch 10 Step 851 Train Loss: 0.5420
Epoch 10 Step 901 Train Loss: 0.4586
Epoch 10 Step 951 Train Loss: 0.4465
Epoch 10 Step 1001 Train Loss: 0.4547
Epoch 10 Step 1051 Train Loss: 0.4723
Epoch 10 Step 1101 Train Loss: 0.4919
Epoch 10 Step 1151 Train Loss: 0.4806
Epoch 10 Step 1201 Train Loss: 0.4935
Epoch 10 Step 1251 Train Loss: 0.4842
Epoch 10 Step 1301 Train Loss: 0.4396
Epoch 10 Step 1351 Train Loss: 0.4542
Epoch 10 Step 1401 Train Loss: 0.5853
Epoch 10 Step 1451 Train Loss: 0.4804
Epoch 10 Step 1501 Train Loss: 0.5249
Epoch 10 Step 1551 Train Loss: 0.4598
Epoch 10 Step 1601 Train Loss: 0.4994
Epoch 10 Step 1651 Train Loss: 0.4985
Epoch 10 Step 1701 Train Loss: 0.5108
Epoch 10 Step 1751 Train Loss: 0.5279
Epoch 10 Step 1801 Train Loss: 0.4538
Epoch 10: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0745 Validation Top 20 DE MSE: 0.1408. 
Epoch 11 Step 1 Train Loss: 0.5031
Epoch 11 Step 51 Train Loss: 0.4954
Epoch 11 Step 101 Train Loss: 0.4403
Epoch 11 Step 151 Train Loss: 0.4679
Epoch 11 Step 201 Train Loss: 0.4874
Epoch 11 Step 251 Train Loss: 0.4689
Epoch 11 Step 301 Train Loss: 0.4644
Epoch 11 Step 351 Train Loss: 0.5385
Epoch 11 Step 401 Train Loss: 0.4699
Epoch 11 Step 451 Train Loss: 0.5102
Epoch 11 Step 501 Train Loss: 0.4578
Epoch 11 Step 551 Train Loss: 0.4814
Epoch 11 Step 601 Train Loss: 0.4682
Epoch 11 Step 651 Train Loss: 0.4732
Epoch 11 Step 701 Train Loss: 0.5194
Epoch 11 Step 751 Train Loss: 0.4986
Epoch 11 Step 801 Train Loss: 0.4367
Epoch 11 Step 851 Train Loss: 0.5348
Epoch 11 Step 901 Train Loss: 0.5049
Epoch 11 Step 951 Train Loss: 0.4718
Epoch 11 Step 1001 Train Loss: 0.5084
Epoch 11 Step 1051 Train Loss: 0.4679
Epoch 11 Step 1101 Train Loss: 0.4973
Epoch 11 Step 1151 Train Loss: 0.4830
Epoch 11 Step 1201 Train Loss: 0.4531
Epoch 11 Step 1251 Train Loss: 0.5030
Epoch 11 Step 1301 Train Loss: 0.4813
Epoch 11 Step 1351 Train Loss: 0.4853
Epoch 11 Step 1401 Train Loss: 0.4482
Epoch 11 Step 1451 Train Loss: 0.4758
Epoch 11 Step 1501 Train Loss: 0.5421
Epoch 11 Step 1551 Train Loss: 0.5070
Epoch 11 Step 1601 Train Loss: 0.4706
Epoch 11 Step 1651 Train Loss: 0.4664
Epoch 11 Step 1701 Train Loss: 0.5042
Epoch 11 Step 1751 Train Loss: 0.4555
Epoch 11 Step 1801 Train Loss: 0.4444
Epoch 11: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0743 Validation Top 20 DE MSE: 0.1430. 
Epoch 12 Step 1 Train Loss: 0.4514
Epoch 12 Step 51 Train Loss: 0.5023
Epoch 12 Step 101 Train Loss: 0.5545
Epoch 12 Step 151 Train Loss: 0.5144
Epoch 12 Step 201 Train Loss: 0.4277
Epoch 12 Step 251 Train Loss: 0.5382
Epoch 12 Step 301 Train Loss: 0.5084
Epoch 12 Step 351 Train Loss: 0.5136
Epoch 12 Step 401 Train Loss: 0.4613
Epoch 12 Step 451 Train Loss: 0.4738
Epoch 12 Step 501 Train Loss: 0.5149
Epoch 12 Step 551 Train Loss: 0.5076
Epoch 12 Step 601 Train Loss: 0.4866
Epoch 12 Step 651 Train Loss: 0.4383
Epoch 12 Step 701 Train Loss: 0.4970
Epoch 12 Step 751 Train Loss: 0.4407
Epoch 12 Step 801 Train Loss: 0.4581
Epoch 12 Step 851 Train Loss: 0.4487
Epoch 12 Step 901 Train Loss: 0.4832
Epoch 12 Step 951 Train Loss: 0.5169
Epoch 12 Step 1001 Train Loss: 0.4724
Epoch 12 Step 1051 Train Loss: 0.4976
Epoch 12 Step 1101 Train Loss: 0.4883
Epoch 12 Step 1151 Train Loss: 0.4757
Epoch 12 Step 1201 Train Loss: 0.4637
Epoch 12 Step 1251 Train Loss: 0.4512
Epoch 12 Step 1301 Train Loss: 0.4963
Epoch 12 Step 1351 Train Loss: 0.4675
Epoch 12 Step 1401 Train Loss: 0.4770
Epoch 12 Step 1451 Train Loss: 0.4766
Epoch 12 Step 1501 Train Loss: 0.5096
Epoch 12 Step 1551 Train Loss: 0.4325
Epoch 12 Step 1601 Train Loss: 0.4950
Epoch 12 Step 1651 Train Loss: 0.4367
Epoch 12 Step 1701 Train Loss: 0.4653
Epoch 12 Step 1751 Train Loss: 0.4945
Epoch 12 Step 1801 Train Loss: 0.4780
Epoch 12: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0704 Validation Top 20 DE MSE: 0.1348. 
Epoch 13 Step 1 Train Loss: 0.4697
Epoch 13 Step 51 Train Loss: 0.5346
Epoch 13 Step 101 Train Loss: 0.4310
Epoch 13 Step 151 Train Loss: 0.4569
Epoch 13 Step 201 Train Loss: 0.4655
Epoch 13 Step 251 Train Loss: 0.4588
Epoch 13 Step 301 Train Loss: 0.4622
Epoch 13 Step 351 Train Loss: 0.4431
Epoch 13 Step 401 Train Loss: 0.4910
Epoch 13 Step 451 Train Loss: 0.5341
Epoch 13 Step 501 Train Loss: 0.5090
Epoch 13 Step 551 Train Loss: 0.4558
Epoch 13 Step 601 Train Loss: 0.5508
Epoch 13 Step 651 Train Loss: 0.4775
Epoch 13 Step 701 Train Loss: 0.5379
Epoch 13 Step 751 Train Loss: 0.4781
Epoch 13 Step 801 Train Loss: 0.4819
Epoch 13 Step 851 Train Loss: 0.4730
Epoch 13 Step 901 Train Loss: 0.5023
Epoch 13 Step 951 Train Loss: 0.4485
Epoch 13 Step 1001 Train Loss: 0.5121
Epoch 13 Step 1051 Train Loss: 0.4868
Epoch 13 Step 1101 Train Loss: 0.4743
Epoch 13 Step 1151 Train Loss: 0.4989
Epoch 13 Step 1201 Train Loss: 0.4828
Epoch 13 Step 1251 Train Loss: 0.4291
Epoch 13 Step 1301 Train Loss: 0.5269
Epoch 13 Step 1351 Train Loss: 0.5418
Epoch 13 Step 1401 Train Loss: 0.5041
Epoch 13 Step 1451 Train Loss: 0.4792
Epoch 13 Step 1501 Train Loss: 0.4584
Epoch 13 Step 1551 Train Loss: 0.5226
Epoch 13 Step 1601 Train Loss: 0.4820
Epoch 13 Step 1651 Train Loss: 0.4884
Epoch 13 Step 1701 Train Loss: 0.5350
Epoch 13 Step 1751 Train Loss: 0.5026
Epoch 13 Step 1801 Train Loss: 0.5247
Epoch 13: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0721 Validation Top 20 DE MSE: 0.1379. 
Epoch 14 Step 1 Train Loss: 0.4574
Epoch 14 Step 51 Train Loss: 0.4144
Epoch 14 Step 101 Train Loss: 0.4805
Epoch 14 Step 151 Train Loss: 0.4549
Epoch 14 Step 201 Train Loss: 0.4864
Epoch 14 Step 251 Train Loss: 0.4760
Epoch 14 Step 301 Train Loss: 0.4751
Epoch 14 Step 351 Train Loss: 0.5568
Epoch 14 Step 401 Train Loss: 0.5134
Epoch 14 Step 451 Train Loss: 0.4318
Epoch 14 Step 501 Train Loss: 0.4451
Epoch 14 Step 551 Train Loss: 0.5155
Epoch 14 Step 601 Train Loss: 0.4454
Epoch 14 Step 651 Train Loss: 0.5201
Epoch 14 Step 701 Train Loss: 0.4472
Epoch 14 Step 751 Train Loss: 0.4300
Epoch 14 Step 801 Train Loss: 0.4387
Epoch 14 Step 851 Train Loss: 0.5068
Epoch 14 Step 901 Train Loss: 0.4526
Epoch 14 Step 951 Train Loss: 0.4797
Epoch 14 Step 1001 Train Loss: 0.5725
Epoch 14 Step 1051 Train Loss: 0.4875
Epoch 14 Step 1101 Train Loss: 0.5003
Epoch 14 Step 1151 Train Loss: 0.5279
Epoch 14 Step 1201 Train Loss: 0.4530
Epoch 14 Step 1251 Train Loss: 0.4748
Epoch 14 Step 1301 Train Loss: 0.5273
Epoch 14 Step 1351 Train Loss: 0.5365
Epoch 14 Step 1401 Train Loss: 0.4965
Epoch 14 Step 1451 Train Loss: 0.5054
Epoch 14 Step 1501 Train Loss: 0.4889
Epoch 14 Step 1551 Train Loss: 0.4921
Epoch 14 Step 1601 Train Loss: 0.4949
Epoch 14 Step 1651 Train Loss: 0.4450
Epoch 14 Step 1701 Train Loss: 0.4957
Epoch 14 Step 1751 Train Loss: 0.4415
Epoch 14 Step 1801 Train Loss: 0.4738
Epoch 14: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0720 Validation Top 20 DE MSE: 0.1325. 
Epoch 15 Step 1 Train Loss: 0.4692
Epoch 15 Step 51 Train Loss: 0.5410
Epoch 15 Step 101 Train Loss: 0.4735
Epoch 15 Step 151 Train Loss: 0.4972
Epoch 15 Step 201 Train Loss: 0.4349
Epoch 15 Step 251 Train Loss: 0.5394
Epoch 15 Step 301 Train Loss: 0.4689
Epoch 15 Step 351 Train Loss: 0.5136
Epoch 15 Step 401 Train Loss: 0.5073
Epoch 15 Step 451 Train Loss: 0.4988
Epoch 15 Step 501 Train Loss: 0.4790
Epoch 15 Step 551 Train Loss: 0.5051
Epoch 15 Step 601 Train Loss: 0.4619
Epoch 15 Step 651 Train Loss: 0.4429
Epoch 15 Step 701 Train Loss: 0.4907
Epoch 15 Step 751 Train Loss: 0.4738
Epoch 15 Step 801 Train Loss: 0.4834
Epoch 15 Step 851 Train Loss: 0.4756
Epoch 15 Step 901 Train Loss: 0.5221
Epoch 15 Step 951 Train Loss: 0.4324
Epoch 15 Step 1001 Train Loss: 0.4448
Epoch 15 Step 1051 Train Loss: 0.4940
Epoch 15 Step 1101 Train Loss: 0.4619
Epoch 15 Step 1151 Train Loss: 0.4336
Epoch 15 Step 1201 Train Loss: 0.4667
Epoch 15 Step 1251 Train Loss: 0.5375
Epoch 15 Step 1301 Train Loss: 0.5511
Epoch 15 Step 1351 Train Loss: 0.5099
Epoch 15 Step 1401 Train Loss: 0.4622
Epoch 15 Step 1451 Train Loss: 0.4491
Epoch 15 Step 1501 Train Loss: 0.4873
Epoch 15 Step 1551 Train Loss: 0.4569
Epoch 15 Step 1601 Train Loss: 0.4837
Epoch 15 Step 1651 Train Loss: 0.4624
Epoch 15 Step 1701 Train Loss: 0.4864
Epoch 15 Step 1751 Train Loss: 0.5361
Epoch 15 Step 1801 Train Loss: 0.4525
Epoch 15: Train Overall MSE: 0.0019 Validation Overall MSE: 0.0027. 
Train Top 20 DE MSE: 0.0730 Validation Top 20 DE MSE: 0.1388. 
Done!
Start Testing...
Best performing model: Test Top 20 DE MSE: 0.1500
Start doing subgroup analysis for simulation split...
test_combo_seen0_mse: 0.004377845
test_combo_seen0_pearson: 0.98997153578728
test_combo_seen0_mse_de: 0.16961332
test_combo_seen0_pearson_de: 0.8395570264956724
test_combo_seen1_mse: 0.0040416988
test_combo_seen1_pearson: 0.990797138779456
test_combo_seen1_mse_de: 0.13332382
test_combo_seen1_pearson_de: 0.7442441988327622
test_combo_seen2_mse: 0.0042298506
test_combo_seen2_pearson: 0.990169126085293
test_combo_seen2_mse_de: 0.17824638
test_combo_seen2_pearson_de: 0.8926684910220766
test_unseen_single_mse: 0.0023173634
test_unseen_single_pearson: 0.994630606761274
test_unseen_single_mse_de: 0.1619236
test_unseen_single_pearson_de: 0.9470097876512257
test_combo_seen0_pearson_delta: 0.5150202520431333
test_combo_seen0_frac_opposite_direction_top20_non_dropout: 0.21999999999999997
test_combo_seen0_frac_sigma_below_1_non_dropout: 0.85
test_combo_seen0_mse_top20_de_non_dropout: 0.20670047
test_combo_seen1_pearson_delta: 0.5711039779191286
test_combo_seen1_frac_opposite_direction_top20_non_dropout: 0.1517241379310345
test_combo_seen1_frac_sigma_below_1_non_dropout: 0.882758620689655
test_combo_seen1_mse_top20_de_non_dropout: 0.18923904
test_combo_seen2_pearson_delta: 0.6320752204719082
test_combo_seen2_frac_opposite_direction_top20_non_dropout: 0.043750000000000004
test_combo_seen2_frac_sigma_below_1_non_dropout: 0.871875
test_combo_seen2_mse_top20_de_non_dropout: 0.20592251
test_unseen_single_pearson_delta: 0.455376428588715
test_unseen_single_frac_opposite_direction_top20_non_dropout: 0.1962962962962963
test_unseen_single_frac_sigma_below_1_non_dropout: 0.9018518518518518
test_unseen_single_mse_top20_de_non_dropout: 0.17342559
Done!
wandb: - 0.001 MB of 0.001 MB uploadedwandb: \ 0.001 MB of 0.038 MB uploadedwandb: | 0.038 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen0_mse ‚ñÅ
wandb:                                      test_combo_seen0_mse_de ‚ñÅ
wandb:                    test_combo_seen0_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen0_pearson ‚ñÅ
wandb:                                  test_combo_seen0_pearson_de ‚ñÅ
wandb:                               test_combo_seen0_pearson_delta ‚ñÅ
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen1_mse ‚ñÅ
wandb:                                      test_combo_seen1_mse_de ‚ñÅ
wandb:                    test_combo_seen1_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen1_pearson ‚ñÅ
wandb:                                  test_combo_seen1_pearson_de ‚ñÅ
wandb:                               test_combo_seen1_pearson_delta ‚ñÅ
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                         test_combo_seen2_mse ‚ñÅ
wandb:                                      test_combo_seen2_mse_de ‚ñÅ
wandb:                    test_combo_seen2_mse_top20_de_non_dropout ‚ñÅ
wandb:                                     test_combo_seen2_pearson ‚ñÅ
wandb:                                  test_combo_seen2_pearson_de ‚ñÅ
wandb:                               test_combo_seen2_pearson_delta ‚ñÅ
wandb:                                                  test_de_mse ‚ñÅ
wandb:                                              test_de_pearson ‚ñÅ
wandb:               test_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:                          test_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                                     test_mse ‚ñÅ
wandb:                                test_mse_top20_de_non_dropout ‚ñÅ
wandb:                                                 test_pearson ‚ñÅ
wandb:                                           test_pearson_delta ‚ñÅ
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout ‚ñÅ
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout ‚ñÅ
wandb:                                       test_unseen_single_mse ‚ñÅ
wandb:                                    test_unseen_single_mse_de ‚ñÅ
wandb:                  test_unseen_single_mse_top20_de_non_dropout ‚ñÅ
wandb:                                   test_unseen_single_pearson ‚ñÅ
wandb:                                test_unseen_single_pearson_de ‚ñÅ
wandb:                             test_unseen_single_pearson_delta ‚ñÅ
wandb:                                                 train_de_mse ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                             train_de_pearson ‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                    train_mse ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                train_pearson ‚ñÇ‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                                training_loss ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÜ
wandb:                                                   val_de_mse ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:                                               val_de_pearson ‚ñÑ‚ñÅ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá
wandb:                                                      val_mse ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                                  val_pearson ‚ñÇ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:   test_combo_seen0_frac_opposite_direction_top20_non_dropout 0.22
wandb:              test_combo_seen0_frac_sigma_below_1_non_dropout 0.85
wandb:                                         test_combo_seen0_mse 0.00438
wandb:                                      test_combo_seen0_mse_de 0.16961
wandb:                    test_combo_seen0_mse_top20_de_non_dropout 0.2067
wandb:                                     test_combo_seen0_pearson 0.98997
wandb:                                  test_combo_seen0_pearson_de 0.83956
wandb:                               test_combo_seen0_pearson_delta 0.51502
wandb:   test_combo_seen1_frac_opposite_direction_top20_non_dropout 0.15172
wandb:              test_combo_seen1_frac_sigma_below_1_non_dropout 0.88276
wandb:                                         test_combo_seen1_mse 0.00404
wandb:                                      test_combo_seen1_mse_de 0.13332
wandb:                    test_combo_seen1_mse_top20_de_non_dropout 0.18924
wandb:                                     test_combo_seen1_pearson 0.9908
wandb:                                  test_combo_seen1_pearson_de 0.74424
wandb:                               test_combo_seen1_pearson_delta 0.5711
wandb:   test_combo_seen2_frac_opposite_direction_top20_non_dropout 0.04375
wandb:              test_combo_seen2_frac_sigma_below_1_non_dropout 0.87187
wandb:                                         test_combo_seen2_mse 0.00423
wandb:                                      test_combo_seen2_mse_de 0.17825
wandb:                    test_combo_seen2_mse_top20_de_non_dropout 0.20592
wandb:                                     test_combo_seen2_pearson 0.99017
wandb:                                  test_combo_seen2_pearson_de 0.89267
wandb:                               test_combo_seen2_pearson_delta 0.63208
wandb:                                                  test_de_mse 0.15003
wandb:                                              test_de_pearson 0.82355
wandb:               test_frac_opposite_direction_top20_non_dropout 0.15315
wandb:                          test_frac_sigma_below_1_non_dropout 0.88288
wandb:                                                     test_mse 0.00368
wandb:                                test_mse_top20_de_non_dropout 0.18937
wandb:                                                 test_pearson 0.99156
wandb:                                           test_pearson_delta 0.54669
wandb: test_unseen_single_frac_opposite_direction_top20_non_dropout 0.1963
wandb:            test_unseen_single_frac_sigma_below_1_non_dropout 0.90185
wandb:                                       test_unseen_single_mse 0.00232
wandb:                                    test_unseen_single_mse_de 0.16192
wandb:                  test_unseen_single_mse_top20_de_non_dropout 0.17343
wandb:                                   test_unseen_single_pearson 0.99463
wandb:                                test_unseen_single_pearson_de 0.94701
wandb:                             test_unseen_single_pearson_delta 0.45538
wandb:                                                 train_de_mse 0.07301
wandb:                                             train_de_pearson 0.90758
wandb:                                                    train_mse 0.00193
wandb:                                                train_pearson 0.99572
wandb:                                                training_loss 0.53636
wandb:                                                   val_de_mse 0.13879
wandb:                                               val_de_pearson 0.87588
wandb:                                                      val_mse 0.00268
wandb:                                                  val_pearson 0.99386
wandb: 
wandb: üöÄ View run geneformer_NormanWeissman2019_split5 at: https://wandb.ai/zhoumin1130/New_gears/runs/5grim295
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zhoumin1130/New_gears
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240922_025400-5grim295/logs
